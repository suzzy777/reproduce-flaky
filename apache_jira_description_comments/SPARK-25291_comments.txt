[~ifilonenko] I can have a look a bit weird, but kind of expected as these tests are the only ones that use fabric8io client to connect to the running pod. There is no good way to do this right now.

I will try to debug it.

[~skonto] the problem stems from the executors not being created in time for the reading of logs. As such, the tests fail. As such, it is required that we block on executor creation via a Watcher and only read the logs when executors are up. In essence:

 val executorPods = kubernetesTestComponents.kubernetesClient
 .pods()
 .withLabel("spark-app-locator", appLocator)
 .withLabel("spark-role", "executor")
 .list()
 .getItems
 executorPods.asScala.foreach { pod =>
 executorPodChecker(pod)
 }

runs after the spark-submit command and it takes an arbitrary period of time for the executors to get spun up. If the k8s client which is reading the executor logs returns 0 pods it won't check over the executor pods. As such, this flakiness occurs when the executor pod isn't always checking the executor pods that are made. In terms of the flakiness for the PySpark tests it seems that the executor pods are setting: .set("spark.executor.memory", "500m") in the SparkConf and as such are expecting 884 instead of the 1408. So that error seems to be related to the PySpark test framework. 

User 'ifilonenko' has created a pull request for this issue:
https://github.com/apache/spark/pull/22415

