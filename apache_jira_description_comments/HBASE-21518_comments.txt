Attached output of https://builds.apache.org/view/H-L/view/HBase/job/HBase-Flaky-Tests/job/branch-2.0/1988

I think I know what is happening here. After testDeleteWithFailover succeeds teardown starts and shuts down cluster.
 2018-11-26 08:48:11,860 DEBUG [Time-limited test] util.JVMClusterUtil(247): Shutting down HBase Cluster

RegionServer and master is closed.
{noformat}
2018-11-26 08:48:13,003 INFO  [RS:0;asf901:44139] regionserver.HRegionServer(1134): Exiting; stopping=asf901.gq1.ygridcore.net,44139,1543222082929; zookeeper connection closed.
2018-11-26 08:48:13,004 INFO  [RegionServerTracker-0] master.RegionServerTracker(172): RegionServer ephemeral node deleted, processing expiration [asf901.gq1.ygridcore.net,44139,1543222082929]
2018-11-26 08:48:13,005 DEBUG [RegionServerTracker-0] master.DeadServer(134): Added asf901.gq1.ygridcore.net,44139,1543222082929; numProcessing=1
2018-11-26 08:48:13,005 INFO  [Shutdown of org.apache.hadoop.hbase.fs.HFileSystem@3e6c0222] hbase.MiniHBaseCluster$SingleFileSystemShutdownThread(221): Hook closing fs=org.apache.hadoop.hbase.fs.HFileSystem@3e6c0222
2018-11-26 08:48:13,005 INFO  [RegionServerTracker-0] master.ServerManager(585): Processing expiration of asf901.gq1.ygridcore.net,44139,1543222082929 on asf901.gq1.ygridcore.net,45952,1543222082842
2018-11-26 08:48:13,005 INFO  [Time-limited test] util.JVMClusterUtil(335): Shutdown of 2 master(s) and 1 regionserver(s) complete
{noformat}
New ServerCrashProcedure added basically at the same time.
{noformat}
2018-11-26 08:48:13,016 DEBUG [RegionServerTracker-0] procedure2.ProcedureExecutor(1092): Stored pid=12, state=RUNNABLE:SERVER_CRASH_START; ServerCrashProcedure server=asf901.gq1.ygridcore.net,44139,1543222082929, splitWal=true, meta=true
2018-11-26 08:48:13,017 DEBUG [RegionServerTracker-0] procedure.MasterProcedureScheduler(339): Add ServerQueue(asf901.gq1.ygridcore.net,44139,1543222082929, xlock=false sharedLock=0 size=1) to run queue because: the exclusive lock is not held by anyone when adding pid=12, state=RUNNABLE:SERVER_CRASH_START; ServerCrashProcedure server=asf901.gq1.ygridcore.net,44139,1543222082929, splitWal=true, meta=true
2018-11-26 08:48:13,017 DEBUG [RegionServerTracker-0] assignment.AssignmentManager(1320): Added=asf901.gq1.ygridcore.net,44139,1543222082929 to dead servers, submitted shutdown handler to be executed meta=true
2018-11-26 08:48:13,019 DEBUG [PEWorker-1] procedure.MasterProcedureScheduler(349): Remove ServerQueue(asf901.gq1.ygridcore.net,44139,1543222082929, xlock=false sharedLock=0 size=0) from run queue because: queue is empty after polling out pid=12, state=RUNNABLE:SERVER_CRASH_START; ServerCrashProcedure server=asf901.gq1.ygridcore.net,44139,1543222082929, splitWal=true, meta=true
{noformat}
Sub procedure started to place meta table but there is no server running, this procedure stuck and the teardown retries this until test timeout.
{noformat}
2018-11-26 08:48:13,041 INFO  [PEWorker-1] assignment.AssignProcedure(254): Starting pid=13, ppid=12, state=RUNNABLE:REGION_TRANSITION_QUEUE, locked=true; AssignProcedure table=hbase:meta, region=1588230740; rit=OFFLINE, location=asf901.gq1.ygridcore.net,44139,1543222082929; forceNewPlan=false, retain=true
2018-11-26 08:48:13,042 DEBUG [PEWorker-1] procedure2.RootProcedureState(153): Add procedure pid=13, ppid=12, state=RUNNABLE:REGION_TRANSITION_DISPATCH, locked=true; AssignProcedure table=hbase:meta, region=1588230740 as the 3th rollback step
2018-11-26 08:48:13,192 WARN  [master/asf901:0] assignment.AssignmentManager(1670): No servers available; cannot place 1 unassigned regions.
{noformat}

ServerManager#expireServer checks if cluster is shutting down and in this case does not create ServerCrashProcedure for dead servers. This AtomicBoolean variable is set to true when ServerManager#shutdownCluster method is called, however, there are 2 ServerManager instances and on expireServer a different one is checked.

I added some debug logs with hashcodes where you can see that clusterShutdown was set to true (hash=1980707837) but later on during shutdown the variable contains false (hash=416244779) that's why ServerCrashProcedure is created which hangs since there are no Master.
{noformat}
2018-11-29 15:43:21,948 INFO [Thread-81] master.ServerManager(160): ServerManager initialized. clusterShutdown false, thread 210, hash 1980707837
2018-11-29 15:43:23,929 INFO [Thread-81] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 210, hash=1980707837
2018-11-29 15:43:23,929 INFO [Thread-81] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 210, hash=1980707837
2018-11-29 15:43:29,732 INFO [Thread-80] master.ServerManager(160): ServerManager initialized. clusterShutdown false, thread 209, hash 416244779
2018-11-29 15:43:29,820 INFO [Thread-80] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 209, hash=416244779
2018-11-29 15:43:29,820 INFO [Thread-80] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 209, hash=416244779
2018-11-29 15:43:29,937 INFO [Time-limited test] master.ServerManager(904): Set clusterShutdown to true, thread 14, hash 1980707837
2018-11-29 15:43:30,985 INFO [RegionServerTracker-0] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 461, hash=416244779
2018-11-29 15:43:30,986 INFO [RegionServerTracker-0] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 461, hash=416244779
2018-11-29 15:48:29,851 INFO [master/172.30.65.195:0.Chore.1] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 417, hash=416244779
2018-11-29 15:48:29,852 INFO [master/172.30.65.195:0.Chore.1] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 417, hash=416244779
2018-11-29 15:48:29,852 INFO [master/172.30.65.195:0.Chore.1] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 417, hash=416244779
2018-11-29 15:53:32,277 INFO [master/172.30.65.195:0.Chore.1] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 417, hash=416244779
2018-11-29 15:53:32,277 INFO [master/172.30.65.195:0.Chore.1] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 417, hash=416244779
2018-11-29 15:53:32,277 INFO [master/172.30.65.195:0.Chore.1] master.ServerManager(913): Called isClusterShutdown. clusterShutdown=false, thread 417, hash=416244779{noformat}

With the help of [~busbey] we figured out that both masters were considered as active masters. This patch adds an extra check whether master is already stopped so a previously killed master cannot claim it is still active.

+1

| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:orange}-0{color} | {color:orange} test4tests {color} | {color:orange}  0m  0s{color} | {color:orange} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 11s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  2m  4s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 14s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 15s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  5s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 29s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 11s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green}  9m 15s{color} | {color:green} Patch does not cause any errors with Hadoop 2.7.4 or 3.0.0. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}134m 14s{color} | {color:green} hbase-server in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}174m 12s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:b002b0b |
| JIRA Issue | HBASE-21518 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12950045/HBASE-21518-v1.patch |
| Optional Tests |  dupname  asflicense  javac  javadoc  unit  findbugs  shadedjars  hadoopcheck  hbaseanti  checkstyle  compile  |
| uname | Linux 345b56ce27bf 4.4.0-139-generic #165~14.04.1-Ubuntu SMP Wed Oct 31 10:55:11 UTC 2018 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build@2/component/dev-support/hbase-personality.sh |
| git revision | master / fdddc47e77 |
| maven | version: Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T18:33:14Z) |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC3 |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/15152/testReport/ |
| Max. process+thread count | 4421 (vs. ulimit of 10000) |
| modules | C: hbase-server U: hbase-server |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/15152/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



[~stack]: Shall I commit it back until branch-2.0?

So, other UTs can be flaky because of this too? 

Yes, any UT that has multiple masters and knocks one over.

Pushed to brach-2.1, branch-2 and master. [~stack] let me know if you'd like this on branch-2.0.

Results for branch master
	[build #638 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/638/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/638//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/638//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/638//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


Results for branch branch-2.1
	[build #648 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/648/]: (/) *{color:green}+1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/648//General_Nightly_Build_Report/]




(/) {color:green}+1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/648//JDK8_Nightly_Build_Report_(Hadoop2)/]


(/) {color:green}+1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/648//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


Nice find boys. Yes please for branch-2.0 [~psomogyi].

Results for branch branch-2
	[build #1533 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/1533/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/1533//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/1533//JDK8_Nightly_Build_Report_(Hadoop2)/]


(/) {color:green}+1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/1533//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


Reopening to push to branch-2.0.

Results for branch branch-2.0
	[build #1128 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1128/]: (/) *{color:green}+1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1128//General_Nightly_Build_Report/]




(/) {color:green}+1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1128//JDK8_Nightly_Build_Report_(Hadoop2)/]


(/) {color:green}+1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1128//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


