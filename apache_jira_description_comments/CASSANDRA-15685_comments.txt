Walking the workflow I see its this branch https://github.com/newkek/cassandra/commits/cci-test2. which is based off https://github.com/newkek/cassandra/commit/b9c7a90e8a928cb22cd2b7c4ecd9f8dbc711e10e.  This issue should be fixed with

{code}
commit dfc279a22a5563ac7a832a586914d5410426e9b7
Author: David Capwell <dcapwell@gmail.com>
Date:   Sat Mar 21 20:57:50 2020 -0700

    Fix flaky test o.a.c.d.test.*RepairCoordinatorFastTest

    patch by David Capwell; reviewed by Ekaterina Dimitrova and Benjamin Lerer
    for CASSANDRA-15650
{code}

That commit has its own issues (being fixed in CASSANDRA-15684 and CASSANDRA-15683), but should have fixed this test. The root cause that was that nodetool repair command had a bug where it doesn't handle the case error notification was not seen but complete was (notifications are lossy).  If you update to latest do you still see this test as flakey?

Hey [~dcapwell] afaict I have only seen this test failing on this run, I have not seen it fail on other more recent builds happening after CASSANDRA-15650 indeed. Opened this ticket since I couldn't find a report of this failure. I'm ok closing it if it's been fixed in CASSANDRA-15650

Let's close but if you see it after upgrade then should reopen

Agreed

Happened again on trunk.

{code}
junit.framework.AssertionFailedError
	at org.apache.cassandra.distributed.test.PreviewRepairTest.testWithMismatchingPending(PreviewRepairTest.java:97)
{code}

[~newkek] I was wrong, I had forgotten that test doesn't use the nodetool api and instead relies on notifications.  Notifications are lossy so its possible to not see the error notification but see the complete.  I fixed this issue with the *RepairCoordinator*Test but that fix was in nodetool, so this test is still impacted.

I sent a patch switching from notification to nodetool.  I plan to run this in a loop tonight with lower resources.

One thing that this doesn't fix is that, if the NOTIFICATION notification gets dropped then this will still be flakey.  Unless we move the message to the success message (I am a fan of this), then not sure how to solve.

because node tool doesn't expose the output, I am tempted to remain using notifications, but poll at the end to get the final result.

moved out of PA since I don't think the patch I sent solves this completely.  The main issue is we need a notification which is lossy, so we need to move this to the completion message and poll for that in the tests (aka can't use nodetool).

I am jumping between a few things so neglecting this right now.  I "feel" like we need to make the completion message contain the notification message so we can recover from a missed message; with that the test can always wait for complete then poll one last time for the status; that should make it no longer flakey.  This can't be done via nodetool, so the current logic is better off.

Hi [~dcapwell],
Still working on this one? Let me know if you need a hand

[~e.dimitrova] I stopped working on it as I lost my time =(.  If you want, feel free to pick it up! =D

The current state is that the test requires knowing the result of the previous repair which only happens as a JMX notification, which is lossy; if the notification is lost, the test fails.

I was planning to append the result to the completion message as we can get that via polling, for this reason the test could not use the nodetool API but would need to poll directly (which it does today).  I also had a patch a bit back to fix nodetool to detect failures in case of JMX loosing the error message, this polling would need to do similar (so should probably be a common function).

Now, the change to the completion message is better IMO since it also fixes this for operators (if the notification is lost you don't know if there was a conflict), since polling will also see the latest message and will report it correctly. 

For now I will unassigned myself.

Managed to reproduce the failure after 189 runs on Mac, limiting the cores to 2. (Thanks for the hints [~dcapwell])

Attached the full log for completeness.



Test fixed [here | https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15685]
I ran successfully the whole class 500 times (as the fixed repair function is used by all test methods), log attached
Cassandra-wise the issue was already fixed in CASSANDRA-15650. The issue was in this test the StorageService#repair function is directly called so the additional polling from the RepairRunner was not happening for this test.

[~dcapwell], [~blerer], may I ask you for a review, please?


* https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15685#diff-584fc321d5855ec6340e5477d8e7e469R373. if you make it this far it means await was already signaled, so this should be a noop 
* https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15685#diff-584fc321d5855ec6340e5477d8e7e469R376 this fixes the failed case, but what about the preview repair notification getting lost?

The test has two issues
* not see the error, so believe the job was success; you handle this case
* not see the notification, so unable to update wasInconsistent; currently not handled.

To handle the second case, you would need to update the completion message then check for that in the test.  This is desirable since nodetool has the same issue, so moving it to complete would mean we always log it.

Ok, as we talked, for completion in case of regression it is good to cover also the case of lost NOTIFICATION to ensure we never got issues in the future with the last assert in this test. 
Moving back to in progress to add that too, thanks

Is there a ticket for the lossy notifications in general?


For others, Ekaterina and I chatted offline.

The preview repair tests assert that previews did not report inconsistencies, but since it uses a AtomicBoolean to handle concurrent cases, and the default is false, it can lead to a false negative; if we don't see the notification then we wouldn't flip the boolean from false to true.  Every test asserts false, so it is not failing right now, but it also won't detect regressions.

The issue is also true in the nodetool case as it may not see the notification so isn't aware of the results.

Thanks Ekaterina for all the hard work! =D

After a couple of hundred more runs of this test (my gut feeling told me that I miss something), it was confirmed that the lossy notifications are not the primary issue with this test.
In some cases even if we catch the notifications for success/error and the flags "success" and "wasConsistent" are properly set, still the PreviewRepair shows that the Incremental Repair is still running.

{code:java}
[junit-timeout] java.lang.RuntimeException: Repair session 82ff3420-9737-11ea-b32d-7fa12d874715 for range [(-1,9223372036854775805], (9223372036854775805,-1]] failed with error An incremental repair with session id 82eff1e0-9737-11ea-b32d-7fa12d874715 finished during this preview repair runtime
{code}

Turns out getting the notification doesn't always mean that the rest of the nodes are already informed about the completion. I can easily increase the time before preview repair starts.
But we were considering with [~dcapwell] to open a ticket as there might be other parts of the code or tools relying only on the notifications for completion. Worth to be checked.
Also, I am gonna check tomorrow in detail how we can improve this test not to rely on timing but probably some metadata. 

Any updates on this [~e.dimitrova] ?

[~bdeggleston] [~marcuse] could we get your thoughts on this?  preview repair isn't allowed to be run concurrently with IR, but IR completes before the message is handled on the participant; the below is impacted by this race condition.

{code}
nodetool repair foo bar
# does not look to be running concurrently with the above IR, since it was notified of success
# but the commit message may not have been seen yet which cause this to be concurrent with the above
nodetool repair foo bar --validate
{code}

See here https://github.com/apache/cassandra/blob/ec1808a34f9aa5ae0b956c1527828566f4ba2be5/src/java/org/apache/cassandra/repair/consistent/CoordinatorSession.java#L234-L244.  We send the message but don't make sure its ack on.

What do we think about moving fixver to 4.0-beta on this?

[~jmckenzie], personally, I think we can move it to 4.0-beta. My reasoning is that we might not be able to run preview repair(the participant is still not aware of the IR being completed already)  and have to start it again later but there is no data loss or something. [~dcapwell] are you ok with this reasoning? 


sorry for the delay.

I am fine with the test getting fixed without changing IR, though this adds an unexpected edge case for users; though it is expected to be rare in production.

We might fix the IR but it is not a blocker now as there is no defect and it is rare case. It will be taken care in beta, it is not a blocker. That is what I meant

Back to this work. [~blerer], [~bdeggleston], [~marcuse], may I ask for your expert advice? Is fixing the test enough here or [this behavior | https://issues.apache.org/jira/browse/CASSANDRA-15685?focusedCommentId=17121396&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17121396]   should also be considered? Thank you in advance :)

lets fix the test and open a new jira for any other improvements?

Thanks [~marcuse],  I raised CASSANDRA-15930 for handling of the mentioned edge case. 

Considering CASSANDRA-15930 I added only sleep of 3 seconds before running repair preview in this test which should be enough to fix the issue. 

I ran it 300 times locally with the 2 cores successfully. 

[Patch |https://github.com/ekaterinadimitrova2/cassandra/commit/8aa799a38247475ce96a5421625ff4a7a88b44d9]

[~marcuse], [~dcapwell], anyone of you wants to do a quick review? Thanks in advance!

your code shows 1 second, are you targeting 1 or 3 seconds?

I would prefer exec into each process to check the status but I can live with sleeps for now...

My bad, it is 1. Patch with 1s, tested with 1s. 
Thanks!

[~brandon.williams] do you mind to commit if you don't see any issue, please? The edge case will be fully handled in CASSANDRA-15930

Committed (with CHANGES entry removed since we don't list test fixes there)

I wasn't clear so being clear; +1

