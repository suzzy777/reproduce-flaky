Thanks for reporting [~weichiu]. I have not seen this issue though. I just randomly checked several DNs in our 3.1 cluster, with number of blocks from 330K to 1024K, I did not see this exception.

That error and the lag that causes it are exactly why the directory scanner throttle test is flaky.  When working on that test, I noticed that we occasionally see that lock-held-too-long message, and it correlates with the directory scanner taking much longer than usual to complete a scan.  So, yes it's a performance issue, but, no, it's not a regression.

I got the same problem on version 2.8.4-amzn-1 too.  And it seems that datanode will restart automatically for some reason after that. 

 
{code:java}
2019-06-26 05:27:21,615 WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl (VolumeScannerThread(/mnt/hdfs)): Lock held time above threshold: lock identifier: org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl lockHeldTimeMs=768 ms. Suppressed 8 lock warnings. The stack trace is: java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:1033)

... // here is some regular info log

2019-06-26 05:27:36,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode (main): STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG: user = hdfs
{code}
so, how to avoid this?

how about using readwritelock instead of lock?

I believe HDFS-14476 solves the same issue and so I'll mark this as a duplicate.

