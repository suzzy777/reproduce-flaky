cc [~jpeach@apache.org], you may have more context on it. Would you mind to take a look?

cc [~xujyan], [~dlm]

Looks like the resource statistics are not correct. I don't really see why that would happen. Probably the way to debug this is to leave the scratch filesystem mounted and poke at it with {{xfs_quota}}.

{code}
[01:07:51]W:	 [Step 10/10] 1048576 bytes (1.0 MB) copied, 0.00128219 s, 818 MB/s
[01:07:51] :	 [Step 10/10] ../../src/tests/containerizer/xfs_quota_tests.cpp:559: Failure
[01:07:51]W:	 [Step 10/10] I0429 01:07:51.865185 17604 slave.cpp:825] Agent terminating
[01:07:51] :	 [Step 10/10] Value of: usage1->executors(0).statistics().disk_used_bytes()
[01:07:51] :	 [Step 10/10]   Actual: 196608
{code}

We could be calling {{usage}} too fast, in another test we use {{while(true)}} to keep calling {{usage()}}. In don't think we need to do this in every test and our focus in this test is not {{usage()}}, we can just remove this expectation.

[~jpeach@apache.org] I can take care of this if you don't have time. :)

Tested this on master w/ fedora23 VM and can't get it to fail. Timing issue?

Maybe this is a flaky test. Did you try testing with `sudo /bin/mesos-test.sh --gtest="*NoCheckpointRecovery*" --gtest_repeat=-1 --gtest_break_on_failure`?

Earlier I did 1000 iterations with no error. This time I got a different failure after 114:

{code}
I0430 11:10:54.666662 11591 exec.cpp:150] Version: 0.29.0
I0430 11:10:54.678015 11620 exec.cpp:225] Executor registered on agent ccb4b4e9-46f5-48ec-bc72-ae5504a67357-S0
Registered executor on fedora-23
Starting task ea4ece2b-fb5a-40a2-bfcd-ff79a31566d8
sh -c 'dd if=/dev/zero of=file bs=1048576 count=1; sleep 1000'
Forked command at 11627
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000696071 s, 1.5 GB/s
I0430 11:10:54.691599 11620 exec.cpp:399] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 11627
Sent SIGTERM to the following process trees:
[
-+- 11627 sh -c dd if=/dev/zero of=file bs=1048576 count=1; sleep 1000
 \--- 11629 sleep 1000
]
/opt/home/src/mesos.git/src/tests/containerizer/xfs_quota_tests.cpp:575: Failure
Failed to wait 15secs for _recover
*** Aborted at 1462039869 (unix time) try "date -d @1462039869" if you are using GNU date ***
PC: @          0x184da1e testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 4417 (TID 0x7f7794df78c0) from PID 0; stack trace: ***
    @     0x7f778d1e49f0 (unknown)
    @          0x184da1e testing::UnitTest::AddTestPartResult()
    @          0x1842197 testing::internal::AssertHelper::operator=()
    @          0x17a0570 mesos::internal::tests::ROOT_XFS_QuotaTest_NoCheckpointRecovery_Test::TestBody()
    @          0x186bbcc testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1866bd2 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x184761c testing::Test::Run()
    @          0x1847dd4 testing::TestInfo::Run()
    @          0x1848425 testing::TestCase::Run()
    @          0x184ef63 testing::internal::UnitTestImpl::RunAllTests()
    @          0x186c893 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1867748 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x184dc3f testing::UnitTest::Run()
    @           0xf640f1 RUN_ALL_TESTS()
    @           0xf63ce9 main
    @     0x7f778bcfc580 __libc_start_main
    @           0xa35039 _start
Segmentation fault
{code}

About the original failure, it's about the disk speed isn't it? i.e., put sleep 1 before dd to simulate it?

And the 2nd error should be because we have placed 

{code}
  Future<Nothing> _recover =
    FUTURE_DISPATCH(_, &MesosContainerizerProcess::___recover);
{code}

below

{code}
slave = StartSlave(detector.get(), flags);
{code}

It should above it.

Both are race conditions.

Thanks for the help [~xujyan]. Patch is in https://reviews.apache.org/r/47001/.

Interesting I got a recovery failure after 5000 iterations.

{code}
/opt/home/src/mesos.git/src/tests/containerizer/xfs_quota_tests.cpp:606: Failure
Value of: xfs::getProjectId(sandbox).isNone()
  Actual: false
Expected: true
*** Aborted at 1462425273 (unix time) try "date -d @1462425273" if you are using GNU date ***
PC: @          0x184d566 testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 8269 (TID 0x7f860ea948c0) from PID 0; stack trace: ***
    @     0x7f8606e819f0 (unknown)
    @          0x184d566 testing::UnitTest::AddTestPartResult()
    @          0x1841cdf testing::internal::AssertHelper::operator=()
    @          0x17a0bfb mesos::internal::tests::ROOT_XFS_QuotaTest_NoCheckpointRecovery_Test::TestBody()
{code}

I verified the XFS project state and somehow the isolator failed to remove the project ID.

[~jpeach@apache.org] I committed the patch you had

{noformat:title=}
commit 95e670cd41a33e68afab701f9c28dd968a6f8011
Author: James Peach <jpeach@apache.org>
Date:   Wed May 11 16:43:05 2016 -0700

    Fix race conditions in ROOT_XFS_QuotaTest.NoCheckpointRecovery.
    
    There are two race conditions in
    ROOT_XFS_QuotaTest.NoCheckpointRecovery. The first is when we were
    checking the disk resources consumed without knowing whether the dd
    command had completed. We can just eliminate this check since other
    tests cover the resource usage case. The second race was installing the
    MesosContainerizerProcess::___recover expectation after starting the
    slave. We need to install this before starting.
    
    Review: https://reviews.apache.org/r/47001/
{noformat}

Weird check failures though. Any theories? Let me see if I can repro.

Unfortunately it didn't fail for me after 10000 iterations... let's close this one and file a new one if it fails again and more info can be captured.

