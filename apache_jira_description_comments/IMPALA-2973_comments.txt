[~dhecht] - not sure if this is reproducible or serious enough to go into 2.5.0.

This test looks inherently flaky to me.  [~alan@cloudera.com], why can we expect this assert to never fire?  The process execution could be delayed occasionally, especially when running in a VM and/or with code coverage enabled.  Is there someway you can get the coverage you want without be susceptible to timing flakiness?

Also hit in one of the gvms: http://sandbox.jenkins.cloudera.com/job/impala-external-gerrit-verify-merge/1949/
{code}
Error Message

AssertionError: Hash join timing too low for Fragment non-child: 999.982us 0 assert False
Stacktrace

query_test/test_hash_join_timer.py:114: in test_hash_join_timer
    self.__verify_join_time(non_child_time, "Fragment non-child")
query_test/test_hash_join_timer.py:129: in __verify_join_time
    assert False, "Hash join timing too low for %s: %s %s" %(comment, duration, duration_ms)
E   AssertionError: Hash join timing too low for Fragment non-child: 999.982us 0
E   assert False
Standard Error

SET disable_codegen=False;
SET abort_on_error=1;
SET exec_single_node_rows_threshold=0;
SET batch_size=0;
SET num_nodes=0;
-- executing async: localhost:21000
select /*+straight_join*/ count(*) from functional.alltypes a CROSS join  (select distinct * from functional.alltypes where int_col >= sleep(5)) b where a.id>b.id and a.id=99;
{code}


Also in http://sandbox.jenkins.cloudera.com/job/impala-external-gerrit-verify-merge/1950/


When I created the test, the hash join time takes ~50ms and way less than 1sec. The number was very stable when I ran it. The test is also run in serial and with codegen disabled. There shouldn't be that much variance.

Have you manually check if the hash join timer is correct? If so, seems like we have improved cross join, then we should increase the size of the LHS and RHS to make the timing more stable. We need a upper bound and lower bound because we used to report 0 time when we had a bug in the hash join timing.

The case when the time was too high was a code coverage build. I don't think we should assume that anything will complete in 1 second when code coverage is enabled.

This type of time dependent test assumes a somewhat predictable execution env. 
For this particular one, this doesn't really help with code coverage. So, there isn't much value in running it in code coverage run. 

However, other time dependent test (such as detecting timeout, or testing the #DDL throughput) can also run into problem when running in an extremely slow env (such as code coverage). 



Really, there are two issues being tracked here:

1) Time too high: this happens with code coverage, and not a blocker for the release.

2) Time too low: this looks like something may have changed recently.  Are all the failures after Jim's change to use MONOTONIC_COARSE?

[~jbapple], could you look at #2 from that angle?

I.e. I wonder if 999.982 usec is equal to one jiffy on the EC2 images we use.

On the cloudcat image I was using, it was 250 usec, but it could vary depending on the instance size.

Also, this bug showed up the first time before the COARSE change got in, but they could be two different bugs manifesting similarly.

Actually, scratch that - it was *2500* usec.

Can you share the link to the first failure before COARSE change got in?

[~alan@cloudera.com] I was referring to this bug and the comments on it that happened before yesterday, all of which were before COARSE got in.

[~alan@cloudera.com], as far as I'm aware, just those GVM's linked in the comments above. I'm not sure how to determine whether they were run with Jim's change or not.

Okay, Jim's change has: CommitDate: Mon Feb 22 21:26:46 2016 +0000
And the first GVM failed started on Feb 22 at 17:03:19, so wouldn't have had that change.

But it does appear something may have changed recently to cause case #2 to show up.
[~tarmstrong], you mentioned it looked like timers were generally broken.  Can you comment here?

On an EC2 cluster, I'm seeing timings in the Impalad web profile that are off by oders of magnitude. E.g. below non-child time is 206ms but BuildTime is 4.7us:

{code}
      AGGREGATION_NODE (id=3):(Total: 3s009ms, non-child: 206.145ms, % non-child: 6.85%)
        ExecOption: Codegen Enabled
         - BuildTime: 4.731us
         - GetResultsTime: 0.000ns
         - HTResizeTime: 0.000ns
         - HashBuckets: 0 (0)
         - LargestPartitionPercent: 0 (0)
         - MaxPartitionLevel: 0 (0)
         - NumRepartitions: 0 (0)
         - PartitionsCreated: 0 (0)
         - PeakMemoryUsage: 24.00 KB (24576)
         - RowsRepartitioned: 0 (0)
         - RowsReturned: 1 (1)
         - RowsReturnedRate: 0
         - SpilledPartitions: 0 (0)
{code}

Or here, in the profile I got from CM without the pretty-printed units:
{code}
        AGGREGATION_NODE (id=24)
          - BuildTime: 2427460
          - GetNewBlockTime: 1095830606
          - GetResultsTime: 449753
          - HTResizeTime: 939412
          - HashBuckets: 32768
          - InactiveTotalTime: 0
          - LargestPartitionPercent: 2
          - MaxPartitionLevel: 0
          - NumRepartitions: 0
          - PartitionsCreated: 16
          - PeakMemoryUsage: 2691418
          - PinTime: 0
          - RowsRepartitioned: 0
          - RowsReturned: 1002
          - RowsReturnedRate: 1
          - SpilledPartitions: 0
          - TotalTime: 691361593143
          - UnpinTime: 23697
{code}

Disregard my previous comment, i was misunderstanding the profile.

[~jbapple], even though not due to your change, would you still be able to dig into this? In particular case #2 where timing is lower.  And maybe we should adjust the test in the mean time to unblock gvms.  If you aren't able to look at, I think [~dtsirogiannis] said he could take it over.

[~dhecht] Sure thing

I think the appearance of my change landing only AFTER the lower bound started being violated is actually a time-zone confusion.

If you look at http://gerrit.cloudera.org:8080/#/c/2204/ , it was cherry-picked at 1:26pm.

I think this fix is flaky. I just got a test failure on my gvm because duration was less than a millisecond, which should still be valid given the new lower bound:

14:51:51  TestHashJoinTimer.test_hash_join_timer[test cases: ['select /*+straight_join*/ count(*) from functional.alltypes a CROSS join  (select distinct * from functional.alltypes where int_col >= sleep(5)) b where a.id>b.id and a.id=99', 'NESTED LOOP JOIN'] | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none] 
14:51:51 query_test/test_hash_join_timer.py:114: in test_hash_join_timer
14:51:51     self.__verify_join_time(non_child_time, "Fragment non-child")
14:51:51 query_test/test_hash_join_timer.py:129: in __verify_join_time
14:51:51     assert False, "Hash join timing too low for %s: %s %s" %(comment, duration, duration_ms)
14:51:51 E   AssertionError: Hash join timing too low for Fragment non-child: 999.995us 0
14:51:51 E   assert False



Sorry my gvm wasn't rebased

As Dan mentioned, there were two parts of this. The lower bound one was the blocker one. That seems fixed; I'm adjusting the priority for the upper bound part, which Dan noted is not a blocker as it's only for code coverage.

Here is the partial (and temporary) fix that was checked in for 2.5:

http://gerrit.cloudera.org:8080/#/c/2298/

Hopefully this fixes the test when running under code coverage:

http://gerrit.cloudera.org:8080/2608

While the ASAN job is still red, it is not red because of this test.

