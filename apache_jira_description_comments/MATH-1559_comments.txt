bq. can mess up the probability

How often does it happen?
I'm a bit wary that changing the tolerance conveys the wrong impression about the obviously expected result.


It happens quite often – whenever the probability p(2) is added to the sum in the first or second place in MathArrays.normalizeArray() line 909. Somehow when this happens, the sum won't be 1, it'll be 0.999..., then the output is: p(2) * 1/0.999... = 0.5/0.999... so it's not exactly 0.5. 

!image-2020-10-28-10-32-34-088.png!

And just to clarify, the order of the probabilities to sum can be different because a HashMap is used to store each value and its occurrences in the constructor of EnumeratedRealDistribution (line 88). HashMaps do not guarantee any order of iteration.

I agree that changing the tolerance is not solving this issue directly, it's just making the test always pass. It'll take some deeper knowledge about the code and the MathArrays module in particular to solve the underlying issue. 

bq. It happens quite often

Strange.  I don't recall ever seeing this test fail.
Could this be a platform issue?


I think the test is failing due to randomness in the HashMap. I found the test to be flaky using a tool called NonDex ([implementation here|https://github.com/TestingResearchIllinois/NonDex] and the [paper here|http://mir.cs.illinois.edu/awshi2/publications/FSEDEMO2016.pdf]), which uses different random seeds for testing so that wrong assumptions about Java APIs (eg. HashMaps are ordered) are caught. 

When I simply run mvn test, the test doesn't fail either. Empirically the failing orders doesn't seem to occur when the HashMap implementation is not instrumented by the NonDex tool. Still, I am able to produce the exact trace when it does fail (see screenshot above), and I think my reasoning should be correct.

