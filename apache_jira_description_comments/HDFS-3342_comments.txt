This error is easy to reproduce. Put a medium size file (I used 4MB) in HDFS, and then run something like:
{code}
$ hadoop fs -cat vmlinuz | perl -e 'while (1) { sleep 5; $x += 5; print "$x\n"; }'
{code}

After 8 minutes (the default write timeout) you'll see a SocketTimeoutException in the DN logs.


Hi Toddï¼Œ
how to cause the Exception in hbase?

Hi [~tlipcon] and [~aoxiang], thanks for reporting this issue and looking at it earlier. I'm assigning it to myself and will try to look at it hopefully soon.


Hi [~tlipcon],

The reason I'm looking at this issue is, this is still happening in recent use and confused user. Would you please help review the patch? Thaks a lot.

I was able to reproduce the issue and see the log:
{code}
14/10/04 21:12:04 INFO datanode.DataNode: Failed to send data: java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/172.17.186.17:42010 remote=/172.17.186.17:60227]
14/10/04 21:12:04 WARN datanode.DataNode: DatanodeRegistration(172.17.186.17, datanodeUuid=95f0a627-b010-453b-a432-c147d012c814, infoPort=42075, ipcPort=42022, storageInfo=lv=-56;cid=CID-13a9b341-3a15-405e-8d07-a719ec9be2ac;nsid=1866275128;c=0):Got exception while serving BP-326257059-172.17.186.17-1412481724026:blk_1073741825_1001 to /172.17.186.17:60227
java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/172.17.186.17:42010 remote=/172.17.186.17:60227]
        at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)
        at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:547)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:716)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:486)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:111)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:69)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:225)
        at java.lang.Thread.run(Thread.java:724)
14/10/04 21:12:04 ERROR datanode.DataNode: haus03.sjc.cloudera.com:42010:DataXceiver error processing READ_BLOCK operation  src: /172.17.186.17:60227 dst: /172.17.186.17:42010
java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/172.17.186.17:42010 remote=/172.17.186.17:60227]
        at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)
        at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:547)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:716)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:486)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:111)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:69)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:225)
        at java.lang.Thread.run(Thread.java:724)
{code}

I found that the top portion 
{code}
14/10/04 21:12:04 INFO datanode.DataNode: Failed to send data: java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/172.17.186.17:42010 remote=/172.17.186.17:60227]
{code} 
was introduced by HDFS-3555 for same issue. But the fix there still thows exception, which is not handled, thus we are seeing the reported error.

I'm submitting a patch that made the output 
{code}
14/10/05 10:56:57 INFO datanode.DataNode: Failed to send data: java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/172.17.186.17:42010 remote=/172.17.186.17:41933]
14/10/05 10:56:57 WARN datanode.DataNode: DatanodeRegistration(172.17.186.17, datanodeUuid=2a87010c-c9fe-4b4e-a249-0d8bf11a8f41, infoPort=42075, ipcPort=42022, storageInfo=lv=-56;cid=CID-ba2f8c8b-7e49-4514-b74c-201c1e9508ad;nsid=1860548702;c=0):Got exception while serving BP-269685814-172.17.186.17-1412528857362:blk_1073741825_1001 to /172.17.186.17:41933
java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/172.17.186.17:42010 remote=/172.17.186.17:41933]
        at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)
        at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:550)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:730)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:677)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:490)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:226)
        at java.lang.Thread.run(Thread.java:724)
14/10/05 10:56:57 INFO datanode.DataNode: Likely the client has stopped reading, disconnecting it (haus03.sjc.cloudera.com:42010:DataXceiver error processing READ_BLOCK operation  src: /172.17.186.17:41933 dst: /172.17.186.17:42010; java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/172.17.186.17:42010 remote=/172.17.186.17:41933])
{code}


{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12673007/HDFS-3342.001.patch
  against trunk revision 16333b4.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8324//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8324//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8324//console

This message is automatically generated.

The findbugs issues are not introduced by the change made here. I created a separate jira HDFS-7194 for them. 


Hi Yongjun, thanks for working on this,

Looking at the new output you posted, it looks like it quashes the ERROR log, but we still end up with 3 log prints for the same issue, and one is still at WARN. Wouldn't an ideal solution print just a single log message at INFO? Also note that if someone has the log level set to WARN (happens in production deployments), they'll see the scary stack trace but not the new log print you added. It'd also be nice to not have stack trace spam in this situation, since it's somewhat expected.

LMK what you think, thanks again.

HI [~andrew.wang],

Thanks a lot for the review and comments!

Good catch of yours. Indeed, if user set the log level to WARN, then the new message I added won't be seen.  

The "WARN" message was there before I made this change, and it's intended to report the stack trace all IOException. The new message I added tried to say "Likely the client has stopped reading..".  When there is a SocketTimeoutException, I guess there may be other cases of SocketTimeoutException than the one we are dealing here. I was worried that taking out the WARN message would cause missed reporting of some other cases.  That's why I used the word "Likely".

To address your comment, I added similar statement to the WARN msg and uploaded a new rev (002), so similar msg will be printed at WARN log level. I wonder whether it looks good to you.

Thanks again.


{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12675985/HDFS-3342.002.patch
  against trunk revision e90718f.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:red}-1 eclipse:eclipse{color}.  The patch failed to build with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8464//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8464//console

This message is automatically generated.

The eclipse:eclipse build issue appears to be a glitch, upload same patch again to trigger another run.


{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12676008/HDFS-3342.002.patch
  against trunk revision 7aab5fa.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDecommission
                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting
                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication
                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8467//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8467//console

This message is automatically generated.

Couple of the failed tests appear to be flaky, other twos are fixed recently. Upload same patch again to trigger new run.


{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12676528/HDFS-3342.002.patch
  against trunk revision d71d40a.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestRollingUpgradeDowngrade

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8493//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8493//console

This message is automatically generated.

HI [~andrew.wang],

Thanks for your earlier review. I uploaded a newer rev (003) which might be closest to what you initially commented. Would you please take a look again? You can refer to rev 002 too in case. Thanks.




{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12676920/HDFS-3342.003.patch
  against trunk revision 0942c99.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8521//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8521//console

This message is automatically generated.

LGTM, thanks Yongjun! One log message and a quashed stack trace sounds like a nice improvement to me.

+1 will commit shortly.

Committed to branch-2 and trunk, thanks again YJ.

SUCCESS: Integrated in Hadoop-trunk-Commit #6397 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6397/])
HDFS-3342. SocketTimeoutException in BlockSender.sendChunks could have a better error message. Contributed by Yongjun Zhang. (wang: rev c2866ac34d063a4d2e150fe35632111b37a1514d)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java


Thanks Andrew!


FAILURE: Integrated in Hadoop-Yarn-trunk #729 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/729/])
HDFS-3342. SocketTimeoutException in BlockSender.sendChunks could have a better error message. Contributed by Yongjun Zhang. (wang: rev c2866ac34d063a4d2e150fe35632111b37a1514d)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java


FAILURE: Integrated in Hadoop-Hdfs-trunk #1918 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1918/])
HDFS-3342. SocketTimeoutException in BlockSender.sendChunks could have a better error message. Contributed by Yongjun Zhang. (wang: rev c2866ac34d063a4d2e150fe35632111b37a1514d)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt


FAILURE: Integrated in Hadoop-Mapreduce-trunk #1943 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1943/])
HDFS-3342. SocketTimeoutException in BlockSender.sendChunks could have a better error message. Contributed by Yongjun Zhang. (wang: rev c2866ac34d063a4d2e150fe35632111b37a1514d)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt


