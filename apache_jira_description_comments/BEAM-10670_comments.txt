This issue is assigned but has not received an update in 30 days so it has been labeled "stale-assigned". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.

This issue was marked "stale-assigned" and has not received a public comment in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.

This issue is P2 but has been unassigned without any comment for 60 days so it has been labeled "stale-P2". If this issue is still affecting you, we care! Please comment and remove the label. Otherwise, in 14 days the issue will be moved to P3.

Please see https://beam.apache.org/contribute/jira-priorities/ for a detailed explanation of what these priorities mean.


This issue was marked "stale-P2" and has not received a public comment in 14 days. It is now automatically moved to P3. If you are still affected by it, you can comment and move it back to P2.

[~lcwik]

Will there be any error or warning if Pipelines continue to use --experiments=use_deprecated_read afterwards ?

This ticket seems possibly out of date now, but I did want to come in and leave a comment that, until something is fixed with PubsubIO on the DirectRunner (and I'd assume any other runner other than dataflow...), using use_deprecated_reads is the only reasonable option for running pipelines that use it, otherwise they'll go for long periods of time without emitting any elements.

Following update to 2.28.0 the GBIF pipelines ([https://github.com/gbif/pipelines)] started to have issue. This affected the production pipelines on GBIF.org running on spark/Yarn and those for ala.org.au which run on Spark standalone on AWS.

I'm told that they observed significant memory increase, slower performance, and failing jobs with "GC overhead exceeded" for simple AvroIO to AvroIO pipelines. Both now run with the use_deprecated_read option.

We can help running tests but I think the cause needs to be fixed before the legacy functionality is removed.

I can confirm a degradation of between 15-20% of performance when using the now 'default' execution option on the Spark runner. We tested this via TPC-DS query 3 in a 1000GB input dataset with CSV input via TextIO.

Something puzzling is that I can see a performance degradation even when the inputs are not based on the traditional Read transform for example ParquetIO.withSplit (based on SDF) performance is worse by default that when configured with `–experiments=use_deprecated_read`. Something odd is going on here. Do you think we can get someone to go deeper into this? [~boyuanz] maybe? otherwise probably it is best that we opt out of this for the next release until the performance of the new translation is better.

CC [~kenn]

Is this present in 2.29.0 or is it earlier/later?

I'd be a big +1 on changing the default back to the old non-splittable read implementation.

It seems like a good path forward would be:
 * Change default back to the old impl (in 2.30)
 * Ask users to begin experimenting with the new SDF based implementation, fix any performance issues that come up during this period.  There's a couple enumerated in this ticket, and I think there's a few on the mailing list as well
 * Once those are ironed out, change the default back to the SDF implementation

 

+1 [~SteveNiemitz] I also think is a good idea to do this. 2.30.0 cut is today so maybe we should include this into the burn list.
WDYT [~kenn] [~boyuanz] ?

 

These ideas make sense. They should not require big changes. In fact the Spark translator already should directly translate a composite if there is a translator for it: https://github.com/apache/beam/blob/8e6695643b69da1ab83c9aad16a913cf6e442823/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java#L397

So the idea is this:

 - Every BoundedSource-based Read transform should still have Read in its expansion. I don't know if this was changed. The SDK should be able to build one graph and have it work optimally on runners based on their preferences.
 - Inside that expansion there may be SDF, but any runner can ignore it, or have a flag to enable it. Again, I don't think the SDK should change its expansion.

Can you itemize where this does not work? Was the SparkRunner changed? Were the IOs made incompatible?

CC [~heejong] and [~chamikara]

I also want to make sure things work well for portability. But the design of the pipeline proto and the whole model is entirely aimed at having both benefits.

I think this bug might have the wrong title now? Anyhow I think that every transform should ideally have an SDF in their expansion (this is true?) but the runners should execute however they want. What needs to be changed back in the SparkRunner? This is important and we should cherrypick for 2.30.0

For entry points that require SDF, it will be nice to still support them. But whenever possible using the most efficient way of executing is preferred.

What's the next step for this issue? This is currently a blocker for 2.30.0 release.

[~heejong] I just opened [https://github.com/apache/beam/pull/14755] to make the non SDF translation the default for Spark runner because of the reported performance issues. If it all goes well I will submit the cherry pick just afterand we should be good to go.

I am not sure however if this would be enough for [~SteveNiemitz] request too, if I remember well he reported the issues on Direct runner, or was it a different one?

Yeah my issues were with the usability of the direct runner.  It seems like there were also issues reported with Flink as well?

I have issues with PubsubIO + Direct Runner too, the messages are consumed from pubsub side, but nothing is happening in the Beam side (stuck or something). 

Additionally, when I upgraded from beam 2.24.0 -> 2.25.0, I can workaround this by using `–experiments=use_deprecated_read`, but seems like starting from version 2.26.0, this option no longer works.

This is a blocker for me to upgrade beam version since there is no way to revert back to old implementation.

For info I just merged the PR that returns back Spark runner to use the traditional Read translation. Given all the other comments above it is a good idea to do the same for all runners, but I won't be able to do it at least until the next week.  [~kenn] [~boyuanz] maybe someone else can take care of rolling back this on the other runners so we can unblock the release in the meantime?

I can help this. I believe at this moment, the only runner going with SDF is Dataflow one.

Thanks a lot [~boyuanz]

Making the default back to old read implementation because of the performance regression is done for 2.30.0.
The issue described by the title is still unresolved. So bumping up the fix version to 2.31.0.

I suggest cloning the bug and having it closed with Fix Version = 2.30.0 so it shows up in the automatic release notes.

This issue is assigned but has not received an update in 30 days so it has been labeled "stale-assigned". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.

This issue was marked "stale-assigned" and has not received a public comment in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.

This Jira ticket has a pull request attached to it, but is still open. Did the pull request resolve the issue? If so, could you please mark it resolved? This will help the project have a clear view of its open issues.

This issue has been migrated to https://github.com/apache/beam/issues/20530

