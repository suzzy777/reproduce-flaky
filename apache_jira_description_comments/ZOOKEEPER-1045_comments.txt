Thanks for the security work on ZK, folks. 

I have a question - how is the quorom peer protocol different from the client-server protocol? Any rough estimate on the ETA for a patch on this issue?

Devaraj,
 The server to server protocol is very different from client to server, which makes it harder to implement kerberos in the quorom peer protocols. I dont think we'll have this in 3.4.0 release. Maybe 3.5?

Hi Mahadev,

Our team started to use zookeeper recently, and we think we need the server to server authentication to prevent other server joining the cluster. Any plan for this jira?

Thanks

Hi Sasha,

Will SSL based Cert authentication solve your problem?. Mind that there are two channels/tcp connections between Quorum Peers one for election and other for ZAB. One can create a CA cert for a ZK cluster and use that to sign the cert of each ZK node. Which will ensure that nodes signed by this CA cert, i.e part of this cluster, can connect to each other.

Let me know if this works for your use case.
Thanks
Powell.

Hi Powell,

That should work. Is that already implemented and where can I find the instructions? Or it's going to be implemented as part of this jira?

Thanks
Shasha

Hi Shasha,

I am working on a patch at this time its still work in progress. What version of ZK are you looking to use will it be 3.4.x or 3.5.x?.

thanks
Powell.

Thanks for working on this [~geek101]! Happy to help with reviewing & merging those patches. What branch are your patches based on (3.4 or 3.5)?

How does this ticket compare to ZOOKEEPER-1000? Do they solve different problems?  Or alternate solutions to the same problem?

Hi Raul,

For now I have written something that tries to replace QuorumCnxManager class using Netty 4.1 for ZOOKEEPER-901, which tries to address both issues of SSL and serialized connect.

Which would work something like this:
 1. Initialize as VoteBroadcast(Set<QuorumServer>) (QuorumPeer will do that)
 2. Then use it as follows FLE.sendNotifications(msg) -> VoteBroadcast.broadcast(msg) and FLE.WorkerReceiver.run() -> VoteBroadcast.getVotes(). 
     I am providing addServer() and removeServer() methods will could address 3.5.x I think(not sure yet!).

I was hoping to use this stuff for Learner but I at this point in time SSL Sockets for Learner seems like a better way to get SSL working for it.
The transport/encode/decode is pretty entrenched there and making all of that async seems risky just to get SSL and implementing streaming interface on top of Netty channels seems like increasing complexity just to get SSL. Hence I am leaning towards SSL Sockets for Learner side of things. Let me know what you think or if I have gotten that wrong.

I will post a patches for 3.4 first since I am most familiar with it then work my way upstream. I will post two patches one for QCM and other for Learner. I have yet to start with Learner side of things.

Thanks
Powell.


Both are the same unless the author of the bug wants more complex auth that what I have proposed, perhaps duplicating this with ZOOKEEPER-1000 and moving the discussion there is best.

3.4.6

I would vote for keeping this ticket open for SASL server-server authentication. SSL is better than nothing, but SASL support would be nice.

I agree, but it'd be nice to make the distinction between the two jiras clear. We can also use SSL authentication, so I assume we can narrow down the scope of this jira to just SASL or perhaps leave the decision of whether to use SSL authentication at all to this jira and have a note in the other.  

Mutual SSL support, seems simpler (if I'm not mistaken), and is adequate for the basic blocker we have, of wanting to have a cluster with nodes spanning multiple datacenters (e.g. with remote observer nodes, etc.).  SASL is probably overkill for that.

Thanks everyone for the interests and useful discussions. I agree with [~fpj] to make a clear distinction between the two jira issues. I could see the idea of this jira is to provide an authentication mechanism among the quorum peers using SASL. I'm currently working on SASL + Kerberos based solution in branch-3.4. I'm assigning the issue to myself and will upload a patch soon.

Thank you [~geek101] for the efforts in building SSL way, we could use ZOOKEEPER-1000 for this solution. My personal opinion is to implement SSL solution in 3.5.* or trunk as Netty + SSL feature for the client-server communication is available from branch-3.5 onwards.

[~geek101] this looks good, but I don't fully understand the semantics of VoteBroadcast.broadcast(msg). The problem I see is that you don't want to block the call until everyone receives the message, but at the same time you need to deliver votes to late joiners.

One suggestion is to write a short design doc explaining the reasoning for this proposal.

I'm attaching patch to supports QuorumPeer authentication using the SASL(Kerberos/Digest) mechanism. This patch is based on branch-3.4. Also, please refer PR: https://github.com/apache/zookeeper/pull/49. Any questions and comments are very welcome.

Following are the changes:
# Please refer {{src/java/main/org/apache/zookeeper/server/quorum/auth/README.md}} to see the configurations.
# Introduced {{QuorumConnectionThread}}, through which the connection will be established between the quorum peers asynchronously. This will not block other connection requests.
# Added {{org.apache.zookeeper.util.SecurityUtils}} to reduce the code duplication
# Added {{org.apache.zookeeper.server.quorum.QuorumAuthPacket}}, jute buffer for messaging.
# Refer QuorumAuthClient and QuorumAuthServer for the major auth logic.
# Included tests to verify Digest mechanism
# Included tests to verify the Kerberos. I've used {{MiniKdc}} way of testing from the {{HDFS}} and taken few test classes from that project. This code base is quite big and added few test jar dependencies {{apache.directory.server}}

Thanks a lot [~ivank@yahoo-inc.com], [~hongchaod], [~fpj], [~phunt], [~rgs] for the offline discussions and advice.

Pending Work:
# Need to support upgrade execution path. I'll update the proposal to support this soon.


Hi Shasha,

If you want to try out SSL based implementation please refer to ZOOKEEPER-1000 I have posted links to source. This is implemented for 3.4.x branch. Feel free to let us know what you requirements are this will help us refine the solution.

thanks
Powell.

All API are async , will return immediately. The module does try to hold the invariant of ensuring that current vote is sent to peers in all cases(adding peers, reconnecting peers, msg tx errors etc). Will come with unit-tests to verify correctness. I will certainly write a design doc and publish that soon. The goal is to address ZOOKEEPER-901 and its related issues. Perhaps we should move this discussion there.

Two points of divergence from current implementation w.r.t FLE:
1. FLE does not to put the Vote into per Peer queue via manager.toSend() anymore only has to send call broadcast(vote) when ever it likes. QCM will take of sending the new Vote, FLE asked it to , if it knows this is a new Vote for peer(s) its managing. There are no outgoing queue to each Peer in QCM either, when it connects to a peer it just sends current Vote it has.

2. FLE unlike now will call getVotesBlockingQueue() (instead of manager.pollRecvQueue()) will get current votes of all peers that QCM knows now and any future vote received that is different than last one sent for every peer since the first call. 

From my understanding this will eliminate unnecessary transitions of FLE since currently it has to digest all the messages received when it calls pollRecvQueue() since last round, this is because QCM's Rx/Tx is always alive until QuorumPeer shuts it down, which is when QuorumPeer is shutdown. 

There is also an API call called getVotes() which simply returns current vote view that QCM knows, but too keep FLE simple/same with current implementation I will add getVotesBlockingQueue() to mimic pollRecvQueue().

Few points regarding implementation:
1. QCM will use a single thread executor. Hopefully this will address the concern of starting and stoping multiple threads to each peer.

2. This thread is shared by Netty to handle all TCP channels and also tasks in QCM to perform operations (like new channel/write and queue mgmt).

3. Netty handlers are written to be thread safe so one could pass more threads but I think one thread should be enough to handle handful of QuorumPeers talking part time.

4. Will try to keep FLE changes to minimum, only touching interfaces to manager.

Please let me know if I assumed something wrong and all feedback/comments are welcome. 

{quote} Pending Work:
 1. Need to support upgrade execution path. I'll update the proposal to support this soon. {quote} 
I've captured the design thoughts that comes in my mind and attached a draft document which describes the proposal. It would be really great to see the feedback from the community about the proposal. Thanks!

Is there much user demand for Kerberos support for inter-zk channels?. Will ZK have to always get token from KDC first before authenticating a peer?. I am not quite familiar with SASL Java API can you shed some light into the system level process. Does this provide encryption of the data traffic using the shared secret key?.



bq. Is there much user demand for Kerberos support for inter-zk channels?. 

SASL approach is a good choice considering that SASL(Kerberos) is already supported between zkclient-zksever communications quite some time. I think users/admins will be much more comfortable with SASL deployment. Also, it will be easy for them to upgrade the existing clusters which is secured with SASL(kerberos) way.

bq. Will ZK have to always get token from KDC first before authenticating a peer?. I am not quite familiar with SASL Java API can you shed some light into the system level process.

Yes, Initial authentication takes place between the Kerberos client and the KDC server. In ZooKeeper,every QuorumPeer will have a Kerberos client and  at the beginning gets token from the KDC server. For the initial TGT can either choose {{kinit USERNAME}} or use the local keytab file. I've tried capturing few configurations needed for implementing this feature in [README.md|https://github.com/apache/zookeeper/pull/49/files#diff-ecca263dd60ba00b624638323e799bdfR53]

For example,
{code}
QuorumServer {
       com.sun.security.auth.module.Krb5LoginModule required
       useKeyTab=false
       useTicketCache=true
       principal="zkquorum/localhost@EXAMPLE.COM";
};
{code}

{code}
QuorumServer {
       com.sun.security.auth.module.Krb5LoginModule required
       useKeyTab=true
       keyTab="/path/to/keytab"
       storeKey=true
       useTicketCache=false
       debug=false
       principal="zkquorum/localhost@EXAMPLE.COM";
};
{code}

I hope the following links will help to understand Kerberos, SASL java API:

http://www.roguelynn.com/words/explain-like-im-5-kerberos/
https://docs.oracle.com/javase/7/docs/technotes/guides/security/sasl/sasl-refguide.html
https://software.intel.com/sites/manageability/AMT_Implementation_and_Reference_Guide/default.htm?turl=WordDocuments%2Fintroductiontokerberosauthentication.htm

bq. Does this provide encryption of the data traffic using the shared secret key?.

As per my understanding, Kerberos provides facilities to make sure that messages are not changed as they travel across the network. The messages can optionally be encrypted so only the parties that know the session key can examine their contents. Java SASL provides QOP settings. "auth-conf" â€“ This stands for authentication, integrity and confidentiality.  This setting guarantees that data exchanged between client and server is encrypted and is not readable by a "man in the middle". Truly I haven't explored much to implement this part. I hope experienced folks can shed some light on this.

Regarding the QOP settings, use of auth-int (integrity checking to guard against man-in-the-middle tampering) or auth-conf (encryption to prevent man-in-the-middle reading data) requires wrapping and unwrapping the data exchanged between client and server so that the SASL code is given an opportunity to inspect the data, either to validate it hasn't been tampered or encrypt/decrypt.  This is accomplished by passing the stream data through a couple of special methods in the SASL API.

http://docs.oracle.com/javase/7/docs/api/javax/security/sasl/SaslClient.html#unwrap(byte[],%20int,%20int)

http://docs.oracle.com/javase/7/docs/api/javax/security/sasl/SaslClient.html#wrap(byte[],%20int,%20int)

http://docs.oracle.com/javase/7/docs/api/javax/security/sasl/SaslServer.html#unwrap(byte[],%20int,%20int)

http://docs.oracle.com/javase/7/docs/api/javax/security/sasl/SaslServer.html#wrap(byte[],%20int,%20int)

This means that supporting auth-int or auth-conf would require more coding work for us compared to just plain auth.  I haven't looked at this specific patch to see if it tried to do this.  The last time I considered supporting the full range of QOP settings, it looked like it was going to be a very intrusive change to the existing ZooKeeper codebase.  I was looking at the client-server connection though, not the quorum peer connections.

In Hadoop, we implement this with special subclasses of {{InputStream}} and {{OutputStream}} that do the SASL {{wrap}}/{{unwrap}} calls internally and then delegate to another underlying stream.  This has proven to be a pretty elegant design, because it encapsulates the SASL wrapping and unwrapping from the rest of the Hadoop codebase.  The rest of the code doesn't need to worry about whether auth or auth-int or auth-conf is in effect.  It just reads from/writes to streams.

Is it considered ok to enforce, a strong CA system like,  Zookeeper to connect to an external entity to get authenticated to form a quorum. Will the external entity be considered equal or more reliable than Zookeeper?. There is a difference in reading a key from local filesystem and reading key over socket from a remote machine. I see two layers of issues here,  single path networking to the authentication server and the HA capabilities of the authentication server it self.

Doesn't Kerberos have requirement of timeout for session token etc?. Is Kerberos used widely for data transfer protocol channels?. From my understanding it is pretty common to control user access to systems via Kerberos etc, I am unsure however w.r.t inter-cluster / inter-server channels. 

A quick survey of comparable/semi-comparable projects here:

- Etcd  [TLS/SSL for inter-node encryption | https://github.com/coreos/etcd/blob/master/Documentation/security.md]
- Consul  [TLS/SSL for inter-node encryption | https://www.consul.io/docs/internals/security.html]
- Cassandra [TLS/SSL for inter-node encryption | https://wiki.apache.org/cassandra/InternodeEncryption]
- Mongdb [TLS/SSL for intern-node encryption ? | http://s3.amazonaws.com/info-mongodb-com/MongoDB_Security_Architecture_WP.pdf]

Two concerns that I have are , is it architecturally ok to enforce ZK to talk to an external server(perhaps on regular intervals) to form a quorum and if that is ok then is this the most widely used/requested feature by users.

bq. Doesn't Kerberos have requirement of timeout for session token etc?. Is Kerberos used widely for data transfer protocol channels?

I can speak to how this is done in Hadoop.  The Hadoop daemons do authenticate via Kerberos, using a keytab file.  The login is done once during initial startup of the daemon.  After that, the daemon can authenticate to other remote daemons using the Kerberos ticket in SASL authentication.

There are some edge cases that need to be handled.  Kerberos tickets have a maximum lifetime, after which it is no longer possible to renew.  To handle this, Hadoop's RPC layer is capable of detecting an authentication failure during a connection attempt, and it will handle it by doing an automatic relogin of the same principal from the same keytab that was used during process startup.

Another issue is that Kerberos infrastructure typically attempts to detect replay attacks by checking for multiple login attempts for the same principal within a short window.  To handle that, we apply some backoff logic before trying again.

It's tricky stuff, but it's solvable, and it has worked well for Hadoop.

bq. Two concerns that I have are , is it architecturally ok to enforce ZK to talk to an external server(perhaps on regular intervals) to form a quorum and if that is ok then is this the most widely used/requested feature by users.

You're right that overall availability then becomes tied to availability of the KDC.  I don't have any perspective to offer on which approach is more widely requested by ZooKeeper users.  I haven't personally received any requests for quorum peer authentication myself.

I think simple TLS/SSL is more important to have, and I expect significantly easier to implement and test, and to use operationally.  A kerberos might be a nice to have feature (but should not be prioritized ahead of basic SSL, IMHO). 

{quote}Two concerns that I have are , is it architecturally ok to enforce ZK to talk to an external server(perhaps on regular intervals) to form a quorum and if that is ok then is this the most widely used/requested feature by users.{quote}

The patch already supports DIGEST-MD5, so Krb isn't required to have auth.

Encryption of the links is a separate concern, and shouldn't be done as part of this change. There's already a JIRA for it (ZOOKEEPER-1000).

Thanks a lot [~cnauroth] for the thoughts. Its really useful sharing. Currently in my patch I haven't considered QOP in quorum peer connections. How about pushing the basic SASL solution first and later will enhance the feature by including the QOP support based on the community interests.

[~rakeshr] would you be able to post a review board for this? It's a pretty big patch to comment on in a jira.

Thanks a lot [~phunt] for your time. There is a pull request https://github.com/apache/zookeeper/pull/49 available, does this works for you?

Yes, the patch is quite big. Should I split up the bigger patch into multiple smaller sub jira tasks for making reviews and commits easy?

I had looked on RB. We typically use that for reviews because it's apache infrastructure - things like review comments might be lost on non-apache hosted infra. Note that RB comments are also appended to the JIRA. gh.com is really intended to be mirror for the canonical apache git.

Can you post it to RB? I wouldn't recommend splitting it up. Let me look through it in more detail on RB and see where we're at.

Thanks!

I had looked on RB. We typically use that for reviews because it's apache infrastructure - things like review comments might be lost on non-apache hosted infra. Note that RB comments are also appended to the JIRA. gh.com is really intended to be mirror for the canonical apache git.

Can you post it to RB? I wouldn't recommend splitting it up. Let me look through it in more detail on RB and see where we're at.

Thanks!

OK, I will create RB soon.

Please find the review board ticket URL: [Review Board Ticket Number - 47354|https://reviews.apache.org/r/47354]. Also, am attaching the same patch in this jira.

[~rakeshr] seems you didn't attach the update to this JIRA? (I'll get it from RB for now)

Hi [~rakeshr], two suggestions:
1. It looks like the https://reviews.apache.org/r/47354 is based on {noformat} https://github.com/apache/zookeeper/pull/49/files {noformat}, which is different comparing to the patch attached to the JIRA. Do you mind to update the patch attached as well, if the current patch attached is out dated?

2. The patch is targeting branch 3.4, so it might be better to rename the patch attached to the JIRA with branch name to reflect this.


[~rakeshr] In RB the entire ZOOKEEPER-1506 which is available in git branch-3.4 is removed. I think it is because your working branch on git 
{noformat}
https://github.com/rakeshadr/zookeeper-1/tree/ZOOKEEPER-1045
{noformat} which {noformat}
https://github.com/apache/zookeeper/pull/49/
{noformat} based on is not up to date with branch-3.4 (your branch does not contain ZOOKEEPER-1506 for example.).
You may want to rebase the working branch with upstream branch-3.4 first.

Thanks [~hanm] for the feedback, I will rebase the patch in latest branch-3-4 code and upload by today(IST).

I'm attaching the patch rebased in latest branch-3-4 (the same is uploaded to RB as well). Kindly review it again and let me know your feedback. Thanks!

Thanks [~ikelly] for helping in implementation and reviews. Also, thanks [~rgs] for the offline reviews.

Attached another patch with the following diff:
# build related changes.
# removed unwanted BaseSysTest modifications.

Thanks a lot [~phunt], [~hanm] for the reviews. I've updated [PR-49|https://github.com/apache/zookeeper/pull/49] and [RB-47354|https://reviews.apache.org/r/47354/] uploaded the latest changes. Kindly review it again when you get a chance.


I'm planning to review this, just been swamped at the office. I have some time scheduled tomorrow to look...

I can see a lot of refactoring is done for SaslServer and SaslClient code (moving to SecurityUtils class - not sure about rational though). We have an outstanding JIRA: https://issues.apache.org/jira/browse/ZOOKEEPER-2159 that is long overdue for review to address pluggable authentication and it would be great to take that one in consideration as well.

Applied the patch locally on branch 3.4 and these tests failed consistently for me:

QuorumCnxManagerTest
Zab1_0Test
MiniKdcTest
QuorumKerberosAuthTest



[~rakesh_r] Would you have some cycles to have a look at ZOOKEEPER-2159 and see how those changes impact this jira and vice versa?

Sure, I will take a look at it and get back to you.

Attached test case failure logs to this jira on behalf of Michael.

Thank you [~yufeldman] for the comments. 

I have created {{SecurityUtils}} to avoid code duplication. Like you know, the idea of my jira is to provide authentication among Zookeeper servers using SASL(Kerberos/Digest) mechanism. IIUC the idea of ZOOKEEPER-2159 is to "extend the existing client-server authentication infrastructure by supporting more auths". I couldn't see any dependency between these jiras apart from touching common code base. I feel both the jira tasks can go parallely, because both are proposing two different ideas. Any of the jira can be committed first based on the community votes +1s. That way we get chance to accelerate multiple imprv/feature developments and add more imprvs/features to ZooKeeper project simulataneously. The jira which is going second needs to refactor their code on top of the first because both are touching common code {{SaslServer}} and {{SaslClient}} classes. It doesn't mean that they both are dependent, right?

[~yufeldman], appreciate your effort. I will try to find some time in the coming week and help in pushing your idea in.

[[~rakeshr] I would be interested to extend pluggable authentication not just to client/server but also server/server.
I can certainly do it as a follow up to your JIRA, but would be greta at least to coordinate.

OK, that sounds good. I'm happy to coordinate with you once this feature is in.

OK, that sounds good. I'm happy to coordinate with you once this feature is in.

These are failing for me (note this is on my mac):

{noformat}
    [junit] Running org.apache.zookeeper.server.quorum.QuorumCnxManagerTest
    [junit] Tests run: 18, Failures: 1, Errors: 4, Skipped: 0, Time elapsed: 63.558 sec
    [junit] Running org.apache.zookeeper.server.quorum.Zab1_0Test
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] Running org.apache.zookeeper.server.quorum.auth.MiniKdcTest
    [junit] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 0.344 sec
    [junit] Running org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.143 sec
{noformat}


attached failing tests as 1045_failing_phunt.tar.gz

Thanks [~phunt], [~hanm] for the help in testing and reporting issues.

Sorry for the delay. I suspect few cases from the logs:

*Case-1)* {code}
Multiple copies of resource named 'schema/ou=schema/cn=apachedns/ou=objectclasses/m-oid=1.3.6.1.4.1.18060.0.4.2.3.9.ldif' located on classpath at urls
{code}
Could you please check the multiple copies of m-oid=1.3.6.1.4.1.18060.0.4.2.3.9.ldif in the test.dir?
{{(build_dir_path)/partitions/schema/ou=schema/cn=apachedns/ou=objectclasses/m-oid=1.3.6.1.4.1.18060.0.4.2.3.9.ldif}}. Could you tell me your maven version, am suspecting the dependency with maven version. I could see a similar case DIRSERVER-2066.

*Case-2)* I'm wondering how the socket creation is working in Zab1_0Test.java test class. I have used the same approach used in this test for creating the socket {{#getSocketPair()}}
{code} 
java.net.SocketException: Invalid argument
	at java.net.PlainSocketImpl.socketConnect(Native Method)
{code}

{code}
Zab1_0Test.java
    static Socket[] getSocketPair() throws IOException {
        ServerSocket ss = new ServerSocket();
        ss.bind(null);
        InetSocketAddress endPoint = (InetSocketAddress) ss.getLocalSocketAddress();
        Socket s = new Socket(endPoint.getAddress(), endPoint.getPort());
        return new Socket[] { s, ss.accept() };
    }

QuorumCnxManagerTest.java
    private static Socket getSocketPair() throws IOException {
        ServerSocket ss = new ServerSocket();
        ss.bind(null);
        InetSocketAddress endPoint = (InetSocketAddress) ss
                .getLocalSocketAddress();
        Socket s = new Socket(endPoint.getAddress(), endPoint.getPort() + 1);
        s.setSoTimeout(5000);
        return s;
    }
{code}
Could you please set {{-Djava.net.preferIPv4Stack=true}} and try once.

*Case-3)* Zab1_0Test test time out:

To support upgrade flow, NullQuorumAuthServer uses {{QuorumAuth.nextPacketIsAuth(din);}} logic to know whether received packet is auth or not. If yes then reply back QuorumAuth.Status.ERROR to the connecting quorum client.

For example, assume we have three servers peer0, peer1, peer2. Auth is disabled initially. During upgrade, admin will enable auth in peer0. Now, this server starts sending {{Authpacket}} to peer1 and peer2. With the above mentioned packet identification logic, peer1 detects an {{AuthPacket}} and reply ERROR to the peer0 client as these servers doesn't support auth. Please look at the unit test case {{QuorumAuthUpgradeTest#testRollingUpgrade()}} to understand more about the described use case.

But in Zab1_0Test tests, quorum peers doesn't have auth enabled. In that case, reading packet using {{QuorumAuth.nextPacketIsAuth(din);}} logic will wait and timed out by throwing IOException and continue to next step in FLE. On the other side Leader is getting timed out by not seeing any connecting followers and failing the test case.

{code}
java.lang.InterruptedException: Timeout while waiting for epoch from quorum
	at org.apache.zookeeper.server.quorum.Leader.getEpochToPropose(Leader.java:888)
	at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:387)
	at org.apache.zookeeper.server.quorum.Zab1_0Test$LeadThread.run(Zab1_0Test.java:96)
{code}


[~phunt], do you have suggestion to handle this case?

Hi [~rakeshr],
I am looking into these failed tests as well, I may have something later. Here are just quick replies for your question:

Case-1) - My maven version is 3.3.9. I was running tests on my mac. 
Case-2) - Tried to set {noformat}-Djava.net.preferIPv4Stack=true{noformat}, still got timeout.

[~rakeshr], Regarding Case-1 {noformat} Multiple copies of resource named 'schema/ou=schema/cn=apachedns/ou=objectclasses/m-oid=1.3.6.1.4.1.18060.0.4.2.3.9.ldif' located on classpath at urls {noformat}, I suspect that apacheds-all in ivy.xml might be the culprit, as that is the one that causes conflicts with others.  A similar problem gets fixed in https://issues.apache.org/jira/browse/HADOOP-10100.  Also tried maven 3.2.5, no luck. 


[~hanm], For Case-2) - Could you please run {{Zab1_0Test.java}} tests in your env without applying my patch and please let me know the status. QuorumCnxManagerTest.java test cases are using similar logic of creating sockets from this test {{Zab1_0Test#getSocketPair()}}.

Hi [~rakeshr], Zab1_0Test passed locally on vanilla branch-3.4:
{noformat}
junit.run:
    [junit] Running org.apache.zookeeper.server.quorum.Zab1_0Test
    [junit] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.75 sec
{noformat}

Oh, I'm wondering whats causing {{java.net.SocketException: Invalid argument}} in QuorumCnxManagerTest tests:(. Only diff I could see with Zab1_0Test is, I've incremented port number like {{endPoint.getPort() + 1}}.

{code}
Invalid argument
java.net.SocketException: Invalid argument
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:198)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at java.net.Socket.<init>(Socket.java:425)
	at java.net.Socket.<init>(Socket.java:241)
	at org.apache.zookeeper.server.quorum.QuorumCnxManagerTest.getSocketPair(QuorumCnxManagerTest.java:642)
	at org.apache.zookeeper.server.quorum.QuorumCnxManagerTest.testLearnerHandlerAuthFailed(QuorumCnxManagerTest.java:282)
{code}

Looks like the Zab1_0Test regressed, because it passed before applying the patch. The change to Zab1_0Test is trivial, so I think something might go wrong in the server logic. I am curious, is Zab1_0Test test passing for you [~rakeshr]? Both Pat and I got same error message for this test. 

Sorry for the confusion. Zab1_0Test is failing for me also and I have described this problem as *Case-3)*. I'm preparing a solution for this and will share by tomorrow. 

The reason why I asked to re-test {{Zab1_0Test}} without my patch is to understand the {{QuorumCnxManagerTest.testLearnerHandlerAuthFailed}} failure case. This is the case I've described earlier as *Case-2)*. From the {{TEST-*-QuorumCnxManagerTest.txt}} logs shared by both of you, I could see that the failure is due to {{java.net.SocketException}} which I described in my above comment's exception log trace. Interesting thing is that, I have copied this socket creation logic from {{Zab1_0Test#getSocketPair()}} and this logic is happily working in Zab1_0Test but its failing in QuorumCnxManagerTest in your MAC machine. I'm really not understanding the root cause of {{java.net.SocketException: Invalid argument}}. QuorumCnxManagerTest is passing in my windows machine, will you be able to debug this flow in your mac machine and let me know the arguments value which results in SocketException. Thanks!

Thanks [~rakeshr] for more context. FYI, QuorumCnxManagerTest also fails on Ubuntu, with a similar but not completely same stack trace comparing on mac.
{code}
Testcase: testNullQuorumAuthServerWithValidQuorumAuthPacket took 0.002 sec
	Caused an ERROR
Connection refused
java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:198)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at java.net.Socket.<init>(Socket.java:425)
	at java.net.Socket.<init>(Socket.java:241)
	at org.apache.zookeeper.server.quorum.QuorumCnxManagerTest.getSocketPair(QuorumCnxManagerTest.java:642)
	at org.apache.zookeeper.server.quorum.QuorumCnxManagerTest.testNullQuorumAuthServerWithValidQuorumAuthPacket(QuorumCnxManagerTest.java:494)

{code}

I'll debug in this test case on mac and let you know when I get something. Currently, I am trying to figure out the root cause of the Case-1, with my theory that it could be resovled by replacing the apacheds-all with individual dependencies that javacc reports missing after I removed apacheds-all. Just to confirm that you don't see Case-1 errors on your local development environment (windows), right?

Thanks [~hanm] for your time and analysis.

bq. FYI, QuorumCnxManagerTest also fails on Ubuntu, with a similar but not completely same stack trace comparing on mac.
QuorumCnxManagerTest is passing the port value as {{endPoint.getPort() + 1}}, just increment by 1. Thats the only one difference between {{Zab1_0Test#getSocketPair}} and {{QuorumCnxManagerTest#getSocketPair}} usage, right?

bq. Just to confirm that you don't see Case-1 errors on your local development environment (windows), right?
Yes, its passing in my local Windows machine.

[~rakeshr] Some good news: MiniKdcTest and QuorumKerberosAuthTest is now fixed and verified locally on my mac and Ubuntu. You can check out the fix here at https://goo.gl/Y6DbXW. What I did is to copy the latest MiniKDC from Hadoop Common [1], and updated ivy.xml by removing most of the dependencies we introduced in the patch. Here is the patch https://goo.gl/Lh4r7d that you could apply on top of your patch. I could also generate a new patch that combined your patch and my patch and attach to the JIRA, if you like, just let me know.
Please give it a try and see if it works on windows :)

[1] https://github.com/apache/hadoop/blob/916140604ffef59466ba30832478311d3e6249bd/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java

[~rakeshr] Regarding {noformat} java.net.SocketException: Invalid argument {noformat}: If we just use the port number without increment it by one, then most failing test cases pass. I am wondering if it's legitimate to increment the port number by one here and then fed it back to construct a Socket. The exception is thrown from native code, so I have not identified the exact place where the exception is created. I hope I can get a better idea when I capture the context and exact location that threw the exception, as that could provide some insights on why the exception is generated. 

Thanks [~hanm] for the great support in analyzing and fixing the failures. I'm attaching new patch fixing all the three cases. Its passing in my local Windows machine, can you please try it in your mac machine.

*Case-1)* Merged latest Apache Kerby changes from HDFS. I've included the dependencies explicitly to run the test cases using IDE eclipse. {{ant eclipse}} will add {{.classpath}} entries using these dependencies list.
*Case-2)* This is due to invalid arg socket exception and removed unnecessary increment of port number
*Case-3)* Modified the rolling upgrade logic by introducing one more config. Please refer QuorumAuthUpgradeTest#testRollingUpgrade() unit test case to understand the new sequence of steps.


[~rakeshr] Thanks for quick update! Verified on mac / Ubuntu, latest patch fixes all previous major failures we've seen. I still get one test case of QuorumCnxManagerTest failing consistently:
{noformat}
Testcase: testAuthClientBadCredToNoAuthServerWithLowerSid took 20.073 sec
        FAILED
Not connected to peer
junit.framework.AssertionFailedError: Not connected to peer 
  at org.apache.zookeeper.server.quorum.QuorumCnxManagerTest.assertEventuallyConnected(QuorumCnxManagerTest.java:655)
  at org.apache.zookeeper.server.quorum.QuorumCnxManagerTest.testAuthClientBadCredToNoAuthServerWithLowerSid(QuorumCnxManagerTest.java:275)
{noformat}

OK, its passing for me consistently. Could you please send/attach me the logs corresponding to this failure.

[~rakeshr] The full log appertain to the failure (testAuthClientBadCredToNoAuthServerWithLowerSid) was posted in my previous comment. The rest of test cases in QuorumCnxManagerTest all passed.

[~hanm], I could see a chance of the following sock connection refused exception while connecting to {{election port}}. I think this is because the {{Listener}} is binding the election server port in a separate thread. I've tried an attempt by adding grace period in the tests before connecting to this port. Could you please verify it again using the latest patch.

{code}
2016-05-29 10:43:57,029 [myid:] - WARN  [Thread-1:QuorumCnxManager@502] - Cannot open channel to 1 at election address /0.0.0.0:11235
java.net.ConnectException: Connection refused
{code}

[~rakeshr] All tests passed with latest patch :)

Wow! Thanks [~hanm] for the great help in correcting the cases. Now, I'll start preparing new patch fixing the review board comments.

Attached new patch fixing Michael's comments.

[~rakeshr] Thanks for updating the patch. Latest patch (r6) LGTM and +1.

I noticed that these tests are failing: InvalidSnapshotTest, SerializationPerfTest, DeserializationPerfTest. Given that these tests are also failing for me locally on branch-3.4 before patch 1045 is applied, I think these failures are separate issues. [~rakeshr] Could you please confirm if these tests are failing for you as well, before and after apply 1045 to branch 3.4?

Also FYI I am going to do some high level testing this week as end user, and will report back if I find any issues. 



bq. Could you please confirm if these tests are failing for you as well, before and after apply 1045 to branch 3.4?
These tests are passing for me. Could you please share logs for more analysis. Since its failing without this patch I hope the failures are not related.
bq. Also FYI I am going to do some high level testing this week as end user, and will report back if I find any issues.
Thank you.

[~rakeshr] All things are good now. I can't reproduce what I reported - all tests passed for me. Might because I messed up my working branch somehow. 

[~hanm] Good to hear that all test cases passed in your machine:)

+1, great work [~rakeshr]!

Thank you [~rgs] for the reviews and feedback.

Hi [~rakeshr]. Is the RB updated? I'll probably get a chance to take another review pass soon and it would help if RB is up to date. Thanks!

Thank you. Yes, both jira and RB has updated patches.

Thanks Rakesh, can you also describe whether you've done any cluster level testing, and if so to what extent? I'm particularly interested in b/w compat (with 3.4.x releases) and rolling upgrade testing.

Few months back(Feb month) I have done one round of basic tests using old patch. Installed 3 node cluster (all participants) uses 3.4.7 release. Say, ZK1, ZK2, ZK3. Upgraded to 3.4.9-SNAPSHOT release starting from ZK1, ZK2, ZK3. During each step of upgrade, verified the service availability using zkclients.

I haven't done rolling upgrade testing using the latest patch and configurations. I will setup a kerb env and test it again. BTW [~hanm] has told about testing, did you get a chance to experience it. Would be great to see your feedback. Thanks!

To understand about the upgrade steps, please refer the steps mentioned in {{QuorumAuthUpgradeTest.java}} unit test. While updating the cwiki page will use this content and add rolling upgrade section in that.

The unit tests are passing for me now. Kudos. I'll be looking at the code in more depth and running some integration scenarios next.

I noticed a small nit related to the build. After running the tests a new directory "${test.data.kerberos.dir}" was created. It's probably an issue with the build.xml?

{noformat}
phunt@phunt-MBP13:~/dev/zookeeper-br34[branch-3.4]$ ls
$\{test.data.kerberos.di\}/	README.txt			build.xml			ivysettings.xml
CHANGES.txt			README_packaging.txt		conf/				src/
LICENSE.txt			bin/				docs/				test.log
NOTICE.txt			build/				ivy.xml
{noformat}


[~rakeshr] High level testing on my side is working in progress, will definitely provide my feedback once I finished it (ETA end of this week.).

Another small problem: 3.4.x support Java 1.6 and later. However the code fails to compile with 1.6 - 5 warnings and 1 error (this is the core code, I didn't get to the tests):

{noformat}
    [javac] Compiling 165 source files to /Users/phunt/dev/zookeeper-br34/build/classes
    [javac] /Users/phunt/dev/zookeeper-br34/src/java/main/org/apache/zookeeper/Shell.java:276: warning: [serial] serializable class org.apache.zookeeper.Shell.ExitCodeException has no definition of serialVersionUID
    [javac]   public static class ExitCodeException extends IOException {
    [javac]                 ^
    [javac] /Users/phunt/dev/zookeeper-br34/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java:782: warning: [deprecation] org.apache.zookeeper.server.quorum.LeaderElection in org.apache.zookeeper.server.quorum has been deprecated
    [javac]             le = new LeaderElection(this);
    [javac]                      ^
    [javac] /Users/phunt/dev/zookeeper-br34/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java:785: warning: [deprecation] org.apache.zookeeper.server.quorum.AuthFastLeaderElection in org.apache.zookeeper.server.quorum has been deprecated
    [javac]             le = new AuthFastLeaderElection(this);
    [javac]                      ^
    [javac] /Users/phunt/dev/zookeeper-br34/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java:788: warning: [deprecation] org.apache.zookeeper.server.quorum.AuthFastLeaderElection in org.apache.zookeeper.server.quorum has been deprecated
    [javac]             le = new AuthFastLeaderElection(this, true);
    [javac]                      ^
    [javac] /Users/phunt/dev/zookeeper-br34/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java:809: warning: [deprecation] org.apache.zookeeper.server.quorum.LeaderElection in org.apache.zookeeper.server.quorum has been deprecated
    [javac]             electionAlg = new LeaderElection(this);
    [javac]                               ^
    [javac] /Users/phunt/dev/zookeeper-br34/src/java/main/org/apache/zookeeper/server/quorum/LearnerHandler.java:174: closeStream(java.io.Closeable) in org.apache.zookeeper.common.IOUtils cannot be applied to (java.net.Socket)
    [javac]            IOUtils.closeStream(sock);
    [javac]                   ^
    [javac] 1 error
    [javac] 5 warnings
{noformat}


Question: do we need to distinguish between QuorumServer/QuorumClient and "Server" - the existing Server that we already need in the jaas.conf in order to support Client->Server communication (ZooKeeper object clients I'm talking about here, the traditional client, vs the "server client).

What's the benefit of breaking out (adding) QuroumServer and QuorumClient vs just using the existing "Server"?

Yes, I also observed this and will correct it in the next patch.

Thanks for identifying this case. I will replace {{IOUtils.closeStream(sock);}} this with {{sock.close()}} call.

Thanks [~phunt] for the reviews.

In future if someone wants to build data encryption over SASL only for the client-server communications and not between quorum-quorum then they can do it independently. Also, I have tried keeping the client-server and quorum-quorum configurations separately for better code maintenance. How about renaming client to learner and keeping the sections like,

QuorumServer and QuorumLearner ?

That seems to make sense to me on it's face. I assumed it was something like that but wasn't sure.

Would we recommend that folks use the same credentials for Server/QuorumServer/QuorumClient? Or should they use different for each?

bq. Would we recommend that folks use the same credentials for Server/QuorumServer/QuorumClient? Or should they use different for each?

All three can use same credentials. I couldn't see any problem in sharing the credentials. But IMHO we can recommend to use different credentials for client-server(Server) and server-server communications(QuorumServer/QuorumClient) in order to clearly separate the communication paths. Whats your opinion?

For the previous comment, to avoid confusions about the "server client, should I modify {{QuorumClient}} to {{QuorumLearner}}?

I'll check with one of our internal security folks about setup/configuration recommendation wrt creds, I'm not sure I know the right answer.

Re the naming I think using learner insead of client would be more consistent and less confusing for folks - a "real" client vs a server talking to another server. [~fpj] et. al. thoughts?

Talked with Mike, one of our security people, and he mentioned that the preference would be to configure all of the server roles with the same credentials. So Server/QuorumServer/QuorumLearner would use the same creds. Sounded to me like an ease of use thing, afaict there was not specific "security reason" for going either way.

[~rakeshr] Update on testing against latest (June05) patch: I set up a 3 node cluster and verified these using manual testing and some in house automations:
# ZK works with default configuration (none SASL enabled on server or client.).
# ZK works with SASL + Digest (SASL server to server enabled, client to server not enabled.).
# ZK not working with SASL + Kerberos (I am 90% certain I had a bad Kerberos set up so this is not a real red alarm.).

Plan to verify:
# SASL + Digest (with both server to server and server to client enabled.).
# SASL + Kerberos (same as above.).
# Rolling upgrade.

Thanks [~hanm] for the effort. Its really a good news. IMHO, we could capture all these test scenarios performed with the patch and prepare a test report doc showing all these cases.

FYI, am preparing another patch fixing [~phunt]'s review comments. I think there won't be any major logic changes, I will try to attach patch soon.

OK, thanks for the updates. I will make a note of this and will include this point while doing feature doc in cwiki.

Hey [~rakeshr]:
I think it is a very good idea to formalize the tests we did and the tests we plan to cover in a doc - pretty much like a test plan / report so we can capture things and for the record. I'll submit the doc early next week. 

Thanks for updating the patch, looking forward to it. It looks like I am going to spend some time fixing my Kerberos setup while waiting for the latest patch :)



Thanks [~hanm]

Attached new patch addressing [~phunt]'s review comments. Kindly review it again. Thanks!

Following are the major changes:-
bq. '{test.data.kerberos.dir' was created. It's probably an issue with the build.xml?
Done.
bq. src/java/main/org/apache/zookeeper/server/quorum/LearnerHandler.java:174: closeStream(java.io.Closeable) in org.apache.zookeeper.common.IOUtils cannot be applied to (java.net.Socket). However the code fails to compile with 1.6 - 5 warnings and 1 error IOUtils.closeStream(sock);
Done. Fixed 1 error case induced by my patch. I haven't touched other 5 warnings which already exists in the code base.
bq. should we say "client" here or "learner"? just wondering re usability/understandability (vs a "real" zk client)
I've replaced {{quorum client}} terminology with {{quorum learner}} for better ubderstandability. I've tried replacing all the occurances including configurations and class names. I think this renaming has changed the code more compare to the other review comment fixes.

Apart from all the above, this patch fixed all the review comments given in [review ticket|https://reviews.apache.org/r/47354/]

[~rakeshr] One thing I spotted today is at end of Readme file:
bq. quorum.auth.kerberos.servicePrincipal=".. Service Principal ..Defaulting to zkquorum/localhost"

This belongs to zoo.cfg, so it would be better to move this section up and make it part of zoo.cfg configuration section. I am speaking from a user perspective because I was not aware this value is important and it should be set in zoo.cfg so I got auth errors when login KDC, and took a while to figure out. When we have the wiki / user guide page set up, we'd also do something similar by making sure the configurations belong to same sections are clustered together.

Thanks for the inputs. Sure, will take care this point.

Hey [~rakeshr], I am thinking upgrade / compatibility test scenarios and IIUC, the existing SASL based ZK client to ZK servers authentication and the new SASL based ZK server to server authentication is orthogonal in terms of JAAS configuration settings: e.g. there are existing (default) login context like 'Client' and 'Server' for ZK client to server auth, and the new (default) login context like 'QuorumLearner/Server' is for server to server auth. So, a client which already uses the SASL client-to-server auth who upgrades from old 3.4 would not have to change any existing configurations settings for client-to-server auth continues working. Later when client wants to enable server SASL based auth they just need to add Quorum* login contexts to their JAAS config file without impacting any of existing functionality, correct? 

Overall I am trying to identify if there is a case that this patch might include any backward compatibility break changes even in cases the feature is included but not enabled. From what I read the code so far we seem to be fully backward compatible in that after applying the patch what worked will continue working without any config changes (assume the SASL server-server feature is off by default), so we are good. Can you please double check and confirm this?

Yes, exactly. We need to ensure that the addition of this feature shouldn't introduce any compatibility issues whoever is upgrading from old {{3.4.x}} version to the new {{3.4.x'}}. All the existing functionality should work fine with (1) the feature is disabled, (2) during rolling upgrade steps, (3) after the successful upgrades with feature enabled.

There will be many combinations like below. All these may not be the actual deployment cases but to be on the safer side we could consider it while reviewing/testing.
||client-server||server-server||
|No Auth|No Auth(disabled)|
|Sasl Auth|No Auth(disabled)|
|No Auth|Sasl Auth|
|Sasl Auth|Sasl Auth|




I was thinking through the QuorumServer/QuorumLearner question I asked a bit more today, reviewing more of the patch from this perspective, and I realized that while we are doing authentication, we don't seem to be doing authorization. I don't see it mentioned in this JIRA, but I believe that we need to address both authn and authz with this patch. I suspect the original intent was to do both, even though the subject mentions authn only. The reason I feel so is that if we don't do authz then anyone with valid kerberos credentials, regardless the principal, can authenticate and operate as a member of the quorum. Do I have that right? From what I can tell in the code, with the patch applied, we are just authenticating, and not validating that the principal has rights to operate as a quorum member.

Here's this code in SaslServerCallbackHandler for example:

{noformat}
    private void handleAuthorizeCallback(AuthorizeCallback ac) {
        String authenticationID = ac.getAuthenticationID();
        String authorizationID = ac.getAuthorizationID();

        LOG.info("Successfully authenticated client: authenticationID=" + authenticationID
                + ";  authorizationID=" + authorizationID + ".");
        ac.setAuthorized(true);
{noformat}

Am I reading this right or are we indeed doing authz somewhere?

fwiw: notice hbase, they seem to be checking authz in their handler: https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcServer.java

bq. Am I reading this right or are we indeed doing authz somewhere?
Thanks for bringing this point. The patch doesn't contains any new logic to do the authorization. It is just relying on the existing ZooKeeper {{SaslServerCallbackHandler}}, but I understand its not sufficient and could do extra verification logic similar to hbase like you mentioned. 
{code}
if (ac != null) {
//...
       if (authid.equals(authzid)) {
          ac.setAuthorized(true);
        } else {
          ac.setAuthorized(false);
        }
        if (ac.isAuthorized()) {
             //...
            ac.setAuthorizedID(authzid);
        }
}
{code}
Its new to me and will explore more on this. I have quickly gone through the link, but I failed to find any unit test case to verify the behavior. Please point me if you come across any way to simulate through java unit test. Thanks!.

The handleAuthz method from hbase seems deceptively simple to me - there must be some setup ahead of time so that the simple string comparison can be done? Regardless we may need to have two such callbacks given the mechanism for quorum auth will be different from true client->server auth (authz I mean - acl support with kerberos for clients)

bq. Regardless we may need to have two such callbacks given the mechanism for quorum auth will be different from true client->server auth (authz I mean - acl support with kerberos for clients)
Yes, How about introducing {{SaslQuorumCallbackHandler}} for separating out the new logic from client-server authz.

bq. The handleAuthz method from hbase seems deceptively simple to me -  there must be some setup ahead of time so that the simple string comparison can be done?
Sorry, I failed to understand this part. Are you pointing out that the string comparison is not sufficient?


No, not that it's insufficient, but I was wondering what else was going on in order for a simple string comparison to be used.

For example. String comparison might be fine in that situation - iiuc they are comparing ids. If we were to use user/host@realm  principals that would be a problem. user@realm should work ok. However if the credentials provided to each of the servers is the same (user@realm) that would be a bit less secure than providing each of the servers individual credentials with user/host@realm principals. In the former case (user/host@realm) we couldn't do a simple string comparison without some prior code handling that complexity. Perhaps what hbase is doing is using user@realm, I'm not sure, I'll try to find out (lmk if you come across it).

Following are few references from hbase and hdfs to get more idea.

I could see Hbase using user/host@realm in their principals, hbase [principal format|https://github.com/apache/hbase/blob/master/hbase-common/src/main/resources/hbase-default.xml#L958]. Also, HDFS has the string comparison logic, [SaslRpcServer.java|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java#L356]. HDFS [principal format|https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml#L1582]. I'm wondering how user/host@realm is working in hbase and hdfs. I'm exploring more about this part.




This is kind of a side comment on this topic, but please make sure you support the case where all the ZK hosts run as the same Kerberos principal. You don't have to support *only* that case, of course, but it's definitely how I would be deploying ZK when using Kerb auth.

The reason for running all the service instances with the same Kerb principal is to enable clients to do Kerberos AuthN to all the ZK hosts using a single DNS name, which is pretty common, I think; we certainly do it, so that we can scale out the ensemble for more throughput as needed. Since they're pointed at a single DNS name, the clients should always construct the same service principal name, so the client will get a ticket that's only good for a single Kerberos service principal. All the services must be running as that same principal, otherwise they won't be able to crack the Kerberos ticket. Basically, since the clients can't see a difference between the servers (due to the shared DNS name), and since the clients are authenticating the servers' Kerberos identity, the servers have to be identical (according to Kerberos identity). 

Dan, it happens that this patch does support this use case where server credentials between nodes are shared. I just validated that in my Kerberos cluster where nodes share same service principal in config and also same keytabs (in fact, that is the only case so far I am able to get my cluster working with server to server auth -_-). I am not sure if the support of such use case is implicit, or by chance, or by design. I'll let others comment, but what you pointed out seems a reasonable use case.

Hey [~rakeshr], 
I've validated that server to server Kerberos SASL auth working, when servers share same credentials (same service principal name + same full qualified domain [I was using the DNS name of my KDC/Kadmin server] + same keytabs) deployed on all nodes. 

For the cases where each server has a distinct Kerberos credential, it's not working yet. The error is consistent ('GSS initiate failed' - with various categories of errors depends on my combinations of configurations.). I am not sure if it is a misconfiguration, or it is a bug. I'll try figure out. I think it would be helpful if we could provide a reference configuration for the use case where each server has different credential, because this information is currently not available in either the readme or the test code (which all use shared credentials). This info could be initially put in the cwiki and ultimately we might want to bring them back to the in xdoc.

Also during the setup of the cluster and validation process I find out some issues so I left my comments in review board. One thing worth to address is to log the exception (and maybe call stack as well) when GSS failed to initialize - I find that is very useful for me to debug my setup.

My plan next is:
# Validate rolling upgrade (with shared credentials).
# Figure out why none shared credential not working and add more tests to UT to cover this use case.
# Submit test coverage doc.

Thanks [~dbenediktson] for the interest in this work and sharing the use case.
bq. please make sure you support the case where all the ZK hosts run as the same Kerberos principal
Yes, you can configure the same Krb credentials for client-server and server-server communications. As part of this jira, there is no changes to the existing client-server communication path, this will work as it is. I will try to add few details about the server-server auth configs.

For the server-server auth, Kerb principal should be same for all the servers to allow communicating each other. Since each server will talk to all the other servers to form quorum it is required to know each others Krb principal. This jira introduces {{QuorumServer}} section where admin can configure the Krb principal of self (also it represents the principal of other quorum peer server), learner will use this and can contact them. 

In the below example config, should use same {{principal="zkquorum/localhost@EXAMPLE.COM";}} in all the servers.
{code}
QuorumServer {
       com.sun.security.auth.module.Krb5LoginModule required
       useKeyTab=true
       keyTab="/path/to/keytab"
       storeKey=true
       useTicketCache=false
       debug=false
       principal="zkquorum/localhost@EXAMPLE.COM";
};
{code}

Few days back there was a discussion about configuring [different Kerb credentials|https://issues.apache.org/jira/browse/ZOOKEEPER-1045?focusedCommentId=15339198&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15339198] for client-server(Server) and server-server(QuorumServer/QuorumLearner) communications. Please refer this to understand more.

We will be supporting Krb credentials in the following ways. I'd appreciate if you can test the same in your env and see its working.
1) all ZK hosts sharing same Kerb principal for both client-server and server-server
2) client-server(Server) uses {{principal_1}} and server-server(QuorumServer/QuorumLearner) uses {{principal_2}}.

bq.I've validated that server to server Kerberos SASL auth working, when servers share same credentials (same service principal name + same full qualified domain+ same keytabs) deployed on all nodes.
Thanks [~hanm] for the confirmation.

bq. For the cases where each server has a distinct Kerberos credential, it's not working yet. 
[~hanm], please let me the QuorumServer principal values. Could you share the {{jaas.config}} of all the servers and the failure logs for better debugging.

bq. For the server-server auth, Kerb principal should be same for all the servers to allow communicating each other. 

IIUC, this means that we only support a single (shared) Kerberos principal / credential across all servers for server to server communication, and if so, the failure of my validation against the case where servers use different Kerberos principal is a by design, because I was using different Kerberos principals on each server for server to server auth validation. [~rakeshr]

[~hanm] Yes, exactly. Presently we will be supporting only single(shared) Kerb principal across all the servers. We could capture this point clearly in our test report documentation and later the same can be used to update {{cwiki page}} as well. In future, if anyone has a use case of different Kerb principal then we can discuss/extend the implementation to support the same later. IMHO, its not required to handle those complex case now. Does that make sense to you?

Yeah just want to confirm because you were asking my server configs ... if the failure is a by design then we can move on this time :-)

Yes, its by design we supports only single(shared) Kerb principal across all the servers. Thanks for sharing your thoughts and testing efforts.

Upload a file that documents the test design, what's been validated so far and what's next. 

[~rakeshr] Just uploaded the doc that logged things related to tests and validation. I've also validated rolling upgrade today. There are a couple of remaining validations which I marked as 'not started' in the doc. I think we are in good shape minus the pending authorization feature. 

[~hanm] Awesome!, overall the test report looks good. I'm adding few minor comments, please take a look at it.

- *Review Comment-1)* Please add the git revision number of the {{branch3-4}}, you have taken for testing, that will be helpful for future references. Can add something like,
{code}
Branch-3-4 version:
Time:July 01, 2016, UTC+09:00.
Git Information:
Revision: 6b6a63bbbda920315d3d24b61ed3344a78a981b6
{code}

- *Review Comment-2)* Rolling upgrade should be supported and all existing features should continue work unconditionally, with or without this feature being enabled (full backward compatibility), before, in the middle, and after rolling upgrade.
*Comment:* Please mention, from which version of 3.4.x used for rolling upgrade testing. For example, rolling upgrade from {{3.4.6}} version to {{3.4.9-SNAPSHOT}} version.

- *Review Comment-3)* Rolling upgrade verification:
*Comment:* In this section, it would be good to add few more extra details. We can say, rolling upgrade should do in three steps and after every step admin has to {{"Ensure that all the servers has completed this step. Only after that, move on to next step"}}. I'm adding the below sample for your information, please refer this and update accordingly.
{code}
Rolling upgrade should do in three steps:

step-1) Stop the servers one by one, then set the following flags in the server 'zoo.cfg' and restart it back.
quorum.auth.enableSasl=true, quorum.auth.learnerRequireSasl=false and quorum.auth.serverRequireSasl=false. Ensure that all the servers has completed this step. Now, move on to next step.

step-2) Stop the servers one by one, then set 'quorum.auth.learnerRequireSasl=true' flag in the server 'zoo.cfg' and restart it back. Ensure that all the servers has completed this step. Now, move on to next step.

step-3) Stop the servers one by one, then set 'quorum.auth.serverRequireSasl=true' flag in the server 'zoo.cfg' and restart it back. Now, all the servers are fully upgraded and running in secured mode.

Verified everything works after restarting each server and every step.
{code}

- *Review Comment-4)* If time permits, please add two more test scenarios:
*Scenario-1)* I hope you are adding servers as LearnerType.Participant. Please add one server as LearnerType.OBSERVER with sasl. For example, you can configure in zoo.cfg as "server.1:localhost:2181:3181:observer"
*Scenario-2)* Add a fourth server to a quorum of server which is already upgraded to sasl. Probably you can perform this together with the above scenario by adding fourth server as Observer.

I think, I need to update the feature document describing the internals. I will give priority to that and update this week or next.

[~rakeshr] Thanks for putting up the feedback! I'll update the test doc early next week.

Attached new patch with the following changes:

# Fixed [~hanm]'s review comments given in the review ticket. Also, fixed one review comment given in this jira to modify {{ReadMe.md}} content.
# Since the current patch is supporting  only single (shared) Kerberos principal I've provided simple string comparison for authorization. Presently {{quorum.auth.kerberos.servicePrincipal}} holds the shared principal value. One idea to support each of the servers individual credentials is by extending the configuration with respective {{myid}} value as shown below. IMHO, we could push the basic patch first and discuss individual credentials logic and their authorization part separately as the current patch is getting bigger and bigger. [~phunt], whats your opinion?
{code}
quorum.auth.kerberos.servicePrincipal.1 = "QuorumServer1"
quorum.auth.kerberos.servicePrincipal.2 = "QuorumServer2"
quorum.auth.kerberos.servicePrincipal.3 = "QuorumServer3"
{code}
# Exposed {{fleTimeTaken}} value via jmx bean attribute, which can be used to see the total time taken for LE.
# Added few more unit test cases to cover newly introduced {{ConfigException}} cases.

Adding two more points to the above list:

# Please include the {{Java version}} used for the ZooKeeper project compilation as 3.4.x supports Java 1.6 and later.
# Few weeks back I've tried integration with Hbase project. I've mvn compiled/tested hbase "branch-1.2" (https://github.com/apache/hbase.git) project using zookeeper-3.4.9-SNAPSHOT.jar(contains latest qp sasl feature code) artifact. I got SUCCESS result. Probably you can capture this info in the validation report.
I've used the following revision for the integration testing.
{{HBase git revision da52e0cdf109199b157ccaedbd891336221c439b}}

[~rakeshr] Sure, will do. Thanks for providing additional data points from testing perspective.

New patch lgtm with regards to address comments I made since last CR. Thanks for the work Rakesh! 

bq. the current patch is supporting only single (shared) Kerberos principal

I don't think we want to make this simplification. I asked around for the other components (e.g. hdfs/hbase) and they assign a single cred to each of the services - user/host@realm. Their authz check will verify the user and realm, but of course the hosts differ across servers/processes. In asking why they do this rather than the shared approach they said it was due to ease of management - in our case the servers likely already have user/host@realm, and two if a particular host/credential is compromised it's much easier to change. I believe we should do similar, if not for security then for consistency.

bq. Exposed fleTimeTaken

I this this is a great idea, however is it possible to move to another jira? It would be easy to fasttrack that into releases. My concern is that this patch is already pretty big/complicated and I'd really like to focus on the auth pieces. Again, great idea to add this to the system though.

bq. new tests

It looks like we don't have any tests to verify the authz aspect of the change? The two new tests, their comments seem to say they same thing and afaict doesn't verify authz functionality?



bq.  I believe we should do similar, if not for security then for consistency.
Agreed. Previously in this jira, [~dbenediktson] has brought up a case where all the ZK hosts run with the same Kerberos principal. So now we have two cases, need to support both {{_HOST}} based principal name and {{shared}} principal name. I'm assuming there won't be any need of supporting mixture of both of these like, few quorum servers with "zkquorum/_HOST@" pattern in their principal name and few others are having constant name "zkquorum@". 

Following is an idea to support both the cases. Welcome comments.

*Case-1)* {{_HOST}} based principal -> For example, zkquorum/_HOST@EXAMPLE.COM
zoo.cfg has the following configuration which has the 'host' information. This host address {{addr.getCanonicalHostName()}} will be used to replace the {{_HOST}} pattern. We will make use of the existing data structure {{QuorumCnxManager#view}} map to get the respective server's host name. While connecting to the respective server, first the quorum learner will check {{quorum.auth.kerberos.servicePrincipal}} configuration has {{_HOST}} name pattern then convert the Kerberos principal name to a valid name by replacing the {{_HOST}} part. Myid will be used as the key to get the respective quorum server address from the {{#view}}.
{code}
server.0=host1:11222:11223:participant
server.1=host2:11225:11226:participant
server.2=host3:11228:11229:participant 
{code}

*Case-2)* Shared principal -> For example, zkquorum@EXAMPLE.COM
While connecting to peer servers, first the quorum learner will check {{quorum.auth.kerberos.servicePrincipal}} configuration has {{_HOST}} name pattern, if not then will directly use the value as Kerberos principal name and continue with the authentication process.

*Case-3* Mixture of {{_HOST}} based and constant principal
This won't be supported. We will supports only two valid principal names, either all servers should have "_HOST" based principal name or all servers shares same principal name.

bq. I this this is a great idea, however is it possible to move to another jira? 
OK, I will push this separately

bq. It looks like we don't have any tests to verify the authz aspect of the change? 
Yes, will include test case for this.

[~rakeshr]
There is one flaky test (testRollingUpgrade) in latest (July 12) version of patch. Attach the test log. 
Would be good to fix this test to reduce the flaky tests. The rest of tests passed my pressure tests.

Rakesh - I don't think we can rely on the "host" from the zoo.cfg file, it might be the host name, it might be an IP address, it might be FQDN, might not match whatever is in the kerberos credential.

I chatted with the HDFS and HBase folks briefly, and what they mentioned to me was that they look at the user and domain portion of the user/host@domain principal, and don't compare the host portion. This is why it's a bit more complicated than a simple string comparison as we originally had it in this patch. This would provide the authz at the user and domain level, while not constraining the host. Given we aren't using shared credentials I believe this is sufficient - the ZK servers would authenticate each of the zk servers with kerberos, then check that the user and domain is correct. ZK servers would have dedicated user name such as "zk-quorum-peer" or something like that. Another way to say it, is that we are authorizing the servers by user id, which is unique to an ensemble of quorum peers, and that is sufficient within a domain.

Note that I'm not an expert at this stuff myself. Does what I'm saying make sense? Perhaps we should get on a hangout and discuss f2f for a bit? I'm open to that if you think it would help figure out the right approach.

I had a chat with [~esteban] and he was telling me that I'm probably mistaken, that indeed there is some configurational aspect here that allows multiple types of matching. Esteban can you give some insight? Obviously I'm missing something... Thanks.

I had a look at Hadoop's SaslRpcClient.getServerPrincipal() - one could easily make the argument that we ought to stick to a similar implementation. It *does* look like it considers the host in the comparison.  (I'm assuming that this is the correct hadoop function to look at...)  It can also match a principal against a regex pattern for genericness. 

But if ZK has a history of not ensuring that hostnames in zoo.cfg don't match the kerberos credential, then we probably can't realistically use it. And if there is a separate history of it using kerberos principals without a host, then we have to consider that as well.

So if we ignore the host part of the principal, that opens us up to an attack where any zookeeper keytab can be used on any host, so if one is compromised it can be reused elsewhere. Not great. But you do at least have to steal one keytab.

How about we add an optional "require host in kerberos principal" flag, default it to false.  Then when comparing principals, we split out user / host @ domain, compare the user and domain, and then depending on the value of the flag compare the host.  This gives us comptibility and then optional security. And we can move towards flipping that flag to true later.



bq. But if ZK has a history of not ensuring that hostnames in zoo.cfg don't match the kerberos credential
Just checked, looks like the hostname (AKA 'instance' in the context of primary/instance@realm of Kerberos) does participate in the game:
https://github.com/apache/zookeeper/blob/trunk/src/java/main/org/apache/zookeeper/server/ZooKeeperSaslServer.java#L104


Also, I think all Keberos related configurations such as principals should be put in jaas.conf instead of zoo.cfg, right? 

Re Hadoop common SaslRpcClient.getServerPrincipal():
https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java#L317

It takes the hostname as an integral part of the credential when doing compare.

The whole credential is generated here I believe:
https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java#L194

So [~rakeshr] just refer to the previous "how to get the name from IP" question looks like what Hadoop common did is to get the FQDN from the IP using InetAddress.getLocalHost().getCanonicalHostName().

Thanks a lot [~phunt], [~yoderme], [~hanm] for your time and comments. Sorry for the delay in reply, I took some time to read the hadoop logic. I've referred Hadoop SecurityUtil and tried a logic to resolve the hostname, please refer attached {{HOST_RESOLVER-ZK-1045.patch}}. This can be verified using the {{TestHost.java}} class. Can we do similar host resolver in ZooKeeper ?

bq. I don't think we can rely on the "host" from the zoo.cfg file, it might be the host name, it might be an IP address, it might be FQDN, might not match whatever is in the kerberos credential.
[~phunt], I thought of avoiding extra configurations to configure the peer details for our authentication mechanism. Since we have the host details of all the peers in zoo.cfg, while connecting, a current server can make use of this zk quorum view and can get the respective ipaddress/hostname/fqdn using {{sid}}. How about adopting {{HostResolver}} logic from Hadoop for resolving this address, please refer the attached patch {{HOST_RESOLVER-ZK-1045.patch}}, which can be used to prepare ServerPrincipal {{SecurityUtil.getServerPrincipal()}}?

bq. I chatted with the HDFS and HBase folks briefly, and what they mentioned to me was that they look at the user and domain portion of the user/host@domain principal, and don't compare the host portion. This is why it's a bit more complicated than a simple string comparison as we originally had it in this patch. This would provide the authz at the user and domain level, while not constraining the host. Given we aren't using shared credentials I believe this is sufficient - the ZK servers would authenticate each of the zk servers with kerberos, then check that the user and domain is correct
Agreed. I think, we should also implement the same logic. I'm waiting to see the responses of host resolver part first and then implement the authorization logic. OK?

bq. How about we add an optional "require host in kerberos principal" flag, default it to false. Then when comparing principals, we split out user / host @ domain, compare the user and domain, and then depending on the value of the flag compare the host. This gives us comptibility and then optional security. And we can move towards flipping that flag to true later.
Yes, this is a good idea. [~phunt], shall we introduce a flag?

Coming into this late and getting up to speed - I hope to have a look at the patch in the next few days. I've successfully patched against branch-3.4 with both patches as follows:

{code}
git checkout remotes/upstream/branch-3.4
git log -1
commit 82ea70cc128336411ff83f1fd177f9a62aa1e14e
Author: Flavio Paiva Junqueira <fpj@apache.org>
Date:   Wed Aug 10 14:11:48 2016 +0000

    Fix command handling in the C client shell (phunt via fpj)
{code}

Then I applied the two most recent patches:
{code}
cat 0001-ZOOKEEPER-1045-br-3-4.patch | patch -p1
cat HOST_RESOLVER-ZK-1045.patch | patch -p1
{code}

both of which applied without errors or warnings.

I'm running {{ant test}} now. Do those steps get me up to date with the latest development on this issue?

Thanks, Eugene

Thank you [~ekoontz] for your interest in this feature. Please feel free to add your comments/questions.

It seems, the patch you are are referring is quite old one. Could you please use the latest patch [ZOOKEEPER-1045-br-3-4.patch|https://issues.apache.org/jira/secure/attachment/12817493/ZOOKEEPER-1045-br-3-4.patch]. 

{{HOST_RESOLVER-ZK-1045.patch}} is an independent patch, idea is to prepare {{QuorumServer}} kerberos principal by resolving the host address of ZK server to InetAddress.getLocalHost().getCanonicalHostName() and expects principal like {{zkquorum/host1@EXAMPLE.COM}}. This principal will be used by the quorum peer client to talk to quorum peer server during FLE. As you know, one can configure ZK server details as host name or ipaddress or fqdn. I'm planning to integrate this utility function into the main patch once I get a +1 vote for this approach.

Thank you [~rakeshr] for the correction - I'm now using the latest, correct patch. 

I've been trying to dig into this stuff more, but it's pretty complex and I haven't had much time. I did check with one of the Hadoop folks recently that deals with security, and he mentioned that they don't use Kerberos for authz. So I'm not sure if should apply the same logic here. I'm trying to get ahold of some hbase folks that would know more, because I believe they do use kerberos for authz, but I've been too swamped I'm afraid. Am trying.

I'm not sure I know the answers to these. I'm going to try and touch base with some hbase folks before the end of this week, sorry for the delay. I believe the hbase code already has support for this. If I can't make progress we should just do our best. Again, sorry for delaying all this.

I did additional research on this. In particular I met with Esteban who's very familiar with the Hadoop and HBase code. We spent a considerable amount of time discussing and looking through the hbase/hadoop code. We couldn't find any direct parallels between what they are doing and what we are considering however. That said we came up with the following proposal. The idea being to simplify the implementation, allow expansion/extension in future, and in particular ensure that what we have initially is secure. Here's my proposal:

1) first, authenticate the remote entity (learner) and ensure they have valid kerberos credentials - we already have that in the patch.

2) if the learner has a principal of the form user/host@realm we compare just the user and realm, and not the host. If a credential with user@realm is provided we compare the user and realm similarly.

In neither case will we compare the host. This means that a particular user in realm can operate from any host as a quorum peer (if the user and realm match). It simplifies things greatly as we don't have to configure each/every ZK server in the ensemble with the principal of every other ZK server participating in the ensemble. If there are multiple ZK services (ensembles) in a realm then the user will need to ensure that differing user names are provided in the principal, otherwise servers from "other" ZK clusters could join services they are not meant to join. We should ensure that the documentation calls this out.

IIUC our current patch correctly this just means that we need to parse/compare the user and realm when authorizing. In future if there is the need to enhance this functionality in some way we can add additional configuration options for that.

Thoughts?


I'm a little bit concerned about rogue zookeepers... but this is way better than nothing, and good enough for a first cut. +1.


[~yoderme] can you expand on "rogue" comment? The ZK server process will have to have access to a valid "zookeeper ensemble X participant in the specified realm" credential regardless. (where X can be a specific ensemble)

Thanks for the useful discussion and proposal. Comparing only the user and realm parts sounds good to me. I'm preparing another patch incorporating the same. I hope I will be able to upload new patch by tomorrow(IST).

Sorry to be vague, I just meant a principal zookeeper/<bad host>@realm where you didn't intend that principal from the <bad host> to be able to join.  There is a small hole where if an attacker can somehow get credentials with a principal for the desired user but for a different or unintended host, then they can talk to the zk servers. But you know this already, and like I said this is not a bad tradeoff especially for a first cut.

What [~yoderme] describes would also be a problem in our environment. Let's say I decide to use the "user" zk1 for my cluster and thus set up Kerberos credentials for zk1/host1.example.com and zk1/host2.example.com. I have no way to prevent another user from getting Kerberos credentials for zk1/badhost.example.com, and I don't want them to be able to join my cluster. I would prefer a way to specify the full credential names for participants that I want to allow in my cluster.


That's reasonable. How do other systems typically ensure that the code doing the authz has knows the <good host> then? They specify the principals on every process doing authz? In our case every zk server would need to know the principals of all ensemble members? We have the list of servers already in the zoo.cfg for example, however the server address can be anything - e.g. ip address. Would it make sense to require that the server addresses in zoo.cfg match the host used in the principal?

bq. I have no way to prevent another user from getting Kerberos credentials for zk1/badhost.example.com

Hi Jason. Wouldn't you just not provide credentials to anyone (other than yourself) for the user zk1 within your realm?

Typically with Kerberos principals the bit in front of the slash identifies the type of service rather than a specific user. E.g. HTTP/host.example.com is a typical principal that would be used for a web server. So the fact that I can get credentials for HTTP/host1.example.com does not typically imply that I have any control over who gets credentials for HTTP/host2.example.com. Where I work we have many thousands of hosts controlled by many different teams, and self-service tooling for getting Kerberos credentials. If I own host1.example.com I can control who can get HTTP/host1.example.com or zookeeper/host1.example.com or whatnot. But if somebody else owns host2.example.com they can get principals for those same service types on their host. At least in our environment, even if I make up a unique service type like "zk1" there is no way for me to limit the ability of others to get zk1/<hostname> credentials on their hosts.

I think it is reasonable to require that people use hostnames rather than IP addresses in zoo.cfg when using Kerberos and to use that as an authorization list. Kerberos clients typically compute the credential name they expect the server to use based on the hostname that the client is connecting to, so folks in a Kerberos environment will typically have a functioning name service like DNS.

bq. if somebody else owns host2.example.com they can get principals for those same service types on their host. At least in our environment

I see. That's very useful information that I wasn't aware of. Thanks Jason.

bq. Kerberos clients typically compute the credential name they expect the server to use based on the hostname that the client is connecting to, so folks in a Kerberos environment will typically have a functioning name service like DNS.

so in this case we'd expect the host names to all match? zoo.cfg/kerberos/dns. I think that's reasonable.

[~rakeshr] - is that what you found when looking at the Hadoop code? Your comment earlier about "InetAddress.getLocalHost().getCanonicalHostName()" being used.

bq. We have the list of servers already in the zoo.cfg for example, however the server address can be anything - e.g. ip address. Would it make sense to require that the server addresses in zoo.cfg match the host used in the principal?

I like the idea of using zoo.cfg as the source of authorization information. Kerberos is there for solving authentication problem, to do authorization we need a system that stores the set of information such as authorized user info, and I think that system is separate with Kerberos. In our case, using zoo.cfg seems the best solution given it's already there and contain everything we need. Other approach I can think of is using a dedicated znode that stores such information (user+host combinations) which is bootstrapped configured at cluster setup time, but that is way more complicated.

One approach we could do is to compute the FQDN for the _host part in user/_host@REALM, regardless of what's put in zoo.cfg (it could be a DNS name, or IP address). The implicit requirement here is the cluster deployment environment has to have a naming service, which is required anyway to run Kerberos. So we don't need to explicitly configure the _host part in any of the zookeeper configuration files, we infer them. I believe Hadoop did something similar by specifying principal names using _HOST string as a placeholder for the hostname in the configuration file which is replaced at runtime appropriately wherever needed. This approach does not require any additional set up or configurations on users side, which I think might be good from usability point of view.

To sum up, I think:
* ZK quorum authenticates a ZK process asking for joining quorum through Kerberos.
* ZK quorum authorizes the process by comparing the full principal name that composed of user/host@realm. The full string must match.

bq. Your comment earlier about "InetAddress.getLocalHost().getCanonicalHostName()" being used.
That's how Hadoop RPC authenticates - though the motivation seems a little different than what we are doing. What Hadoop RPC did is to make sure a client that making RPC is what it advertised who it is by comparing the configured principal names with the principal names computed at runtime from the client making PRC - so IIUC this is still an authentication problem, not authorization problem. That said I think we can still reuse this same code to compute the host name. 

bq. This is kind of a side comment on this topic, but please make sure you support the case where all the ZK hosts run as the same Kerberos principal. You don't have to support only that case, of course, but it's definitely how I would be deploying ZK when using Kerb auth.

Recently I find out a feature of KDC that it will treat repeated attempts to log in with the same Kerberos principal within a short period of time as replay attacks and will reject such login requests. Since we plan to support shared Kerberos credential, we might hit this issue. Not sure how likely we will get shot but it would be good to have some retry with backup code in login if we don't have now in case this happen.

One more comment regarding authorization part - it seems to me that all the information we authorize against is pretty static but with dynamic reconfiguration that is not the case. How would we authorize against something that is not pre-configured? This is not an immediate issue, though I want to raise awareness of this so we can have a solution that is extensible that makes it easier to port this to 3.6.

I haven't followed this closely, so I apologize if this was already discussed. I'd like to add to what Michael is saying - it would actually be better if we
do support dynamic server addition and removal in this feature, otherwise we're introducing two new features in 3.5 and asking the users to choose 
between them. It would be better if new features are compatible with existing ones unless there's a very good reason.

re reconfig that's a great catch. But what are you saying, further weight to not use zoo.cfg? Or is there a specific issue you think needs to be addressed directly.

I personally don't know if there's any issue, I mainly would like to see a description of how new servers can be added/removed in terms of authentication. 

bq. Recently I find out a feature of KDC that it will treat repeated attempts to log in with the same Kerberos principal within a short period of time as replay attacks and will reject such login requests.

Yes, excellent point.  FWIW, Hadoop's RPC framework handles this case with a brief backoff and retry to work around the case of getting flagged as a replay attack.

Thanks everyone for putting all the use cases and active discussions. Let me try to summarize all the problems and proposed solutions.

*Point-1)* ??Prevent another host with the same krb user and realm to join my zookeeper cluster, for example {{zk/<badhost>@EXAMPLE.COM}}??

Like [~phunt], [~hanm] explained we could make use of zoo.cfg as the source of authorization information and replace the _HOST part in {{user/_HOST@REALM}}. Since admins can configure ZK server details as host name or ipaddress or fqdn in zoo.cfg, server should have a mechanism to resolve this to fully qualified domain name for this IP address. Sometime back I've attached {{HOST_RESOLVER-ZK-1045.patch}} idea (thanks to hadoop project [hadoop SecurityUtil ref.|https://github.com/apache/hadoop/blob/branch-2.8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java#L570]), which is an independent patch to prepare QuorumServer kerberos principal by resolving the host address of ZK server to InetAddress.getLocalHost().getCanonicalHostName() and expects principal like zk/host1@EXAMPLE.COM. This principal will be used by the quorum peer learner to talk to another quorum peer server during FLE. Here the implicit requirement is, admin has to ensure that the configured kerberos principal name should be resolved to fully qualified domain name for this IP address.

* For authorization every server will compare the full principal name that composed of {{user/host@realm}}. For doing this every server will cross check with the list of known quorum server principals built from zoo.cfg file.
+Ensemble-1 :-+
quorum server principal list => {{zk/host1@EXAMPLE.COM, zk/host2@EXAMPLE.COM, zk/host3@EXAMPLE.COM}}
+Ensemble-2 :-+
quorum server principal list => {{zk/host3@EXAMPLE.COM, zk/host4@EXAMPLE.COM, zk/host5@EXAMPLE.COM}}

* For authentication, quorum learner server will get the remote quorum server principal name and then do authentication. For example, host1 will get host2 principal {{zk/host2@EXAMPLE.COM}} and do authenticate.

Does this make sense?

*Point-2)* ??Feature of KDC that it will treat repeated attempts to log in with the same Kerberos principal within a short period of time as replay attacks and will reject such login requests. Since we are supporting shared Kerberos credential, we might hit this issue.??

Good catch, Michael. [~cnauroth], I hope you are pointing me to the hadoop code [Client.java#L699|https://github.com/apache/hadoop/blob/branch-2.8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java#L699], isn't it? I'll try to understand this part and get back to you.

*Point-3)* ??How would we authorize against something that is not pre-configured? Basically the dynamic reconfiguration of servers (addition and removal). Also, supports upgrade from 3.4 to 3.5 and above.??

[~shralex], IIUC, dynamic reconfig feature is continue using the zoo.cfg configuration file to keep the quorum info and while processing the reconfiguration request, it will always update the zoo.cfg file and ensure this file is uptodate. In that case each server will get the details of newly added server and during this time we should accommodate the logic of updating the {{quorum server principal list}} with the newly added server or removed server details, if any. But there is a case, <badhost> server tries to join {{zk/<badhost>@EXAMPLE.COM}}, I think this has to be restricted at the reconfig command execution side rather than FLE, probably ZOOKEEPER-2014 jira will help to resolve this problem, am I missing anything?

Rakesh, I will try to take a look more closely soon, so maybe I'm missunderstanding. But to your question - during a reconfig, zoo.cfg is only updated to point to a new dynamic configuration file. That new file contains the up-to-date server list, zoo.cfg only contains a pointer. This update happens during the configuration commit. But authorization should probably happen before, maybe when a reconfig is processed or maybe even before that - when a new server boots and tries to connect to the leader. Does the leader check new server connections ? If that server wasn't initially declared, would it be able to connect ?

bq. But authorization should probably happen before, maybe when a reconfig is processed or maybe even before that - when a new server boots and tries to connect to the leader. Does the leader check new server connections ? If that server wasn't initially declared, would it be able to connect ?
Perhaps we should identify the exact place where it needs to do the authorization. I can say, this jira is proposing authentication and authorization at the beginning of FLE process. That means, when a learner tries to connect to remote quorum server, first it will do the authn and authz, on success it will proceed to FLE process, otw reject the connecting server. Again, after FLE when a Follower/Observer tries to {{connectToLeader}} it does authn and authz steps.

As per my above comment, when a new server tries to connect to leader, leader will cross check with its quorum server principal list(built from zoo.cfg). So here the leader should have the information about the newly connecting server. Will the reconfig commit happens before this?

Good summary! +1 on point 1 and 2.

Re reconfig and 1045:

bq. But what are you saying, further weight to not use zoo.cfg? Or is there a specific issue you think needs to be addressed directly.

The specific issue I think, which was pointed out by Alex is that zoo.cfg does not contain up to date server list during reconfig, so if a server that's perfect valid ask to join quorum the request will be denied with current auth mechanism because quorum peer will not find the server on zoo.cfg.

bq. I think this has to be restricted at the reconfig command execution side rather than FLE, probably ZOOKEEPER-2014 jira will help to resolve this problem, am I missing anything?

ZOOKEEPER-2014 will hopefully ensure that all servers participating reconfig are valid (because only admin or admin equivalent parties can issue the command, if an admin is rogue, all bets are off), and with this it should be good enough from security point of view such that we can skip the downstream auth checks on these servers. The problem here is, the downstream logic (auth check in FLE) has to be aware of that these new servers are from reconfig so it can skip the checks, otherwise these servers will fail auth check (with current logic that solely rely on zoo.cfg that does not contain up to date server info, unless we do something different). 

I see two solutions here:
* When in reconfig mode, skip auth checks for quorum peers. Not sure how exactly we would do this, but I imagine we could possibly have some flags associated with the reconfig context and pass that around. 
* Or, we don't treat reconfig as a special case, instead, we still do auth checks of quorum peers. To do that, we need up to date server info, which we should read from the zoo.cfg.dynamic files. 

the usual sequence is probably this:
1) New server is booted
2) FLE
3) New server connects to leader and adopts current config
4) someone issues reconfig to add new server (which should already be connected)
5) reconfig commits, updating dynamic config file.

it is possible to add a server which didn't connect yet, i.e., execute steps 1-3 after steps 4-5, but this imposes a limit on reconfig: in order for reconfig to start working it requires a quorum / majority of the new configuration servers to be already connected to the current leader. For example, if you have servers A, B, C and assume server C is down and you want to add server D. You need a majority of the new config, which is 3 servers, so you will only be able to add D if it connects before reconfig is invoked (or if C recovers).

another option is to have a write command updating authentication info, which must be invoked before reconfig. 

Its interesting, could you please explain more about this. Can we rely on {{zoo.cfg}} for doing authz with this approach?

We discussed this with Rakesh offline, and here's a summary of what I propose. Any comments appreciated!

1. Make auth checks on both sides of a connection - both receiving and connecting, for both FLE connections as well as connections to leader.

2. in 3.4, create a separate file for the auth list, and link it from zoo.cfg, similarly to the way I link the dynamic config file from zoo.cfg. 
This will make updating the file easier in 3.5 (see below). 

3. In 3.5 support dynamic addition/removal of permissions (this may be very similar to dynamic reconfig): store the auth list in a znode, 
create a new command for addition/removal/query from the auth list. Whenever the auth list is updated, also update the on-disk auth file. 

3.1. before a server can be added to the cluster using dynamic reconfig, it needs to be added using dynamic auth list command

3.2 when a server is removed from the cluster using dynamic reconfig, the auth list isn't changed. It may continue to include the server,
and you may add it back to the cluster in the future. (membership info is separate from auth info - for example the auth list can have 10 servers
that you intend to add to the cluster some time in the future).

3.3. when a server is removed from the auth list, all existing connections to that server from other servers can be dropped upon commit
of the auth change command.





Thanks [~shralex] and [~rakeshr] for the design proposal!

bq. update the on-disk auth file.
I am wondering why do we need an on disk auth file in addition to the znode that stores the list of auth info for servers added through reconfig? My understanding is we need to solve the problem of persisting such information such as every server in ensemble can query it, if so, can we just rely solely on a new dedicated znode (similar as /zookeeper/config for reconfig)?

bq. it needs to be added using dynamic auth list command
I am assuming here it refers to new commands and APIs to be added for 3.5 that specifically for user to add auth info. I am wondering why we don't update existing reconfig commands / APIs such that the first thing it will do is to add such auth info. Reusing existing reconfig APIs / commands to serialize auth info sounds easier to implement.


You probably need a config file at least for initialization. Then, if you're able to load the database before leader election you may not need the config file anymore.
I don't remember how that works.

Its possible to reuse existing reconfig commands. But it may be better to add new flags for authentication rather than tie it with membership changes.
For example when you remove a server you don't necessarily want to remove its auth info because that would imply different semantics than what removal
means now. Currently removal doesn't shut it down and clients can gracefully migrate if they want to, we could also add it back in a different role (there one
scenario where this two step process is necessary to change roles).

But yeah, one can add more options to reconfig besides -add and -remove to handle authentication info, the question is whether this would save anything - I'm
heavily reusing the set and get APIs anyhow.


Thanks a lot [~phunt], [~shralex], [~hanm] for the discussions and suggestions. I've tried an initial attempt to do the authorization using the hostnames from {{zoo.cfg}}.  Kindly review and let me know the feedback. To keep the implementation simple, this patch expects fqdn should be configured in the zoo.cfg. Later this could be enhanced by supporting ipaddress/hostname and could use the approach in the patch {{HOST_RESOLVER-ZK-1045.patch}}

bq. 2. in 3.4, create a separate file for the auth list, and link it from zoo.cfg, similarly to the way I link the dynamic config file from zoo.cfg. 
As an initial attempt I've used zoo.cfg based approach for the authorized hosts. I agree we could enhance this using separate file for the auth list or znode approach etc. How about push this patch first and later we could discuss and implement solution through another jira.

bq. 3. In 3.5 support dynamic addition/removal of permissions (this may be very similar to dynamic reconfig): store the auth list in a znode, 
bq. create a new command for addition/removal/query from the auth list. Whenever the auth list is updated, also update the on-disk auth file.
I've plans to raise a separate jira for forward porting the solution to branch3.5 through another jira. I will make a note of these points and will consider while implementing the same.


Thanks Rakesh, sure, creating separate Jiras for these things sounds like a good idea.

Attached another patch where I've tried to fix the flaky test reported by Michael [earlier|https://issues.apache.org/jira/browse/ZOOKEEPER-1045?focusedCommentId=15388151&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15388151]. Hi [~hanm], it would be great if you could re-run your pressure tests using the latest patch and let me know the feedback. Thanks!

Stress tests for latest patch run OK. I'll provide more feedback after reviewing the patch in detail.

Great, thanks for your time and early feedback.

[~rakeshr] fyi this rolling upgrade test is still flaky, though with a different error message comparing to the previous error that you fixed. I got 2 failures out of 30 test pass on all unit tests during weekend. I'll see if I can fix the test after finishing review of the patch.

[~rakeshr] I am wondering if we can consolidate auth check in QuorumCnxManager and remove auth checks in Learner and LearnerHandler (please refer to my review board comments on the exact place). I think it is sufficient to do auth checks in QuorumCnxManager as that is the abstraction layer where every connection request / response being made, and once we did the check there there is no need to check at higher level abstractions such as in Learner. I tried to remove checks in Learner and LearnerHandler, all tests passed except three tests failed in QuorumCnxManagerTest, and these tests fail because the way they were implemented essentially by pass QuorumCnxManager, which is not what would happen in practice for production code.

I've done another review pass on the patch. The authorization logic added between revision 8 and 9 looks good to me. I've left some notes on the review board and I think all those issues are none blocking. 

Reg this point there was a discussion in the review ticket and others please refer the same. Summary of the discussion is to keep the auth in both peer-peer and learner-leader quorum formation phase.

Attached another patch in the [review ticket 47354|https://reviews.apache.org/r/47354/] addressing [~hanm]'s comments. Majorly I've used the {{QuorumConnectionThread}} only if the 'quorumSaslEnabled' to process the connection requests asynchronously. In the existing code path(non-auth) it will continue connecting to the peers sequentially. This separation is basically done to give the confidence to all by not touching the existing code flow, and keeps a different code path for this feature. I hope this change will help to push the feature safely into the {{branch3.4}}.

[~fpj], [~cnauroth], [~rgs], do you have some cycles to review the latest patch. Appreciate your feedback. Thanks!

Thanks [~hanm] for the reviews. I'm attaching the latest patch from the review board, also modified the {{README.md}} by including the _HOST wildcard pattern details.

Test plan update now with many new stuff in the patch (i.e. authorization piece). Anyone interested in this JIRA could help testing by executing some of the test cases there.

I've attached another patch with few changes to the authorization part by adding {{SecurityUtils#getServerPrincipal()}} "_HOST" replacing logic.

Attached feature document describing the configurations and upgrade steps. Please refer the same for understanding the latest patch and I hope this would help to setup secure cluster for testing the patch. Appreciate feedback.

Thanks [~hanm] for the offline reviews. 

Attached new patch with the following changes:
# Renamed {{quorumcnxn.threads.size}} to {{quorum.cnxn.threads.size}}
# Removed {{README.md}} from the patch as I attached feature doc pdf doc separately in jira
# Replaced {{IOUtils.cleanup(LOG, sock)}} with simple {{sock.close();}} as java-1.6 shows compilation errors.

Update test report posted as https://goo.gl/qNR45M (open to everyone for comment). 

The gist out of the report is:
* We need to decide how to support shared Kerberos principals. Currently zkquorum@REALM.COM does not work, but it should be easy to fix this.
* There is a corner case about impersonating server (a server with a valid Kerberos credential from another server in ensemble.). My feeling is this is a corner case that we could either postpone or document - security wise it seems fine, because we support shared kerberos credential there is no way we can prevent impersonating (shared Kerberos credential is an extreme, as shared Kerberos credential effectively would disable authorization).

I left a few minor comments on the reviewboard here:

https://reviews.apache.org/r/47354/#comment223707

I will try to get up to speed with the discussion thus far in the JIRA and hopefully have something else to add.

I posted details of a worked example to dev@, using the 3.4 patch on this issue applied to 3.4.9. The feature seems to basically work in that I can see a quorum bootstrap with successful authentication, and specifying an incorrect principal in the configuration of an instance or making the keytab unreadable will prevent that. Suggestions on a hammer test to try now?

One nit I noticed is with Java 8 (OpenJDK 8u112 specifically) - and I believe recent versions of Java 7 will have the same behavior - if you do not use precisely the form <principal>/_HOST for quorum.auth.kerberos.servicePrincipal the JRE will throw exceptions during configuration file processing. The instructions in the PDF attached to this issue suggest you can use other  name formats like <principal>@<realm> or <principal>/<fqdn>@<realm> but I had trouble with those. Could be a local JRE issue or have been operator error I suppose. You may want to try out a few variations when testing this feature.



Thanks a lot [~ekoontz] for the reviews. I will incorporate your code improvement suggestions in the next patch. 

Thanks a lot [~hanm], [~apurtell] for the comments.
I agree to both of you. The shared principal format {{<principal>@<realm>}} won't work well with the current patch. Its my mistake and I will modify the shared principal section saying that the supported format is "<principal>/hostname@<realm>". Presently ZooKeeper supports the server principal name in the format {{<principal>/<hostname>@<realm>}}, [zk branch-3.4 code reference|https://github.com/apache/zookeeper/blob/branch-3.4/src/java/main/org/apache/zookeeper/server/ZooKeeperSaslServer.java#L55] and [GSSName.html#NT_HOSTBASED_SERVICE|https://docs.oracle.com/javase/7/docs/api/org/ietf/jgss/GSSName.html#NT_HOSTBASED_SERVICE]. From your comments IIUC, the current patch works well if admin configures Kerberos principal in the format "<principal>/hostname@<realm>".

For the shared principal, it needs to configure like => <principal>/127.0.0.1@<realm> or <principal>/localhost@<realm>.

Also, the current patch will support authorization if admin configured Kerberos principal name in the format <principal>/FQDN@<realm>.

bq. shared Kerberos credential is an extreme, as shared Kerberos credential effectively would disable authorization
[~hanm], Presently am doing the authorization of the connecting hosts at the QuorumServer side. It does by comparing the FQDN(host details) from the QuorumLearner's principal and the zoo.cfg server details of QuorumServer. This is done after the authentication and in the SaslQuorumServerCallbackHandler. If we need to support authz irrespective of the Kerberos principal configured, I have another idea for the authz logic.

Say, A and B is forming quorum. Since the connecting peer sends 'QuorumAuthPacket' to the peer server (A -> B), now I'm planning to keep A's FQDN in his 'QuorumAuthPacket' and B can read this and cross check against zoo.cfg. This will be done outside the CallbackHandler and before the authentication. Also, this new authz logic wouldn't have any dependency with the Kerberos and this is a separate logic common for Digest/Kerberos. In general, I would say this new authz is a kind of {{"hostname white listing"}} using the {{zoo.cfg}} server details.

Auth Packet format:-
{code}
    class QuorumAuthPacket {
        long magic;
        int status;
        buffer token;
        ustring hostname;
    }
{code}

Principal name:-
<principal>/hostname@<realm>

Server details:-
{code}
server.1=FQDN1:port:port
server.2=FQDN2:port:port
server.3=FQDN3:port:port
{code}

If everyone agrees to this, I will quickly start prototyping this and upload a patch. Welcome comments. Thanks!

[~rakeshr]: Regarding your proposal of doing authorization with shared Kerberos principal by sending hostname as part of auth packet, my thoughts are authentication and authorization has to be done together and authorization has a hard dependency on authentication. If an entity is not authenticated, it seems not making much sense to try to authorize it, as what it claims might be total bogus. In shared Kerberos credential case, there is no way to authenticate that the names sent from a server is genuine as opposed to the none shared Kerberos case where we have names encoded in keytabs, which will be authenticated as part of Kerberos. So, maybe we just don't not solve this shared kerberos credential authorization problem? If user wants authorization they can use none-shared kerberos credential.



bq. There is a corner case about impersonating server (a server with a valid Kerberos credential from another server in ensemble.). My feeling is this is a corner case that we could either postpone or document - security wise it seems fine, because we support shared kerberos credential there is no way we can prevent impersonating (shared Kerberos credential is an extreme, as shared Kerberos credential effectively would disable authorization).
Good catch [~hanm]. I think it is not required to specifically authorizing the Learner's host details against the authz lists. I agreed to capture this recommendations in my feature document. Any of the servers(the server hosts configured in zoo.cfg) in the ensemble can join quorum with a valid Kerb principal. Lets assume we have three servers 1,2 & 3. It is highly recommended to configure the host based Kerb principal to the respective servers like,
{code}
server.1=FQDN1:2080:2181    and its Kerb principal name should be 'zkquorum/FQDN1@EXAMPLE.COM'
server.2=FQDN2:2080:2181    and its Kerb principal name should be 'zkquorum/FQDN2@EXAMPLE.COM'
server.3=FQDN3:2080:2181    and its Kerb principal name should be 'zkquorum/FQDN3@EXAMPLE.COM'
{code}
Impact of interchanging the principal. For example, assume admin has configured a valid {{zkquorum/FQDN2@EXAMPLE.COM}} principal(ensured proper keytab in place) in server.3's {{jaas.config}} instead of {{FQDN3}}. Server.3 will create an instance of {{SaslServer}} using this principal, which is a valid one. But all the Learners will think that Server.3 has service principal {{zkquorum/FQDN3@EXAMPLE.COM}} and tries to authenticate using this and will endup in auth failures. So Server.3 will never get chance to become LEADER due to not successfully authenticating any of the connecting Learners. Since 1 & 2 has proper {{jaas.config}} principal entries, these both will successfully participate in quorum formation and one of them will become LEADER. For convenience, assume 1 became LEADER. Now, what happens to 3. Since he has valid kerberos principal he can findout the Leader server and prepares Leader's Kerb principal {{zkquorum/FQDN1@EXAMPLE.COM}} and joins quorum as FOLLOWER {{connectToLeader()}}.

bq. my thoughts are authentication and authorization has to be done together and authorization has a hard dependency on authentication
Yes, you are correct.

bq. In shared Kerberos credential case, there is no way to authenticate that the names sent from a server is genuine as opposed to the none shared Kerberos case where we have names encoded in keytabs, which will be authenticated as part of Kerberos.
Passing host details via {{QuorumAuthPacket}} is one proposal. *I'd like to know anybody has a strong use case which needs authorization of host for both Digest and shared Kerb principal*. Thanks!

bq. If user wants authorization they can use none-shared kerberos credential.
This will make the implementation simple. I'd like to hear comments from other folks as well. Welcome thoughts. Thanks!


Few updates:-

Attached new patch fixing Eugene Koontz's minor comments.
* typo: should be initializeAuth(..)
* Rename variable {{clientLoginContext}} to {{learnerLoginContext}}
* Rename variable {{clientRequireSasl}} to {{learnerRequireSasl}}

Also, attached new feature document. I've updated the doc mentioning that,
- Authorization is not supported in single shared Kerb principal format.
- Authorization is not supported in Digest.
- For the shared principal, admin has to configure {{quorum.auth.kerberos.servicePrincipal= servicename/localhost}}

I reviewed the docs and they look great. Rakesh what is your plan re the docs once 1045 lands, are you going to just add them to the bottom of this page as a new section?
https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zookeeper+and+SASL
or are you planning to create a new wiki page? I assume you're not planning to include them in the patch as part of the forrest documentation?

Thanks [~phunt] for the reviews.

bq. I assume you're not planning to include them in the patch as part of the forrest documentation?
Yes, I'm thinking to add this into our cwiki page.

bq. are you planning to create a new wiki page? 
My plan is to include this into {{Zookeeper and SASL}} section. I could see the current [Zookeeper and SASL|https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zookeeper+and+SASL] page is too large. I'm thinking to separate out ZooKeeper client-server and server-server sections into sub-pages something similar to [Zab in words|https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zab+in+words] page.

Just a suggestion,
* Zookeeper and SASL
#- Introduction
#- Client-Server mutual authentication
#- Server-Server mutual authentication
#- Appendix: Kerberos, GSSAPI, SASL, and JAAS

Sounds like a good approach to me on the docs. Thanks Rakesh.

Thanks [~phunt] for the reply. I'll come back to the documentation part once I get +1 votes for the patch. I hope the feature doc attached in this jira can be used as a user guide to test this feature.

One thing I noticed in the docs while doing some testing today - it doesn't make things clear that "servicename" has to be substituted with the proper service name. I think we need to make this more clear, perhaps by referring to <servicename> (etc...) instead. Also the docs should talk a bit about how to properly set this, perhaps to fit with your [~rakeshr] existing example. Rakesh can you update that?


[~rakeshr] does org.apache.zookeeper.server.quorum.QuorumCnxManager#observerCounter need to be protected from concurrent access? It looks like it does given it might be decremented under the async recv? Did you check the other accesses as part of adding the thread executor/async calls?

OK, I'll take care this part in the next document revision.

Good catch [~phunt].
bq. does org.apache.zookeeper.server.quorum.QuorumCnxManager#observerCounter need to be protected from concurrent access? It looks like it does given it might be decremented under the async recv?
Yes, this need to be protected from concurrent access. I will change {{int}} to {{AtomicInteger}} and use {{observerCounter.getAndDecrement()}} instead of {{observerCounter--}}.

bq. Did you check the other accesses as part of adding the thread executor/async calls?
Sure, I will do a self-review on the thread execution path.

Sounds good. fwiw that's the only one I noticed. both send and recv async paths.

Attached new document. Following are the changes:
- described servicename,
- changed realm {{YOUR-REALM.COM}} to {{EXAMPLE.COM}}. This is to sync with our existing cwiki SASL page,
- corrected few indentation.

I've reviewed the async paths and uploaded new patch with the following changes:
- Used AtomicInteger  for {{observerCounter}}.
- Added counter to count connection processing threads. Now, I've added log messages in each LE iteration. Later if requires can expose bean attribute. Also, I hope this will be helpful to tune the configuration property {{quorum.cnxn.threads.size}}

I've reviewed the code (including the recent change which I had one small comment on) and overall I think we are good at this point. Any other issues, please speak up otw I'll commit this as soon as my recent review comment is addressed.

I did extensive testing last week. I tried running multiple cluster sizes, tried running new server against old server, also went through the rolling upgrade testing part of the document and that worked fine. afaict at this point we're ready to commit.

One small note on the rolling upgrade doc - it might be good to mention that step 1 is a good time to upgrade the code itself?

Thanks a lot [~phunt] for the feedback. Attached new patch addressing the code review comments. Also, updated doc mentioning that {{restart the server with the new ZooKeeper 3.4.10 binaries}}

Does the doc make it clear which version(s) this is supported in? 3.4.10+ for now, eventually 3.5 (we need to work on the "forward" port one this patch goes into 3.4). I recommend you add to the doc if this is not already overly clear.

Added below paragraph in the {{Introduction}} to describe the supported version. Also, raised ZOOKEEPER-2639 JIRA to do the forward porting activity. Please take a look at it.
{code}
Presently, this feature is supported only in ZooKeeper 3.4.10+ version and is implemented using JIRA ZOOKEEPER-1045. The feature code will be forward ported to ZooKeeper 3.5.x and 3.6.x versions in a separate JIRA task ZOOKEEPER-2639.
{code}

That's a good addition. Doc lgtm.

Any insight into how big a job it will be to port to 3.5/trunk (ZOOKEEPER-2639)? 

Reviewed and committed to 3.4.10. Great to see this one land -- thanks Rakesh!

[~phunt], thank you for the helpful code review feedback and the commit!

Thank you very much to all contributors for your time and helpful discussions in making this feature!

I hope following are some of the major parts to be considered. I'm planning to do the code changes after 3.4.10 release. I think, we could target this for 3.5.4 release. Any comments?
# Provide a mechanism to build {{authzHosts}} for dynamicReconfig servers.
# Additional auth cases of dynamic joining of servers should be implemented.
# Refactoring requires to incorporate QuorumCnxManager code changes.

Sounds good. These captures the following up work to forward port 1045 to master / 3.5.x. Just add one additional note, we talked about one issue previously that Kerberos treats frequent login attempts as replay attacks, so we'd need some code to deal with this. What Chris commented earlier:

bq. Hadoop's RPC framework handles this case with a brief backoff and retry to work around the case of getting flagged as a replay attack.

This would need on both 3.4.x and 3.5 I think. 

Thanks [~hanm] for adding that point. IIUC, this has to be covered in both client-server and server-server sasl layers. How about discuss & implement this separately via another jira task?

Github user hanm closed the pull request at:

    https://github.com/apache/zookeeper/pull/147


-1 overall.  GitHub Pull Request  Build
      

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 58 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 3.0.1) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/879//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/879//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/879//console

This message is automatically generated.

+1 overall.  GitHub Pull Request  Build
      

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 58 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 3.0.1) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/880//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/880//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/880//console

This message is automatically generated.

FAILURE: Integrated in Jenkins build ZooKeeper-trunk #3631 (See [https://builds.apache.org/job/ZooKeeper-trunk/3631/])
ZOOKEEPER-2935: [QP MutualAuth]: Port ZOOKEEPER-1045 implementation from (phunt: rev 75411ab34a3d53c43c2d508b12314a9788aa417d)
* (edit) src/java/main/org/apache/zookeeper/server/quorum/Leader.java
* (edit) src/java/test/org/apache/zookeeper/server/quorum/LearnerTest.java
* (edit) src/java/test/org/apache/zookeeper/server/quorum/LearnerHandlerTest.java
* (add) src/java/test/org/apache/zookeeper/server/quorum/auth/MiniKdc.java
* (edit) src/zookeeper.jute
* (add) src/java/test/org/apache/zookeeper/server/quorum/auth/QuorumAuthTestBase.java
* (edit) src/java/main/org/apache/zookeeper/server/quorum/Observer.java
* (edit) src/java/main/org/apache/zookeeper/server/quorum/FastLeaderElection.java
* (edit) src/java/main/org/apache/zookeeper/server/quorum/QuorumPeerMain.java
* (add) src/java/main/org/apache/zookeeper/server/quorum/auth/QuorumAuthLearner.java
* (edit) src/java/main/org/apache/zookeeper/server/ServerCnxn.java
* (edit) src/java/main/org/apache/zookeeper/server/quorum/Follower.java
* (add) src/java/main/org/apache/zookeeper/server/quorum/auth/SaslQuorumAuthServer.java
* (add) src/java/test/org/apache/zookeeper/server/quorum/auth/QuorumKerberosAuthTest.java
* (edit) build.xml
* (edit) src/java/main/org/apache/zookeeper/server/auth/SaslServerCallbackHandler.java
* (edit) src/java/main/org/apache/zookeeper/client/ZooKeeperSaslClient.java
* (edit) src/java/main/org/apache/zookeeper/server/quorum/Learner.java
* (add) src/java/main/org/apache/zookeeper/server/quorum/auth/SaslQuorumAuthLearner.java
* (add) src/java/test/org/apache/zookeeper/server/quorum/auth/KerberosSecurityTestcase.java
* (add) src/java/main/org/apache/zookeeper/server/quorum/auth/SaslQuorumServerCallbackHandler.java
* (edit) src/java/main/org/apache/zookeeper/Login.java
* (edit) src/java/test/org/apache/zookeeper/server/quorum/Zab1_0Test.java
* (add) src/java/test/data/kerberos/minikdc.ldiff
* (add) src/java/test/org/apache/zookeeper/server/quorum/auth/QuorumAuthUpgradeTest.java
* (add) src/java/main/org/apache/zookeeper/server/quorum/auth/NullQuorumAuthLearner.java
* (add) src/java/test/data/kerberos/minikdc-krb5.conf
* (add) src/java/test/org/apache/zookeeper/server/quorum/auth/MiniKdcTest.java
* (edit) src/java/main/org/apache/zookeeper/server/quorum/LearnerHandler.java
* (edit) src/java/main/org/apache/zookeeper/server/quorum/QuorumCnxManager.java
* (add) src/java/main/org/apache/zookeeper/util/SecurityUtils.java
* (edit) ivy.xml
* (edit) src/java/test/org/apache/zookeeper/server/quorum/QuorumPeerTestBase.java
* (add) src/java/test/org/apache/zookeeper/server/quorum/auth/QuorumDigestAuthTest.java
* (add) src/java/main/org/apache/zookeeper/SaslClientCallbackHandler.java
* (edit) src/java/test/org/apache/zookeeper/test/CnxManagerTest.java
* (add) src/java/main/org/apache/zookeeper/server/quorum/auth/QuorumAuthServer.java
* (edit) src/java/test/org/apache/zookeeper/server/quorum/EphemeralNodeDeletionTest.java
* (add) src/java/main/org/apache/zookeeper/server/quorum/auth/QuorumAuth.java
* (edit) src/java/main/org/apache/zookeeper/server/quorum/QuorumPeerConfig.java
* (edit) src/java/test/org/apache/zookeeper/server/quorum/FLEBackwardElectionRoundTest.java
* (edit) src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java
* (add) src/java/main/org/apache/zookeeper/server/quorum/auth/NullQuorumAuthServer.java
* (edit) src/java/test/org/apache/zookeeper/server/quorum/RaceConditionTest.java
* (edit) src/java/main/org/apache/zookeeper/server/ZooKeeperSaslServer.java
* (edit) src/java/test/org/apache/zookeeper/server/quorum/FLELostMessageTest.java
* (add) src/java/test/org/apache/zookeeper/server/quorum/auth/QuorumKerberosHostBasedAuthTest.java
* (edit) src/java/test/org/apache/zookeeper/test/FLEPredicateTest.java
* (edit) src/java/test/config/findbugsExcludeFile.xml
* (edit) src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java
* (add) src/java/test/org/apache/zookeeper/server/quorum/auth/KerberosTestUtils.java


Patch provided with [#ZOOKEEPER-24443]Â is not available in theÂ org.apache.zookeeper.util.SecurityUtils.java in release version 3.5.5. can some provideÂ on details if this is missed or any alternate way provided to fix the issue.

Â 

ERROR [NIOServerCxnFactory.SelectorThread-2:SecurityUtils@249] - server principal name/hostname determination error: 
 java.lang.StringIndexOutOfBoundsException: String index out of range: -1
 at java.lang.String.substring(String.java:1967)
 at org.apache.zookeeper.util.SecurityUtils.createSaslServer(SecurityUtils.java:174)
 at org.apache.zookeeper.server.ZooKeeperSaslServer.createSaslServer(ZooKeeperSaslServer.java:44)
 at org.apache.zookeeper.server.ZooKeeperSaslServer.<init>(ZooKeeperSaslServer.java:38)
 at org.apache.zookeeper.server.NIOServerCnxn.<init>(NIOServerCnxn.java:104)
 at org.apache.zookeeper.server.NIOServerCnxnFactory.createConnection(NIOServerCnxnFactory.java:848)
 at org.apache.zookeeper.server.NIOServerCnxnFactory$SelectorThread.processAcceptedConnections(NIOServerCnxnFactory.java:479)
 at org.apache.zookeeper.server.NIOServerCnxnFactory$SelectorThread.run(NIOServerCnxnFactory.java:392)

Â 

{quote}ZOOKEEPER-24443 is not available in the org.apache.zookeeper.util.SecurityUtils.java in release version 3.5.5
{quote}
Jira number is not correct. Could you please let me know the expected jira.
{quote}server principal name/hostname determination error: 
 java.lang.StringIndexOutOfBoundsException: String index out of range: -1
{quote}
It expects Kerberos principals in the form of {{servicename/fully.qualified.domain.name@EXAMPLE.COM}}, here "servicename" has to be substituted with the proper service name. 
 For example, {{zkquorum/myhost1.foo.com@EXAMPLE.COM}} and {{zkquorum/myhost2.foo.com@EXAMPLE.COM}} for the hosts - {{myhost1.foo.com}} and {{myhost2.foo.com}}

Hope you are referring [ZooKeeper cwiki page|https://cwiki.apache.org/confluence/display/ZOOKEEPER/Server-Server+mutual+authentication] , it has details about the configs.

[~rakeshr] Here is the link -Â https://issues.apache.org/jira/browse/ZOOKEEPER-2433 typed wrong JIRA.

*Work around:*
 ZooKeeper-2433 is internally setting hostname as {{null}}, then principal becomes {{zkquorum/null@EXAMPLE.COM}}. Is it possible for you to explicitly append {{'/null'}} to your principal and give a try ?
{code:java}
DEBUG serviceHostname is 'null' 
DEBUG servicePrincipalName is 'zkquorum' 
DEBUG SASL mechanism(mech) is 'GSSAPI'
{code}
One question, are you looking for both client-server and server-server authentication via SASL mechanism ?

