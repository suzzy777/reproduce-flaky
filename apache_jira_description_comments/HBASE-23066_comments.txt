I took a shot at creating a patch based off of branch-1.4, calling the new configuration option "hbase.rs.prefetchcompactedblocksonwrite". Please consider this improvement in upcoming release.

cc: [~anoop.hbase]

please start with a patch for the master branch. please create your patch using {{git format-patch}} so that authorship information is included.

On first glance patch looks good to me [~jacob.leblanc]. Have you tested this in your cluster running on AWS? 

I'd like to see some benchmarking to show effectiveness please.

Also some measurement of impact when the working set doesn't fit in the cache.

Added patch for master. After I added to TestCacheOnWrite, I ran into errors with "Too many open files" when running it. It would occur when chmod was run on one of the region files or a child thread was created on around the 50th test iteration. Running only the test I added worked fine, as did running all of the tests after I adjusted my system's open file limit. I'm not sure what threads/open files/processes are getting left around in this test but my addition seems to be pushing it over the edge. Is this a problem? Is it expected to have >1024 open file limit when running large and/or integration tests?

Thanks for taking a look at this!

I have not tested on our cluster yet. Our production cluster is the one hitting performance problems after compaction and given that EMR is a managed service where I'm not exactly sure what is being deployed (does Amazon customize any part of the server jar?) I need to somehow verify that replacing the class files in the jar with my patched versions isn't going to cause any harm before trying the patch on that cluster. I'll try patching on a test environment first and I'll also try to get some confirmation through AWS support that I won't be overriding customized changes by replacing a class file, particularly the HStore class. 

So I'll try to do some testing and get some performance numbers, but it will take me a bit of time.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  1m 16s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} dupname {color} | {color:green}  0m  0s{color} | {color:green} No case conflicting files found. {color} |
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 38s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 57s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 29s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  5m  3s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 34s{color} | {color:green} master passed {color} |
| {color:blue}0{color} | {color:blue} spotbugs {color} | {color:blue}  5m  4s{color} | {color:blue} Used deprecated FindBugs config; considering switching to SpotBugs. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m  2s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  1m 38s{color} | {color:red} hbase-server: The patch generated 7 new + 46 unchanged - 0 fixed = 53 total (was 46) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  5m 12s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 18m  2s{color} | {color:green} Patch does not cause any errors with Hadoop 2.8.5 2.9.2 or 3.1.2. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 48s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}266m 23s{color} | {color:red} hbase-server in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 33s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}329m 59s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.2 Server=19.03.2 base: https://builds.apache.org/job/PreCommit-HBASE-Build/919/artifact/patchprocess/Dockerfile |
| JIRA Issue | HBASE-23066 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12981228/HBASE-23066.patch |
| Optional Tests | dupname asflicense javac javadoc unit spotbugs findbugs shadedjars hadoopcheck hbaseanti checkstyle compile |
| uname | Linux d72a4cd1542f 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | dev-support/hbase-personality.sh |
| git revision | master / 52f5a85bfc |
| Default Java | 1.8.0_181 |
| checkstyle | https://builds.apache.org/job/PreCommit-HBASE-Build/919/artifact/patchprocess/diff-checkstyle-hbase-server.txt |
| unit | https://builds.apache.org/job/PreCommit-HBASE-Build/919/artifact/patchprocess/patch-unit-hbase-server.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/919/testReport/ |
| Max. process+thread count | 4970 (vs. ulimit of 10000) |
| modules | C: hbase-server U: hbase-server |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/919/console |
| versions | git=2.11.0 maven=2018-06-17T18:33:14Z) findbugs=3.1.11 |
| Powered by | Apache Yetus 0.11.0 https://yetus.apache.org |


This message was automatically generated.



Can we declare a warmup threshold on the Table schema? If the compacted HFile greater than this threshold, do not cache it, just a suggestion.

I've run some performance tests to demonstrate the effectiveness of the patch.

I have not patched our production cluster yet as I'm waiting on confirmation from AWS service team that I won't be overwriting AWS-specific changes in the HStore class, but I've done some sampling on a test cluster.

Basic setup is an EMR cluster running 1.4.9 backed by S3 as well as ganglia installed to capture the metrics. I have a stress tester executing about 1000 scans per second on a 1.5 GB region. Prefetching is enabled, and I have one region server that is unpatched or has the new configuration setting disabled, and one region server that is patched and has the new configuration option enabled. I then execute the following test:

1. Move the region to the desired region server (either patched or unpatched).
2. Wait for prefetching to complete and for mean scan times to normalize.
3. Execute a major compaction on the target region.
4. Check region server UI / logs to see when the compaction completes.
5. Collect data from ganglia.

One issue I identified with my test is the scans aren't as random as they should be so I believe data after compaction is getting cached on read more quickly than it otherwise should be on the unpatched server if my scans were truly random. I can improve the test, but results still validate the patch.

Baseline mean scan time was about 20 - 60 milliseconds. After compaction the results were:

Trial 1 (unpatched): mean scan time peaked at over 27000 milliseconds, and stayed above 5000 milliseconds for 3 minutes
Trial 2 (unpatched): mean scan time peaked at over 27000 milliseconds, and stayed above 5000 milliseconds for 3.5 minutes
Trial 3 (patched): mean scan time peaked to 282 milliseconds for one time sample
Trial 4 (patched): mean scan time peaked at just over 1300 milliseconds and remained abover 1000 milliseconds for 30 seconds
Trial 5 (patched): no noticable spike in mean scan time

I've attached a picture of a graph of the results.

Regarding getting some numbers on if the size of the data exceeds the cache, I'm not sure what is being asked for?

My thinking is that behavior in that regard is not going to be any different. Keep in mind: this setting only applies when prefetching is already enabled for the column family. In other words we are already going to read the new file entirely into cache. Enabling this setting will only do a little bit earlier while we are writing it out to circumvent a glut of cache misses that kill performance for a period of time after compaction finishes. So if other data will be evicted with the setting enabled, then it would be evicted without this patch as well. This is also why I'm not sure a per-table setting such as a warmup threshold is needed. In fact I'd be happy if this was the default setting as I don't see any negatives, but I'd understand keeping it disabled by default for risk purposes.

I think [~jacob.leblanc] says  for cloud related use cases having a bigger cache the cache on write after compactions should benefit them - considering the fact that this feature is disabled by default and it is enabled only when prefetch is enabled.
The results that are attached here  shows very positive impact. 
[~jacob.leblanc] - you want to prepare a patch for master?
[~javaman_chen]
bq.  If the compacted HFile greater than this threshold, do not cache it, just a suggestion.
You mean after every compaciton or in general if the hfile count increases certain level do not cache? 


{quote}You mean after every compaciton or in general if the hfile count increases certain level do not cache?
{quote}
As time goes on, HFile will grow larger(because of Compaction), and it's data may get colder and colder, In some scenarios, only the latest time window data is accessed, so warmup the large HFile seems unnecessary.

bq.As time goes on, HFile will grow larger(because of Compaction), and it's data may get colder and colder, In some scenarios, only the latest time window data is accessed, so warmup the large HFile seems unnecessary.
got it. thanks [~javaman_chen]. More of a size based threshold. 

[~ram_krish] the attached HBASE-23066.patch is for master. This is my first patch submission, can you please let me know next steps? 

[~jacob.leblanc]
Can you create a PR using github? That will trigger the CI for running the tests. And it is easy to merge the patch too. 

Not sure how folks are notified of new PRs, but I submitted one for this, including some checkstyle fixes that were in the previous patch. Please review, thank you!

You may have to add a clear release note for this - as how to use this feature and clearly highlight it is used only when prefetch is turned ON.

[~busbey] - you want to have a look at the patch and the charts added by [~jacob.leblanc].

[~jacob.leblanc]
Is the patch still applicable on trunk/2.0 branches?

 Any more reviews here? I can commit this if no other reviews are pending here. Will wait for another day or so.

Right now if the prefetch is turned on, we will do the data prefetch as part of the HFile open. Once a compaction is over and committed, the new file reader gets opened and we will give the prefetch job to the prefetch thread pool. There are by default 4 threads only here. So this is not so aggressive prefetch we can see. Also it avoids the need to do one extra HFile read for the caching by the prefetch thread.
With this new config, what we do is write to cache along with the HFile create itself. Blocks are added to cache as and when it is written to the HFile. So its aggressive. Ya it helps to make the new File data available from time 0 itself. The concern is this in a way demands 2x cache size. Because the compacting files data might be already there in the cache.  While the new file write, those old files are still valid. The new one is not even committed by the RS.  The size concern is big when it is a major compaction!  The comment from @chenxu seems valid. Should we see that angle also?

On a side note, (Not related to this issue) when we have cache on write ON as well as prefetch also On, do we do the caching part for the flushed files twice? When it is written, its already been added to cache. Later as part of HFile reader open, the prefetch threads will again do a read and add to cache!


bq.On a side note, (Not related to this issue) when we have cache on write ON as well as prefetch also On, do we do the caching part for the flushed files twice? When it is written, its already been added to cache. Later as part of HFile reader open, the prefetch threads will again do a read and add to cache!
I checked this part. Seems we just read the block and if it is from cache we just return it. Because HfileReaderImpl#readBlock() just return if the block is already cached.

bq.The comment from @chenxu seems valid. Should we see that angle also?
Ok. We can see that but it is part of this JIRA or should we raise another JIRA and address it. 

bq. On a side note, (Not related to this issue) when we have cache on write ON as well as prefetch also On, do we do the caching part for the flushed files twice? When it is written, its already been added to cache. Later as part of HFile reader open, the prefetch threads will again do a read and add to cache!

Has thought about this recently, maybe there need a conf key to decide which operations(flush, compaction, regionOpen, bulkload) can trigger the prefetch. In this way, if cacheonwrite is enabled, we can exclude the flush and compaction operations when doing prefetch, I can do this in another JIRA.

[~jacob.leblanc]
You want to do the [~javaman_chen] angle of adding threshold based on some size based config - in this JIRA or another one? 
I can do that in another JIRA if you are busy with other things.
[~javaman_chen], [~anoop.hbase] - FYI.


On the new conf name 'hbase.rs.prefetchcompactedblocksonwrite', it would be better not to call it or relate it to prefetch.  Cache on write related config is there. It should have been named like cache on flush. But leave that aside. At least this conf can clear say cache on compaction.  So it is clear that the caching happens as part of compaction write. Also IMHO no need to check that based on whether prefetch is on or not!  Make this conf name and doc clear what it is doing and what is the size expectations.  Any way we have another jira to discuss whether all compacted files to follow this cache on write or not.

Apologies for the delayed response on my part.

Just as an FYI, we manually deployed this patch (my original version) to our production environment and have been running it with good results for about a month and a half.
{quote}With this new config, what we do is write to cache along with the HFile create itself. Blocks are added to cache as and when it is written to the HFile. So its aggressive. Ya it helps to make the new File data available from time 0 itself. The concern is this in a way demands 2x cache size. Because the compacting files data might be already there in the cache. While the new file write, those old files are still valid. The new one is not even committed by the RS.
{quote}
Without evictblocksonclose enabled (it looks like it is disabled by default) then wouldn't the old file data still be in cache even after compaction is finished? Granted once compaction is done it will no longer be accessed, age out, and be evicted if necessary but the same amount of data is read into the cache both with and without this new setting, is it not? When prefetching is enabled the only difference is the caching of the new file is done a little bit earlier, but other than that it seems caching requirements are the same. I'm not sure I understand why 2x cache size is needed - perhaps I am missing something. Having eviceblocksonclose enabled does change things and means you would need 2x cache size compared to normal as you change the ordering of caching/evicting.
{quote}Also IMHO no need to check that based on whether prefetch is on or not! Make this conf name and doc clear what it is doing and what is the size expectations.
{quote}
Coming from the perspective of my organization's requirements - this would not work well for us as we only want data to be cached on compaction for tables where prefetching is enabled:

The clear intention of enabling prefetching on a table is to keep as much data in the read cache as possible to ensure consistently fast reading, but without this configuration there are consistently huge drops in read performance whenever compaction is done because large parts of the table are essentially dropped from the cache (actually the pre-compaction data is still there unless evictblocksonclose is enabled, but the pre-compaction data is for the old file names which will never be accessed again after compaction is finished so it's the same as dropping the data). This configuration is to mitigate that effect to better achieve read performance sought by prefetching. The intention is *not* just to cache everything that gets compacted.

So caching all compacted data on all tables does not meet this requirement and in fact would cause problems if it were to be used. In our use cases we have several tables where we write and compact a lot but where we don't want to prefetch those tables into our cache. Caching all blocks on compaction would cause big problems where we'd evict data we care about in favor of data we will never/rarely read.

An alternative to having this setting contingent on prefetching would be to have a CACHE_BLOCKS_ON_COMPACTION as part of ColumnFamilyDescriptor. Then we could choose to turn it on for the same CFs where we also have prefetching. This seems like a bigger code/documentation change, whereas my original intention on this patch was to keep it small and focused for the only use case I could think of (why else would someone want to cache blocks during compaction except if they were prefetching?). But if a per-column family setting is preferred, then I could try making those changes.

I welcome input from you experts. Thanks!

bq.have a CACHE_BLOCKS_ON_COMPACTION as part of ColumnFamilyDescriptor.
The latest patch what Ram published yesterday is having this aspect also.  We were discussing it offline. The allow cache on write for compacted files should be a global config as well as one can override it at Table or CF levels. On HTD or HCD level, user can do setConfigure with same config name and override the global value.

{quote}The latest patch what Ram published yesterday is having this aspect also. We were discussing it offline. The allow cache on write for compacted files should be a global config as well as one can override it at Table or CF levels. On HTD or HCD level, user can do setConfigure with same config name and override the global value.
{quote}
OK, that sounds great! I did check on that PR and saw only the global setting, which is why I suggested that for my organization's use case at least we would need a table or CF specific setting. Let me know if there's anything else needed on my end.

There is global config.  The global config value can be overridden at a CF level via
ColumnFamilyDescriptorBuilder#setConfiguration(final String key, final String value)
Using the same global config name as the Key

bq.Without evictblocksonclose enabled (it looks like it is disabled by default) then wouldn't the old file data still be in cache even after compaction is finished? 
No. Once compaction is done, the old compacted away files get closed with evictBlocks = true always. It wont honor this conf then.  The conf is more used while closing of region.

[~jacob.leblanc]
Thanks for your detailed write up. I saw the reply from [~anoop.hbase]. hope it helps.

How ever since you have prefetch also enabled - it means every time your cache was always getting loaded asynchronously and that was helping you in a big way always.
Can you just give some rough numbers on you cache size and the number of blocks that you always see in your cache? Is there a sporadic raise in your block count and if so by how much and hope your cache size is good enough to have them.

[~jacob.leblanc]
If you are fine with the latest PR - I can just merge them and work on the other sub task to make this configuration based on a size so that all the older files' blocks are not cacheed.  

BTW - confirmed seeing the code that it is easy to trigger this per table/family also. Even using shell or the java client.

{quote}There is global config. The global config value can be overridden at a CF level via
ColumnFamilyDescriptorBuilder#setConfiguration(final String key, final String value)
Using the same global config name as the Key

BTW - confirmed seeing the code that it is easy to trigger this per table/family also. Even using shell or the java client.
{quote}
Ah, OK. I wasn't aware there was a free-form way to set configuration at CF level, as I saw all of the explicit properties listed there. Thanks.
{quote}No. Once compaction is done, the old compacted away files get closed with evictBlocks = true always. It wont honor this conf then. The conf is more used while closing of region.
{quote}
OK, I see now in HStore.removeCompactedFiles that evictBlocks is set explicitly to true every time. Thanks for clarification on that. So with caching compacted blocks on write there will be a temporary rise in cache size until the compacted files are evicted after compaction is done. Got it.
{quote}Can you just give some rough numbers on you cache size and the number of blocks that you always see in your cache? Is there a sporadic raise in your block count and if so by how much and hope your cache size is good enough to have them.
{quote}
We have two production environments currently with our largest currently hosting 25.4 TB of data, with about 14 TB we are trying to keep "hot" in the cache for quick reading, with about 4 TB of that kept "IN_MEMORY" since we want fast phoenix searching on the data and care more about keeping it in cache. We currently are running 48 region servers with 1 TB on-disk bucket cache each, but that's probably overprovisioned as we've scaled up due to some recent issues. We can probably shoot for around 24 RS which would mean about 600 GB of data we are trying to keep in bucket cache. With the number of regions we have, we have plenty of spare space in bucket cache for handling many large compactions happening at once with this setting. In reality our used space for the cache will be a bit higher because other reading is still done beyond the 14 TB we prefetch, but it's OK if LRU blocks get evicted. I haven't done a deep analysis to see if there are sporadic spikes in cache size during compactions. 1 TB bucket caches may seem large but with the way costs are in AWS, storage is relatively cheap compared to compute resources so it's more cost effective for us to host fewer region servers with hosting more data with large caches rather than smaller caches where we would need more region servers to maintain performance (our compute nodes are probably 5x more expensive than the 1 TB of storage per month).
{quote}If you are fine with the latest PR - I can just merge them and work on the other sub task to make this configuration based on a size so that all the older files' blocks are not cacheed.
{quote}
I am fine with it. Thanks for the attention on this!

 

[~jacob.leblanc] - Pushed this to master. Tomorrow will push this to branch-2.x (latest). I have some issues in pulling the code - since my VM is not working.
BTW thanks for the nice patch.

Results for branch master
	[build #1564 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/1564/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/1564//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1564//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1564//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


This broke nightly and is currently breaking precommit runs after it ran. Unless it's something simple we will probably need to revert and reapply once it's fixed

I reverted d561130e82 on master branch because after revert {{TestCacheOnWrite}} consistently passes. with d561130e82 it consistently fails.

{code}
commit ec9bd20ec8423d49641d0d30704eaee178480d0e (HEAD -> master, origin/master, origin/HEAD)
Author: Sean Busbey <busbey@apache.org>
Date:   Wed Dec 11 10:17:46 2019 -0600

    Revert "HBASE-23066 Allow cache on write during compactions when prefetching is (#919)"
    
    TestCacheOnWrite failing all the time.
    
    This reverts commit d561130e82c5956f0dd9fff5c6f6240a686d3d6a.
{code}

Please ensure the test works reliably before pushing again. Also please set the contributor info as the git author using {{--author "Some User <some@example.com>"}}

Thanks [~busbey]. Once I have my VM back I will check this. 

Results for branch master
	[build #1565 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/1565/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/1565//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1565//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1565//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


[~ram_krish] thanks so much for making the code changes for this!

Not to be a pain, but any chance on getting this merged into branch-1.4 or whatever the next "stable" version will be (not sure if there will be a 1.4.13)? So far at least Amazon chooses the version marked "stable" ([https://hbase.apache.org/downloads.html]) to include in new EMR releases and since the main use case for this is cloud-based, it would be great to see it part of EMR. Thanks!

This is a big change in behavior and I'd rather not see it in a maintenance release.

1.6.0 should move towards release soon and the plan is to get the stable pointer moved off of branch-1.4 to 1.6+ soon.

Also could someone please update the jira subject so it better reflects what was implemented?

[~busbey]
Generally the nightly builds will execute the excluded tests also and once it starts passing will remove it from the exclude list right? Am just waiting so that I can merge this to branch-2 also?
Ya I think better to put this in branch-1.6 only. 

[~javaman_chen]
Do you want to take a stab at trying to cache the index/bloom blocks also - the one you were mentioning? May be then we should do while flushes also then. Let me know. If not I can try it out. 

Results for branch master
	[build #1570 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/1570/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/1570//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1570//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1570//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


The failed test passed now. Will push it to branch-2.

bq. Generally the nightly builds will execute the excluded tests also and once it starts passing will remove it from the exclude list right? Am just waiting so that I can merge this to branch-2 also?

nightly doesn't run excluded tests. there's a dedicated job that runs them more frequently:

https://builds.apache.org/view/H-L/view/HBase/job/HBase-Find-Flaky-Tests/job/master/lastSuccessfulBuild/artifact/dashboard.html

when test failures have aged out of both nightly (~5 runs) and the untrusted runner (~20 runs) then it's put back into regular rotation.

Results for branch branch-2
	[build #2388 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2388/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2388//General_Nightly_Build_Report/]




(/) {color:green}+1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2388//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2388//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


Pushed to branch-2 and master. Lets raise a backport JIRA for branch-1.6. 
Confirmed that in branch-2 also the test cases passed with this change. 

Results for branch branch-1
	[build #1248 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/1248/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/1248//General_Nightly_Build_Report/]


(/) {color:green}+1 jdk7 checks{color}
-- For more information [see jdk7 report|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/1248//JDK7_Nightly_Build_Report/]


(/) {color:green}+1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/1248//JDK8_Nightly_Build_Report_(Hadoop2)/]




(x) {color:red}-1 source release artifact{color}
-- See build output for details.


