I created [https://github.com/apache/beam/pull/6375] which cleans this up. Floating point division is always the correct thing to do here; I'd like to understand under what circumstances this would be made worse. (Note that for a real pipeline, the actual timings can be noisy.)

 

Note that the code in question should not be (that) performance sensitive (with the exception of adding to the batch and checking the batch size), as the complicated logic is called once per batch (which should be reasonably expensive, and is taken into account).

Submitted pr/6375.
 * Simplifies the aging out of old data. Not only was the old formula hard to
understand, but it meant that bad data could stick around and poison the
estimates forever.
 * Adds a variance parameter allowing the batch size to vary over a fixed
range giving a broader base for the linear regression.
 * Uses numpy when available to do the regression. This is both much more
efficient and allows for less error-prone expression of more complicated
analysis.

The algorithm was also changed to:
 * Eliminates outliers, both using Cook's distance and just throwing out the
top (often high-variance and high-influence) 20% when there is sufficient
data.
 * Weight by the inverse of batch size, to provide a more stable fixed size
estimate (which the default "overhead" target is sensitive to).

These changes were tested against a large TFT pipeline and found to produce
more uniform batch sizes and similar, possibly slightly improved, total
runtimes and total costs.

