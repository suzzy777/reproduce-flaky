since this was a single flap, going to try 100 runs and see if it repros again. here: http://cassci.datastax.com/view/Parameterized/job/parameterized_dtest_multiplexer/101/

1 test did fail out of 100 trials, so this test does need some repair.

This might be worth one more trial of like 200, because it was discovered that the multiplexer was running a smaller heap than the standard job run (multiplexer has now been fixed), which could maybe affect the results.

Running this test 200x here:

http://cassci.datastax.com/view/Parameterized/job/parameterized_dtest_multiplexer/163/

with xlarge instances.

Saw 8 failures, all of which looked like this:

{code}
Error Message

concurrent rebuild should not be allowed, but one rebuild command should have succeeded.
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-w6QEHl
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'start_rpc': 'true'}
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc2> discovered
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.1 dc1> discovered
--------------------- >> end captured logging << ---------------------
Stacktrace

  File "/usr/lib/python2.7/unittest/case.py", line 329, in run
    testMethod()
  File "/home/automaton/cassandra-dtest/rebuild_test.py", line 106, in simple_rebuild_test
    msg='concurrent rebuild should not be allowed, but one rebuild command should have succeeded.')
  File "/usr/lib/python2.7/unittest/case.py", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File "/usr/lib/python2.7/unittest/case.py", line 506, in _baseAssertEqual
    raise self.failureException(msg)
"concurrent rebuild should not be allowed, but one rebuild command should have succeeded.\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-w6QEHl\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'start_rpc': 'true'}\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc2> discovered\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.1 dc1> discovered\n--------------------- >> end captured logging << ---------------------"
{code}

EDIT: Ah, interesting -- this is _with_ vnodes, so it looks like it can fail with or without.

might help to know the actual value when we fail with this message: https://github.com/riptano/cassandra-dtest/pull/1097

another run here (maybe the new assertion will help figure out what's going on) http://cassci.datastax.com/view/Parameterized/job/parameterized_dtest_multiplexer/170/

We don't seem to be printing out the actual rebuild errors, just the count.

I'm not sure it matters:

https://github.com/riptano/cassandra-dtest/blob/d2c93023ebe26a3aff98a85bd62deb96c9403c49/rebuild_test.py#L81-L97

It's going to be a failing {{nodetool}} call one way or the other. (Side note: I'm making a PR to make the two error-handling cases use the same logic.)

That PR also improved error handling, and here's the error I get running locally:

{code}
======================================================================
ERROR: rebuild_test.py:TestRebuild.simple_rebuild_test
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/mambocab/cstar_src/cassandra-dtest/rebuild_test.py", line 96, in simple_rebuild_test
    rebuild()
  File "/home/mambocab/cstar_src/cassandra-dtest/rebuild_test.py", line 88, in rebuild
    raise e
NodetoolError: Nodetool command '/home/mambocab/.ccm/repository/gitCOLONtrunk/bin/nodetool -h localhost -p 7200 rebuild dc1' failed; exit status: 1; stdout: nodetool: Unable to find sufficient sources for streaming range (-3287869951390391138,-1624006824486474209] in keyspace system_auth with RF=1.If you want to ignore this, consider using system property -Dcassandra.consistent.rangemovement=false.
See 'nodetool help' or 'nodetool help <command>'.

-------------------- >> begin captured logging << --------------------
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: cluster ccm directory: /tmp/dtest-QDgw0_
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None, 'phi_convict_threshold': 5, 'start_rpc': 'true'}
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc2> discovered
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.1 dc1> discovered
--------------------- >> end captured logging << ---------------------

----------------------------------------------------------------------
{code}

-So, looks like we're calling {{rebuild}} incorrectly.- This shouldn't fail this way. The thing to do here, as the error says, may be to set {{cassandra.consistent.rangemovement}} to {{false}}.

EDIT: for the record, this was the PR:

https://github.com/riptano/cassandra-dtest/pull/1106

That is also what happens on CI:
http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/14/testReport/rebuild_test/TestRebuild/simple_rebuild_test/

So, given that we have a one node cluster, and are now rebuilding a second node we just added, I don't see why we would suddenly start failing to find a source for a range of system_auth. Marking this as a bug.

This problem could have been around for a while but not caught because of bad error handling in the test.

True, but I maintain that there are enough sources for system_auth for all ranges up, at the time of rebuild.

This test became flaky after CASSANDRA-11848, because before that the local node could be considered a source, while now sources are restricted only to dc2, so since {{system_auth}} uses {{SimpleStrategy}} depending on the token arrangement there could or not be sources from dc2. Fix is to either use {{-Dcassandra.consistent.rangemovement=false}} or update {{system_auth}} to use {{NetworkTopologyStrategy}} with 2 dcs..

bq. True, but I maintain that there are enough sources for system_auth for all ranges up, at the time of rebuild.

Oh, I agree -- just saying that passing tests in the CassCI history may be the result of incorrect test code.

[~yukim] If you agree with Paulo that this is a test issue, feel free to assign this ticket to me and I'll deal with it.

Yes, we need to change dtest for what Paulo said.

I've filed [dtest PR 1133|https://github.com/riptano/cassandra-dtest/pull/1133] to address the problem where {{rebuild}} would fail to find sources for {{system_auth}}. This test still fails locally when nodetool allows concurrent rebuilding. Reassigning to [~yukim] to address that issue.

dtest PR here: https://github.com/riptano/cassandra-dtest/pull/1146

submitted [multiplexer run|https://cassci.datastax.com/view/Parameterized/job/parameterized_dtest_multiplexer/189/]

+1, multiplexer results look good. Merged PR. Posting PR body here for posterity:

{noformat}
Current test implementation didn't trigger rebuild concurrently even though we are using thread in my local laptop. I think the same thing is happening in CI since it is hitting assertion error that indicates it.

So I changed the code to run nodetool in background first, and then immediately trigger second one in foreground, and the latter should fail at that point.

Rebuild completion from the former is guarded by watching log.
{noformat}

I commented on the dtest PR to remove the known_failure annotation - after this PR was merged, I saw a new failure on this test as part of the [daily 3.9 dtest run|http://cassci.datastax.com/job/cassandra-3.9_dtest/21/testReport/rebuild_test/TestRebuild/simple_rebuild_test/]. This also reproduces fairly easily on my local machine; not sure why the multiplexer didn't hit it. I think the PRed fix is still prone to races.

What is probably happening here is that the second rebuild is completing before the first, so the first rebuild fails while the second succeeds, but we expect the second rebuild to fail.

I think we should modify the test to insert more data (10k or 100k keys) with stress and set {{streaming_throughput_mb_per_sec=1}} similar to bootstrap and replace_address tests, trigger the first rebuild and {{watch_log_for("rebuilding from")}}, and then trigger the second rebuild and expect it to fail to make this more deterministic.

bq. the second rebuild is completing before the first, so the first rebuild fails while the second succeeds

Right, but increasing keys will also increase amount of time for test to complete by large. I will experiment with some numbers.

New PR here: https://github.com/riptano/cassandra-dtest/pull/1158

+1, PR merged after [new multiplexer 92x results|https://cassci.datastax.com/view/Parameterized/job/parameterized_dtest_multiplexer/207/artifact/test_stdout.000.txt] look good.

