This is probably a timeout, since this test [absolutely relies on timing|https://github.com/apache/cassandra-dtest/blame/trunk/jmx_test.py#L201], but we can improve the error a bit:

{noformat}
===Flaky Test Report===

test_compactionstats failed and was not selected for rerun.
        <class 'Exception'>
        Compaction({taskUuid}, {progress} / {total} bytes)@{uuid}(keyspace1, standard1) did not match Major compaction(633887a0-2fd6-11ee-b69d-fd97692b5e82, 0 / 426 bytes)@7ad54392-bcdd-35a6-8417-4e047860b377(system, local)
        [<TracebackEntry /home/drift/cassandra-dtest/jmx_test.py:220>]

===End Flaky Test Report===
{noformat}

||Branch||CI||
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-18709-trunk]|[dtest repeat|https://app.circleci.com/pipelines/github/driftx/cassandra/1155/workflows/96cb8a44-1394-42ce-a1c9-2bf70bd14af0/jobs/39962]

+1 to this patch - [https://github.com/driftx/cassandra-dtest/commit/40a3dd695e6f25b13145adc76dcddf6cf7644396]

Should we raise a bit that timing, too, for slow CI environments?

I think it's trickier than that, the timing is delicately balancing a few things: compaction has started, system keyspaces have completed, and the major has not finished.  It needs to be after the first two, but before it completes.  I think we can better solve that by only compacting the keyspace we care about and checking the logs and a small sleep, like [this|https://github.com/driftx/cassandra-dtest/commit/60878d6eb19da942a091cfcd539b631a0935310a].  Let's see how that holds up:

||Branch||CI||
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-18709-trunk]|[dtest repeat|https://app.circleci.com/pipelines/github/driftx/cassandra/1155/workflows/aa8df94c-6fef-4ea7-93b5-2379e0450b2c/jobs/39964]|

Good call, +1!

Committed, thanks!

[~brandon.williams], It seems like we have more work to do here. Please check this failure:

[https://ci-cassandra.apache.org/job/Cassandra-trunk/1658/testReport/junit/dtest-novnode.jmx_test/TestJMX/test_compactionstats/]

The sleep after compaction starts probably isn't long enough, but let's [quit blindly sleeping|https://github.com/driftx/cassandra-dtest/commit/208ad2e72a93d9af2c5e7ad5228f581f860ac7b4] instead:

||Branch||CI||
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-18709-trunk]|[dtests repeat|https://app.circleci.com/pipelines/github/driftx/cassandra/1163/workflows/0172eded-c157-42c2-898c-63dbfe7e3d2f/jobs/40892]

+1

Committed.

I see this fails in 

https://app.circleci.com/pipelines/github/instaclustr/cassandra/2977/workflows/4b077cb3-fd1c-4606-9c9d-62508240c1a6/jobs/104227/tests

I am also hitting this timeout locally. Maybe we changed the logs recently so it waits on something which is not there?

{code}
ccmlib.node.TimeoutError: 21 Aug 2023 12:01:19 [node1] after 600.59/600 seconds Missing: ['Compacting'] not found in system.log:
 Head: INFO  [main] 2023-08-21 11:50:43,273 YamlConfigura
 Tail: ...2 - Major compaction will not result in a single sstable - repaired and unrepaired data is kept separate and compaction runs per data_file_directory.
self = <jmx_test.TestJMX object at 0x7f03ffc08dd8>
{code}

I hit this on 3.11.

That's probably a timeout, -but in any case it only shares the test with this ticket and is otherwise unrelated- and it looks like 3.11 doesn't support major compaction here.

I realize this test runs on all branches 3.0+, so I suggest we run it a few times in all of them, like 10 times on the repeatable runner after the test is fixed. (not full CI)

it fails both on 3.0 and 3.11.

e.g this is 3.0 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2988/workflows/e0bedf8c-6811-4920-b23b-bdc487bcaf9d/jobs/104754/tests#failed-test-1

it fails pretty consistently, e.g look at this 

https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-18767-3.0

That's from time of 18767 for 3.0, I just took Brandon's patch. It fails there too (not saying it started _to fail there_, all I am saying is that it was apparently a problem back then as well).

Simple fix [here|https://github.com/driftx/cassandra-dtest/commit/705ac61e3c6d3161941d0a78cb8478796587fd9f].  I think something must be wrong with generate.sh and 3.x branches because I don't get the option to run repeated tests, even though I configured it.  But I did start the dtests for [3.11|https://app.circleci.com/pipelines/github/driftx/cassandra/1223/workflows/ed05e503-4afd-4375-8602-6ef1807e2699] and [3.0|https://app.circleci.com/pipelines/github/driftx/cassandra/1222/workflows/822f21c6-223c-40c9-a568-568bd62cbf51].

For completeness, we started to do this slightly different here 

https://github.com/apache/cassandra-dtest/commit/94142323a

bq.  I think something must be wrong with generate.sh and 3.x branches because I don't get the option to run repeated tests

Repeated tests config should be added manually for Python DTests. Changed tests are not detected automatically as Python DTests do not even reside in the C* repo.

You should be able to run something like this to generate the config:

{code:java}
./generate.sh -p -e REPEATED_DTESTS=jmx_test.py::TestJMX::test_compactionstats
{code}


I knew that, I just forgot to define any tests apparently.  Here's [3.11|https://app.circleci.com/pipelines/github/driftx/cassandra/1225/workflows/8f150648-a29b-454a-a585-9b261e8b253c] and [3.0|https://app.circleci.com/pipelines/github/driftx/cassandra/1224/workflows/eb2ff487-27b6-40a6-ab36-5c2771fcc18e].  Given the nature of the [fix|https://github.com/driftx/cassandra-dtest/commit/705ac61e3c6d3161941d0a78cb8478796587fd9f] I think we can skip the other branches.

+1 on green CI (still running)

3.0 needs to check the debug.log, here's [3.0|https://app.circleci.com/pipelines/github/driftx/cassandra/1226/workflows/4bfadc64-8537-4644-bb90-1ee42a23802d] and [3.11|https://app.circleci.com/pipelines/github/driftx/cassandra/1227/workflows/64e89fc2-3ed2-4f8f-b7a9-946e91806730] again.  I'll check back and commit if those are good.

Everything passed, [committed|https://github.com/apache/cassandra-dtest/commit/738d5de93def153338003a26e304a77463a7fd2a].

[~brandon.williams] thank you for your prompt resolution of this issue!

