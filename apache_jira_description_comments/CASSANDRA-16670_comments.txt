[~edimitrova] I see you played with this test. Probably this is the one you recently referred to as having to have to move to the 'long' section. It does almost timeout for me locally on my laptop and the stdout seems to be around the 10m timeout for the long tests which makes sense.

I have put up a proposal to raise only the timeout of this class but you know this test better than me. Wdyt?

Same old Jenkins bugs.... when you open everything has passed, no failures. :( 
{quote}Probably this is the one you recently referred to as having to have to move to the 'long' section
{quote}
Yes, this is one of the two unit test classes I moved. The reason was that they are parameterized and there is no easy way to raise the timeout only per class. Check also this [discussion|https://the-asf.slack.com/archives/CK23JSY2K/p1620402158398900] for reference. 

So I guess if this is again borderline (wasn't the case when I ran it in a loop locally :( ), we should split the class instead of being parameterized. I can look at it tomorrow with fresh eyes as it is already 10.30 pm now.

^my PR raises the timeout only for this class. Is that ok or am I misunderstanding sthg?

EDIT: I mean: you were adding a rule that affects at the method level. I am adding a _class_ rule that affects the whole class. I tested by lowering to 1m and see it fail so it is effective indeed.

I added a fix for {{InsertUpdateIfConditionTest}} also for convenience. With that one we can either move to the {{long}} section or raise timeout. I have mixed thoughts about it as it's fast locally so I don't have a strong preference...

When you run with the rules locally it works fine, but did you try in a full CI? From what I remember and I read in the mentioned discussion, the rules are overwritten by the build.xml timeouts when you run full CI.

We can verify this again by pushing a dev branch to Jenkins with low build.xml timeout and higher timeout on a class level.

I will push a run later, thanks

About _InsertUpdateIfConditionTest,_ I haven't looked at it and its timings but if it exceeds the unit tests' timeout significantly and qualifies for long test, without exceeding also the long tests' timeouts, I would move it to a long test.  If it needs just a bit of time, and not frequently - I would add a bit of time probably on a class level.

My reasoning comes from the point that if it exceeds the timeout just a bit - we don't have to raise its timeout to a long test and wait for its timeouts too long on a failure. 

Very low timeout in build.xml using your patch and it timed out:

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/856/workflows/57238d25-d581-4117-872a-422c9f785b6d/jobs/5096/tests

icwym now. {{JStackJUnitTask}} sets the watchdog to the timeout in the build.xml. So despite the class rule working whichever timeout is hit first, the lowest one, wins. Works both locally and on CI, I just hadn't happened to test the unlucky path.

Given we can confirm we can't raise timeout at a class level the only solution I can think of is to move them to the 'long' section I am afraid. They still run per commit on CI. wdyt? I don't see other options...

Moving to long tests will work fine for  _InsertUpdateIfConditionTest_ but _ViewComplexTest_ is already moved to be a long test. So for that one I guess we will have no choice but to split the class as also David mentioned in the chat. 

[~e.dimitrova] I just realized we can't happily move things to the 'long' section. CI works under std, cdc and compression variants but not for the 'long' section, moving a test to long removes testing coverage so:
A. InsertUpdateIfConditionTest has been split into 3 themes: std, collections and statics and it does _not_ move to 'long' section
B. ViewComplexTest has been split into 3 themes: std, deletions and TTL that should be good enough for the 'long' section.

The q I have now is double: first wdyt? and secondly is whether we should split ViewComplexTest further so it can go back away from 'long' to the std test path bc of test coverage reasons?

I agree with you, we need both test classes back away from 'long', unfortunately. :-( 
While on this, can you also do the same for _ViewFilteringTest_, please? As It seems I didn't have to move that one too as part of CASSANDRA-16613.
Thank you!

Done. CI lgtm but I am afraid we won't know if we're flying under the timeouts until we merge and we get a few runs in jenkins. I have all splits around the 2m mark or less.

Looks good, I left only a few small questions/suggestions.
{quote}we won't know if we're flying under the timeouts until we merge and we get a few runs in jenkins.
{quote}
I think that if a back-of-the-envelope calculation is done, that should help. What I mean - we can check the timings of the splits and the timeout for the long tests. The tests were not exceeding with a lot the timeout. We check the ratio between the splits and the whole long test and the timeout for testsome target and that should give us a good idea where we are. Then CI run of course, but that rough calculation should help to establish a good baseline I think. Worst case scenario we can loop them now when we have the multiplexer :) 

Last typo review comment addressed.

As soon as you did the timing calculation and we have green CI, I think this is ready for commit. Thank you 

CI is green already, please find links in the PR. Back of the envelope calculation is how splits were generated already. But there is big divergence between local, jenkins  devbranch and jenkins commit CI runs. So I don't think we can go much further i.e.. timeout is around 10m and there are 3 to 4 splits which should be under the new 4m timeout...

Posting CI runs here for general visibility CI [j11,|https://app.circleci.com/pipelines/github/bereng/cassandra/319/workflows/1dd551f5-e09e-42ab-bdf9-5d821e360d0d]CI [j8.|https://app.circleci.com/pipelines/github/bereng/cassandra/319/workflows/60191f62-f2df-493e-aaef-9995c52e89b4]

Ready for commit, thank you

PRs rebased and new CIs on it's way once circle gets back online as it's non responsive for me atm. Will commit asap circle is back.

Holding off commit due to massive CI failures on these tests for unknown reasons yet.

Thanks [~bereng].

I took a quick look and I am wondering whether there was some drivers update maybe?

Those were not failing up to now and now I see this:
{code:java}
Error Message
An unexpected protocol error occurred on host localhost/127.0.0.1:46739. This is a bug in this library, please report: Must not send frame with WARNING flag for native protocol version < 4

Stacktrace
com.datastax.driver.core.exceptions.ProtocolError: An unexpected protocol error occurred on host localhost/127.0.0.1:46739. This is a bug in this library, please report: Must not send frame with WARNING flag for native protocol version < 4 at com.datastax.driver.core.exceptions.ProtocolError.copy(ProtocolError.java:66) at com.datastax.driver.core.exceptions.ProtocolError.copy(ProtocolError.java:27) at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:35) at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:293) at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:58) at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:45) at org.apache.cassandra.cql3.CQLTester.executeNet(CQLTester.java:956) at org.apache.cassandra.cql3.ViewComplexTest.testBaseTTLWithSameTimestampTest(ViewComplexTest.java:641) Caused by: com.datastax.driver.core.exceptions.ProtocolError: An unexpected protocol error occurred on host localhost/127.0.0.1:46739. This is a bug in this library, please report: Must not send frame with WARNING flag for native protocol version < 4
{code}
 //CC [~aholmber] and [~samt] 

All back to normal. Some docker changes in the CI scripts were the culprit although I can't tell how the schema and driver problems came about. In any case I'll be committing this today.

ViewComplexTest seems to be the only split that timeouts out sometimes, so I almost go it right. It might need another split #justfyi

Thank you. I reopened the ticket so we can fix those. I can also look at this if you don't have the time

Not  a big fan of reopening, I prefer a 'linear' hsitory. I'll open a new quick ticket thx.

[~e.dimitrova] Were you able to find why the errors that you mentioned in the [comment|https://issues.apache.org/jira/browse/CASSANDRA-16670?focusedCommentId=17355084&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17355084] happened? We see a similar errors where the driver version is 4.13.0 and the server version is 4.0.5. 

Hi [~jaid] , this was more than a year ago and unfortunately I do not recall the details and the logs are gone but there were a bunch of changes happening pre-release and I remember it was something temporary. We haven't seen it anymore in our test environment, as far as I can see.

I think I noticed in ASF Slack you figured out your issue?

 

thanks for your response. Yes I figured out the issue by looking at the error message but my only confusion was why it started all of a sudden. The server was upgraded almost 10 days back but the error started only 2 days ago.

I'm not going to be of much help here but I remember those driver errors just came and went away. Somebody must have fixed them somewhere else as the fix for this ticket was completely unrelated to drivers.

