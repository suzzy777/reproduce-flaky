This appears to be a flaky issue. From the cluster logs, it was not clear what the cause of failure was.

Since the file is not generated on the fly, it can't be a case of failure due to errors while generating the file. Additionally, the code path to determine invalid metadata is deterministic and should be hit as long as the file contains fewer values than expected. In the cluster logs, the test failed because all fragments completed successfully.

I tried to recreate the error by running the test in an infinite loop with the same build parameters. At the end of a 24 hour run, it completed over 10000 successful iterations without any failure. We can wait and see if the issue is reproduced in the next week or so. If not, we can close it until it reappears.

https://jenkins.impala.io/job/ubuntu-16.04-from-scratch/2822/testReport/junit/query_test.test_scanners/TestParquet/test_parquet_exec_option____batch_size___0___num_nodes___0___disable_codegen_rows_threshold___0___disable_codegen___False___abort_on_error___1___debug_action___None___exec_single_node_rows_threshold___0____table_format__parquet_none_/ seems similar too. We expect an error that doesn't show up.

I think it is closer to IMPALA-7378. In this case, the error was logged and the query status was set appropriately. Here is the line in the log:
{code:java}
ee_tests/impalad.ip-172-31-28-177.ubuntu.log.INFO.20180730-233654.20124:I0731 00:52:07.904188 105583 query-state.cc:403] Instance completed. instance_id=b04c3db85fca19f5:2927aa8300000001 #in-flight=13 status=PARQUET_BAD_VERSION_NUMBER: File 'hdfs://localhost:20500/test-warehouse/bad_magic_number_parquet/bad_magic_number.parquet' has an invalid version number: XXXX
{code}
However, I can't find all the logs for this query. (Specifically the coordinator logs.)

I will look at possible race conditions where an error status might be set without propagating it back to the coordinator.

Saw what looks like another occurrence of a lost scanner error in orc-type-checks.test " table column DECIMAL(2,0) is map to column decimal(10,5) in ORC file": https://jenkins.impala.io/job/ubuntu-16.04-from-scratch/2892/testReport/junit/query_test.test_scanners/TestOrc/test_type_conversions_exec_option____batch_size___0___num_nodes___0___disable_codegen_rows_threshold___0___disable_codegen___False___abort_on_error___1___debug_action___None___exec_single_node_rows_threshold___0____table_format__orc_def_block_/

Commit 2e5df138aaf4354fd4ada69b627842c34fef2e05 in impala's branch refs/heads/master from poojanilangekar
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=2e5df13 ]

IMPALA-7335/IMPALA-7418: Add logs to HdfsScanNode to debug the issues

IMPALA-7335 and IMPALA-7418 are failing some builds on jenkins.
However, there is no deterministic method to reproduce them
locally and hence it is difficult to figure out the cause of the
failure. From the existing logs, it appears that the status
generated by HdfsScanNode::ProcessSplit() is lost. This log
would help determine the condition when the failures occur.

Change-Id: I68698c90031edc6ee8c31e9ce3d52dade9d8f6f1
Reviewed-on: http://gerrit.cloudera.org:8080/11174
Reviewed-by: Bikramjeet Vig <bikramjeet.vig@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Commit 2e5df138aaf4354fd4ada69b627842c34fef2e05 in impala's branch refs/heads/master from poojanilangekar
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=2e5df13 ]

IMPALA-7335/IMPALA-7418: Add logs to HdfsScanNode to debug the issues

IMPALA-7335 and IMPALA-7418 are failing some builds on jenkins.
However, there is no deterministic method to reproduce them
locally and hence it is difficult to figure out the cause of the
failure. From the existing logs, it appears that the status
generated by HdfsScanNode::ProcessSplit() is lost. This log
would help determine the condition when the failures occur.

Change-Id: I68698c90031edc6ee8c31e9ce3d52dade9d8f6f1
Reviewed-on: http://gerrit.cloudera.org:8080/11174
Reviewed-by: Bikramjeet Vig <bikramjeet.vig@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Saw this again. Let me see if your log message got in there. :)

Here is the log corresponding to the query failure:

 
{code:java}
I0816 00:20:01.948418  6716 hdfs-scan-node.cc:336] Non-ok status returned by ProcessSplit = Error converting column: 0 to TINYINT for Scan node (id=0, status_ = ok, done_ = 1)
{code}
So the theory discussed earlier was partially correct. The done flag was already set to true when ProcessSplit() returned. (This is because the RangeComplete() function is called from ProcessSplit() even when the Scanner runs into an error). In other cases where the test didn't fail, the done_ flag was set to false. *However, the status_ is not set to canceled.* (I still need to understand why).

There is  another potential issue with the HdfsScanNode. In the case the scanners encounter errors, they still enqueue the RowBatch into the batch_queue_ (which seems alright). However, HdfsScanNode::GetNextInternal() calls the GetBatch() function and in case it acquires a non null RowBatch, it returns Status::OK() without checking the status_ of the ScanNode. A caveat here is that even if the function were to check the status_, it could still see an Ok status because the scanner thread updates the status only after ProcessSplit() returns. (Yet it seems semantically incorrect to return an OK status without inspecting the status_ variable.)

I think delaying the calls to RangeComplete() alone might not fix the problem because GetNextInternal() could still return an OK status for a scan node which has already encountered an error.

[~tarmstrong] [~bikramjeet.vig] What would be a clean approach to handle this issue? 

 

I would probably look at the pieces of state currently used for synchronisation between threads and figure out what the possible states and state transitions are currently and what needs to happen during the state transitions (e.g. a thread blocked on batch_queue_ needs to be woken up on some transitions). Then we can probably figure out a simpler set of states without the incorrect transition that is currently possible. It looks like the relevant state is:
* HdfsScanNodeBase::progress_
* HdfsScanNode::done_
* HdfsScanNode::status_
* HdfsScanNode::thread_state_::batch_queue_::shutdown_

We should also look at KuduScanNode since it might have the same bug.

From my understanding of the HdfsScanNode, here are the set of transitions/dependencies among the state variables:
 # HdfsScanNode::thread_state_::batch_queue_::shutdown_ is only modified once (false -> true) in the HdfsScanNode::SetDoneInternal(). When the value is changed, it is guaranteed that done_ is set to true and reader_context_ is cancelled. It is read in GetBatch() and AddMaterializedRowBatch() functions. *It acts as a point of synchronization for GetNext() since it invokes GetNextInternal() which invokes GetBatch(). If the batch returned is NULL, the status_ is returned to the FragmentInstanceState.* 
 # HdfsScanNodeBase::progress_ is initialized in HdfsScanNode::Open() and decremented each time HdfsScanner::Close() is called. It is only decremented after initialization. *When ScannerThread() or GetNext() functions view its value as 0**, they invoke SetDone() which transitions the done_ flag, cancels all ranges in the reader_context_ and sets the shutdown_ flag to true.*  It is used in ThreadTokenAvailableCb to avoid spawning excessive threads, however this doesn't lead to any non-trivial failure. The extra thread spawned can simply terminate.
 # HdfsScanNode::status_ It transitions in two places. ThreadTokenAvailableCb() [when thread creation fails] and ScannerThread() [when ProcessSplit() returns an error]. *In both cases, modification of status_ is followed by a call to SetDoneInternal().*
 # HdfsScanNode::done_ It transitions only via SetDoneInternal.() However, it is used by multiple threads to signal the completion of the Scan. (It can be due to completing all ranges, encountering a non-recoverable error or reaching the scan node's limit).

Here are the dependencies among the transitions.

progress_  --> done_ --> shutdown_ & status_ --> done_ --> shutdown_

Additionally, progress_ -> status_ is a dependency which is not deterministically followed. One approach to fix the race would be to update the status_ and invoke SetDoneInternal() in HdfsScanNode::ProcessSplit() before invoking scanner>Close() in case of failures (a similar flow was a part of the code before IMPALA-2667). This will ensure that the first non-ok status always updates the status_ before done_ and shutdown_ flags are set.

[~poojanilangekar] setting status_ before calling Close() in the event of an error does indeed seem like the minimal fix for this. It does feel like this could be cleaned up a bit to make it easier to understand but we could go ahead with that for now. It would be good to add a regression test for this (e.g. using the DebugAction mechanism to add a delay at the right point to induce the race if it's reasonably possible to do so.

I have not been able to reproduce this locally. I think reproducing this would require a specific scheduling of 3 threads (two scanner threads and the thread which calls GetNext).

The threads need to be scheduled in the following order:
 # ScannerThread A hits an error while scanning, closes the scan range (thereby decrementing the progress_) and is swapped out before it returns from ProcessSplit(). 
 # ScannerThread B isn't assigned a ScanRange, checks if the progress_ is done, calls SetDoneInternal() and set shutdown_ flag to true. 
 # The FragmentStateInstance thread calls GetNext, notices that the shutdown_ thread is set and returns the status_ value. 

The issue is that steps 2 and 3 need to complete before ScannerThread A is woken up. Is there a way to deterministically cause this?

Commit ad3e1c94abc9a4c9bf37b5c42dafd149537375dc in impala's branch refs/heads/master from poojanilangekar
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=ad3e1c9 ]

IMPALA-7335: Fix a race in HdfsScanNode

This change fixes the race between the done_ flag and the status_
variable in the HdfsScanNode. Previously, a scanner thread would
mark its scan range as complete even if it ran into an error.
Another scanner thread could notice that all scan ranges have
completed and set the done_ flag before the status_ variable is
updated with the non-ok status. This caused the main thread to
return an ok status despite the scanner thread running into an
error. This change ensures  that when a scanner thread runs into
an error, it updates the status_ before marking its scan range as
complete.

Testing: Ran core tests.
Since there is no deterministic method to reproduce the
race, this change does not include any regression tests.

Additionally, this change also fixes IMPALA-7430 by removing the
logs added to debug this race.

Change-Id: Id470818846a5c69ad28a6ff695069702982aa793
Reviewed-on: http://gerrit.cloudera.org:8080/11337
Reviewed-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Commit 449fe73d2145bd22f0f857623c3652a097f06d73 in impala's branch refs/heads/master from [~tarmstrong@cloudera.com]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=449fe73 ]

IMPALA-7662: fix error race when scanner open fails

This is very similar to IMPALA-7335, except happens
when 'progress_' is incremented in the call chain
HdfsScanNode::ProcessSplit
-> HdfsScanNodeBase::CreateAndOpenScanner()
-> HdfsScanner::Close()

The fix required restructuring the code so that
SetDoneInternal() is called with the error *before*
HdfsScanner::Close(). This required a refactoring because
HdfsScanNodeBase doesn't actually know about SetDoneInternal().

My fix is to put the common logic between HdfsScanNode and
HdfsScanNodeMt into a helper in HdfsScanNodeBase, then in
HdfsScanNode, make sure to call SetDoneInternal() before
closing the scanner.

I also reworked HdfsScanNode::ProcessSplit() to handle error propagation
internally. I think the joint responsibility between ProcessSplit() and
its caller for handling errors made things harder than necessary.

Testing:
Added a debug action and test that reproduced the race before the fix.

Change-Id: I45a61210ca7d057b048c77d9f2f2695ec450f19b
Reviewed-on: http://gerrit.cloudera.org:8080/11596
Reviewed-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


