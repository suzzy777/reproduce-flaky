[~ifesdjeen] can you look into this?

||Branch||Apache CI||
|[CASSANDRA-16229|https://github.com/driftx/cassandra/tree/CASSANDRA-16229]|[ci-cassandra|https://ci-cassandra.apache.org/job/Cassandra-devbranch/309/]|

The test was manually removing endpoints from the gossiper and doing endpoint cleanup incorrectly - after notifying StorageService about a change in state, it should handle all this as necessary.  Doing it manually creates a race somewhere.

Thanks for the patch! Agree with your analysis. The manual endpoint removal does not seem to be necessary, and could cause race with handling the left status. 

A nit for the test case is to add an explicit check after removing node3 from the ring. It checks that from the perspective of the remaining nodes, there should be just 2 nodes. 
{code:java}
cluster.run(GossipHelper.removeFromRing(cluster.get(3)), 1, 2);
// assert that node1 and node2 only sees 2 nodes in the ring. 
cluster.run(inst -> inst.runsOnInstance(() -> {
    Assert.assertEquals("There should be 2 remaining nodes in ring", 
                        2, StorageService.instance.effectiveOwnershipWithPort(KEYSPACE).size());
}), 1, 2);
{code}

Thanks [~brandon.williams]. The patch looks good to me.

I second what [~yifanc] proposed.

Committed w/nit test added, thanks!

