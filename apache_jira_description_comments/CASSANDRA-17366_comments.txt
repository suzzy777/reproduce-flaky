-What I know so far is that these were not flaky on commit, as they pass fine there but later on begin failing.  Locating that exact point is an ongoing bisect.-

That is not true, this is actually just more difficult to bisect than I originally gauged.  [Here|https://app.circleci.com/pipelines/github/driftx/cassandra/424/workflows/3ce471bc-244d-486e-aa62-cca36e85517b/jobs/4996/tests#failed-test-0] is a circle run near the time these tests were committed illustrating the failure.

Confirming that is [this|https://app.circleci.com/pipelines/github/driftx/cassandra/425/workflows/d4949949-441a-47b4-b97d-12fba3574c98/jobs/5003/tests#failed-test-0] failure on 4.0 also.

I've added a longer wait for nodes to come up [here|https://github.com/driftx/cassandra-dtest/tree/CASSANDRA-17366] and also raise an error when we exhaust the time without all the nodes returning.  Repeated runs against it: [4.0|https://app.circleci.com/pipelines/github/driftx/cassandra/427/workflows/d994077f-ec7e-424e-b7f4-cb8ef2649a0c/jobs/5006] , [trunk|https://app.circleci.com/pipelines/github/driftx/cassandra/426/workflows/937969fa-605c-4970-b3d9-4913e935d075/jobs/5007]

Thanks for checking back for when it was committed. Our jobs to run in a loop the tests were added after that time, this bisect just shows me one more time how useful and important is for us to be running new tests in a loop before commit. I am glad they were added.

So on commit and now it was failing with the same frequency, right? But I saw you ran it only with 4.0 to confirm that. 

I just ran it with the same circle config with trunk  [here|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1499/workflows/2b7e411a-78a2-44f8-9d2c-c6b645df953c/jobs/9754/parallel-runs/2?filterBy=ALL]

It hasn't finished yet, there are 3 failures... if there are more maybe we need to investigate what made it fail more often? WDYT?

I've been hitting this today while working on a different issue with the in-jvm BootstrapTest failing on cluster close (CASSANDRA-17524).  I can hit it on my MacBook after many runs and have instrumented the MigrationCoordinator with a bit more debug output. Will report back what I find.

Got to the bottom of the in-jvm dtest version and I don't think it will impact python dtests.  The in-jvm dtest override the instance uptime calculation for {{MigrationManager}}, but not for {{MigrationCoordinator}} so it was not scheduling the schema pulls immediately as the test thought instances had been up for > 60s (which was not true).

Thanks for the update, [~jonmeredith].

As for this test, it turns out my patch actually doesn't do anything, since we make sure the nodes are all running [right afterward|https://github.com/apache/cassandra-dtest/blob/trunk/gossip_test.py#L98].  I'll keep digging on this since that leaves us with increasing the schema delay, which at 30s seems like it should be long enough already.

These tests set up and then stop a cluster, subsequently starting it with some combination of seeds, and then nodes in parallel.  The problem is that the setup doesn't guarantee it will wait long enough for the cluster to be completely established, though C* is fast enough to do so anyway, _almost_ all of the time.  To ensure the setup is complete before shutting down, the nodes should wait for the CQL interface to become available after the initial startup. [This dtest branch|https://github.com/driftx/cassandra-dtest/tree/CASSANDRA-17366] does that, and here's 400 runs on [4.0|https://app.circleci.com/pipelines/github/driftx/cassandra/438/workflows/df01fde1-1ff1-4007-bdc2-a9de8e358a16/jobs/5145] and [trunk|https://app.circleci.com/pipelines/github/driftx/cassandra/439/workflows/c00617e8-904c-44d7-9f4f-de565e4878cf/jobs/5143].

+1, thanks

Committed, thanks for the review.

