I haven't seen this one recently, and It was seen before very often. Though trying to reproduce test_failed_snitch_update_property_file_snitch on current 5.0 and the commit where I saw it, I cannot do it running it hundreds of times (500).

The Jenkins link is expired. I tried to find the run in nightlies to check in the logs what happened - build 16666 is missing.

I will try with the other test mentioned. 

I did not manage to reproduce the issue with the second test in 500 runs on top of a commit from the time when the ticket was raised.

I will try to dig more into Butler for other examples and hopefully builds we can find still in nightlies. 

I cannot do any grep for nightlies; it just hangs, or I can find a failing build with this error in the past 30 (in Butler in Jenkins), and I cannot reproduce it. 

[~mck], [~Bereng], [~brandonwilliams] anyone of you with a recipe for dealing with nightlies? It just hangs for me. 

A long time ago, [~mck] gave this on another ticket:
{code:java}
find Cassandra-trunk-dtest*/label=cassandra,split=* -name nosetests.xml -exec grep -q ${DTEST_CLASS} {} \; -exec grep -l "name=\"${DTEST_NAME}\" time=\"[0-9.]*\"><error message" {} \;
{code}
I tried it with the two test names mentioned in this ticket on trunk, but I got "no matches found: Cassandra-trunk-dtest*/label=cassandra,split=*". Perhaps something changed in time. 

What do I do wrong? Any ideas?

If you check the trunk runs id and the trunk-dtest runs you'll see they are 211 apart more or less. Go into nightlies trunk-dtest [folder|https://nightlies.apache.org/cassandra/trunk/Cassandra-trunk-dtest-novnode/] around id 1455, download the previous/later 5 folders and with some luck you'll have the logs you need. My 2cts

Thanks [~Bereng] , it was valuable advice. I actually realized I can go to this [link|https://nightlies.apache.org/cassandra/ci-cassandra.apache.org/job/Cassandra-trunk-dtest/] starting from [ci-cassandra.apache.org/|https://nightlies.apache.org/cassandra/ci-cassandra.apache.org/] on the home page of nightlies. Even if the Jenkins exact failure page is expired, I was able to find [this page|https://nightlies.apache.org/cassandra/ci-cassandra.apache.org/job/Cassandra-trunk-dtest/1451/] where we see the same test name failure we discuss in the ticket and the dtests run being associated with build 1666. Then I go to the list you pointed to me and find the logs for tests run 1451. 

Now I need to know the split which was not visible on the [mentioned page|https://nightlies.apache.org/cassandra/ci-cassandra.apache.org/job/Cassandra-trunk-dtest/1451/], but clicking on the expired Jenkins page I can see in the [URL|https://ci-cassandra.apache.org/job/Cassandra-trunk-dtest/1451/jdk=jdk_11_latest,label=cassandra-dtest,split=28/testReport/junit/dtest.materialized_views_test/TestMaterializedViews/test_gcgs_validation/] - split28. :)  

HA! Actually the URL to the expired Jenkins page also points which is the test run we need - 1451, not only the split! We can use that in the future. The tricky part is that URL is available if we go through the dtest job page, not through the main page of failures - the one pasted in the ticket. 

Yep those are the breadcrumbs to follow :-)

We can add to our Build Lead page that when people open a ticket for a test failure based on Jenkins - they should add the build number for the specific test suite job, not the main cassandra build and split numbers. This will make it easier to investigate later, using nightlies logs if we cannot reproduce.  

These are the logs we need for this ticket - [https://nightlies.apache.org/cassandra/trunk/Cassandra-trunk-dtest/1451/Cassandra-trunk-dtest/jdk=jdk_11_latest,label=cassandra-dtest,split=28/]

This is for test_gcgs_validation failure

I also saw today [https://ci-cassandra.apache.org/job/Cassandra-trunk/1708/testReport/dtest.materialized_views_test/TestMaterializedViews/test_drop_table/]  
{code:java}
Error Message
failed on teardown with "Failed: Unexpected error found in node logs (see stdout for full details). Errors: [[node2] 'ERROR [PendingRangeCalculator:1] 2023-09-15 13:30:52,532 JVMStabilityInspector.java:70 - Exception in thread Thread[PendingRangeCalculator:1,5,PendingRangeCalculator]\njava.lang.AssertionError: Unknown keyspace system_traces\n\tat org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:324)\n\tat org.apache.cassandra.db.Keyspace.lambda$open$0(Keyspace.java:162)\n\tat org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:105)\n\tat org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:251)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:162)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:151)\n\tat org.apache.cassandra.service.PendingRangeCalculatorService.lambda$new$1(PendingRangeCalculatorService.java:58)\n\tat org.apache.cassandra.concurrent.SingleThreadExecutorPlus$AtLeastOnce.run(SingleThreadExecutorPlus.java:60)\n\tat org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)']"

Stacktrace
Unexpected error found in node logs (see stdout for full details). Errors: [[node2] 'ERROR [PendingRangeCalculator:1] 2023-09-15 13:30:52,532 JVMStabilityInspector.java:70 - Exception in thread Thread[PendingRangeCalculator:1,5,PendingRangeCalculator]\njava.lang.AssertionError: Unknown keyspace system_traces\n\tat org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:324)\n\tat org.apache.cassandra.db.Keyspace.lambda$open$0(Keyspace.java:162)\n\tat org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:105)\n\tat org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:251)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:162)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:151)\n\tat org.apache.cassandra.service.PendingRangeCalculatorService.lambda$new$1(PendingRangeCalculatorService.java:58)\n\tat org.apache.cassandra.concurrent.SingleThreadExecutorPlus$AtLeastOnce.run(SingleThreadExecutorPlus.java:60)\n\tat org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)']
{code}
 

So from the logs from nightlies we can see the following race is happening on startup of node 3:
{code:java}
INFO  [MigrationStage:1] 2023-08-10 18:39:32,992 DefaultSchemaUpdateHandler.java:221 - Applying schema change due to received mutations: SchemaTransformationResult{2e847089-a5be-3705-a1aa-4a7f9b3ebe36 --> d03783d7-b468-3c1a-82f1-8e30b2edde8b, diff=KeyspacesDiff{created=[KeyspaceMetadata{name=system_auth, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[system_auth.cidr_groups, system_auth.cidr_permissions, system_auth.identity_to_role, system_auth.network_permissions, system_auth.resource_role_permissons_index, system_auth.role_members, system_auth.role_permissions, system_auth.roles], views=[], functions=[], types=[]}], dropped=[], altered=[]}}
DEBUG [MigrationStage:1] 2023-08-10 18:39:32,993 DefaultSchemaUpdateHandler.java:259 - Schema updated: SchemaTransformationResult{2e847089-a5be-3705-a1aa-4a7f9b3ebe36 --> d03783d7-b468-3c1a-82f1-8e30b2edde8b, diff=KeyspacesDiff{created=[KeyspaceMetadata{name=system_auth, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[system_auth.cidr_groups, system_auth.cidr_permissions, system_auth.identity_to_role, system_auth.network_permissions, system_auth.resource_role_permissons_index, system_auth.role_members, system_auth.role_permissions, system_auth.roles], views=[], functions=[], types=[]}], dropped=[], altered=[]}}
DEBUG [PendingRangeCalculator:1] 2023-08-10 18:39:32,997 TokenMetadata.java:876 - Starting pending range calculation for system_traces
DEBUG [PendingRangeCalculator:1] 2023-08-10 18:39:32,997 TokenMetadata.java:881 - Pending range calculation for system_traces completed (took: 0ms)
DEBUG [PendingRangeCalculator:1] 2023-08-10 18:39:32,997 TokenMetadata.java:876 - Starting pending range calculation for system_distributed
DEBUG [PendingRangeCalculator:1] 2023-08-10 18:39:32,997 TokenMetadata.java:881 - Pending range calculation for system_distributed completed (took: 0ms)
INFO  [MigrationStage:1] 2023-08-10 18:39:32,998 Keyspace.java:366 - Creating replication strategy system_auth params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}
DEBUG [MigrationStage:1] 2023-08-10 18:39:32,998 Keyspace.java:370 - New replication settings for keyspace system_auth - invalidating disk boundary caches
ERROR [PendingRangeCalculator:1] 2023-08-10 18:39:33,000 JVMStabilityInspector.java:70 - Exception in thread Thread[PendingRangeCalculator:1,5,PendingRangeCalculator]
java.lang.AssertionError: Unknown keyspace system_auth
    at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:324)
    at org.apache.cassandra.db.Keyspace.lambda$open$0(Keyspace.java:162)
    at org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:105)
    at org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:251)
    at org.apache.cassandra.db.Keyspace.open(Keyspace.java:162)
    at org.apache.cassandra.db.Keyspace.open(Keyspace.java:151)
    at org.apache.cassandra.service.PendingRangeCalculatorService.lambda$new$1(PendingRangeCalculatorService.java:58)
    at org.apache.cassandra.concurrent.SingleThreadExecutorPlus$AtLeastOnce.run(SingleThreadExecutorPlus.java:60)
    at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:829)
DEBUG [GossipStage:1] 2023-08-10 18:39:33,002 MigrationCoordinator.java:428 - Reported schema d03783d7-b468-3c1a-82f1-8e30b2edde8b at endpoint /127.0.0.1:7000
INFO  [GossipStage:1] 2023-08-10 18:39:33,006 TokenMetadata.java:539 - Updating topology for /127.0.0.1:7000
INFO  [MigrationStage:1] 2023-08-10 18:39:33,008 ColumnFamilyStore.java:493 - Initializing system_auth.cidr_groups
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,011 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system_auth.cidr_groups
INFO  [GossipStage:1] 2023-08-10 18:39:33,010 TokenMetadata.java:539 - Updating topology for /127.0.0.1:7000
DEBUG [PendingRangeCalculator:1] 2023-08-10 18:39:33,026 HeapUtils.java:133 - Heap dump creation on uncaught exceptions is disabled.
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,034 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 5)
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,034 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data0}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data1}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data2}], positions=null, ringVersion=5, directoriesVersion=0} for system_auth.cidr_groups
INFO  [GossipStage:1] 2023-08-10 18:39:33,034 Gossiper.java:1467 - Node /127.0.0.2:7000 is now part of the cluster
DEBUG [GossipStage:1] 2023-08-10 18:39:33,038 StorageService.java:3187 - Node /127.0.0.2:7000 state NORMAL, token [-149492677599243657, -2026557007112229685, -3846561308092176084, -4910902297865239480, -6169322889008892514, -8291017789011127624, 1019462245579770721, 2068586045908961809, 3004499218700477229, 3420136878272117109, 4631817224944098254, 5029140077292811045, 6287560793238254133, 7321861439913658500, 7748147941194787890, 9098349767769465773]
INFO  [MigrationStage:1] 2023-08-10 18:39:33,055 ColumnFamilyStore.java:493 - Initializing system_auth.cidr_permissions
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,058 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system_auth.cidr_permissions
DEBUG [GossipStage:1] 2023-08-10 18:39:33,060 StorageService.java:3097 - New node /127.0.0.2:7000 at token -149492677599243657
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,061 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 5)
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,061 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data0}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data1}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data2}], positions=null, ringVersion=5, directoriesVersion=0} for system_auth.cidr_permissions
DEBUG [GossipStage:1] 2023-08-10 18:39:33,061 StorageService.java:3097 - New node /127.0.0.2:7000 at token -2026557007112229685
DEBUG [GossipStage:1] 2023-08-10 18:39:33,062 StorageService.java:3097 - New node /127.0.0.2:7000 at token -3846561308092176084
DEBUG [GossipStage:1] 2023-08-10 18:39:33,062 StorageService.java:3097 - New node /127.0.0.2:7000 at token -4910902297865239480
DEBUG [GossipStage:1] 2023-08-10 18:39:33,063 StorageService.java:3097 - New node /127.0.0.2:7000 at token -6169322889008892514
DEBUG [GossipStage:1] 2023-08-10 18:39:33,065 StorageService.java:3097 - New node /127.0.0.2:7000 at token -8291017789011127624
DEBUG [GossipStage:1] 2023-08-10 18:39:33,065 StorageService.java:3097 - New node /127.0.0.2:7000 at token 1019462245579770721
DEBUG [GossipStage:1] 2023-08-10 18:39:33,066 StorageService.java:3097 - New node /127.0.0.2:7000 at token 2068586045908961809
DEBUG [GossipStage:1] 2023-08-10 18:39:33,066 StorageService.java:3097 - New node /127.0.0.2:7000 at token 3004499218700477229
DEBUG [GossipStage:1] 2023-08-10 18:39:33,066 StorageService.java:3097 - New node /127.0.0.2:7000 at token 3420136878272117109
DEBUG [GossipStage:1] 2023-08-10 18:39:33,066 StorageService.java:3097 - New node /127.0.0.2:7000 at token 4631817224944098254
DEBUG [GossipStage:1] 2023-08-10 18:39:33,066 StorageService.java:3097 - New node /127.0.0.2:7000 at token 5029140077292811045
DEBUG [GossipStage:1] 2023-08-10 18:39:33,066 StorageService.java:3097 - New node /127.0.0.2:7000 at token 6287560793238254133
DEBUG [GossipStage:1] 2023-08-10 18:39:33,066 StorageService.java:3097 - New node /127.0.0.2:7000 at token 7321861439913658500
DEBUG [GossipStage:1] 2023-08-10 18:39:33,066 StorageService.java:3097 - New node /127.0.0.2:7000 at token 7748147941194787890
DEBUG [GossipStage:1] 2023-08-10 18:39:33,066 StorageService.java:3097 - New node /127.0.0.2:7000 at token 9098349767769465773
INFO  [MigrationStage:1] 2023-08-10 18:39:33,080 ColumnFamilyStore.java:493 - Initializing system_auth.identity_to_role
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,083 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system_auth.identity_to_role
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,084 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 6)
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,085 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data0}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data1}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data2}], positions=null, ringVersion=6, directoriesVersion=0} for system_auth.identity_to_role
INFO  [MigrationStage:1] 2023-08-10 18:39:33,094 ColumnFamilyStore.java:493 - Initializing system_auth.network_permissions
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,095 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system_auth.network_permissions
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,097 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 6)
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,098 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data0}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data1}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data2}], positions=null, ringVersion=6, directoriesVersion=0} for system_auth.network_permissions
INFO  [MigrationStage:1] 2023-08-10 18:39:33,107 ColumnFamilyStore.java:493 - Initializing system_auth.resource_role_permissons_index
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,109 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system_auth.resource_role_permissons_index
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,111 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 6)
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,111 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data0}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data1}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data2}], positions=null, ringVersion=6, directoriesVersion=0} for system_auth.resource_role_permissons_index
INFO  [MigrationStage:1] 2023-08-10 18:39:33,121 ColumnFamilyStore.java:493 - Initializing system_auth.role_members
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,122 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system_auth.role_members
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,124 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 6)
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,124 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data0}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data1}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data2}], positions=null, ringVersion=6, directoriesVersion=0} for system_auth.role_members
INFO  [MigrationStage:1] 2023-08-10 18:39:33,135 ColumnFamilyStore.java:493 - Initializing system_auth.role_permissions
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,137 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system_auth.role_permissions
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,140 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 6)
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,141 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data0}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data1}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data2}], positions=null, ringVersion=6, directoriesVersion=0} for system_auth.role_permissions
INFO  [MigrationStage:1] 2023-08-10 18:39:33,152 ColumnFamilyStore.java:493 - Initializing system_auth.roles
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,154 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system_auth.roles
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,156 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 6)
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,156 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data0}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data1}, DataDirectory{location=/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-3qsf1zsl/test/node3/data2}], positions=null, ringVersion=6, directoriesVersion=0} for system_auth.roles
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,167 TokenMetadata.java:876 - Starting pending range calculation for system_auth
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,167 TokenMetadata.java:881 - Pending range calculation for system_auth completed (took: 0ms)
DEBUG [GossipStage:1] 2023-08-10 18:39:33,167 MigrationCoordinator.java:428 - Reported schema d03783d7-b468-3c1a-82f1-8e30b2edde8b at endpoint /127.0.0.2:7000{code}
[~jlewandowski] , can you take a look? You've worked a lot on those issues in the past year or two.

What would be a good way forward? 

So, I can see from the logs that the metadata is still null. We throw in the PendingRangeCalculator:1, but then, in the meantime, MigrationStage:1 still calls the PendingCalculationRanges method from Schema#createKeyspace, and things continue smoothly:
{code:java}
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,167 TokenMetadata.java:876 - Starting pending range calculation for system_auth
DEBUG [MigrationStage:1] 2023-08-10 18:39:33,167 TokenMetadata.java:881 - Pending range calculation for system_auth completed (took: 0ms){code}
The tests finished successfully. It is just the previous error is reported on teardown, if I didn't misread anything... 

This also reminds me a bit of CASSANDRA-18636. 

I found also this failure on trunk today:

[https://ci-cassandra.apache.org/job/Cassandra-trunk-dtest-large-novnode/1245/jdk=jdk_17_latest,label=cassandra-dtest-large,split=2/testReport/junit/dtest-large-novnode.consistency_test/TestAvailability/test_network_topology_strategy_each_quorum/]

Unfortunately, the ccm tar located [here|https://nightlies.apache.org/cassandra/trunk/Cassandra-trunk-dtest-large-novnode/1246/Cassandra-trunk-dtest-large-novnode/jdk=jdk_17_latest,label=cassandra-dtest-large,split=2/] is empty, so I cannot confirm 100% for sure whether it is the same issue. The only thing is the test actually passed again, there is just this teardown error.

I've found the issue, putting a sleep in this method:

{code:java}
    /**
     * Update (or insert) new keyspace definition
     *
     * @param ksm The metadata about keyspace
     */
    private synchronized void load(KeyspaceMetadata ksm)
    {
        Preconditions.checkArgument(!SchemaConstants.isLocalSystemKeyspace(ksm.name));
        KeyspaceMetadata previous = distributedKeyspaces.getNullable(ksm.name);

        if (previous == null)
            loadNew(ksm);
        else
            reload(previous, ksm);

        distributedKeyspaces = distributedKeyspaces.withAddedOrUpdated(ksm);
        Uninterruptibles.sleepUninterruptibly(100 , TimeUnit.MILLISECONDS);
        distributedAndLocalKeyspaces = distributedAndLocalKeyspaces.withAddedOrUpdated(ksm);
    }
{code}

makes it reliably reproducible, which means that it probably applies to 4.1 as well

+1 on the latest patch. For anyone who follows - the describe statement problem discovered while working on this issue was pulled in its ticket - CASSANDRA-18921

Running final CI here ([~jlewandowski]  has some issues with CircleCI): 

4.1: [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=18747-4.1-final]

5.0: https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=18747-5.0-final

trunk: https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=18747-trunk-final

[~jlewandowski], please feel free to commit on clean CI runs.

There are some failures, though most of them are infrastructure related; the one which is not, does not look like something related / pls confirm

Indeed, there are two failures on the 5.0 CI run but I feel both are random:
h5. testCompactionInvalidPartitionDeletion - I did not find a ticket. Running a repeated run on the current cassandra-5.0 without the patch here. - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2540/workflows/64120a59-71fa-4dde-984a-d1cf8a4543fb/jobs/44327/parallel-runs/2?filterBy=ALL] ; 

With the patch - https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2541/workflows/00547b95-8790-4a73-997b-68d145a2eeef/jobs/44440

testOneNetworkInterfaceProvisioning-.jdk11 - It does seem like the infra. 

While we may still want to look at any of them, I do not see a reason to block this patch from committing. I do not see how those can be triggered from this patch. 

 

As discussed in the PR, we also have to open a ticket for the MigrationCoordinatorTest flaky test.

It seems to me that the proposed solution is going in the opposite direction of where it should go. The issue mainly comes from the fact that we have duplicated some information in a multithreaded code. Rather than making that logic more complex we should simplify it an remove the duplication. Looking at where those 2 variables are used and how they get used I really do not see the need for the {{distributedAndLocalKeyspaces}} variable. Am I missing something?

I looked at the code of 4.0 and 4.1 and thinking a bit more about it, I do not understand why the keyspaces were split into several groups.
Some methods look also wrong. There seem to be some confusions between what is called distributed keyspaces, non-system keyspaces and local keyspaces.
It feels to me that we should revisit that code more carefully. 

Regarding the first comment, you are probably right, fresh look was needed indeed;

Regarding the second question - if you mean local and distributed - because local are not synchronized across the cluster

Some methods stayed there as they were previously.

{quote}I looked at the code of 4.0 and 4.1 and thinking a bit more about it, I do not understand why the keyspaces were split into several groups.
Some methods look also wrong. There seem to be some confusions between what is called distributed keyspaces, non-system keyspaces and local keyspaces.
{quote}
Indeed, good catch! Thank you! I was told the two variables were for optimization on the hot path, which is rewritten anyway with TCM in 5.0, but I didn't check it out of the current patch further myself. (I should have...)

+1 on the latest patch; thanks for fixing everything, [~jlewandowski]!

Few problems with testing...

On 4.1, one junit test failed but I don't believe that is related:
https://app.circleci.com/pipelines/github/jacek-lewandowski/cassandra/977/workflows/f6f81939-5b1e-4e17-b756-bfb8791dc23d/jobs/38838/tests

On 5.0, it turned out that {{ShortPaxosSimulationTest}} is failing. I've retried both locally and on CI with {{cassandra-5.0}} and it fails there as well:
https://app.circleci.com/pipelines/github/jacek-lewandowski/cassandra/1013/workflows/132ec030-5eae-4622-ad92-ffea76cbb5a4/jobs/39718
Additionally, {{LeaveAndBootstrapTest}} turned out to be flaky. I did some investigation and discovered that this particular test is very sensitive to the execution speed - when the test is executed a bit slower, it fails consistently in the same way (both {{cassandra-5.0}} and this feature branch). The key temporal option in this case is either Gossiper delay/interval or quarantine delay. The failed runs were slower and Gossiper check state task was executed wiping some endpoint from the state map due to elapsed quarantine time. Thas is clearly visible in the logs. I'm hoping to fix this issue by increasing the quarantine time so that it is less likely to elapse during the test.

On trunk, all tests passed.


Test links in the PRs

{quote}[https://app.circleci.com/pipelines/github/jacek-lewandowski/cassandra/977/workflows/f6f81939-5b1e-4e17-b756-bfb8791dc23d/jobs/38838/tests]
{quote}
CASSANDRA-18447 - the jamm issue is the same technically... So not related;
{quote}ShortPaxosSimulationTest
{quote}
CASSANDRA-18944 
{quote}Additionally, {{LeaveAndBootstrapTest}} turned out to be flaky
{quote}
CASSANDRA-18045, we can post your investigations there so someone can solve the problem?

[~jlewandowski] do you have a jvm test upgrade build result for this patch?  I rebased and was trying to merge CASSANDRA-18952 and now it's failing at org.apache.cassandra.distributed.upgrade.MixedModeRepairTest#testRepairDuringMajorUpgrade.  When I run the test I see the following error causing repair to fail

{code}
INFO  [node1_Repair-Task:1] 2023-10-27 10:25:54,600 SubstituteLogger.java:169 - ERROR 17:25:54 Repair e22ac8f0-74ed-11ee-b538-b1617eaf5e43 failed:
java.lang.NoSuchMethodError: 'com.google.common.collect.ImmutableSet org.apache.cassandra.schema.Schema.getKeyspaces()'
	at org.apache.cassandra.metrics.CompactionMetrics$1.getValue(CompactionMetrics.java:75)
	at org.apache.cassandra.metrics.CompactionMetrics$1.getValue(CompactionMetrics.java:70)
	at org.apache.cassandra.db.compaction.CompactionManager.getPendingTasks(CompactionManager.java:2199)
	at org.apache.cassandra.service.ActiveRepairService.verifyCompactionsPendingThreshold(ActiveRepairService.java:622)
	at org.apache.cassandra.service.ActiveRepairService.prepareForRepair(ActiveRepairService.java:635)
	at org.apache.cassandra.repair.RepairCoordinator.prepare(RepairCoordinator.java:430)
	at org.apache.cassandra.repair.RepairCoordinator.runMayThrow(RepairCoordinator.java:288)
	at org.apache.cassandra.repair.RepairCoordinator.run(RepairCoordinator.java:257)
{code}

Ok weird, it passes in Jenkins but not Circle https://ci-cassandra.apache.org/view/Cassandra%205.0/job/Cassandra-5.0-jvm-dtest-upgrade/97/jdk=jdk_11_latest,label=cassandra,split=6/testReport/org.apache.cassandra.distributed.upgrade/MixedModeRepairTest/testRepairDuringMajorUpgrade__jdk11_arch_x86_64_python2_7/. https://app.circleci.com/pipelines/github/dcapwell/cassandra/2327/workflows/c52543ec-6147-4cc2-9694-f58a129da702/jobs/37368/tests.  The error makes no sense as this isn't reflection or JMX... so why a signature change matters isn't clear to me...

ok maybe a cache issue... I can't see the SHA circle used... but if I rebuild the jars in circle and rerun against my same SHA; the test passes...

sorry to bother!


