I've hit this flaky test as well.  It seems to happen when there is activity on the host machine that opens or closes files while the test is running.

It looks like the original bug that this is testing (AVRO-2286) always left one file descriptor open.

A quick test on my machine, this catches about 5-10 failures over about 20 seconds.
{code:java}
@Test
public void testFlakiness() throws IOException {
  int failures = 0;
  for (int i = 0; i < 100000; i++)
    try {
      testForLeakingFileDescriptors();
    } catch (AssertionError e) {
      failures++;
    }
  assertEquals(failures, 0);
}
{code}
It's probably safe to just retry the test a few times (very few) if it happens to fail, and consider it successful if it succeeds once.  I guess it's possible to give us a false success if one file descriptor was leaked but garbage collection fuzzed its detection, but at least when it eventually DOES fail, there will actually be a bug behind it.

Any suggestions how we can best retry the test in gh actions?

I just linked a PR that loops a maximum of three times, but finishes with success if it succeeds once.  It should be fine with Github actions... but that's tricky to prove with flaky tests!

Commit 568ee67c2d0c8d62049741ae78dc91910aa1b6f2 in avro's branch refs/heads/master from RyanSkraba
[ https://gitbox.apache.org/repos/asf?p=avro.git;h=568ee67 ]

AVRO-3033: Fix flaky file descriptor test (#1078)

It looks like before fixing the bug AVRO-2286, the number of openFilesBeforeOperation was always greater than openFilesAfterOperation. In practice, now they're usually the same value, which is promising evidence that there isn't a resource leak there.

The flakiness of the test comes from the fact that "something" might be fuzzing either of them off by one. I used to see this in travis and local builds too (but infrequently). It might be a background, system-process or an action like garbage collection. I don't think it's instability of the machine, it's just not as deterministic as we'd like.

Commit 568ee67c2d0c8d62049741ae78dc91910aa1b6f2 in avro's branch refs/heads/dependabot/nuget/lang/csharp/nunit3testadapter-3.17.0 from RyanSkraba
[ https://gitbox.apache.org/repos/asf?p=avro.git;h=568ee67 ]

AVRO-3033: Fix flaky file descriptor test (#1078)

It looks like before fixing the bug AVRO-2286, the number of openFilesBeforeOperation was always greater than openFilesAfterOperation. In practice, now they're usually the same value, which is promising evidence that there isn't a resource leak there.

The flakiness of the test comes from the fact that "something" might be fuzzing either of them off by one. I used to see this in travis and local builds too (but infrequently). It might be a background, system-process or an action like garbage collection. I don't think it's instability of the machine, it's just not as deterministic as we'd like.

Commit 568ee67c2d0c8d62049741ae78dc91910aa1b6f2 in avro's branch refs/heads/dependabot/nuget/lang/csharp/NUnit.ConsoleRunner-3.12.0 from RyanSkraba
[ https://gitbox.apache.org/repos/asf?p=avro.git;h=568ee67 ]

AVRO-3033: Fix flaky file descriptor test (#1078)

It looks like before fixing the bug AVRO-2286, the number of openFilesBeforeOperation was always greater than openFilesAfterOperation. In practice, now they're usually the same value, which is promising evidence that there isn't a resource leak there.

The flakiness of the test comes from the fact that "something" might be fuzzing either of them off by one. I used to see this in travis and local builds too (but infrequently). It might be a background, system-process or an action like garbage collection. I don't think it's instability of the machine, it's just not as deterministic as we'd like.

Commit 568ee67c2d0c8d62049741ae78dc91910aa1b6f2 in avro's branch refs/heads/dependabot/nuget/lang/csharp/System.Reflection.Emit.Lightweight-4.7.0 from RyanSkraba
[ https://gitbox.apache.org/repos/asf?p=avro.git;h=568ee67 ]

AVRO-3033: Fix flaky file descriptor test (#1078)

It looks like before fixing the bug AVRO-2286, the number of openFilesBeforeOperation was always greater than openFilesAfterOperation. In practice, now they're usually the same value, which is promising evidence that there isn't a resource leak there.

The flakiness of the test comes from the fact that "something" might be fuzzing either of them off by one. I used to see this in travis and local builds too (but infrequently). It might be a background, system-process or an action like garbage collection. I don't think it's instability of the machine, it's just not as deterministic as we'd like.

Commit 568ee67c2d0c8d62049741ae78dc91910aa1b6f2 in avro's branch refs/heads/dependabot/nuget/lang/csharp/System.CodeDom-5.0.0 from RyanSkraba
[ https://gitbox.apache.org/repos/asf?p=avro.git;h=568ee67 ]

AVRO-3033: Fix flaky file descriptor test (#1078)

It looks like before fixing the bug AVRO-2286, the number of openFilesBeforeOperation was always greater than openFilesAfterOperation. In practice, now they're usually the same value, which is promising evidence that there isn't a resource leak there.

The flakiness of the test comes from the fact that "something" might be fuzzing either of them off by one. I used to see this in travis and local builds too (but infrequently). It might be a background, system-process or an action like garbage collection. I don't think it's instability of the machine, it's just not as deterministic as we'd like.

