Did you reproduce on ASF machines or locally? And if so, do you have some more details on the OS / setup?

[~tnachen] Have you seen this issue before? The changed (single line) code in the health check shouldn't influence your test - so Connor and I am a bit puzzled. Maybe this exposed a race?

[~bmahler] Is the issue persistent or sporadic? Have you tried to revert and rerun?

I've seen another problem of flakiness on the consecutive failures, and I think this is probably exposed with the extra communication going on. What I saw happened in another test was the "sleep 20" command finishes before the expected health checks comes in, for some reason if it's being processed really slowly. If that happens the test can fail, I have a patch for the other test to make it sleep for 120 seconds. I think for now I can update the patch to make all commands to sleep at a longer time to avoid problems like this.

[~nnielsen] it was on a private CI VM. I think the only thing of interest about the environment is that sometimes the underlying machine is heavily loaded and the VM is starved of CPU resources. Also, we run more CI builds than what's running on ASF currently, which seems to expose more of the flaky tests.

[~tnachen] will you be sending a patch to fix to increase the timeout? Still seeing this flakiness, we can see if it persists with the higher timeout.

Fix was included here:

{noformat}
commit 656b0e075c79e03cf6937bbe7302424768729aa2
Author: Timothy Chen <tnachen@apache.org>
Date:   Wed Aug 6 11:34:03 2014 -0700

    Re-enabled HealthCheckTest.ConsecutiveFailures test.

    The test originally was flaky because the time to process the number
    of consecutive checks configured exceeds the task itself, so the task
    finished but the number of expected task health check didn't match.

    Review: https://reviews.apache.org/r/23772
{noformat}

This is still flaky with similar output (on Twitter's slower Jenkins VMs)

{noformat:title=}
[ RUN      ] HealthCheckTest.GracePeriod
Using temporary directory '/tmp/HealthCheckTest_GracePeriod_yueyFP'
I0909 14:46:17.281755   928 leveldb.cpp:176] Opened db in 31.441089ms
I0909 14:46:17.291883   928 leveldb.cpp:183] Compacted db in 10.067315ms
I0909 14:46:17.291951   928 leveldb.cpp:198] Created db iterator in 7860ns
I0909 14:46:17.291970   928 leveldb.cpp:204] Seeked to beginning of db in 1653ns
I0909 14:46:17.291982   928 leveldb.cpp:273] Iterated through 0 keys in the db in 1365ns
I0909 14:46:17.292042   928 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0909 14:46:17.292481   944 recover.cpp:425] Starting replica recovery
I0909 14:46:17.292635   944 recover.cpp:451] Replica is in EMPTY status
I0909 14:46:17.293318   944 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0909 14:46:17.293438   944 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0909 14:46:17.293599   944 recover.cpp:542] Updating replica status to STARTING
I0909 14:46:17.301344   944 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 7.634928ms
I0909 14:46:17.301401   944 replica.cpp:320] Persisted replica status to STARTING
I0909 14:46:17.301524   944 recover.cpp:451] Replica is in STARTING status
I0909 14:46:17.301959   944 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0909 14:46:17.302042   944 recover.cpp:188] Received a recover response from a replica in STARTING status
I0909 14:46:17.302204   944 recover.cpp:542] Updating replica status to VOTING
I0909 14:46:17.303180   945 master.cpp:286] Master 20140909-144617-16842879-47396-928 (lucid) started on 127.0.1.1:47396
I0909 14:46:17.303235   945 master.cpp:332] Master only allowing authenticated frameworks to register
I0909 14:46:17.303251   945 master.cpp:337] Master only allowing authenticated slaves to register
I0909 14:46:17.303262   945 credentials.hpp:36] Loading credentials for authentication from '/tmp/HealthCheckTest_GracePeriod_yueyFP/credentials'
I0909 14:46:17.303472   945 master.cpp:366] Authorization enabled
I0909 14:46:17.304289   945 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@127.0.1.1:47396
I0909 14:46:17.304360   945 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0909 14:46:17.304728   945 master.cpp:1212] The newly elected leader is master@127.0.1.1:47396 with id 20140909-144617-16842879-47396-928
I0909 14:46:17.304754   945 master.cpp:1225] Elected as the leading master!
I0909 14:46:17.304767   945 master.cpp:1043] Recovering from registrar
I0909 14:46:17.304896   945 registrar.cpp:313] Recovering registrar
I0909 14:46:17.305984   944 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 3.70462ms
I0909 14:46:17.306025   944 replica.cpp:320] Persisted replica status to VOTING
I0909 14:46:17.306130   944 recover.cpp:556] Successfully joined the Paxos group
I0909 14:46:17.306257   944 recover.cpp:440] Recover process terminated
I0909 14:46:17.306478   944 log.cpp:656] Attempting to start the writer
I0909 14:46:17.307101   944 replica.cpp:474] Replica received implicit promise request with proposal 1
I0909 14:46:17.316689   944 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 9.537421ms
I0909 14:46:17.318187   944 replica.cpp:342] Persisted promised to 1
I0909 14:46:17.323650   947 coordinator.cpp:230] Coordinator attemping to fill missing position
I0909 14:46:17.324581   947 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0909 14:46:17.334373   947 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 9.73229ms
I0909 14:46:17.334657   947 replica.cpp:676] Persisted action at 0
I0909 14:46:17.343782   945 replica.cpp:508] Replica received write request for position 0
I0909 14:46:17.343858   945 leveldb.cpp:438] Reading position from leveldb took 30357ns
I0909 14:46:17.354074   945 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 10.144968ms
I0909 14:46:17.354157   945 replica.cpp:676] Persisted action at 0
I0909 14:46:17.354840   945 replica.cpp:655] Replica received learned notice for position 0
I0909 14:46:17.367403   945 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 12.048622ms
I0909 14:46:17.367658   945 replica.cpp:676] Persisted action at 0
I0909 14:46:17.367758   945 replica.cpp:661] Replica learned NOP action at position 0
I0909 14:46:17.373523   950 log.cpp:672] Writer started with ending position 0
I0909 14:46:17.373816   950 leveldb.cpp:438] Reading position from leveldb took 24476ns
I0909 14:46:17.375072   950 registrar.cpp:346] Successfully fetched the registry (0B)
I0909 14:46:17.375094   950 registrar.cpp:422] Attempting to update the 'registry'
I0909 14:46:17.376626   950 log.cpp:680] Attempting to append 116 bytes to the log
I0909 14:46:17.376678   950 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0909 14:46:17.376879   950 replica.cpp:508] Replica received write request for position 1
I0909 14:46:17.387306   950 leveldb.cpp:343] Persisting action (133 bytes) to leveldb took 10.403902ms
I0909 14:46:17.501080   950 replica.cpp:676] Persisted action at 1
I0909 14:46:17.513277   945 replica.cpp:655] Replica received learned notice for position 1
I0909 14:46:17.520561   945 leveldb.cpp:343] Persisting action (135 bytes) to leveldb took 7.256709ms
I0909 14:46:17.520596   945 replica.cpp:676] Persisted action at 1
I0909 14:46:17.520603   945 replica.cpp:661] Replica learned APPEND action at position 1
I0909 14:46:17.520936   945 registrar.cpp:479] Successfully updated 'registry'
I0909 14:46:17.520977   945 registrar.cpp:372] Successfully recovered registrar
I0909 14:46:17.521018   945 log.cpp:699] Attempting to truncate the log to 1
I0909 14:46:17.521086   945 master.cpp:1070] Recovered 0 slaves from the Registry (80B) ; allowing 10mins for slaves to re-register
I0909 14:46:17.521126   945 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0909 14:46:17.521421   943 replica.cpp:508] Replica received write request for position 2
I0909 14:46:17.524828   943 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 3.387047ms
I0909 14:46:17.524853   943 replica.cpp:676] Persisted action at 2
I0909 14:46:17.525019   943 replica.cpp:655] Replica received learned notice for position 2
I0909 14:46:17.533123   943 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.084183ms
I0909 14:46:17.533185   943 leveldb.cpp:401] Deleting ~1 keys from leveldb took 35208ns
I0909 14:46:17.533197   943 replica.cpp:676] Persisted action at 2
I0909 14:46:17.533203   943 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0909 14:46:17.543190   928 containerizer.cpp:89] Using isolation: posix/cpu,posix/mem
I0909 14:46:17.547600   928 sched.cpp:137] Version: 0.21.0
I0909 14:46:17.547931   943 slave.cpp:167] Slave started on 2)@127.0.1.1:47396
I0909 14:46:17.547956   943 credentials.hpp:84] Loading credential for authentication from '/tmp/HealthCheckTest_GracePeriod_QoNlVx/credential'
I0909 14:46:17.548032   943 slave.cpp:274] Slave using credential for: test-principal
I0909 14:46:17.548153   943 slave.cpp:287] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0909 14:46:17.548213   943 slave.cpp:315] Slave hostname: lucid
I0909 14:46:17.548223   943 slave.cpp:316] Slave checkpoint: false
I0909 14:46:17.548688   943 state.cpp:33] Recovering state from '/tmp/HealthCheckTest_GracePeriod_QoNlVx/meta'
I0909 14:46:17.548851   943 sched.cpp:233] New master detected at master@127.0.1.1:47396
I0909 14:46:17.548879   943 sched.cpp:283] Authenticating with master master@127.0.1.1:47396
I0909 14:46:17.548964   943 status_update_manager.cpp:193] Recovering status update manager
I0909 14:46:17.549036   943 authenticatee.hpp:128] Creating new client SASL connection
I0909 14:46:17.549154   943 master.cpp:3653] Authenticating scheduler-6fbcf9ac-7e3e-4c48-9299-937484b45f8b@127.0.1.1:47396
I0909 14:46:17.549244   943 containerizer.cpp:252] Recovering containerizer
I0909 14:46:17.549368   943 authenticator.hpp:156] Creating new server SASL connection
I0909 14:46:17.549542   943 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0909 14:46:17.549567   943 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0909 14:46:17.549671   943 authenticator.hpp:262] Received SASL authentication start
I0909 14:46:17.549715   943 authenticator.hpp:384] Authentication requires more steps
I0909 14:46:17.549762   943 slave.cpp:3202] Finished recovery
I0909 14:46:17.549998   943 authenticatee.hpp:265] Received SASL authentication step
I0909 14:46:17.550092   943 authenticator.hpp:290] Received SASL authentication step
I0909 14:46:17.550118   943 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0909 14:46:17.550130   943 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0909 14:46:17.550139   943 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0909 14:46:17.550148   943 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0909 14:46:17.550153   943 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0909 14:46:17.550156   943 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0909 14:46:17.550168   943 authenticator.hpp:376] Authentication success
I0909 14:46:17.550215   943 slave.cpp:598] New master detected at master@127.0.1.1:47396
I0909 14:46:17.550236   943 slave.cpp:672] Authenticating with master master@127.0.1.1:47396
I0909 14:46:17.550271   943 slave.cpp:645] Detecting new master
I0909 14:46:17.550305   943 authenticatee.hpp:305] Authentication success
I0909 14:46:17.550335   943 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-6fbcf9ac-7e3e-4c48-9299-937484b45f8b@127.0.1.1:47396
I0909 14:46:17.550405   943 status_update_manager.cpp:167] New master detected at master@127.0.1.1:47396
I0909 14:46:17.550433   943 authenticatee.hpp:128] Creating new client SASL connection
I0909 14:46:17.550501   943 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:47396
I0909 14:46:17.550514   943 sched.cpp:476] Sending registration request to master@127.0.1.1:47396
I0909 14:46:17.550565   943 master.cpp:3653] Authenticating slave(2)@127.0.1.1:47396
I0909 14:46:17.550639   943 master.cpp:1331] Received registration request from scheduler-6fbcf9ac-7e3e-4c48-9299-937484b45f8b@127.0.1.1:47396
I0909 14:46:17.550660   943 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0909 14:46:17.550734   943 authenticator.hpp:156] Creating new server SASL connection
I0909 14:46:17.550830   943 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0909 14:46:17.550845   943 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0909 14:46:17.550884   943 master.cpp:1390] Registering framework 20140909-144617-16842879-47396-928-0000 at scheduler-6fbcf9ac-7e3e-4c48-9299-937484b45f8b@127.0.1.1:47396
I0909 14:46:17.550987   943 authenticator.hpp:262] Received SASL authentication start
I0909 14:46:17.551023   943 authenticator.hpp:384] Authentication requires more steps
I0909 14:46:17.551054   943 sched.cpp:407] Framework registered with 20140909-144617-16842879-47396-928-0000
I0909 14:46:17.551128   944 hierarchical_allocator_process.hpp:329] Added framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:17.551139   944 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0909 14:46:17.551144   944 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 5370ns
I0909 14:46:17.551190   944 authenticatee.hpp:265] Received SASL authentication step
I0909 14:46:17.551224   944 authenticator.hpp:290] Received SASL authentication step
I0909 14:46:17.551237   944 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0909 14:46:17.551244   944 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0909 14:46:17.551250   944 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0909 14:46:17.551257   944 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0909 14:46:17.551262   944 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0909 14:46:17.551266   944 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0909 14:46:17.551275   944 authenticator.hpp:376] Authentication success
I0909 14:46:17.551298   944 authenticatee.hpp:305] Authentication success
I0909 14:46:17.551317   944 master.cpp:3693] Successfully authenticated principal 'test-principal' at slave(2)@127.0.1.1:47396
I0909 14:46:17.551373   944 slave.cpp:729] Successfully authenticated with master master@127.0.1.1:47396
I0909 14:46:17.551419   944 slave.cpp:980] Will retry registration in 612556ns if necessary
I0909 14:46:17.551492   944 master.cpp:2843] Registering slave at slave(2)@127.0.1.1:47396 (lucid) with id 20140909-144617-16842879-47396-928-0
I0909 14:46:17.551583   944 registrar.cpp:422] Attempting to update the 'registry'
I0909 14:46:17.554210   947 slave.cpp:980] Will retry registration in 10.787598ms if necessary
I0909 14:46:17.554265   947 master.cpp:2831] Ignoring register slave message from slave(2)@127.0.1.1:47396 (lucid) as admission is already in progress
I0909 14:46:17.554738   950 log.cpp:680] Attempting to append 285 bytes to the log
I0909 14:46:17.554785   950 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0909 14:46:17.554988   950 replica.cpp:508] Replica received write request for position 3
I0909 14:46:17.556380   943 sched.cpp:421] Scheduler::registered took 5.296281ms
I0909 14:46:17.564491   950 leveldb.cpp:343] Persisting action (304 bytes) to leveldb took 9479us
I0909 14:46:17.564528   950 replica.cpp:676] Persisted action at 3
I0909 14:46:17.564771   950 replica.cpp:655] Replica received learned notice for position 3
I0909 14:46:17.573160   945 slave.cpp:980] Will retry registration in 39.366896ms if necessary
I0909 14:46:17.573240   945 master.cpp:2831] Ignoring register slave message from slave(2)@127.0.1.1:47396 (lucid) as admission is already in progress
I0909 14:46:17.574411   950 leveldb.cpp:343] Persisting action (306 bytes) to leveldb took 9.618392ms
I0909 14:46:17.574437   950 replica.cpp:676] Persisted action at 3
I0909 14:46:17.574447   950 replica.cpp:661] Replica learned APPEND action at position 3
I0909 14:46:17.575235   950 registrar.cpp:479] Successfully updated 'registry'
I0909 14:46:17.575330   950 log.cpp:699] Attempting to truncate the log to 3
I0909 14:46:17.575407   950 master.cpp:2883] Registered slave 20140909-144617-16842879-47396-928-0 at slave(2)@127.0.1.1:47396 (lucid)
I0909 14:46:17.575425   950 master.cpp:4126] Adding slave 20140909-144617-16842879-47396-928-0 at slave(2)@127.0.1.1:47396 (lucid) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0909 14:46:17.575579   950 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0909 14:46:17.575672   950 slave.cpp:763] Registered with master master@127.0.1.1:47396; given slave ID 20140909-144617-16842879-47396-928-0
I0909 14:46:17.575770   950 hierarchical_allocator_process.hpp:442] Added slave 20140909-144617-16842879-47396-928-0 (lucid) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0909 14:46:17.575835   950 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-144617-16842879-47396-928-0 to framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:17.575942   950 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140909-144617-16842879-47396-928-0 in 134024ns
I0909 14:46:17.576015   950 slave.cpp:2329] Received ping from slave-observer(2)@127.0.1.1:47396
I0909 14:46:17.576113   950 master.hpp:861] Adding offer 20140909-144617-16842879-47396-928-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-144617-16842879-47396-928-0 (lucid)
I0909 14:46:17.576176   950 master.cpp:3600] Sending 1 offers to framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:17.576396   950 sched.cpp:544] Scheduler::resourceOffers took 27094ns
I0909 14:46:17.576598   950 replica.cpp:508] Replica received write request for position 4
I0909 14:46:17.577487   947 master.hpp:871] Removing offer 20140909-144617-16842879-47396-928-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-144617-16842879-47396-928-0 (lucid)
I0909 14:46:17.577572   947 master.cpp:2201] Processing reply for offers: [ 20140909-144617-16842879-47396-928-0 ] on slave 20140909-144617-16842879-47396-928-0 at slave(2)@127.0.1.1:47396 (lucid) for framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:17.577610   947 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
I0909 14:46:17.577983   947 master.hpp:833] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-144617-16842879-47396-928-0 (lucid)
I0909 14:46:17.578027   947 master.cpp:2350] Launching task 1 of framework 20140909-144617-16842879-47396-928-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-144617-16842879-47396-928-0 at slave(2)@127.0.1.1:47396 (lucid)
I0909 14:46:17.578227   947 slave.cpp:1011] Got assigned task 1 for framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:17.578589   947 slave.cpp:1121] Launching task 1 for framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:17.580807   947 slave.cpp:1231] Queuing task '1' for executor 1 of framework '20140909-144617-16842879-47396-928-0000
I0909 14:46:17.580987   947 containerizer.cpp:394] Starting container '629cfd2d-47dd-4c07-b7fb-f08d8bc921de' for executor '1' of framework '20140909-144617-16842879-47396-928-0000'
I0909 14:46:17.581264   947 slave.cpp:552] Successfully attached file '/tmp/HealthCheckTest_GracePeriod_QoNlVx/slaves/20140909-144617-16842879-47396-928-0/frameworks/20140909-144617-16842879-47396-928-0000/executors/1/runs/629cfd2d-47dd-4c07-b7fb-f08d8bc921de'
I0909 14:46:17.585806   946 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 28464ns
I0909 14:46:17.585907   946 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0909 14:46:17.586352   947 launcher.cpp:137] Forked child with pid '1323' for container '629cfd2d-47dd-4c07-b7fb-f08d8bc921de'
I0909 14:46:17.587218   950 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 10.592079ms
I0909 14:46:17.587266   950 replica.cpp:676] Persisted action at 4
I0909 14:46:17.587563   950 replica.cpp:655] Replica received learned notice for position 4
I0909 14:46:17.587980   947 containerizer.cpp:510] Fetching URIs for container '629cfd2d-47dd-4c07-b7fb-f08d8bc921de' using command '/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src/mesos-fetcher'
I0909 14:46:17.600703   950 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 12.962983ms
I0909 14:46:17.600792   950 leveldb.cpp:401] Deleting ~2 keys from leveldb took 42552ns
I0909 14:46:17.600813   950 replica.cpp:676] Persisted action at 4
I0909 14:46:17.600826   950 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0909 14:46:18.593426   950 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 49754ns
I0909 14:46:19.605854   945 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 170662ns
I0909 14:46:19.613350   950 slave.cpp:2542] Monitoring executor '1' of framework '20140909-144617-16842879-47396-928-0000' in container '629cfd2d-47dd-4c07-b7fb-f08d8bc921de'
I0909 14:46:19.673095   950 slave.cpp:1741] Got registration for executor '1' of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:19.673818   950 slave.cpp:1859] Flushing queued task 1 for executor '1' of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:19.679711   950 slave.cpp:2093] Handling status update TASK_RUNNING (UUID: b4c688dd-c0df-487e-8aa5-f779333369a5) for task 1 of framework 20140909-144617-16842879-47396-928-0000 from executor(1)@127.0.1.1:52801
I0909 14:46:19.679843   950 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b4c688dd-c0df-487e-8aa5-f779333369a5) for task 1 of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:19.679869   950 status_update_manager.cpp:499] Creating StatusUpdate stream for task 1 of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:19.679976   950 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: b4c688dd-c0df-487e-8aa5-f779333369a5) for task 1 of framework 20140909-144617-16842879-47396-928-0000 to master@127.0.1.1:47396
I0909 14:46:19.680114   950 master.cpp:3212] Forwarding status update TASK_RUNNING (UUID: b4c688dd-c0df-487e-8aa5-f779333369a5) for task 1 of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:19.680156   950 master.cpp:3178] Status update TASK_RUNNING (UUID: b4c688dd-c0df-487e-8aa5-f779333369a5) for task 1 of framework 20140909-144617-16842879-47396-928-0000 from slave 20140909-144617-16842879-47396-928-0 at slave(2)@127.0.1.1:47396 (lucid)
I0909 14:46:19.680187   950 slave.cpp:2250] Status update manager successfully handled status update TASK_RUNNING (UUID: b4c688dd-c0df-487e-8aa5-f779333369a5) for task 1 of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:19.680197   950 slave.cpp:2256] Sending acknowledgement for status update TASK_RUNNING (UUID: b4c688dd-c0df-487e-8aa5-f779333369a5) for task 1 of framework 20140909-144617-16842879-47396-928-0000 to executor(1)@127.0.1.1:52801
I0909 14:46:19.680799   950 sched.cpp:635] Scheduler::statusUpdate took 35744ns
I0909 14:46:19.680860   950 master.cpp:2693] Forwarding status update acknowledgement b4c688dd-c0df-487e-8aa5-f779333369a5 for task 1 of framework 20140909-144617-16842879-47396-928-0000 to slave 20140909-144617-16842879-47396-928-0 at slave(2)@127.0.1.1:47396 (lucid)
I0909 14:46:19.680944   950 status_update_manager.cpp:398] Received status update acknowledgement (UUID: b4c688dd-c0df-487e-8aa5-f779333369a5) for task 1 of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:19.681005   950 slave.cpp:1681] Status update manager successfully handled status update acknowledgement (UUID: b4c688dd-c0df-487e-8aa5-f779333369a5) for task 1 of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:20.613318   950 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 33261ns
I0909 14:46:21.614661   943 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 78626ns
I0909 14:46:22.593358   950 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0909 14:46:22.623335   950 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 33767ns
I0909 14:46:23.633633   944 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 61631ns
I0909 14:46:27.799932   947 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 95512ns
I0909 14:46:27.800237   947 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0909 14:46:27.800612   947 slave.cpp:2329] Received ping from slave-observer(2)@127.0.1.1:47396
tests/health_check_tests.cpp:557: Failure
Failed to wait 10secs for statusHealth
tests/health_check_tests.cpp:539: Failure
Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
         Expected: to be called at least twice
           Actual: called once - unsatisfied and active
I0909 14:46:27.815444   928 master.cpp:650] Master terminating
I0909 14:46:27.815640   928 master.hpp:851] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-144617-16842879-47396-928-0 (lucid)
W0909 14:46:27.815795   928 master.cpp:4419] Removing task 1 of framework 20140909-144617-16842879-47396-928-0000 and slave 20140909-144617-16842879-47396-928-0 in non-terminal state TASK_RUNNING
I0909 14:46:27.823565   943 slave.cpp:2361] master@127.0.1.1:47396 exited
W0909 14:46:27.823611   943 slave.cpp:2364] Master disconnected! Waiting for a new master to be elected
I0909 14:46:27.828475   943 slave.cpp:2093] Handling status update TASK_RUNNING (UUID: 5f53830d-cd08-4c57-be42-33be367d3f01) for task 1 in health state unhealthy of framework 20140909-144617-16842879-47396-928-0000 from executor(1)@127.0.1.1:52801
I0909 14:46:27.828613   943 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 5f53830d-cd08-4c57-be42-33be367d3f01) for task 1 in health state unhealthy of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:27.828652   943 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 5f53830d-cd08-4c57-be42-33be367d3f01) for task 1 in health state unhealthy of framework 20140909-144617-16842879-47396-928-0000 to master@127.0.1.1:47396
I0909 14:46:27.828752   943 slave.cpp:2250] Status update manager successfully handled status update TASK_RUNNING (UUID: 5f53830d-cd08-4c57-be42-33be367d3f01) for task 1 in health state unhealthy of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:27.828771   943 slave.cpp:2256] Sending acknowledgement for status update TASK_RUNNING (UUID: 5f53830d-cd08-4c57-be42-33be367d3f01) for task 1 in health state unhealthy of framework 20140909-144617-16842879-47396-928-0000 to executor(1)@127.0.1.1:52801
I0909 14:46:27.843372   945 containerizer.cpp:882] Destroying container '629cfd2d-47dd-4c07-b7fb-f08d8bc921de'
I0909 14:46:28.803546   947 containerizer.cpp:997] Executor for container '629cfd2d-47dd-4c07-b7fb-f08d8bc921de' has exited
I0909 14:46:28.803927   947 slave.cpp:2600] Executor '1' of framework 20140909-144617-16842879-47396-928-0000 terminated with signal Killed
I0909 14:46:28.804935   947 slave.cpp:2093] Handling status update TASK_FAILED (UUID: e2b85656-092d-4851-926b-e6d6d033aa1e) for task 1 of framework 20140909-144617-16842879-47396-928-0000 from @0.0.0.0:0
I0909 14:46:28.804975   947 slave.cpp:3913] Terminating task 1
W0909 14:46:28.805146   947 containerizer.cpp:788] Ignoring update for unknown container: 629cfd2d-47dd-4c07-b7fb-f08d8bc921de
I0909 14:46:28.805538   949 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: e2b85656-092d-4851-926b-e6d6d033aa1e) for task 1 of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:28.805598   949 slave.cpp:2250] Status update manager successfully handled status update TASK_FAILED (UUID: e2b85656-092d-4851-926b-e6d6d033aa1e) for task 1 of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:28.805805   928 slave.cpp:475] Slave terminating
I0909 14:46:28.805834   928 slave.cpp:1414] Asked to shut down framework 20140909-144617-16842879-47396-928-0000 by @0.0.0.0:0
I0909 14:46:28.805845   928 slave.cpp:1439] Shutting down framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:28.805856   928 slave.cpp:2736] Cleaning up executor '1' of framework 20140909-144617-16842879-47396-928-0000
I0909 14:46:28.806020   928 slave.cpp:2811] Cleaning up framework 20140909-144617-16842879-47396-928-0000
[  FAILED  ] HealthCheckTest.GracePeriod (11557 ms)
{noformat}

[~tnachen] After saw the log [~xujyan] posted. The second {{statusUpdate}} is nearly 5 seconds delay after {{14:46:23}}.

{code}
I0909 14:46:23.633633   944 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 61631ns
I0909 14:46:27.799932   947 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 95512ns
I0909 14:46:27.800237   947 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0909 14:46:27.800612   947 slave.cpp:2329] Received ping from slave-observer(2)@127.0.1.1:47396
tests/health_check_tests.cpp:557: Failure
Failed to wait 10secs for statusHealth
tests/health_check_tests.cpp:539: Failure
Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
         Expected: to be called at least twice
           Actual: called once - unsatisfied and active
I0909 14:46:27.815444   928 master.cpp:650] Master terminating
I0909 14:46:27.815640   928 master.hpp:851] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-144617-16842879-47396-928-0 (lucid)
W0909 14:46:27.815795   928 master.cpp:4419] Removing task 1 of framework 20140909-144617-16842879-47396-928-0000 and slave 20140909-144617-16842879-47396-928-0 in non-terminal state TASK_RUNNING
I0909 14:46:27.823565   943 slave.cpp:2361] master@127.0.1.1:47396 exited
W0909 14:46:27.823611   943 slave.cpp:2364] Master disconnected! Waiting for a new master to be elected
I0909 14:46:27.828475   943 slave.cpp:2093] Handling status update TASK_RUNNING (UUID: 5f53830d-cd08-4c57-be42-33be367d3f01) for task 1 in health state unhealthy of framework 20140909-144617-16842879-47396-928-0000 from executor(1)@127.0.1.1:52801
{code}

I think we need add
{code}
@@ -1053,6 +1053,9 @@ TEST_F(HealthCheckTest, DISABLED_GracePeriod)

   driver.launchTasks(offers.get()[0].id(), tasks);

+  AWAIT_READY(statusRunning);
+  EXPECT_EQ(TASK_RUNNING, statusRunning.get().state());
+
   Clock::pause();
{code}
before advance clock. Do you think it is OK to add this and reenable the test case?

Patch: https://reviews.apache.org/r/52432/

{noformat}
Commit: 59f63332aa1d60b7088e1998076ac79467ee712e [59f6333]
Author: Gast√≥n Kleiman <gaston@mesosphere.com>
Date: 5 October 2016 at 18:10:52 GMT+2
Committer: Alexander Rukletsov <alexr@apache.org>
Commit Date: 5 October 2016 at 18:24:28 GMT+2

Updated and reenabled 'HealthCheckTest.GracePeriod'.

This test was disabled in 2014 because it was considered flaky.
The test is now updated and run 1000 times without a single failure.

Review: https://reviews.apache.org/r/52432/
{noformat}

