Not the only one apparently.
{code}
======================================================================
FAIL: test_parameter_accuracy (__main__.StreamingLogisticRegressionWithSGDTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/mllib/tests.py", line 1163, in test_parameter_accuracy
    self.assertAlmostEqual(rel, 0.1, 1)
AssertionError: 0.15264787758306739 != 0.1 within 1 places
{code}

User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/7776

Maybe from streaming? [https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/38935/consoleFull]

A couple initial observations:
- the Scala tests of the same algos are all passing, so if it's something deeper in streaming, it's probably specific to PySpark Streaming
- the estimate and target are close, just not quite within the tolerance. as these are approximations, there will be some error, but as you say, this should be deterministic and was passing before. 
- in developing the original Scala tests, I noticed this could occur if, for some reason, the sequence of streaming updates did not complete in the available time (so it didn't quite converge). an error elsewhere could conceivably cause that to happen. 

ccing [~MechCoder] who did the PySpark StreamingML bindings, did you notice these to be at all flaky during development? 

I'm trying to check to see just how flaky these have been.  But in the meantime, do you have recommendations about disabling the tests vs. relaxing the assertions?  Also, do you have thoughts about whether similar changes should be made to all PySpark Streaming ML tests?

I'd definitely be curious to see if these two tests pass again with a relaxed tolerance (say double the tolerance, which would have fixed the one in your initial comment), while we get to the core of the issue, maybe try that on your PR?

The only other algo to consider making the change in is StreamingKMeans, but the tests there are closer to toy examples, and I think will be less susceptible to small differences in convergence (assuming that's what's going on here), have you you noticed any of those failing as well?

The tests do often pass...which is the really weird thing.  They are not consistently failing.  So perhaps the failure is what you suggested about streaming updates not completing in time.  I'm not that familiar with streaming; what is the best way to address that?

I haven't seen kmeans failures, only the logreg and linreg ones.

Interesting, the lack of failures on KMeans is consistent with the completion idea because  those tests use extremely toy data (< 10 data points) whereas the regression ones use 100s of test data points.

In this (and similar) lines:
https://github.com/apache/spark/blob/master/python/pyspark/mllib/tests.py#L1161

there's a parameter `end_time` that's the time to wait in seconds for it to complete. Looking across these tests, the value fluctuates (5, 10, 15, 20) suggesting that it was hand-tuned, possibly tailored to a local test environment. Bumping that number up for any of the tests showing occasional errors might fix it, though that's a little ad-hoc.

I think things are more robust on the Scala side because there's a full-blown streaming test class that lets test jobs either run to completion, or until a max time out (https://github.com/apache/spark/blob/master/streaming/src/test/scala/org/apache/spark/streaming/TestSuiteBase.scala). So there's just one test-wide parameter, the max time out, and we could safely set that pretty high without wasting time.

Increasing time-out sounds expensive.  But it sounds like zsxwing may have fixed the issue, so we can wait and see if any more failures happen.  Thanks!

Ah, great to hear!

