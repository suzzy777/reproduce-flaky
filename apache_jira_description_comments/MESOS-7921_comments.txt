[~benjaminhindman] [~bmahler]

Attached the full log on ASF CI for this instance.

Added some extra logging and checks to try and suss this out, see https://github.com/apache/mesos/commit/f7ab29936fb1bbca3c3812cc5bb274aaca10b298.

[~xujyan], if you're able to trigger something sooner than the ASF CI build, can you let us know if you get any of these new warning log messages or {{CHECK}} failures? Thank you!

[~benjaminhindman]

In the newly attached FetcherCacheTest.CachedCustomOutputFileWithSubdirectory.log.txt:

{noformat:title=}
W0831 22:06:30.170509 32070 process.cpp:3240] Attempted to spawn already running process version@127.0.1.1:43674
W0831 22:06:30.179316 32070 process.cpp:3240] Attempted to spawn already running process files@127.0.1.1:43674
{noformat}

{noformat:title=}
*** Aborted at 1504217191 (unix time) try "date -d @1504217191" if you are using GNU date ***
PC: @     0x7f43f8cb7956 process::EventQueue::Consumer::empty()
*** SIGSEGV (@0x8) received by PID 32070 (TID 0x7f43fa98c800) from PID 8; stack trace: ***
    @     0x7f43f070f390 (unknown)
    @     0x7f43f8cb7956 process::EventQueue::Consumer::empty()
    @     0x7f43f8ca2be5 process::ProcessManager::resume()
    @     0x7f43f8ca3b80 process::ProcessManager::wait()
    @     0x7f43f8ca8d7d process::wait()
    @     0x7f43f8c4c3c7 process::Latch::await()
    @          0x1ea6749 process::Future<>::await()
    @     0x7f43f7e897f0 mesos::internal::slave::FetcherProcess::Metrics::~Metrics()
    @     0x7f43f7e89d16 mesos::internal::slave::FetcherProcess::~FetcherProcess()
    @          0x195c578 mesos::internal::tests::MockFetcherProcess::~MockFetcherProcess()
    @          0x195c5ce mesos::internal::tests::MockFetcherProcess::~MockFetcherProcess()
    @          0x13b68c1 process::Owned<>::Data::~Data()
    @          0x13bfdfe std::_Sp_counted_ptr<>::_M_dispose()
    @           0xd065ce std::_Sp_counted_base<>::_M_release()
    @           0xd04875 std::__shared_count<>::~__shared_count()
    @          0x139f916 std::__shared_ptr<>::~__shared_ptr()
    @          0x139f932 std::shared_ptr<>::~shared_ptr()
    @          0x139f94e process::Owned<>::~Owned()
    @     0x7f43f7e87b38 mesos::internal::slave::Fetcher::~Fetcher()
    @     0x7f43f7e87b7c mesos::internal::slave::Fetcher::~Fetcher()
    @          0x115ee95 process::Owned<>::Data::~Data()
    @          0x1160f5c std::_Sp_counted_ptr<>::_M_dispose()
    @           0xd065ce std::_Sp_counted_base<>::_M_release()
    @           0xd04875 std::__shared_count<>::~__shared_count()
    @          0x1150d3c std::__shared_ptr<>::~__shared_ptr()
    @          0x1150d58 std::shared_ptr<>::~shared_ptr()
    @          0x1150d74 process::Owned<>::~Owned()
    @          0x13a0054 mesos::internal::tests::FetcherCacheTest::~FetcherCacheTest()
    @          0x13bf7b2 mesos::internal::tests::FetcherCacheTest_CachedCustomOutputFileWithSubdirectory_Test::~FetcherCacheTest_CachedCustomOutputFileWithSubdirectory_Test()
    @          0x13bf7e2 mesos::internal::tests::FetcherCacheTest_CachedCustomOutputFileWithSubdirectory_Test::~FetcherCacheTest_CachedCustomOutputFileWithSubdirectory_Test()
    @          0x23a9eee testing::Test::DeleteSelf_()
    @          0x23b65bf testing::internal::HandleSehExceptionsInMethodIfSupported<>()
Segmentation fault (core dumped)
{noformat}

New failure on ASF CI: https://lists.apache.org/thread.html/bf6cacef549f0822814914b32e281a55ce32a02232bef5070cce512c@%3Cbuilds.mesos.apache.org%3E

Similar to the one posted in the JIRA description.
{noformat:title=}
*** Aborted at 1504241455 (unix time) try "date -d @1504241455" if you are using GNU date ***
I0901 04:50:55.571101 779 registrar.cpp:424] Successfully recovered registrar
I0901 04:50:55.571496 779 master.cpp:1804] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register
I0901 04:50:55.571521 793 hierarchical.cpp:209] Skipping recovery of hierarchical allocator: nothing to recover
PC: @ 0x2b4f0af34c80 process::EventQueue::Consumer::empty()
*** SIGSEGV (@0x8) received by PID 773 (TID 0x2b4f17caa700) from PID 8; stack trace: ***
@ 0x2b4f0f452330 (unknown)
@ 0x2b4f0af34c80 process::EventQueue::Consumer::empty()
@ 0x2b4f0af18c20 process::ProcessManager::resume()
@ 0x2b4f0af27a71 process::ProcessManager::init_threads()::$_9::operator()()
@ 0x2b4f0af279b5 _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvE3$_9vEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
@ 0x2b4f0af27985 std::_Bind_simple<>::operator()()
@ 0x2b4f0af2795c std::thread::_Impl<>::_M_run()
@ 0x2b4f0f711a60 (unknown)
@ 0x2b4f0f44a184 start_thread
@ 0x2b4f0ff7dffd (unknown)
{noformat}

So libprocess GC would delete the managed process upon their exit: https://github.com/apache/mesos/blob/1ae308c2f1344d9e62e094ab11cc195c96eb5c04/3rdparty/libprocess/include/process/gc.hpp#L45

{code:title=}
  virtual void exited(const UPID& pid)
  {
    if (processes.count(pid) > 0) {
      const ProcessBase* process = processes[pid];
      processes.erase(pid);
      delete process;
    }
  }
{code}

What happens when another process who's waiting on it donates the thread to this process which is terminated after it is extracted from the run queue? Could it be destructed before resuming it?
 https://github.com/apache/mesos/blob/1ae308c2f1344d9e62e094ab11cc195c96eb5c04/3rdparty/libprocess/src/process.cpp#L3581-L3587

{code:title=}
  if (process != nullptr) {
    VLOG(2) << "Donating thread to " << process->pid << " while waiting";
    ProcessBase* donator = __process__;
    resume(process);
    running.fetch_sub(1);
    __process__ = donator;
  }
{code}

[~xujyan]: we shouldn't be able to resume if the thread is terminating or cleaned up because it won't be in the run queue.

I did track down at least one bug with how we do garbage collection that could lead to these crashes. Basically, we could have the following sequence of events:

(1) spawn process Foo, this enqueues a dispatch to `GarbageCollector::manage()`.
(2) terminate process Foo
(3) Execute `GarbageCollector::manage()` which calls `link()` and see's that the process is terminated but BEFORE enqueuing an exited event another thread spawns another process Foo which enqueues a dispatch to `GarbageCollector::manage()`. See https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/process.cpp#L3514 for where a thread could get stalled in order to cause this unfortunate ordering. 
(4) Execute `GarbageCollector::manage()` _which overwrites the process that we're managing_.
(5) Execute 'GarbageCollector::exited()` _which deletes the new process that has been spawned not the old process that has terminated_.
(6) Crash because we're using deleted memory.

Here is a patch that removes and simplifies garbage collection: https://reviews.apache.org/r/62053

Unfortunately I still can't trigger this bug myself, [~xujyan] can you try applying this patch and tell me if you still run into the crash? Thank you!

Tried out the patch and it seemed to work. I had run mesos-tests with many iterations and no crash occurred (but the top of tree does in the same environment).

The environment I used is a standard [ubuntu xenial|https://app.vagrantup.com/ubuntu/boxes/xenial64] VM with two cpus configured running on my desktop.

{code}
commit e04d3cf81f542d4d808b0f7eb3247bd34fbc976e
Author: Benjamin Hindman <benjamin.hindman@gmail.com>
Date:   Sat Sep 2 08:00:12 2017 -0700

    Removed garbage collector.
    
    The garbage collector had at least two bugs:
    
    (1) If someone dispatched `manage()` twice in a row the process we're
        waiting for will get overwritten which can wreak havoc depending
        on when the calls to `link()` happen.
    
    (2) The garbage collector was deleting after an exited event rather
        than actually doing a `wait()`.
    
    The simpler implementation that this patch introduces is to just
    delete the process after doing `ProcessManager::cleanup()`.
    
    Review: https://reviews.apache.org/r/62053
{code}

Re-opening since this has shown up in CI a number of times again.

[~benjaminhindman] I took a look at {{process::Latch}}, knowing that it can trigger the crash thanks to traces that [~alexr] shared with me. Here are some example crash paths.

Path 1, produces the stack trace originally shown in this ticket:

{noformat}
T1 creates latch, spawns latch process, and puts it in run queue
T1 waits on latch
T1 ProcessManager::wait on latch see it in BOTTOM

T2 worker thread dequeues the latch process
T2 ProcessManager::resume runs initialize, moves it to READY
T2 ProcessManager::resume sees empty queue
T2 ProcessManager::resume sets to BLOCKED

T3 triggers the latch, terminates the latch process
T3 enqueue TerminateEvent
T3 enqueue sees state BLOCKED
T3 swaps from BLOCKED->READY & enqueues latch process into run queue

T1 extracts latch process from run queue
T1 ProcessManager::resume sees READY
T1 ProcessManager::resume dequeues terminate event
T1 ProcessManager::resume calls ProcessManager::cleanup
T1 ProcessManager::cleanup sets to TERMINATING
T1 ProcessManager::cleanup decommissions event queue
T1 ProcessManager::cleanup waits for latch refs to go away
T1 ProcessManager::cleanup calls SocketManager::exited
T1 ProcessManager::cleanup opens gate
T1 ProcessManager::cleanup deletes the latch process

T2 ProcessManager::resume checks if event queue is empty again (crash)
{noformat}

Path 2, produces the alternate stack trace involving latch shown below:

{noformat}
T1 creates latch, spawns latch process, and puts it in run queue
T1 waits on latch
T1 ProcessManager::wait on latch see it in BOTTOM
T1 ProcessManager::wait extracts latch process from run queue, calls resume
T1 ProcessManager::resume runs initialize, moves it to READY
T1 ProcessManager::resume sees empty queue
T1 ProcessManager::resume sets to BLOCKED

T3 triggers the latch, terminates the latch process
T3 enqueue TerminateEvent
T3 enqueue sees state BLOCKED
T3 swaps from BLOCKED->READY & enqueues latch process into run queue

T2 worker thread dequeues the latch process
T2 ProcessManager::resume sees READY
T2 ProcessManager::resume dequeues terminate event
T2 ProcessManager::resume calls ProcessManager::cleanup
T2 ProcessManager::cleanup sets to TERMINATING
T2 ProcessManager::cleanup decommissions event queue
T2 ProcessManager::cleanup waits for latch refs to go away
T2 ProcessManager::cleanup calls SocketManager::exited
T2 ProcessManager::cleanup opens gate
T2 ProcessManager::resume deletes the latch process

T1 ProcessManager::resume checks if event queue is empty again (crash)
{noformat}

{noformat}
*** Aborted at 1508426752 (unix time) try "date -d @1508426752" if you are using GNU date ***
PC: @     0x7fa9ed827d14 process::EventQueue::Consumer::empty()
*** SIGSEGV (@0x8) received by PID 4537 (TID 0x7fa9ef594800) from PID 8; stack trace: ***
    @     0x7fa9e4e7a390 (unknown)
    @     0x7fa9ed827d14 process::EventQueue::Consumer::empty()
    @     0x7fa9ed813195 process::ProcessManager::resume()
    @     0x7fa9ed814256 process::ProcessManager::wait()
    @     0x7fa9ed819461 process::wait()
    @     0x7fa9ed7bcf3b process::Latch::await()
    @     0x5643c8b83967 process::Future<>::await()
    @     0x7fa9ec8ff81c mesos::internal::slave::FetcherProcess::Metrics::~Metrics()
    @     0x7fa9ec8fff2e mesos::internal::slave::FetcherProcess::~FetcherProcess()
    @     0x7fa9ec8fffb2 mesos::internal::slave::FetcherProcess::~FetcherProcess()
    @     0x5643c804fedb process::Owned<>::Data::~Data()
    ...
{noformat}

One fix I can see here is for {{ProcessManager::resume}} to hold a reference to the process while it's looping over its event queue, and releasing that reference before it calls into cleanup.

A fix is posted here, not sure if this is the best approach:
https://reviews.apache.org/r/63208/

{noformat}
commit 64c9b5cafced6da5e5c209387ba23f182e4f1a7a (HEAD -> master, apache/master, apache/HEAD, bmahler_fix_eventqueue_segfault)
Author: Benjamin Mahler <bmahler@apache.org>
Date:   Sat Oct 21 19:20:00 2017 -0700

    Fixed a crash in ProcessManager::resume due to race.

    When ProcessManager::resume moves the process into a BLOCKED
    state, it's possible that a TerminateEvent is enqueued and the
    process is placed back in the run queue. Another worker thread
    can pick it off the run queue, process the TerminateEvent, and
    delete the process! At this point, when the original thread
    in ProcessManager::resume tries to check if any events were
    enqueued before transitioning to BLOCKED, it will access a
    deleted process and crash.

    Some example crash paths involving process::Latch below.

    Path 1:

    T1 creates latch, spawns latch process, and puts it in run queue
    T1 waits on latch
    T1 ProcessManager::wait on latch see it in BOTTOM

    T2 worker thread dequeues the latch process
    T2 ProcessManager::resume runs initialize, moves it to READY
    T2 ProcessManager::resume sees empty queue
    T2 ProcessManager::resume sets to BLOCKED

    T3 triggers the latch, terminates the latch process
    T3 enqueue TerminateEvent
    T3 enqueue sees state BLOCKED
    T3 swaps from BLOCKED->READY & enqueues latch process into run queue

    T1 extracts latch process from run queue
    T1 ProcessManager::resume sees READY
    T1 ProcessManager::resume dequeues terminate event
    T1 ProcessManager::resume calls ProcessManager::cleanup
    T1 ProcessManager::cleanup sets to TERMINATING
    T1 ProcessManager::cleanup decommissions event queue
    T1 ProcessManager::cleanup waits for latch refs to go away
    T1 ProcessManager::cleanup calls SocketManager::exited
    T1 ProcessManager::cleanup opens gate
    T1 ProcessManager::cleanup deletes the latch process

    T2 ProcessManager::resume checks if event queue is empty again (crash)

    Path 2:

    T1 creates latch, spawns latch process, and puts it in run queue
    T1 waits on latch
    T1 ProcessManager::wait on latch see it in BOTTOM
    T1 ProcessManager::wait extracts latch process from run queue
    T1 ProcessManager::resume runs initialize, moves it to READY
    T1 ProcessManager::resume sees empty queue
    T1 ProcessManager::resume sets to BLOCKED

    T3 triggers the latch, terminates the latch process
    T3 enqueue TerminateEvent
    T3 enqueue sees state BLOCKED
    T3 swaps from BLOCKED->READY & enqueues latch process into run queue

    T2 worker thread dequeues the latch process
    T2 ProcessManager::resume sees READY
    T2 ProcessManager::resume dequeues terminate event
    T2 ProcessManager::resume calls ProcessManager::cleanup
    T2 ProcessManager::cleanup sets to TERMINATING
    T2 ProcessManager::cleanup decommissions event queue
    T2 ProcessManager::cleanup waits for latch refs to go away
    T2 ProcessManager::cleanup calls SocketManager::exited
    T2 ProcessManager::cleanup opens gate
    T2 ProcessManager::resume deletes the latch process

    T1 ProcessManager::resume checks if event queue is empty again (crash)

    Review: https://reviews.apache.org/r/63208
{noformat}

