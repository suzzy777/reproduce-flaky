Hi [~sql_forever], thanks for report this bug. It seems that this test failed when loading iceberg_partitioned. Have you even put the test files to hdfs manually like this?
{code:java}
// testdata/datasets/functional/functional_schema_template.sql
`hadoop fs -mkdir -p /test-warehouse/iceberg_test && \
hadoop fs -put -f ${IMPALA_HOME}/testdata/data/iceberg_test/iceberg_partitioned /test-warehouse/iceberg_test/
{code}
I've already rebuild code in my own environment by ninja and asan, but I can create external Iceberg table and query normally, like this:
{code:java}
 CREATE EXTERNAL TABLE functional_parquet.iceberg_partitioned (                                                           
   id INT,                                                                                                                
   user STRING,                                                                                                           
   action STRING,                                                                                                         
   event_time TIMESTAMP                                                                                                   
 )                                                                                                                        
 PARTITION BY SPEC                                                                                                        
 (                                                                                                                        
   event_time HOUR,                                                                                                       
   action IDENTITY                                                                                                        
 )                                                                                                                        
 STORED AS ICEBERG                                                                                                        
 LOCATION 'hdfs://localhost:20500/test-warehouse/iceberg_test/iceberg_partitioned'                                        
 TBLPROPERTIES ('iceberg.catalog'='hadoop.tables', 'iceberg.file_format'='parquet');
select count(1) from functional_parquet.iceberg_partitioned;{code}
 

Hi [~skyyws],  Sorry I have not tried copying the test files to hdfs, and thanks a lot for trying it out. 

Since the error was seen with running test_scanners.py.  I wonder if the queries executed prior to the DDL in question have an impact. 

Hi [~sql_forever], we need to create these test tables manually before execute tests, if you want to verify a specific test. Or you can run $IMPALA_HOME/bin/{{run-all-tests}}{{.sh to execute whole impala tests, impala server will create tests tables automatically, all DDL statements in functional_schema_template.sql will be executed before run tests, more details about impala test, you can refer: https://cwiki.apache.org/confluence/display/IMPALA/How+to+load%2C+run%2C+and+create+new+Impala+tests}}

[~gaborkaszab][~boroknagyz][~skyyws] can one of you pick this up?

Hi [~tarmstrong], I don't think this is a flaky bug in fact. As I commented above,  we need to create test tables manually before execute a specific test separately. If we execute a whole impala test by  $IMPALA_HOME/bin/{{run-all-tests}}{{.sh, I think this would not happen. By the way, if there anyone meet this bug?}}

We saw a few failures in our internal tests, I guess it is probably a problem with data loading. It looks like the builds where it failed were S3 jobs where we loaded the data from a HDFS snapshot tarball, so maybe it is related to that data loading process and doesn't occur upstream.

Thanks [~tarmstrong] , I understand. But I don't have a completed test environment about data loading and  S3. And I test other patch by put data to hdfs manually and run single test case, so this bug maybe hard to me to resolve...:( 

 Probably it's similar to IMPALA-10335. We might have changed something about how we load Iceberg tables.

I don't think we have seen this issue again since it was reported, so it was probably temporal.

