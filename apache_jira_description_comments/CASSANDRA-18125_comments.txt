Thanks for the report!

Is there any chance you could provide a little more information around the following?

1.) Are secondary indexes in use?
2.) Is this an overwrite-heavy write workload?
3.) Are off-heap memtables in use?

In general further details would be great, including any involved schema, and any log messages around memtable flushes that occur either side of the error.

Thanks for your feedback, of course I can provide more details.

1. There is no secondary index in use.
2. Not sure what is really heavy but I don't believe it is heavy, each node has ~200GB of data with ttl of 10 days for most of the data (~20GB written by day). About overwrite, I guess you mean updating some cells, yes we usually insert then update a row within a short time frame (~10min).
3. I don't think off-heap is used. If disabled by default, that's not something I enabled

A compaction was running it seems. I got a lot of 
- TWCS skipping check for fully expired SSTables
- TWCS expired check sufficiently far in the past, checking for fully expired SSTables

Here is an extract of the logs around ~1min
{code}
Dec 19, 2022 @ 13:07:51.334COMMIT-LOG-ALLOCATOR org.apache.cassandra.db.ColumnFamilyStore logFlush - Enqueuing flush of T4: 35.156KiB (0%) on-heap, 0.000KiB (0%) off-heap
Dec 19, 2022 @ 13:07:51.334COMMIT-LOG-ALLOCATOR org.apache.cassandra.db.ColumnFamilyStore logFlush - Enqueuing flush of T3: 852.600KiB (0%) on-heap, 0.000KiB (0%) off-heap
Dec 19, 2022 @ 13:07:51.334COMMIT-LOG-ALLOCATOR org.apache.cassandra.db.ColumnFamilyStore logFlush - Enqueuing flush of T2: 41.210KiB (0%) on-heap, 0.000KiB (0%) off-heap
Dec 19, 2022 @ 13:07:51.334COMMIT-LOG-ALLOCATOR org.apache.cassandra.db.ColumnFamilyStore logFlush - Enqueuing flush of T1: 364.268KiB (0%) on-heap, 0.000KiB (0%) off-heap
Dec 19, 2022 @ 13:07:15.561NonPeriodicTasks:1 org.apache.cassandra.io.sstable.SSTable delete - Deleting sstable: /data/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-26-big
Dec 19, 2022 @ 13:07:15.560CompactionExecutor:6104 org.apache.cassandra.db.compaction.CompactionTask runMayThrow - Compacted (ad77aef0-7f95-11ed-bab7-e7d7c3e60f28) 2 sstables to [/data/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-29-big,] to level=0.  5.429KiB to 5.426KiB (~99% of original) in 9ms.  Read Throughput = 593.925KiB/s, Write Throughput = 593.604KiB/s, Row Throughput = ~118/s.  59 total partitions merged to 58.  Partition merge counts were {1:57, 2:1, }
Dec 19, 2022 @ 13:07:15.560NonPeriodicTasks:1 org.apache.cassandra.io.sstable.SSTable delete - Deleting sstable: /data/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-27-big
Dec 19, 2022 @ 13:07:15.551MemtableReclaimMemory:2628 org.apache.cassandra.service.CassandraDaemon uncaughtException - Exception in thread Thread[MemtableReclaimMemory:2628,5,main]java.lang.AssertionError: null
	at org.apache.cassandra.utils.memory.MemtablePool$SubPool.released(MemtablePool.java:193)
	at org.apache.cassandra.utils.memory.MemtableAllocator$SubAllocator.releaseAll(MemtableAllocator.java:151)
	at org.apache.cassandra.utils.memory.MemtableAllocator$SubAllocator.setDiscarded(MemtableAllocator.java:142)
	at org.apache.cassandra.utils.memory.MemtableAllocator.setDiscarded(MemtableAllocator.java:93)
	at org.apache.cassandra.utils.memory.SlabAllocator.setDiscarded(SlabAllocator.java:120)
	at org.apache.cassandra.db.Memtable.setDiscarded(Memtable.java:201)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush$1.runMayThrow(ColumnFamilyStore.java:1216)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Dec 19, 2022 @ 13:07:15.551MemtablePostFlush:2626 org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager archiveAndDiscard - Segment CommitLogSegment(/data/cassandra/commitlog/CommitLog-7-1670343439312.log) is no longer active and will be deleted now
Dec 19, 2022 @ 13:07:15.551MemtablePostFlush:2626 org.apache.cassandra.db.commitlog.CommitLog discardCompletedSegments - Commit log segment CommitLogSegment(/data/cassandra/commitlog/CommitLog-7-1670343439312.log) is unused
Dec 19, 2022 @ 13:07:15.551CompactionExecutor:6104 org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy newestBucket - bucket size 2 >= 2 and not in current bucket, compacting what's here: [BigTableReader(path='/data/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-27-big-Data.db'), BigTableReader(path='/data/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-26-big-Data.db')]
Dec 19, 2022 @ 13:07:15.551CompactionExecutor:6104 org.apache.cassandra.db.compaction.CompactionTask runMayThrow - Compacting (ad77aef0-7f95-11ed-bab7-e7d7c3e60f28) [/data/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-27-big-Data.db:level=0, /data/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-26-big-Data.db:level=0, ]
Dec 19, 2022 @ 13:07:15.550MemtableFlushWriter:3385 org.apache.cassandra.db.ColumnFamilyStore flushMemtable - Flushed to [BigTableReader(path='/data/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-31-big-Data.db')] (1 sstables, 7.193KiB), biggest 7.193KiB, smallest 7.193KiB
Dec 19, 2022 @ 13:07:15.550MemtableFlushWriter:3384 org.apache.cassandra.db.ColumnFamilyStore flushMemtable - Flushed to [BigTableReader(path='/data/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-28-big-Data.db')] (1 sstables, 6.510KiB), biggest 6.510KiB, smallest 6.510KiB
Dec 19, 2022 @ 13:07:15.546PerDiskMemtableFlushWriter_0:3385 org.apache.cassandra.db.Memtable writeSortedContents - Completed flushing /data/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-31-big-Data.db (4.170KiB) for commitlog position CommitLogPosition(segmentId=1670343439574, position=3282)
Dec 19, 2022 @ 13:07:15.545PerDiskMemtableFlushWriter_0:3384 org.apache.cassandra.db.Memtable writeSortedContents - Completed flushing /data/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-28-big-Data.db (1.785KiB) for commitlog position CommitLogPosition(segmentId=1670343439574, position=1779)
Dec 19, 2022 @ 13:07:15.545PerDiskMemtableFlushWriter_0:3384 org.apache.cassandra.db.Memtable writeSortedContents - Writing Memtable-repair_history@685478737(-8.285KiB serialized bytes, 246 ops, -0%/0% of on/off-heap limit), flushed range = (min(-9223372036854775808), max(9223372036854775807)]Dec 19, 2022 @ 13:07:15.545PerDiskMemtableFlushWriter_0:3385 org.apache.cassandra.db.Memtable writeSortedContents - Writing Memtable-parent_repair_history@860004687(5.211KiB serialized bytes, 2 ops, 0%/0% of on/off-heap limit), flushed range = (min(-9223372036854775808), max(9223372036854775807)]Dec 19, 2022 @ 13:07:15.543COMMIT-LOG-ALLOCATOR org.apache.cassandra.db.ColumnFamilyStore logFlush - Enqueuing flush of parent_repair_history: 14.763KiB (0%) on-heap, 0.000KiB (0%) off-heap
Dec 19, 2022 @ 13:07:15.542COMMIT-LOG-ALLOCATOR org.apache.cassandra.db.ColumnFamilyStore logFlush - Enqueuing flush of repair_history: -8.000KiB (-0%) on-heap, 0.000KiB (0%) off-heap
Dec 19, 2022 @ 13:06:22.905ReadStage-1 org.apache.cassandra.utils.memory.BufferPool log - Maximum memory usage reached (512.000MiB), cannot allocate chunk of 8.000MiB
Dec 19, 2022 @ 13:05:36.979NonPeriodicTasks:1 org.apache.cassandra.io.sstable.SSTable delete - Deleting sstable: /data/cassandra/data/X/Y-48b97b403c1011eab0e9f58b98b870bd/nb-2963-big
Dec 19, 2022 @ 13:05:36.979CompactionExecutor:6105 org.apache.cassandra.db.compaction.CompactionTask runMayThrow - Compacted (72b6a320-7f95-11ed-bab7-e7d7c3e60f28) 1 sstables to [] to level=0.  0.000KiB to 0.000KiB (~0% of original) in 1ms.  Read Throughput = 0.000KiB/s, Write Throughput = 0.000KiB/s, Row Throughput = ~0/s.  0 total partitions merged to 0.  Partition merge counts were {}
{code}


Thanks. To be clear, I'm looking for the schema of the table involved in the problematic memtable flush. It looks like quite a few flushes might be candidates, but if there are a lot of these log messages it might be possible to pinpoint a specific Memtable (or else we could perhaps patch to include the table in the error message).

The likeliest explanation is that there is some non-determinism in the Memtable accounting, so we want to pinpoint the potential places this might be occurring, and so the more information we can use to narrow down the candidate code locations the better.

I cannot share the schema. There are 110 tables with 76 keyspaces. One table contains ~75% of the data mainly from a data payload in a {{text}} cell.

Recently we ran into this as well, immediately preceding the exception there was the warn log:
{code:java}
Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK. {code}
(Cassandra 4.0.7 / OpenJDK 11.0.17)

Are you able to (privately) share detailed logs and schema information, so we can perhaps ascertain some information about the workload that might have triggered this problem?

The exception you mention is very unlikely to be related, though it does suggest this is all happening on startup?

In our case 4 out of 5 Cassandra crashed one after another within a 2 hour windows after a couple of months of uptime.

We have ~40 keyspaces with most of the data in Akka Persistence Cassandra Schema: [https://doc.akka.io/docs/akka-persistence-cassandra/current/journal.html]

The ENOMEM log message only occurs at startup, so if your timeline is correct this error occurred after your processes first crashed, not before.

Debugging problems like this requires very detailed information, including full logs and the ability to map table names to specific schema, as well as information about what queries were being run against the processes. Most of the data being in one schema is not very helpful information, unfortunately.

Looks like we may have a reproduction anyway: CASSANDRA-18159 looks plausibly the same issue.

Ok, so the simulator has given us a nice reproduction, and has permitted us to easily trace this back to CASSANDRA-15511. Specifically, we apply the same {{retain}} calculation erroneously to _not yet inserted_ data, thereby corrupting the memtable memory usage accounting.

Simple fix [here|https://github.com/apache/cassandra/pull/2169]

/cc [~blerer]

Thanks [~benedict], I will look at it today.

[~benedict] Do you have some tests that reproduce the problem that can be added to the patch?

Jon Meredith is rustling some up

I've pushed up a couple of different reproducers here

org.apache.cassandra.db.partitions.AtomicBTreePartitionMemtableAccountingTest is probably the way to go in https://github.com/jonmeredith/cassandra/commit/f7452772c70b8c75da096ca1ee4375f113560f69 

At the moment the test fails with Bendict's fix applied as the recreated partition size is not identical. I'm not sure if it's possible to pick a different cloner that will always clone the input to a dedicated buffer to make the test repeatable. I'd like to extend the test with something generative so that we cover a wider range of types, partition deletion, range tombstone and complex column deletion.

My previous attempt triggers the assertion, but doesn't fail the test as the flush exception is not propagated back to the test runner.
https://github.com/jonmeredith/cassandra/commit/41e1193767ba3a0a42ab2d2b2bbac2f32f22d3de

Tests probably need a bit of cleaning up before they should be included in a patch, but wanted to share what I had so far.

I've finished a more comprehensive test and pushed a branch including Benedicts fix.  The open question is whether it is reasonable to try and account accurately account for how much onHeap ownership there is. Benedict came up with a handy way to calculate it, but it currently fails to account for static rows.  The good news is that with the fix I'm unable to trigger the assertion when releasing the allocator.

https://github.com/jonmeredith/cassandra/tree/c18125-repro-trunk

So Benedict has fixed the issues with the code/test as I shared it, but unfortunately there was a copy/paste bug in the test that meant complex column deletion was not being set on the partition update. Back to failing.

I've pushed recent changes, which resolve these non-test issues

1) our cloning behaviour is broken for complex columns, we don’t clone empty byte buffers with cell paths
2) we aren’t counting additional allocations for btree backing arrays when we update one leaf with the contents of another leaf
3) if we merge two cells, we copy the contents whether or not they are already in the memtable
4) when copying back on heap from memtable during read, if we are off heap, we do it at least three times


Pushed up another round of fixes, tests now pass. I'm planning on looking the code over and doing some comparisons of heap usage before/after the changes.

Great progress, [~jonmeredith] . I am watching this closely. I wanted to cut 4.1.1 recently and this one is basically the one which was suggested that it would be a bummer if it was not in. 

Cleaned everything up and squashed for a clean review base as it didn't seem like anybody else had started. There are minor differences between each branch, but fortunately the accounting is the same between the branches.

4.0: [PR|https://github.com/apache/cassandra/pull/2186] [branch|https://github.com/jonmeredith/cassandra/tree/c18125-repro-4.0]
4.1: [PR|https://github.com/apache/cassandra/pull/2187] [branch|https://github.com/jonmeredith/cassandra/tree/c18125-repro-4.1]
trunk: [PR|https://github.com/apache/cassandra/pull/2188] [branch|https://github.com/jonmeredith/cassandra/tree/c18125-repro-trunk]

I temporarily modified the test to output allocation info for the exhaustive test and compared the two sets of output. Numbers seem very comparable - the new logic reports a little lower but as you'd hope never negative like before. 

CI Results (pending):
||Branch||Source||Circle CI||Jenkins||
|cassandra-4.0|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18125-cassandra-4.0-EB9273A6-9018-45C1-8C26-127906CD33BE]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18125-cassandra-4.0-EB9273A6-9018-45C1-8C26-127906CD33BE]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/2311/]|
|cassandra-4.1|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18125-cassandra-4.1-EB9273A6-9018-45C1-8C26-127906CD33BE]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18125-cassandra-4.1-EB9273A6-9018-45C1-8C26-127906CD33BE]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/2312/]|
|trunk|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18125-trunk-EB9273A6-9018-45C1-8C26-127906CD33BE]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18125-trunk-EB9273A6-9018-45C1-8C26-127906CD33BE]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/2313/]|

Analysis of failures - nothing that looks related to the changes. A little suspicious of MemtableSizeTest.testSize[skiplist_sharded], but it is already flaky.

Jenkins failures

+*4.0*+

Test Result (6 failures / +6)
 - dtest-novnode.repair_tests.incremental_repair_test.TestIncRepair.test_multiple_full_repairs_lcs
900s timeout
 - org.apache.cassandra.distributed.upgrade.DropCompactStorageTest.testDropCompactStorage
org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level ALL
CASSANDRA-17674 logged against it, but seems like different failure
passed locally, seems unrelated.
 - org.apache.cassandra.streaming.LongStreamingTest.terminated successfully
junit timeout - perhaps too long a streaming test
passes locally - 2mins
 - org.apache.cassandra.utils.LongBloomFilterTest.testConstrained
junit timeout
passes locally - 1m23sec
 - org.apache.cassandra.repair.RepairJobTest.testNoTreesRetainedAfterDifference-cdc
j11 module system error - looks like trying to measure something inside JFR class.
Unable to make field private final jdk.management.jfr.StreamManager jdk.management.jfr.FlightRecorderMXBeanImpl.streamHandler accessible: module jdk.management.jfr does not "opens jdk.management.jfr" to unnamed module @28975c28
 - org.apache.cassandra.db.partition.PartitionImplementationTest.testRowsWithStatic
junit timeout

+*4.1*+

Test Result (3 failures / -3)
 - dtest-novnode.transient_replication_test.TestTransientReplicationRepairStreamEntireSSTable.test_transient_incremental_repair
missing Incoming stream entireSSTable=...from log
 - org.apache.cassandra.tools.TopPartitionsTest.testServiceTopPartitionsSingleTable
https://issues.apache.org/jira/browse/CASSANDRA-17798
junit.framework.AssertionFailedError: If this failed you probably have to raise the beginLocalSampling duration expected:<1> but was:<0>
at org.apache.cassandra.tools.TopPartitionsTest.testServiceTopPartitionsSingleTable(TopPartitionsTest.java:83)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

+*trunk*+

unable to clean tmp on the arm build

CircleCI failures

+*4.0*+

j8 upgrade - org.apache.cassandra.distributed.upgrade.DropCompactStorageTest - test shutdown timeout

+*4.1*+

j8 org.apache.cassandra.index.sasi.SASICQLTest - testPagingWithClustering-system_keyspace_directory - passes locally, no JIRA, nothing in Butler

j11 org.apache.cassandra.cql3.MemtableSizeTest.testSize[skiplist_sharded] - flaky test with built in retry. Locally fails intermittently on j11 due to java module system exports when trying to measure deep sizes of objects inside the java.desktop module - perhaps related to IDEA java agent or something like that.

+*trunk*+

j8 org.apache.cassandra.db.compaction.CompactionStrategyManagerBoundaryReloadTest
 - https://issues.apache.org/jira/browse/CASSANDRA-18144 - review in progress

 - one shard did not run upgrade tests [https://app.circleci.com/pipelines/github/jonmeredith/cassandra/746/workflows/5e928d77-0717-4ae0-ad1d-7883871c7f8e/jobs/5122]

j11 org.apache.cassandra.db.compaction.CompactionStrategyManagerBoundaryReloadTest also failed
same issue as above.

The patches looks good to me. Thanks [~jonmeredith], [~benedict].

This seems to have made {{o.a.c.cql3.validation.operations.AlterTest#testDropListAndAddListWithSameName}} flaky, as reported in CASSANDRA-18360.

[This run|https://app.circleci.com/pipelines/github/adelapena/cassandra/2758/workflows/f9c1b471-3804-4b9b-acf8-2a1201591088] for the commit immediately before CASSANDRA-18125 succeeds.

[This other run|https://app.circleci.com/pipelines/github/adelapena/cassandra/2757/workflows/ebbe5032-fc89-41c1-9603-f4336f20cdc3] for the 4.1-trunk merge commit of CASSANDRA-18125 fails reproducing the current issue.

[This run|https://app.circleci.com/pipelines/github/adelapena/cassandra/2745/workflows/6eb7b890-40de-4ffe-b1f7-16b26ed19b42] for the current 4.1 branch successes, suggesting that probably the failure only affects trunk. I haven't tested 4.0 yet.

