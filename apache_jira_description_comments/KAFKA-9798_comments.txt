[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1523/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]

 
{noformat}
Stacktracejava.lang.AssertionError: Did not receive all 1 records from topic output-concurrent-windowed-0 within 120000 ms
Expected: is a value equal to or greater than <1>
     but: <0> was less than <1>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinValuesRecordsReceived$6(IntegrationTestUtils.java:691)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:415)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:383)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinValuesRecordsReceived(IntegrationTestUtils.java:687)
	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.waitUntilAtLeastNumRecordProcessed(QueryableStateIntegrationTest.java:1189)
	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shouldAllowConcurrentAccesses(QueryableStateIntegrationTest.java:649)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748){noformat}

[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1526/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]

Different issue:
{quote}[2020-04-01 06:14:42,679] ERROR stream-thread [queryable-state-1-3b9f384a-d048-4bd9-bcc0-934fdfde525d-StreamThread-1] task [2_0] Failed to flush state store windowed-word-count-store-stream-concurrent-0: (org.apache.kafka.streams.processor.internals.ProcessorStateManager:412) org.apache.kafka.streams.errors.ProcessorStateException: Error while executing flush from store windowed-word-count-store-stream-concurrent-0.1585612800000 at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:389) at org.apache.kafka.streams.state.internals.AbstractSegments.flush(AbstractSegments.java:148) at org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore.flush(AbstractRocksDBSegmentedBytesStore.java:198) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.CachingWindowStore.flush(CachingWindowStore.java:297) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.MeteredWindowStore.lambda$flush$4(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:808) at org.apache.kafka.streams.state.internals.MeteredWindowStore.flush(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:402) at org.apache.kafka.streams.processor.internals.StreamTask.prepareCommit(StreamTask.java:315) at org.apache.kafka.streams.processor.internals.TaskManager.commitInternal(TaskManager.java:745) at org.apache.kafka.streams.processor.internals.TaskManager.commitAll(TaskManager.java:733) at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:833) at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:696) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:545) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:504) Caused by: org.rocksdb.RocksDBException: While open a file for random read: /tmp/state-queryable-state-1534964904159798192/queryable-state-1/2_0/windowed-word-count-store-stream-concurrent-0/windowed-word-count-store-stream-concurrent-0.1585612800000/000230.sst: No such file or directory at org.rocksdb.RocksDB.flush(Native Method) at org.rocksdb.RocksDB.flush(RocksDB.java:2394) at org.apache.kafka.streams.state.internals.RocksDBStore$SingleColumnFamilyAccessor.flush(RocksDBStore.java:584) at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:387) ... 17 more [2020-04-01 06:14:42,692] ERROR stream-thread [queryable-state-1-3b9f384a-d048-4bd9-bcc0-934fdfde525d-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down: (org.apache.kafka.streams.processor.internals.StreamThread:524) org.apache.kafka.streams.errors.ProcessorStateException: Error while executing flush from store windowed-word-count-store-stream-concurrent-0.1585612800000 at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:389) at org.apache.kafka.streams.state.internals.AbstractSegments.flush(AbstractSegments.java:148) at org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore.flush(AbstractRocksDBSegmentedBytesStore.java:198) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.CachingWindowStore.flush(CachingWindowStore.java:297) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.MeteredWindowStore.lambda$flush$4(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:808) at org.apache.kafka.streams.state.internals.MeteredWindowStore.flush(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:402) at org.apache.kafka.streams.processor.internals.StreamTask.prepareCommit(StreamTask.java:315) at org.apache.kafka.streams.processor.internals.TaskManager.commitInternal(TaskManager.java:745) at org.apache.kafka.streams.processor.internals.TaskManager.commitAll(TaskManager.java:733) at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:833) at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:696) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:545) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:504) Caused by: org.rocksdb.RocksDBException: While open a file for random read: /tmp/state-queryable-state-1534964904159798192/queryable-state-1/2_0/windowed-word-count-store-stream-concurrent-0/windowed-word-count-store-stream-concurrent-0.1585612800000/000230.sst: No such file or directory at org.rocksdb.RocksDB.flush(Native Method) at org.rocksdb.RocksDB.flush(RocksDB.java:2394) at org.apache.kafka.streams.state.internals.RocksDBStore$SingleColumnFamilyAccessor.flush(RocksDBStore.java:584) at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:387) ... 17 more

...

[2020-04-01 06:14:42,706] ERROR stream-thread [queryable-state-1-3b9f384a-d048-4bd9-bcc0-934fdfde525d-StreamThread-1] task [2_0] Failed to flush state store windowed-word-count-store-stream-concurrent-0: (org.apache.kafka.streams.processor.internals.ProcessorStateManager:412) org.apache.kafka.streams.errors.ProcessorStateException: Error while executing flush from store windowed-word-count-store-stream-concurrent-0.1585612800000 at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:389) at org.apache.kafka.streams.state.internals.AbstractSegments.flush(AbstractSegments.java:148) at org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore.flush(AbstractRocksDBSegmentedBytesStore.java:198) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.CachingWindowStore.flush(CachingWindowStore.java:297) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.MeteredWindowStore.lambda$flush$4(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:808) at org.apache.kafka.streams.state.internals.MeteredWindowStore.flush(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:402) at org.apache.kafka.streams.processor.internals.AbstractTask.executeAndMaybeSwallow(AbstractTask.java:110) at org.apache.kafka.streams.processor.internals.StreamTask.prepareClose(StreamTask.java:459) at org.apache.kafka.streams.processor.internals.StreamTask.prepareCloseDirty(StreamTask.java:420) at org.apache.kafka.streams.processor.internals.TaskManager.shutdown(TaskManager.java:626) at org.apache.kafka.streams.processor.internals.StreamThread.completeShutdown(StreamThread.java:888) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:528) Caused by: org.rocksdb.RocksDBException: While open a file for random read: /tmp/state-queryable-state-1534964904159798192/queryable-state-1/2_0/windowed-word-count-store-stream-concurrent-0/windowed-word-count-store-stream-concurrent-0.1585612800000/000230.sst: No such file or directory at org.rocksdb.RocksDB.flush(Native Method) at org.rocksdb.RocksDB.flush(RocksDB.java:2394) at org.apache.kafka.streams.state.internals.RocksDBStore$SingleColumnFamilyAccessor.flush(RocksDBStore.java:584) at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:387) ... 16 more{quote}

[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5559/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]

{code}
java.lang.AssertionError: Did not receive all 1 records from topic output-concurrent-windowed-0 within 120000 ms
Expected: is a value equal to or greater than <1>
     but: <0> was less than <1>
Stacktrace
java.lang.AssertionError: Did not receive all 1 records from topic output-concurrent-windowed-0 within 120000 ms
Expected: is a value equal to or greater than <1>
     but: <0> was less than <1>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinValuesRecordsReceived$6(IntegrationTestUtils.java:691)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:415)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:383)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinValuesRecordsReceived(IntegrationTestUtils.java:687)
	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.waitUntilAtLeastNumRecordProcessed(QueryableStateIntegrationTest.java:1189)
	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shouldAllowConcurrentAccesses(QueryableStateIntegrationTest.java:649)
{code}

I pushed a hotfix commit into trunk, let's see if it fixed the flaky test.

[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1624/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]

Different error message (Seem the hotfix did not do? The PR should contain the hotfix \cc [~guozhang] ):
{quote}java.nio.file.DirectoryNotEmptyException: /tmp/state-queryable-state-18623682413158663595/queryable-state-1/1_0 at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242) at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) at java.nio.file.Files.delete(Files.java:1126) at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:802) at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:772) at java.nio.file.Files.walkFileTree(Files.java:2688) at java.nio.file.Files.walkFileTree(Files.java:2742) at org.apache.kafka.common.utils.Utils.delete(Utils.java:772) at org.apache.kafka.common.utils.Utils.delete(Utils.java:758) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState(IntegrationTestUtils.java:125) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shutdown(QueryableStateIntegrationTest.java:225){quote}

I looked through the recent traces up to 1809 but did not find a reoccurrence, and I failed to reproduce it locally as well.

I will close this ticket for now and if it happens again let's re-open it.

I think this happen again.

See here: [https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1968/]

 

h3. Stacktrace

java.nio.file.NoSuchFileException: /tmp/state-queryable-state-shouldAllowConcurrentAccesses5845383974269013532/queryable-state-shouldAllowConcurrentAccesses/1_0/.checkpoint.tmp at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116) at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149) at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99) at java.base/java.nio.file.Files.readAttributes(Files.java:1763) at java.base/java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219) at java.base/java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276) at java.base/java.nio.file.FileTreeWalker.next(FileTreeWalker.java:373) at java.base/java.nio.file.Files.walkFileTree(Files.java:2760) at java.base/java.nio.file.Files.walkFileTree(Files.java:2796) at org.apache.kafka.common.utils.Utils.delete(Utils.java:777) at org.apache.kafka.common.utils.Utils.delete(Utils.java:763) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState(IntegrationTestUtils.java:125) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shutdown(QueryableStateIntegrationTest.java:228)

h3. Stacktrace

java.lang.AssertionError: Did not receive all 1 records from topic output-concurrent-shouldAllowConcurrentAccesses within 120000 ms Expected: is a value equal to or greater than <1> but: <0> was less than <1> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinValuesRecordsReceived$6(IntegrationTestUtils.java:691) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:415) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:383) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinValuesRecordsReceived(IntegrationTestUtils.java:687) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.waitUntilAtLeastNumRecordProcessed(QueryableStateIntegrationTest.java:1189) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shouldAllowConcurrentAccesses(QueryableStateIntegrationTest.java:649)

One more try to fix this: https://github.com/apache/kafka/pull/8565

Failed again: [https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2127/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]
{quote}java.lang.AssertionError: Did not receive all 1 records from topic output-concurrent-QueryableStateIntegrationTestshouldAllowConcurrentAccesses within 120000 ms Expected: is a value equal to or greater than <1> but: <0> was less than <1> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinValuesRecordsReceived$6(IntegrationTestUtils.java:747) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinValuesRecordsReceived(IntegrationTestUtils.java:743) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.waitUntilAtLeastNumRecordProcessed(QueryableStateIntegrationTest.java:1159) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shouldAllowConcurrentAccesses(QueryableStateIntegrationTest.java:650){quote}

This is due to a known issue which is fixed in a hotfix --- let's see if it fails again from now on.

Failed again here: [https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2154/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]

Not sure if the hotfix was in this build on not – what hotfix do you refer to?

Sigh... it was after my hotfix indeed, I think it still have something to do with the background producer since that's the only difference with others. 

Looking into that test, I'm now feeling if it is really necessary to have this test given the other `shouldBeAbleToQueryDuringRebalance` etc. WDYT if I just remove that test?

Would like to hear [~vvcephei] opinion on that.

Funny you should ask my opinion, since I was just saying to Bill yesterday that I don’t understand what this test is doing here. Is it’s purpose really to ensure that multiple readers can concurrently access the store? Depending on how you look at the problem of verifying this, it seems either trivial in a unit test or impossible in general. 

Or if it’s verifying that queries can be served while the app is concurrently updating the store, then it seems like this is a basic condition that would be automatically ensured by every single IQ test. 

Haha, fair point. I think the original motivation is to verify the second, and also feel that every IQ test should do this already..

Cool, I will just go ahead and remove this test then.

Great, thanks!

