I ran this test locally 50 times and it passed every time. Therefore, I'm currently not marking this as a release blocker as this could just be an artifact of our test environment (possibly due to the order in which tests are run).

Also, cc [~smilegator] [~cloud_fan]

User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20584

User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20591

I'm reopening this issue due to Parquet leakage is detected.

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/205/testReport/org.apache.spark.sql/FileBasedDataSourceSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/

{code}
Caused by: sbt.ForkMain$ForkError: java.lang.Throwable: null
	at org.apache.spark.DebugFilesystem$.addOpenStream(DebugFilesystem.scala:36)
	at org.apache.spark.DebugFilesystem.open(DebugFilesystem.scala:70)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:538)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:149)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:133)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:400)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:356)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:106)
{code}


User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20619

Since this is not a regression for both Parquet(not a regression) and ORC (new ORC is disabled by default), I'll remove the target version from this to unblock RC4.
cc [~cloud_fan] and [~sameerag]

{{FileBasedDataSourceSuite}} seems still flaky.

 

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/87603/testReport/org.apache.spark.sql/FileBasedDataSourceSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/]

 

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/87600/testReport/org.apache.spark.sql/FileBasedDataSourceSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/]

 

As reported in SPARK-25688 by [~smilegator] , all recent reported failures in 6 month occurs ORC data sources . I'll reinvestigate this further.

I think the issue is probably in orc. Any exception throwing between  https://github.com/apache/orc/blob/b21b5ffcc1efcbd4aef337fa6faae4d25262f8f1/java/core/src/java/org/apache/orc/impl/RecordReaderImpl.java#L252 and https://github.com/apache/orc/blob/b21b5ffcc1efcbd4aef337fa6faae4d25262f8f1/java/core/src/java/org/apache/orc/impl/RecordReaderImpl.java#L273 will leak `dataReader`. For example, cancelling a Spark task may cause https://github.com/apache/orc/blob/b21b5ffcc1efcbd4aef337fa6faae4d25262f8f1/java/core/src/java/org/apache/orc/impl/RecordReaderImpl.java#L273 throw an exception.

I didn't look at parquet. It may have a similar issue.

Thank you, [~zsxwing]. It took some time, but ORC-416 is created as a first partial attempt. I'll try to proceed further.

[~zsxwing]. ORC-416 is a first partial attempt to make ORC robust (as I mentioned previously.)

I think we had better have a more consistent test coverage for Spark task cancellation.

`FileBasedDataSourceSuite` happens to expose the issue for ORC and Parquet, but it's not consistent. Without a consistent reproducible environment, we cannot guarantee that ORC/Parquet has no issue for this.

So, I'm still looking at Spark task cancellation process, too. Please allow me more time.


[~dongjoon] when spark cancels a task, the task thread will get interrupted. I think this is what we need to test in Spark. I don't think Spark needs to have a great test coverage for codes inside third party libraries. It's unlikely we can reproduce this issue consistently without changing the third party libraries, since this will require to cancel a task when it's running some special codes in a third party library.

Yep. Thank you for the guide, [~zsxwing].

As a second approach, ORC-419 is created.

[~zsxwing], [~cloud_fan], and [~smilegator].
Both ORC-416 and ORC-419 are fixed and will be released as next ORC release 1.5.4 and 1.6.0.
And, I'm still looking around both sides (ORC and Spark) for this issue.

Thanks!

I'm closing this issue since this was resolved as a part of SPARK-26427 and I didn't hit this issues until now in our Jenkins environment. Please feel free to reopen this if there is another instance of this failure.

