Where is the sudo added? in the root node or inside the container? 

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m 10s{color} | {color:red} HDDS-1712 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HDDS-1712 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12973486/HDDS-1712.001.patch |
| Console output | https://builds.apache.org/job/PreCommit-HDDS-Build/2747/console |
| versions | git=2.17.1 |
| Powered by | Apache Yetus 0.10.0 http://yetus.apache.org |


This message was automatically generated.



{quote}Where is the sudo added? in the root node or inside the container? {quote}

In the container.

The patch applies to remotes/origin/runner-latest branch in https://github.com/apache/hadoop-docker-ozone repository.

So are you saying that some being root inside a container, becomes root on the base node ? is that what you are claiming ? 

 

Yes, it is possible, see [CVE-2019-5736|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5736].

It was a bug that is fixed. Why will I change my code for an already fixed bug in some other products code base? if that is so; Ozone and Hadoop will have to write special code for all known CVE.  I am sure you understand how insane that is.

So unless you can show me a real issue, I don't want to commit this.

Not an issue, CVE is known and addressed.

For the records: I am totally open the remove sudo, if it's required. But there were multiple problems with the uploaded patch:
 
   1. The attached patch was NOT tested with the existing cluster definitions (smoketests + kubernetes clusters). It breaks existing functionality without fixing the newly introduced problems
   2. Smaller problem but it's not clearly defined how the problem can be reproduced (assuming CVE is fixed in the environment)

[~elek] 
Root can jail break from container, when mounting host level files is allowed, such as mounting /etc/passwd, /proc, /sys/fs.  In the Pull Request #1053, it demonstrates the danger to give hadoop user root privileges without restriction.  By printing a write line to /etc/passwd file, this allows hadoop user to install a root user into host.  Hadoop user has the power to create chaos, when too much privileges is given.  We can remove the risk by giving it non-root privileges access in container.

Hadoop user is given sudo access for binary installation during test runtime.  The flow of package installation logic can happen during compilation or package phase of maven build cycle.  By removing the sudo access, it will force developer to rethink how to instrument test into the running container more efficiently without the duplicated downloads of test framework from internet in the current smoke test.  If we can expand on the idea to build docker image after tarball creation (HDDS-1495) rather than current runner image layout, then forward progress would be easier.  I find it difficult to operate in reactive approach to remove sudo requirement and make the current smoke test work with ozone-runner or hadoop-runner because:

# The sudo code is in a separate branch from smoke test.  I can not make smoke test changes in this ticket because smoke test logic resides in another branch.
# Many binary download and installation during test run.  It takes quite a long time to repeat install binaries during test run.  On flaky internet, the test cases fails more frequently due to inability to install test framework rather than running the tests.
# The current smoke tests and Kubernetes cluster are working with replication factor of 1, and many tests are using empty core-site.xml, hence, the disk operations are not distributed.  Hence, I found the current smoke test confusing because the test parameters are invalid.
# Need on demand configuration changes - maven resource templating allows to modify environment variables prior to startup of test runs.  There is a mismatch between test generated volume and bucket and core-site.xml configuration.  Bucket creation sequence and configuration file generation, and daemon startup are in non-specific order.  The current tests are masking problems because a empty configuration leading to use local disk and allowed some tests to pass.

To properly address those problems, the conversations are much longer ones.  This is my reasoning to narrow the scope of this patch to first step of removing the root power.  Would you be open to fix smoke test on a follow up ticket?



bq. Root can jail break from container, when mounting host level files is allowed, such as mounting /etc/passwd, /proc, /sys/fs. In the Pull Request #1053, it demonstrates the danger to give hadoop user root privileges without restriction. By printing a write line to /etc/passwd file, this allows hadoop user to install a root user into host. Hadoop user has the power to create chaos, when too much privileges is given. We can remove the risk by giving it non-root privileges access in container.

See my comment in the pull request, this is an independent problem. Even without sudo I can do the same (use ubuntu image + mount host path)

bq.  Would you be open to fix smoke test on a follow up ticket?

Definitely not. This patch breaks something which works currently. If some of the mentioned points makes harder to post a proper, fully functional patch, please fix that issue *in advance* . Thanks a lot.


bq. The current smoke tests and Kubernetes cluster are working with replication factor of 1.

I am not sure about kubernetes. Can you please prove this statement (for kubernetes).

[~elek] 

{quote}Definitely not. This patch breaks something which works currently. If some of the mentioned points makes harder to post a proper, fully functional patch, please fix that issue in advance . Thanks a lot.{quote}

This is quite disappointing.  Two branches arrangement makes it not possible to provide fully functional patch upfront.  The docker image must be committed, and produced a version, then the sequent patch can reference to the docker image.  It is not possible to provide a fully functional patches, unless a commit and build tag has been made.

In your own code change, you have done exactly this in HDDS-1799.  You are committing pull request 4 without a fully functional pull request 1105.  If you give yourself a lower standard because you are in control of the source code.  Why do you ask higher standard from others?  You should not use the double standard on others if you can not meet your own terms.

I will provide a second patch for review, but it will not be the exact code to be commit because of the two phase commit issues in current code structure.  Would you be open to 99% functional patch for the second patch?

{quote}I am not sure about kubernetes. Can you please prove this statement (for kubernetes).{quote}

I can't find required core-site.xml values in k8s examples.

{code}
$ pwd
/home/eyang/test/hadoop/hadoop-ozone/dist/src/main/k8s/examples
[eyang@localhost examples]$ grep -R CORE-SITE *
[eyang@localhost examples]$
{code}

How does Kubernetes test work, if core-site.xml contain no configuration?  Please educate me the process of config files to be generated for Kubernetes.

{quote}See my comment in the pull request, this is an independent problem. Even without sudo I can do the same (use ubuntu image + mount host path){quote}

Please demonstrate.  If -u ${UID}:${GID} is enforced, and UID does not have sudo access, and host mounting paths are permissively allowed?  docker -u flag and mounting path can be audited before source code is committed.  By implementing a few simple procedures, this will make Ozone docker image more secure and less abuse on root power.  We should not provide false impression to user that we are starting with -u hadoop, then go behind user's back to run sudo curl install.  Otherwise, it breaks user's trust to use Ozone-runner image.

1. Feel free to upload two patches. As in HDDS-1799, we can commit them in the right order. But we need full fix. If the patches are committed in the right order, all the examples, tests, cluster definitions should work well and they shouldn't be broken. That's exactly the same requirement what HDDS-1799 has.

2. grep for OZONE-SITE instead of CORE-SITE? The workflow is very similar to the docker-compose clusters just using kubernetes configmap instead of env files.

If I understood well we can agree that the mentioned statement was not true and kubernetes examples doesn't use replication factor 1.

bq. Please demonstrate. If -u ${UID}:${GID} is enforced

It's not enforced. As you can add additional mount, the uid lines also can be removed.

{quote}It's not enforced. As you can add additional mount, the uid lines also can be removed.{quote}

Not enforcing security is exactly what went wrong in smoke test code evolution.  Why are we still arguing against enforcing security?

{quote}2. grep for OZONE-SITE instead of CORE-SITE? The workflow is very similar to the docker-compose clusters just using kubernetes configmap instead of env files.{quote}

Kubernetes configmap of trunk looks like this:
{code}
data:
  OZONE-SITE.XML_hdds.datanode.dir: /data/storage
  OZONE-SITE.XML_ozone.scm.datanode.id.dir: /data
  OZONE-SITE.XML_ozone.metadata.dirs: /data/metadata
  OZONE-SITE.XML_ozone.scm.block.client.address: scm-0.scm
  OZONE-SITE.XML_ozone.om.address: om-0.om
  OZONE-SITE.XML_ozone.scm.client.address: scm-0.scm
  OZONE-SITE.XML_ozone.scm.names: scm-0.scm
  OZONE-SITE.XML_ozone.enabled: "true"
  LOG4J.PROPERTIES_log4j.rootLogger: INFO, stdout
  LOG4J.PROPERTIES_log4j.appender.stdout: org.apache.log4j.ConsoleAppender
  LOG4J.PROPERTIES_log4j.appender.stdout.layout: org.apache.log4j.PatternLayout
  LOG4J.PROPERTIES_log4j.appender.stdout.layout.ConversionPattern: '%d{yyyy-MM-dd
    HH:mm:ss} %-5p %c{1}:%L - %m%n'
{code}

There is no core-site.xml generated.  How can the test case be valid?

{quote}If I understood well we can agree that the mentioned statement was not true and kubernetes examples doesn't use replication factor 1.{quote}

I can agree on replication factor of 1 does not apply to k8s tests, but it doesn't change the fact that the current k8s tests uses invalid core-site.xml, hence the test results passing are questionable.

Reopen because security is important.

bq. but it doesn't change the fact that the current k8s tests uses invalid core-site.xml, hence the test results passing are questionable.

Did you try to run it or this is just your guess? Is core-site.xml optional or required? 

bq. Not enforcing security is exactly what went wrong in smoke test code evolution. Why are we still arguing against enforcing security?

I think I talked about a totally different thing. *If* you assume that an adversary can modify docker-compose.yaml (as you did in your PR), then you can't guarantee that uid/gid is used/enforced in docker-compose.yaml 

This whole discussion is pointless. Here is why. The images are like documentation. They are examples. We don't release them to any one. They are like binary artifacts. We cannot in good faith start supporting docker images as first class objects of Ozone releases.

 

There are too many combinations:
 # OS - Which flavor of linux should I use ?
 # JVM  - We ship with JVM 11, I am not sure if Hadoop even formally has finished a run with JVM.
 # All the other applications like profiler, debuggers etc. etc.

realistically, we should add comment into the docker files saying this is an example, and get away from the completely point less discussion.

 

Once again, Apache does not release binary artifacts, so spending too much time on this line of developement is not productive and few weeks later someone will have a different opinion. Like why are we not using JVM 12.. This is a non-ending, fertile for trolling kind of JIRA.

 

We already have documentation that docker images are examples and we should just stick to that.

 

Thanks

Anu

 

Docker images are examples: we have clearly documented that in the Ozone-0.4.1 Documentation. Therefore this discussion is point less. I am resolving this JIRA.

 

[~elek] core-site.xml is required because fs.defaultName needs to be specified.  If there is no core-site.xml with volume and bucket in URL, then the test code does not test Ozone.

[~anu] Doesn't Ozone quick start guide refer to use docker-compose to start the cluster?  This puts Docker image on the critical path for most users to try it out.  Why ask people to try it out with docker, if you have no intention to finish what you started?

bq. If there is no core-site.xml with volume and bucket in URL, then the test code does not test Ozone.

Did you try it out, or is this your expectation?

{quote}Did you try it out, or is this your expectation?{quote}

I tried it out using [Deploy Ozone to Kubernetes|https://cwiki.apache.org/confluence/display/HADOOP/Deploy+Ozone+to+Kubernetes] instructions on Ozone wiki.  No successful cluster deployed.  Is this instruction up to date?

Yes, AFAIK it's fine. If you have any error message, let me know. Happy to help. (But maybe not in this jira, but using the usual channels...)

(I am just wondering: If you can't deploy, how do you know how does it work? How do you know if it's wrong...)

{quote}[~anu] Doesn't Ozone quick start guide refer to use docker-compose to start the cluster? This
{quote}
It does not; that examples also say you can start a cluster on your laptop and you can run the process inside a single docker container. Those are for people who are wondering what Ozone is?. Those are not instructions for the real product. As usual, you are getting confused between the real product and documentation examples.

 

{quote}Yes, AFAIK it's fine. If you have any error message, let me know. Happy to help. (But maybe not in this jira, but using the usual channels...)

(I am just wondering: If you can't deploy, how do you know how does it work? How do you know if it's wrong...){quote}

According to kubctl output, the pod configuration does not have 3 datanodes:

{code}
$ kubectl get pod
NAME         READY   STATUS    RESTARTS   AGE
datanode-0   0/1     Pending   0          11m
om-0         0/1     Pending   0          11m
s3g-0        0/1     Pending   0          11m
scm-0        0/1     Pending   0          11m
{code}

ozone.replication is not set to 1, how does this work?

Pod configuration in json format indicates there is no environment variables for CORE-SITE.XML.  How does this work?

{code}
$ kubectl get pod -o json 
{
    "apiVersion": "v1",
    "items": [
        {
            "apiVersion": "v1",
            "kind": "Pod",
            "metadata": {
                "annotations": {
                    "prdatanodeetheus.io/path": "/prom",
                    "prdatanodeetheus.io/port": "9882",
                    "prdatanodeetheus.io/scrape": "true"
                },
                "creationTimestamp": "2019-07-17T19:30:41Z",
                "generateName": "datanode-",
                "labels": {
                    "app": "ozone",
                    "component": "datanode",
                    "controller-revision-hash": "datanode-5f4d6556b8",
                    "statefulset.kubernetes.io/pod-name": "datanode-0"
                },
                "name": "datanode-0",
                "namespace": "default",
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "blockOwnerDeletion": true,
                        "controller": true,
                        "kind": "StatefulSet",
                        "name": "datanode",
                        "uid": "449168e5-c9b9-443c-b65b-475a97e64710"
                    }
                ],
                "resourceVersion": "99413",
                "selfLink": "/api/v1/namespaces/default/pods/datanode-0",
                "uid": "46f1ad81-312e-4e33-b0a7-8496937511bd"
            },
            "spec": {
                "affinity": {
                    "podAntiAffinity": {
                        "requiredDuringSchedulingIgnoredDuringExecution": [
                            {
                                "labelSelector": {
                                    "matchExpressions": [
                                        {
                                            "key": "component",
                                            "operator": "In",
                                            "values": [
                                                "datanode"
                                            ]
                                        }
                                    ]
                                },
                                "topologyKey": "kubernetes.io/hostname"
                            }
                        ]
                    }
                },
                "containers": [
                    {
                        "args": [
                            "ozone",
                            "datanode"
                        ],
                        "envFrom": [
                            {
                                "configMapRef": {
                                    "name": "config"
                                }
                            }
                        ],
                        "image": "eyang/ozone:0.5.0-SNAPSHOT",
                        "imagePullPolicy": "IfNotPresent",
                        "name": "datanode",
                        "resources": {},
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "volumeMounts": [
                            {
                                "mountPath": "/data",
                                "name": "data"
                            },
                            {
                                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                                "name": "default-token-phlhw",
                                "readOnly": true
                            }
                        ]
                    }
                ],
                "dnsPolicy": "ClusterFirst",
                "enableServiceLinks": true,
                "hostname": "datanode-0",
                "priority": 0,
                "restartPolicy": "Always",
                "schedulerName": "default-scheduler",
                "securityContext": {
                    "fsGroup": 1000
                },
                "serviceAccount": "default",
                "serviceAccountName": "default",
                "subdomain": "datanode",
                "terminationGracePeriodSeconds": 30,
                "tolerations": [
                    {
                        "effect": "NoExecute",
                        "key": "node.kubernetes.io/not-ready",
                        "operator": "Exists",
                        "tolerationSeconds": 300
                    },
                    {
                        "effect": "NoExecute",
                        "key": "node.kubernetes.io/unreachable",
                        "operator": "Exists",
                        "tolerationSeconds": 300
                    }
                ],
                "volumes": [
                    {
                        "name": "data",
                        "persistentVolumeClaim": {
                            "claimName": "data-datanode-0"
                        }
                    },
                    {
                        "name": "default-token-phlhw",
                        "secret": {
                            "defaultMode": 420,
                            "secretName": "default-token-phlhw"
                        }
                    }
                ]
            },
            "status": {
                "conditions": [
                    {
                        "lastProbeTime": null,
                        "lastTransitionTime": "2019-07-17T19:30:41Z",
                        "message": "pod has unbound immediate PersistentVolumeClaims (repeated 4 times)",
                        "reason": "Unschedulable",
                        "status": "False",
                        "type": "PodScheduled"
                    }
                ],
                "phase": "Pending",
                "qosClass": "BestEffort"
            }
        },
        {
            "apiVersion": "v1",
            "kind": "Pod",
            "metadata": {
                "annotations": {
                    "prometheus.io/path": "/prom",
                    "prometheus.io/port": "9874",
                    "prometheus.io/scrape": "true"
                },
                "creationTimestamp": "2019-07-17T19:30:41Z",
                "generateName": "om-",
                "labels": {
                    "app": "ozone",
                    "component": "om",
                    "controller-revision-hash": "om-5df5c7cc57",
                    "statefulset.kubernetes.io/pod-name": "om-0"
                },
                "name": "om-0",
                "namespace": "default",
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "blockOwnerDeletion": true,
                        "controller": true,
                        "kind": "StatefulSet",
                        "name": "om",
                        "uid": "d2bed5a9-e694-4219-a58a-98245726b67d"
                    }
                ],
                "resourceVersion": "99428",
                "selfLink": "/api/v1/namespaces/default/pods/om-0",
                "uid": "b961bde3-c865-42a0-8617-39c384c61f7a"
            },
            "spec": {
                "containers": [
                    {
                        "args": [
                            "ozone",
                            "om"
                        ],
                        "env": [
                            {
                                "name": "WAITFOR",
                                "value": "scm-0.scm:9876"
                            },
                            {
                                "name": "ENSURE_OM_INITIALIZED",
                                "value": "/data/metadata/om/current/VERSION"
                            }
                        ],
                        "envFrom": [
                            {
                                "configMapRef": {
                                    "name": "config"
                                }
                            }
                        ],
                        "image": "eyang/ozone:0.5.0-SNAPSHOT",
                        "imagePullPolicy": "IfNotPresent",
                        "livenessProbe": {
                            "failureThreshold": 3,
                            "initialDelaySeconds": 30,
                            "periodSeconds": 10,
                            "successThreshold": 1,
                            "tcpSocket": {
                                "port": 9862
                            },
                            "timeoutSeconds": 1
                        },
                        "name": "om",
                        "resources": {},
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "volumeMounts": [
                            {
                                "mountPath": "/data",
                                "name": "data"
                            },
                            {
                                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                                "name": "default-token-phlhw",
                                "readOnly": true
                            }
                        ]
                    }
                ],
                "dnsPolicy": "ClusterFirst",
                "enableServiceLinks": true,
                "hostname": "om-0",
                "priority": 0,
                "restartPolicy": "Always",
                "schedulerName": "default-scheduler",
                "securityContext": {
                    "fsGroup": 1000
                },
                "serviceAccount": "default",
                "serviceAccountName": "default",
                "subdomain": "om",
                "terminationGracePeriodSeconds": 30,
                "tolerations": [
                    {
                        "effect": "NoExecute",
                        "key": "node.kubernetes.io/not-ready",
                        "operator": "Exists",
                        "tolerationSeconds": 300
                    },
                    {
                        "effect": "NoExecute",
                        "key": "node.kubernetes.io/unreachable",
                        "operator": "Exists",
                        "tolerationSeconds": 300
                    }
                ],
                "volumes": [
                    {
                        "name": "data",
                        "persistentVolumeClaim": {
                            "claimName": "data-om-0"
                        }
                    },
                    {
                        "name": "default-token-phlhw",
                        "secret": {
                            "defaultMode": 420,
                            "secretName": "default-token-phlhw"
                        }
                    }
                ]
            },
            "status": {
                "conditions": [
                    {
                        "lastProbeTime": null,
                        "lastTransitionTime": "2019-07-17T19:30:41Z",
                        "message": "pod has unbound immediate PersistentVolumeClaims (repeated 4 times)",
                        "reason": "Unschedulable",
                        "status": "False",
                        "type": "PodScheduled"
                    }
                ],
                "phase": "Pending",
                "qosClass": "BestEffort"
            }
        },
        {
            "apiVersion": "v1",
            "kind": "Pod",
            "metadata": {
                "creationTimestamp": "2019-07-17T19:30:41Z",
                "generateName": "s3g-",
                "labels": {
                    "app": "ozone",
                    "component": "s3g",
                    "controller-revision-hash": "s3g-c7b9c5886",
                    "statefulset.kubernetes.io/pod-name": "s3g-0"
                },
                "name": "s3g-0",
                "namespace": "default",
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "blockOwnerDeletion": true,
                        "controller": true,
                        "kind": "StatefulSet",
                        "name": "s3g",
                        "uid": "cbcde998-0558-46df-9ae2-8078c5827856"
                    }
                ],
                "resourceVersion": "99442",
                "selfLink": "/api/v1/namespaces/default/pods/s3g-0",
                "uid": "4bc82c4c-0005-41d8-a9eb-6b8962b6de08"
            },
            "spec": {
                "containers": [
                    {
                        "args": [
                            "ozone",
                            "s3g"
                        ],
                        "envFrom": [
                            {
                                "configMapRef": {
                                    "name": "config"
                                }
                            }
                        ],
                        "image": "eyang/ozone:0.5.0-SNAPSHOT",
                        "imagePullPolicy": "IfNotPresent",
                        "livenessProbe": {
                            "failureThreshold": 3,
                            "httpGet": {
                                "path": "/",
                                "port": 9878,
                                "scheme": "HTTP"
                            },
                            "initialDelaySeconds": 30,
                            "periodSeconds": 10,
                            "successThreshold": 1,
                            "timeoutSeconds": 1
                        },
                        "name": "s3g",
                        "resources": {},
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "volumeMounts": [
                            {
                                "mountPath": "/data",
                                "name": "data"
                            },
                            {
                                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                                "name": "default-token-phlhw",
                                "readOnly": true
                            }
                        ]
                    }
                ],
                "dnsPolicy": "ClusterFirst",
                "enableServiceLinks": true,
                "hostname": "s3g-0",
                "priority": 0,
                "restartPolicy": "Always",
                "schedulerName": "default-scheduler",
                "securityContext": {},
                "serviceAccount": "default",
                "serviceAccountName": "default",
                "subdomain": "s3g",
                "terminationGracePeriodSeconds": 30,
                "tolerations": [
                    {
                        "effect": "NoExecute",
                        "key": "node.kubernetes.io/not-ready",
                        "operator": "Exists",
                        "tolerationSeconds": 300
                    },
                    {
                        "effect": "NoExecute",
                        "key": "node.kubernetes.io/unreachable",
                        "operator": "Exists",
                        "tolerationSeconds": 300
                    }
                ],
                "volumes": [
                    {
                        "name": "data",
                        "persistentVolumeClaim": {
                            "claimName": "data-s3g-0"
                        }
                    },
                    {
                        "name": "default-token-phlhw",
                        "secret": {
                            "defaultMode": 420,
                            "secretName": "default-token-phlhw"
                        }
                    }
                ]
            },
            "status": {
                "conditions": [
                    {
                        "lastProbeTime": null,
                        "lastTransitionTime": "2019-07-17T19:30:41Z",
                        "message": "pod has unbound immediate PersistentVolumeClaims (repeated 4 times)",
                        "reason": "Unschedulable",
                        "status": "False",
                        "type": "PodScheduled"
                    }
                ],
                "phase": "Pending",
                "qosClass": "BestEffort"
            }
        },
        {
            "apiVersion": "v1",
            "kind": "Pod",
            "metadata": {
                "annotations": {
                    "prometheus.io/path": "/prom",
                    "prometheus.io/port": "9876",
                    "prometheus.io/scrape": "true"
                },
                "creationTimestamp": "2019-07-17T19:30:41Z",
                "generateName": "scm-",
                "labels": {
                    "app": "ozone",
                    "component": "scm",
                    "controller-revision-hash": "scm-cfd995757",
                    "statefulset.kubernetes.io/pod-name": "scm-0"
                },
                "name": "scm-0",
                "namespace": "default",
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "blockOwnerDeletion": true,
                        "controller": true,
                        "kind": "StatefulSet",
                        "name": "scm",
                        "uid": "bf7e5480-e241-4c5e-b687-30643122635c"
                    }
                ],
                "resourceVersion": "99453",
                "selfLink": "/api/v1/namespaces/default/pods/scm-0",
                "uid": "aa982362-c10a-444e-b03f-d2414ba4a478"
            },
            "spec": {
                "containers": [
                    {
                        "args": [
                            "ozone",
                            "scm"
                        ],
                        "envFrom": [
                            {
                                "configMapRef": {
                                    "name": "config"
                                }
                            }
                        ],
                        "image": "eyang/ozone:0.5.0-SNAPSHOT",
                        "imagePullPolicy": "IfNotPresent",
                        "livenessProbe": {
                            "failureThreshold": 3,
                            "initialDelaySeconds": 30,
                            "periodSeconds": 10,
                            "successThreshold": 1,
                            "tcpSocket": {
                                "port": 9861
                            },
                            "timeoutSeconds": 1
                        },
                        "name": "scm",
                        "resources": {},
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "volumeMounts": [
                            {
                                "mountPath": "/data",
                                "name": "data"
                            },
                            {
                                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                                "name": "default-token-phlhw",
                                "readOnly": true
                            }
                        ]
                    }
                ],
                "dnsPolicy": "ClusterFirst",
                "enableServiceLinks": true,
                "hostname": "scm-0",
                "initContainers": [
                    {
                        "args": [
                            "ozone",
                            "scm",
                            "--init"
                        ],
                        "envFrom": [
                            {
                                "configMapRef": {
                                    "name": "config"
                                }
                            }
                        ],
                        "image": "eyang/ozone:0.5.0-SNAPSHOT",
                        "imagePullPolicy": "IfNotPresent",
                        "name": "init",
                        "resources": {},
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "volumeMounts": [
                            {
                                "mountPath": "/data",
                                "name": "data"
                            },
                            {
                                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                                "name": "default-token-phlhw",
                                "readOnly": true
                            }
                        ]
                    }
                ],
                "priority": 0,
                "restartPolicy": "Always",
                "schedulerName": "default-scheduler",
                "securityContext": {
                    "fsGroup": 1000
                },
                "serviceAccount": "default",
                "serviceAccountName": "default",
                "subdomain": "scm",
                "terminationGracePeriodSeconds": 30,
                "tolerations": [
                    {
                        "effect": "NoExecute",
                        "key": "node.kubernetes.io/not-ready",
                        "operator": "Exists",
                        "tolerationSeconds": 300
                    },
                    {
                        "effect": "NoExecute",
                        "key": "node.kubernetes.io/unreachable",
                        "operator": "Exists",
                        "tolerationSeconds": 300
                    }
                ],
                "volumes": [
                    {
                        "name": "data",
                        "persistentVolumeClaim": {
                            "claimName": "data-scm-0"
                        }
                    },
                    {
                        "name": "default-token-phlhw",
                        "secret": {
                            "defaultMode": 420,
                            "secretName": "default-token-phlhw"
                        }
                    }
                ]
            },
            "status": {
                "conditions": [
                    {
                        "lastProbeTime": null,
                        "lastTransitionTime": "2019-07-17T19:30:41Z",
                        "message": "pod has unbound immediate PersistentVolumeClaims (repeated 4 times)",
                        "reason": "Unschedulable",
                        "status": "False",
                        "type": "PodScheduled"
                    }
                ],
                "phase": "Pending",
                "qosClass": "BestEffort"
            }
        }
    ],
    "kind": "List",
    "metadata": {
        "resourceVersion": "",
        "selfLink": ""
    }
}
{code} 

Very good question, but I am afraid that this discussion is slightly diverged. I am not sure if this is the best place to discuss how ozone can be used in k8s.

I can confirm that it's possible to run multiple datanodes with the help of k8s example files (So the previous statement was not true about replication factor 1)

{code}
NAME                                 READY   STATUS             RESTARTS   AGE
datanode-0                           1/1     Running            0          7h57m
datanode-1                           1/1     Running            0          7h57m
datanode-2                           1/1     Running            0          7h57m
datanode-3                           1/1     Running            0          7h57m
datanode-4                           1/1     Running            0          7h57m
datanode-5                           1/1     Running            2          7h57m
datanode-6                           1/1     Running            0          7h57m
datanode-7                           1/1     Running            0          7h57m
datanode-8                           1/1     Running            0          7h57m
datanode-9                           1/1     Running            20         7h57m
{code}

If you think it's a bug, please open a new issue.

If you are not sure if it's a bug or in a problem in your environment, please start a discussion on the user mailing list.

And you can also ping me offline and I would be happy to help to start it in your environment. The ozone community meetings are also good places to ask such questions where I am also there and open to help to solve your problems.

[~elek] Your output seems to indicate multiple datanode pods.  This looks different than what I would expected, shouldn't it look like this:

{code}
$ kubectl get pod
NAME         READY   STATUS    RESTARTS   AGE
datanode-0   3/3     Running   0          11m
om-0         1/1     Running   0          11m
s3g-0        1/1     Running   0          11m
scm-0        1/1     Running   0          11m
{code}

Where datanode-0 pod has 3 instances running?  We can take this offline in HDDS-1825.  However, I think it is not fair to ask removal of sudo patch to include a full working smoke test code on kubernetes cluster because the kubernetes cluster code is incomplete.  I can include patch for smoke test to work with docker-compose cluster, if you are open to this.  Thoughts?

[~elek] HDDS-1712.001.hadoop-docker-ozone.patch and HDDS-1712.002.patch should remove sudo together to make Ozone-runner image less powerful.

I can only get 33 out of 110 test case pass on my own test machine without the patch.
When the patch is applied, the same result appears in smoke test report.

I don't have s3 account to validate if s3 test cases would pass.  Please help with the verification.  Thanks

I am -1; on this patch and wasteful discussion. As I have clearly said many times; these are to be treated as examples and documentation, not as part of the product. Unless there is a change in that status, I am not willing to commit this patch.

 

{quote}I am -1; on this patch and wasteful discussion. As I have clearly said many times; these are to be treated as examples and documentation, not as part of the product. Unless there is a change in that status, I am not willing to commit this patch.{quote}

With all due respect, I can not agree on this is just examples and documentation.  According [Alpha cluster|https://hadoop.apache.org/ozone/docs/0.4.0-alpha/runningviadocker.html] documentation, this is the first thing that you ask people to try.  No matter if you try Ozone from binary, or building from source, in all paths, Ozone-runner image is used.  Hence, there is no path that leads to avoid the vulnerable docker image according to Ozone website.  Although there is a path to manually setup without running smoke test and use tarball binary, this path is not documented in any known material.  Hence, this vulernable docker image puts everyone who tries Ozone at risk.  [Security is mandatory|https://www.apache.org/foundation/how-it-works.html#philosophy] is one of Apache's guiding principal.  Please be considerate for others at minimum fully document tarball instructions to avoid the mistake, or simply polish the code to a more presentable state before release.

First of all, there is no vulnerability. That is just FUD that is being spewed by you. If there is a CVE in Docker world; the fix is to upgrade docker. So I completely disagree.

 

Second, the example of running Docker on your machine means that you need to be able to install Docker, which implies that you are an admin on that machine. if not, you cannot run this. Now your argument is that someone can write some code which has some issue and my answer has been that what you are saying can be done with Hadoop as well. someone can write backdoors, and that is why we have committers. To make sure that someone does not do random crap like this.

 

The third and most important point, the quick start guide, explains what Ozone is. It is not a guide on how to run Ozone. I gather that you have never taken a look at the current documentation on trunk or 0.4.1.

 

So I am still against you wasting a countless hour with pointless discussion and I am -1;

 
{quote}Hence, this vulernable docker image puts everyone who tries Ozone at risk
{quote}
This is the random stuff that you keep on saying without any merit each time. Case in point when you told me that Ozone is full of findbugs issues and checkstyle issues. When I asked you to compare with Hadoop you ran away, because like this it was blatantly false.

 

 

 

[~anu]  {quote}Case in point when you told me that Ozone is full of findbugs issues and checkstyle issues. When I asked you to compare with Hadoop you ran away, because like this it was blatantly false.{quote}

With regard to findbug issues, Hadoop does not require Findbugs jar file on the classpath at runtime.  Most of Hadoop findbugs exclusion were to deal with Object serialization generated with protobuf codegen.  The bugs flagged manually because of codegen and unfortunate compatibility reasons with keep up FSImage mutations.  They are only used as last resort.  Ozone uses annotation to suppress findbugs rather quickly and the bugs are not at the same level that is hard to solve in Hadoop.  The usage is very different.  Why having Findbugs on the classpath is not good?  Findbugs depends on older XML parser, which has CVE vulnerabilities.  If we don't need the jar file in the class, please remove it from runtime.  It is hard to identify how people would misuse vulnerabilities when a collections of them are hidden in the software.  Due diligence would help to keep security bugs down.  I offered the patches, and Marton said it's good to fix them.  Whether you accept or reject the patches is your choice.  If you allow sudo in the container, you will only end up with more code that does remote root download and execution at runtime.  This makes Ozone more unpredictable and dangerous.  It will be hard to clean up later.

