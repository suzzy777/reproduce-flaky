I've been trying to reproduce this locally but couldn't so far. It passed [here|https://app.circleci.com/pipelines/github/nastra/cassandra/1/workflows/fe66d000-5fc0-4248-a4cf-2cc8a740377d/jobs/28] and [here|https://app.circleci.com/pipelines/github/nastra/cassandra/2/workflows/dc6ad38a-5949-4c73-9b7f-7f1cedfd790b/jobs/31]

Hi [~eduard.tudenhoefner]
Yes, those are hard to reproduce. They fail only with certain configurations.
It is tricky.
From the CircleCI link, from the commit, this is the config Kevin used when he hit it:
https://github.com/newkek/cassandra/blob/0d2d34e08768b07c6e703bb085316042611e7981/.circleci/config.yml

I will file a different but related JIRA, CASSANDRA-15650 broke this test and causing it to hang consistently.  I checked the original build and see it didn't have CASSANDRA-15650, so the timeout happened before CASSANDRA-15650, but now consistent with CASSANDRA-15650.

Created CASSANDRA-15689 to fix the regression. This shouldn't fix the timeout as that looks to be before CASSANDRA-15650

For completeness, here's the history on Jenkins: https://ci-cassandra.apache.org/view/branches/job/Cassandra-trunk/45/testReport/junit/org.apache.cassandra.distributed.test/CasWriteTest/history/

The current implementation of this test is non-deterministic, it executes two diverging compare-and-set statements in a while-true loop until it breaks QUORUM. The issue is it may throw a WriteTimeoutException before that happens, because the loop creates too many requests. At the time this ticket was opened, the timeout exception was masked due to a poor assert statement, but that specifically has been addressed already.

Another issue was that the generated PK for the INSERT statements was non-final in the test method, it was overwritten at every iteration of the loop, so multiple threads would see only the last assigned value of the PK, which contributed to the overhead.

I have re-written the test to make it deterministic. Now it will intercept the {{PAXOS_PROPOSE_RSP}} messages from the nodes back to the coordinator and replace them with failure responses, such that QUORUM is never reached and it will throw the expected {{CasWriteUnknownResultException}}. This way it removes the "chance" aspect, but still exercises the same code path (which is mostly testing o.a.c.s.StorageProxy::cas).

[PR|https://github.com/grighetto/cassandra/pull/2]

[JVM 11 Tests|https://app.circleci.com/pipelines/github/grighetto/cassandra/17/workflows/f9930861-5765-469f-970e-d4cf6a08563b/jobs/91]

[JVM 8 Tests|https://app.circleci.com/pipelines/github/grighetto/cassandra/17/workflows/2cfc7b5f-6acc-420f-9fcd-ee4abfe3aa8a/jobs/92]

[~gianluca] I was wondering, you managed to run it many times locally without an error, right?
I wanted to attach a log of 100 successful runs but every fifth run the test fails as in the attached screenshot.
Any idea why?

Otherwise I am about to give +1.

Interesting approach. If I am reading the code correctly, the test makes sure the response from peers are always false. So the first CAS backs off and returns unknown result. 

I think overwriting the response is not necessary. You can precisely construct such a contention condition with {{CountDownLatch}} and 2 filters. 
The test can block the PROPOSE req until the second CAS is completed. So the first CAS is guaranteed to get unknown result. Something looks like the following. Therefore, you do not need to open the access to methods in {{Instance.java}} and {{DistributedTestSnitch.java}} and overwrites messages. 
{code:java}
            CountDownLatch ready = new CountDownLatch(1);
            cluster.filters().verbs(Verb.PAXOS_PROPOSE_REQ.id).from(1).to(2, 3).messagesMatching((from, to, msg) -> {
                if (to == 2)
                {
                    // Inject a single CAS request in-between prepare and propose phases
                    cluster.coordinator(2).execute(mkCasInsertQuery((a) -> pk, 1, 2),
                                                   ConsistencyLevel.QUORUM);
                    ready.countDown();
                } else {
                    Uninterruptibles.awaitUninterruptibly(ready);
                }
                return false;
            }).drop(); // the message intercepting syntax is sort of confusing...
{code}



[~e.dimitrova] if you can always reproduce the MetaSpace OOM after several iterations, it is certain that there is a memory leak at MetaSpace.

Probably the {{classLoader}} in {{IsolatedExecutor}} is not closed completely.  

[~e.dimitrova] I've seen metaspace exceptions around the in-jvm tests a couple of months ago, I don't think it's anything specific to this test. At the time I found similar issues reported in this ticket: CASSANDRA-14922. Actually, in order to run the entire in-jvm suite locally, I have to increase -XX:MaxMetaspaceSize.

When I run this test in a bash-loop in the command line, it always passes now and I don't see any metaspace errors. I'm not sure about running within in the IDE as you're there though.

Thanks [~yifanc]! I tested your suggestion and incorporated it in my pull request. It works great and I was able to get rid of the whole message serialization/deserialization dance.

[PR|https://github.com/grighetto/cassandra/pull/2]

[Java 11 Tests|https://app.circleci.com/pipelines/github/grighetto/cassandra/18/workflows/d6459a97-0a44-4720-8f85-0d8db11029a2/jobs/96]

[Java 8 Tests|https://app.circleci.com/pipelines/github/grighetto/cassandra/18/workflows/e549dfac-d53c-4ebf-bcbd-efa537c0b740/jobs/98]

 

Thanks [~gianluca]. LGTM.
+1

_I am not a committer. [~ifesdjeen], would you like to take a look at the CAS test fix?_

+1 from me as well

Thanks for the patch!

Committed as [fdcd0dff216d9e1ad242be1a7d5be3ef67044ac3|https://github.com/apache/cassandra/commit/fdcd0dff216d9e1ad242be1a7d5be3ef67044ac3] to [trunk|https://github.com/apache/cassandra/tree/trunk].


