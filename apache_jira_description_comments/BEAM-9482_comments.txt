Seems like this test has been failing for two weeks.

 

Michał, will you be able to look into this ?

 

(since seems like you updated this test recently. Please unassign if not)

cc: [~lgajowy]

I think I know why the test started failing.

This is all due to "No space left on device" error some time ago, the Kafka cluster started on the K8s didn't get cleaned up. This blocked the ports for all subsequent builds. I don't have sufficient access to clean it up, [~chamikara] can you reassign to someone that will be able to clean the k8s cluster?

Haven't been able to look into this. ccing some folks who might be able to comment regarding access.

cc: [~alanmyrvold] [~yifanzou]

This issue is P2 but has been unassigned without any comment for 60 days so it has been labeled "stale-P2". If this issue is still affecting you, we care! Please comment and remove the label. Otherwise, in 14 days the issue will be moved to P3.

Please see https://beam.apache.org/contribute/jira-priorities/ for a detailed explanation of what these priorities mean.


This issue was marked "stale-P2" and has not received a public comment in 14 days. It is now automatically moved to P3. If you are still affected by it, you can comment and move it back to P2.

I don't see this happening recently, is it a flake? Or has it been resolved?

It's been happening consistently for the last several runs.

Could this be done in the .groovy file using a `ServerSocket` to open 3 random ports, close them, then pass them in the pipeline options?

Something like [https://github.com/apache/beam/pull/15152/files]

The test is still failing consistently.

 

Random port don't might be tricky to implement as the port has to be communicated by Kafka cluster to beam pipeline.

 

I tried [https://github.com/apache/beam/pull/15187] but it is failing with 
{code:java}
//     java.lang.RuntimeException: java.lang.IllegalStateException: Could not find any partitions info. Please check Kafka configuration and make sure that provided topics exist.
        at org.apache.beam.runners.dataflow.ReadTranslator.translateReadHelper(ReadTranslator.java:55)
        at org.apache.beam.runners.dataflow.DataflowRunner$StreamingUnboundedRead$ReadWithIdsTranslator.translate(DataflowRunner.java:1886)
        at org.apache.beam.runners.dataflow.DataflowRunner$StreamingUnboundedRead$ReadWithIdsTranslator.translate(DataflowRunner.java:1882)
        at org.apache.beam.runners.dataflow.DataflowPipelineTranslator$Translator.visitPrimitiveTransform(DataflowPipelineTranslator.java:515)
        at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:593)
        at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:585)
        at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:585)
        at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:585)
        at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:585)
        at org.apache.beam.sdk.runners.TransformHierarchy$Node.access$500(TransformHierarchy.java:240)
        at org.apache.beam.sdk.runners.TransformHierarchy.visit(TransformHierarchy.java:214)
        at org.apache.beam.sdk.Pipeline.traverseTopologically(Pipeline.java:469)
        at org.apache.beam.runners.dataflow.DataflowPipelineTranslator$Translator.translate(DataflowPipelineTranslator.java:448)
        at org.apache.beam.runners.dataflow.DataflowPipelineTranslator.translate(DataflowPipelineTranslator.java:190)
        at org.apache.beam.runners.dataflow.DataflowRunner.run(DataflowRunner.java:991)
        at org.apache.beam.runners.dataflow.DataflowRunner.run(DataflowRunner.java:191)
        at org.apache.beam.sdk.Pipeline.run(Pipeline.java:323)
        at org.apache.beam.sdk.testing.TestPipeline.run(TestPipeline.java:398)
        at org.apache.beam.sdk.testing.TestPipeline.run(TestPipeline.java:334)
        at org.apache.beam.sdk.io.kafka.KafkaIOIT.testKafkaIOReadsAndWritesCorrectlyInStreaming(KafkaIOIT.java:152)

        Caused by:
        java.lang.IllegalStateException: Could not find any partitions info. Please check Kafka configuration and make sure that provided topics exist.
            at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState(Preconditions.java:507)
            at org.apache.beam.sdk.io.kafka.KafkaUnboundedSource.split(KafkaUnboundedSource.java:70)
            at org.apache.beam.runners.dataflow.internal.CustomSources.serializeToCloudSource(CustomSources.java:87)
            at org.apache.beam.runners.dataflow.ReadTranslator.translateReadHelper(ReadTranslator.java:52)
            ... 19 more

{code}

Ankur, are you still working on this? If not, would you know who knows enough about the Kafka tests to own this?

[~chamikara] [~heejong] [~amyrvold] - Could one of you suggest how we can address these test issues? Could we do a one time disk cleanup? Or restart the cluster? Or increase the disk size?

Seems like recent attempts for a permanent solution (picking a random port) failed.

[https://github.com/apache/beam/pull/15152]

[https://github.com/apache/beam/pull/15187]

 

AFAICT this is a local Kubernetes cluster, right ?

Does someone have access to restart Jenkins nodes (as a temporary solution) ?

I checked the jenkins nodes, the ports were not open. I noticed though that the failures began on July 8th, right around the time that a seed job was run on [https://github.com/apache/beam/pull/15090/] (e.g. https://github.com/apache/beam/pull/15090#issuecomment-876639555).

 

Job passed at July 8 6:45pm: [https://ci-beam.apache.org/job/beam_PerformanceTests_Kafka_IO/2505/]

Job failed, then continued to fail, after running on the PR mentioned at July 8 8:19pm: [https://ci-beam.apache.org/job/beam_PerformanceTests_Kafka_IO/2506/]

 

This is curious though because the seed jobs run every 6 hours from HEAD. So it seems like this shouldn't be causing a problem, but the timing is just so supicious.

 

Folks could we fix or disable this test. It is not providing any valuable information. And it is blocking some PRs.

/cc [~kenn]

Adding to {{flake}} label. Also if it is a currently failing test that constitutes an outage and is P0. For now, leaving at P1. We should not have so many unaddressed P1s or flakes; it loses its meaning.

This test is not flaky, it is perma-red. We can either fix it (if it is easy to do so), disable and plan to fix it (if it requires more work), or delete it (if it is not useful.)

IIUC, moving this to a hosted kafka solution would make it a more stable benchmark.

+1 for disabling and potentially re-enabling with a more stable cluster.

This issue is assigned but has not received an update in 30 days so it has been labeled "stale-assigned". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.

This Jira ticket has a pull request attached to it, but is still open. Did the pull request resolve the issue? If so, could you please mark it resolved? This will help the project have a clear view of its open issues.

This issue was marked "stale-assigned" and has not received a public comment in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.

This issue has been migrated to https://github.com/apache/beam/issues/20333

