Experience has shown that folks who aren't used to Github don't realize that annotations are appearing in the source code and they don't actually need to go look at logs.  In the case of Hadoop, I'm sure YETUS-1100 doesn't help.  There are likely other things that aren't getting floated up correctly either.  Just bringing comments back is, frankly, a huge step backward.

While you think bringing comment back is a step backward, the current situation is not good for us. Even the annotations are working as expected, the github comments are still useful for us because:

- The comment notifies us that the build is finished. The GitHub status does not notify.
- The elapsed times are included in the comment. They are important metrics for long running job.
- We can view the history of build summary by just scrolling the screen of the PR.

> Even the annotations are working as expected

 Doing a quick pass,  no, they aren't.  If that was the case, cc, javac, test4test, and "can't apply patch" errors would be showing up properly in the 'Files Changed' output or in the Checks screen.  There have been quite a few Hadoop PRs that aren't showing these annotations. In some cases (like the latter two) that's missing functionality.

> The comment notifies us that the build is finished. The GitHub status does not notify.

Sure it does.  Jenkins updates the status in the PR listing with a yellow dot and a "continuous-integration/jenkins/pr-merge Pending â€” This commit is being built "  as soon as the PR is kicked off. I'm literally looking at the top of the PR chain now as running without even having to look at Jenkins.

> The elapsed times are included in the comment. They are important metrics for long running job.

The community in general hasn't cared about how long stuff runs in probably a decade.  Besides, if Hadoop actually cared about how long stuff ran, this value wouldn't be this high: https://github.com/apache/hadoop/blob/trunk/dev-support/Jenkinsfile#L26 . It has gone up by 5-6 hours since I last looked a few years ago.

> We can view the history of build summary by just scrolling the screen of the PR.

The final history is kept with the PR.  The continual running list isn't particularly useful if commits are getting properly squashed and just adds noise.



bq. It has gone up by 5-6 hours since I last looked a few years ago.

There are many new features and new tests introduced in the few years, and sometimes we have to run all the unit tests for drastic changes (e.g., protocol buffers upgrade). Therefore the timeout is extended.

As long as the Hadoop community refuses to parallelize more modules than just hadoop-hdfs and hadoop-common, it's hard to take the cries about long running tests seriously.  The time could be significantly smaller just by doing that.  And then there are the out of control resource usages that get so bad that even surefire can't keep things boxed in.

Merged the PR to main branch.

Unfortunately, even if we could split the modules, it's not easy to increase the parallelism because the tests will become flaky due to some resource collision. Recently I've decreased the parallelism from 4 to 2 in HDFS-15731.

I'm assuming Hadoop is (still) doing integration tests instead of unit tests or the filehandle leak still there.  Lowering the parallelism is just putting another band-aid on top of a gushing wound.

The point is:  Hadoop's testing is super broken and it is never going to get fixed.  It is sort of pointless to hold Yetus back because of one project that has an awful QA track record anyway.

bq. I'm assuming Hadoop is (still) doing integration tests instead of unit tests or the filehandle leak still there. Lowering the parallelism is just putting another band-aid on top of a gushing wound.

I think your assumption is right. However, I don't think the integration tests can be disabled because they are very important in distributed systems like Apache Hadoop and have discovered a lot of actual bugs.

bq. It is sort of pointless to hold Yetus back because of one project that has an awful QA track record anyway.

* As a Yetus developer: Yeah, agreed that it is pointless to hold Yetus back for one broken project.
* As a Hadoop developer: If Yetus continues to drop the old options such as GitHub PR comments, Hadoop may drop Yetus in the near future because I think it's easier to maintain the scripts rather than fixing the super broken testing. Ideally we must fix the broken testing, but it is too costly.

