[~hanishakoneru], how does this relate to HDFS-11096? Can you leave a comment on that JIRA? Thanks!

Thanks [~hanishakoneru] for reporting this. I do see this error while rolling upgrade testing Hadoop cluster. Especially creating a file while rolling upgrade is in progress and finalizing the upgrade on upgraded version. This blocks rolling upgrade functionality. 

I am bumping the JIRA to blocker as it blocks rolling upgrade.


Given there's no movement of this issue for 2 months and this is not a regression in 3.1.x, I just moved it to 3.1.2

While  Upgrading from hadoop-2.7.3 to Hadoop-3.1.0

JournalNodes' s are all running with hadoop-3.X

 RollingUpgrade of one of the NN loaded fsimage and editLogs, it just stuck  on report Block , without any errors. All Datanodes are still at hadoop-2.7.  block reporting doesn't progress anybody faced ?

++
{code:java}
The reported blocks 0 needs additional 204812  blocks to reach the threshold 1.0000 of total blocks 204812. The number of live datanodes 11 has reached the minimum number 0. {code}
++

 

DN Logs 
{code:java}
2018-08-22 16:11:44,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from -nn.node.com./X.X:8030 with standby state
2018-08-22 16:11:44,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Reported NameNode version '3.1.0' does not match DataNode version '2.7.1-Prod' but is within acceptable limits. Note: This is normal during a rolling upgrade.
2018-08-22 16:11:44,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1018191021-10.115.22.28-1436474857708 (Datanode Uuid 3057a76f-b274-492c-a774-df767a260f09) service to nn.node.com/XX.XX:8030 beginning handshake with NN
2018-08-22 16:11:44,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1018191021-10.115.22.28-1436474857708 (Datanode Uuid 3057a76f-b274-492c-a774-df767a260f09) service to nn.node.com/XX.XX:8030 successfully registered with NN{code}

[~zvenczel] [~hanishakoneru] [~rajeshhadoop] [~leftnoteasy] [~rohithsharma] How to take this forward as we are nearing 3.2 release.

Code freeze for 3.2.0 is nearing (15th Sept) and there are no contributors for this yet. Since this is a blocker, pinging [~zvenczel] [~hanishakoneru] [~rajeshhadoop] [~leftnoteasy] [~rohithsharma] [~vinayrpet] [~rakeshr] [~umamaheswararao] for further steps.

If we wont be able to finish this, I think we will need to move this to next version.

Any updates for this?

Try to fix this issue  and it works fine on my testing cluster.
This issue occurs because namenode writes new layout audit log during upgrading ,but standby namenode can not parse new layout audit log. So We can writes audit log according to the current layout version.
Upload v001 patch

Assign to me and submit the patch

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 16s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  0s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  0s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  6s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 50s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 53s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 54s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 26 new + 480 unchanged - 24 fixed = 506 total (was 504) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 59s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 77m 15s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 29s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}131m 53s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.web.TestWebHdfsTimeouts |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | HDFS-13596 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12963878/HDFS-13596.001.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 6100cf8aa15f 4.4.0-139-generic #165-Ubuntu SMP Wed Oct 24 10:58:50 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / b226958 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/26524/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/26524/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/26524/testReport/ |
| Max. process+thread count | 5379 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/26524/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



Upload v002 patch, fix checkstyle issues

| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 43s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 21m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 10s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m 31s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 17s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  9s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m  5s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 481 unchanged - 24 fixed = 482 total (was 505) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 22s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 30s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 98m 59s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 33s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}166m 14s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | HDFS-13596 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12963985/HDFS-13596.002.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 66a39dc1a67b 3.13.0-153-generic #203-Ubuntu SMP Thu Jun 14 08:52:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 8a59efe |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/26532/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/26532/testReport/ |
| Max. process+thread count | 2934 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/26532/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



Upload v003 patch to fix checkstyle

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 24s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m 20s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m  8s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  1s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 20s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  6s{color} | {color:green} hadoop-hdfs-project/hadoop-hdfs: The patch generated 0 new + 481 unchanged - 24 fixed = 481 total (was 505) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 22s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 44s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}129m 39s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 43s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}195m 10s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.datanode.TestBPOfferService |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | HDFS-13596 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12964008/HDFS-13596.003.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux b98a564cf895 3.13.0-153-generic #203-Ubuntu SMP Thu Jun 14 08:52:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 8a59efe |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/26535/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/26535/testReport/ |
| Max. process+thread count | 2847 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/26535/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



[~hanishakoneru] [~zvenczel] [~andrew.wang] Could you please take a look?

Hey [~ferhui], I took a very quick look, and I'm worried about the potential impact of this. IIUC: When the NN is restarted as 3.x, it will immediately start accepting EC-related ops. But it seems with your patch, until the upgrade is finalized (this is when the layout version will change, correct?), those EC ops will be accepted but not properly persisted in the edit log? Let me know if I am misunderstanding.

[~xkrogen] Thanks for your reply.
{quote}
When the NN is restarted as 3.x, it will immediately start accepting EC-related ops
{quote}
During upgrading namenode starts with arguments "-rollingUpgrade started" , and EC should not be supported.

{quote}
But it seems with your patch, until the upgrade is finalized (this is when the layout version will change, correct?)
{quote}
Yes. Before upgrading done, EC will not be supported

Can you explain / provider a pointer to how EC is not supported during the upgrade? I'm not familiar with such mechanisms in the NameNode.

[~xkrogen]
I think hdfs upgrading steps follow, and during upgrading, will not call EC RPC
# Before upgrading hdfs client version is 2.x, namenode & datanode version is 2.x
# During upgrading hdfs client version is 2.x, namenode & datanode version is 2.x
# After finalized, hdfs client version is 2.x , namenode & datanode version is 3.x
# Upgrade hdfs client from 2.x to 3.x

{quote}
Can you explain / provider a pointer to how EC is not supported during the upgrade?
{quote}
Get your point, maybe EC RPC call should be checked whether EC is supported ?

[~cheersyang] Could you please ask anybody to review it ?

This is an important fix, thanks [~ferhui] for working on this. I am not familiar with this part, so pinging few folks internally to see if they can help to review. Meanwhile, glad to see [~xkrogen] is helping on this :)

I am reviewing the patch.
{quote}This issue occurs because namenode writes new layout audit log during upgrading ,but standby namenode can not parse new layout audit log. So We can writes audit log according to the current layout version.
{quote}
I'm pretty sure you meant to say "edit log" instead of "audit log".

I think not being able to accept EC requests prior to the completion of upgrade, is a reasonable trade-off. You can check layout version within {{FSNamesystem#startFileInt}}, reject when CreateFlag.SHOULD_REPLICATE is false or ecPolicyName is not empty. Other EC RPCs that should be checked include setErasureCodingPolicy.

No tests. But you have done the manual test so that's ok.

Further notes: if we want to support rolling upgrades, we should define the "to" and "from" version supported. I've not done any rolling upgrade test myself. [~ferhui] what "to" and "from" versions do you have? I think if we can support 2.8 it'll make the most of the community happy.

Release note or documentation, please. 
 Things that should be documented – minimum supported versions, caveats,

[~weichiu] Thanks for your review
{quote}
I'm pretty sure you meant to say "edit log" instead of "audit log".
{quote}
Yes

{quote}
what "to" and "from" versions do you have?
{quote}
Rolling upgrade from 2.7.2 to 3.2.0

Try to improve the patch and add document


Maybe another option：we can write new properties after the older ones, so that when restarted they can be ignored.  EC requests will still be accepted and persisted in editlog before finalizing rollingupgrade. 

In this case , it would be like this.

from 
{quote}FSImageSerialization.writeByte(storagePolicyId, out);
 FSImageSerialization.writeByte(erasureCodingPolicyId, out);
 writeRpcIds(rpcClientId, rpcCallId, out);
{quote}
to 
{quote}writeRpcIds(rpcClientId, rpcCallId, out);

FSImageSerialization.writeByte(storagePolicyId, out);
 FSImageSerialization.writeByte(erasureCodingPolicyId, out);
{quote}

[~starphin] Thanks for your comments. If writing new properties, downgrading maybe fail while upgrading has problems

[~weichiu] Upload v004 patch. Could you please take a look? Thanks

Later I will test 2.7 & 2.8, and then add release notes or document 

New properties will be ignored as I checked the code. There ls a length field in every edit log.  To write new properties after older ones is not just about this issue, but a rule for future extension. If not, there's high risks that hdfs metadata being broken after finanizing. Because until then new properties are written in editlogs, which is not verified in the process of rolling upgrade. Metadata will not be recoverable at that time. Maybe I 'm wrong on this, feel free to correct.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 20s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 56s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 23s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  0s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 53s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 744 unchanged - 24 fixed = 745 total (was 768) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 20s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 65m  4s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 32s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}119m  4s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.mover.TestMover |
|   | hadoop.hdfs.TestSafeModeWithStripedFile |
|   | hadoop.hdfs.TestDFSInotifyEventInputStream |
|   | hadoop.hdfs.TestLeaseRecoveryStriped |
|   | hadoop.hdfs.TestErasureCodeBenchmarkThroughput |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy |
|   | hadoop.hdfs.TestErasureCodingExerciseAPIs |
|   | hadoop.hdfs.TestWriteReadStripedFile |
|   | hadoop.hdfs.server.blockmanagement.TestBlockManager |
|   | hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData |
|   | hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer |
|   | hadoop.hdfs.server.namenode.TestQuotaWithStripedBlocksWithRandomECPolicy |
|   | hadoop.hdfs.TestSafeMode |
|   | hadoop.hdfs.TestErasureCodingMultipleRacks |
|   | hadoop.hdfs.server.namenode.TestAddStripedBlockInFBR |
|   | hadoop.hdfs.server.namenode.TestStartup |
|   | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA |
|   | hadoop.hdfs.TestFileChecksum |
|   | hadoop.hdfs.TestErasureCodingAddConfig |
|   | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |
|   | hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewerWithStripedBlocks |
|   | hadoop.hdfs.server.blockmanagement.TestComputeInvalidateWork |
|   | hadoop.hdfs.server.namenode.TestQuotaWithStripedBlocks |
|   | hadoop.hdfs.tools.TestDFSAdmin |
|   | hadoop.hdfs.server.namenode.TestFSImage |
|   | hadoop.hdfs.server.namenode.TestFSEditLogLoader |
|   | hadoop.hdfs.server.namenode.sps.TestStoragePolicySatisfierWithStripedFile |
|   | hadoop.hdfs.server.namenode.TestStripedINodeFile |
|   | hadoop.hdfs.TestErasureCodingPolicyWithSnapshotWithRandomECPolicy |
|   | hadoop.hdfs.TestSetrepIncreasing |
|   | hadoop.cli.TestErasureCodingCLI |
|   | hadoop.hdfs.server.namenode.TestFsck |
|   | hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics |
|   | hadoop.hdfs.TestFileStatusWithRandomECPolicy |
|   | hadoop.hdfs.server.namenode.TestNameNodeMXBean |
|   | hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer |
|   | hadoop.hdfs.server.namenode.TestAddOverReplicatedStripedBlocks |
|   | hadoop.hdfs.TestFileChecksumCompositeCrc |
|   | hadoop.hdfs.TestSafeModeWithStripedFileWithRandomECPolicy |
|   | hadoop.hdfs.TestFileStatusWithDefaultECPolicy |
|   | hadoop.hdfs.server.namenode.TestAddStripedBlocks |
|   | hadoop.hdfs.server.blockmanagement.TestBlockInfoStriped |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |
|   | hadoop.hdfs.TestReconstructStripedFile |
|   | hadoop.hdfs.TestDistributedFileSystem |
|   | hadoop.hdfs.TestDFSStripedInputStream |
|   | hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData |
|   | hadoop.hdfs.server.blockmanagement.TestReconstructStripedBlocksWithRackAwareness |
|   | hadoop.hdfs.TestStripedFileAppend |
|   | hadoop.hdfs.tools.TestECAdmin |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithRandomECPolicy |
|   | hadoop.hdfs.TestUnsetAndChangeDirectoryEcPolicy |
|   | hadoop.hdfs.TestDistributedFileSystemWithECFileWithRandomECPolicy |
|   | hadoop.hdfs.TestErasureCodingPoliciesWithRandomECPolicy |
|   | hadoop.hdfs.TestReadStripedFileWithDNFailure |
|   | hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped |
|   | hadoop.hdfs.client.impl.TestBlockReaderLocal |
|   | hadoop.hdfs.server.namenode.TestCommitBlockWithInvalidGenStamp |
|   | hadoop.hdfs.TestErasureCodingPolicies |
|   | hadoop.hdfs.web.TestWebHdfsTimeouts |
|   | hadoop.hdfs.web.TestWebHDFS |
|   | hadoop.hdfs.server.blockmanagement.TestSequentialBlockGroupId |
|   | hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier |
|   | hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics |
|   | hadoop.hdfs.server.namenode.TestReconstructStripedBlocks |
|   | hadoop.hdfs.TestDFSStripedInputStreamWithRandomECPolicy |
|   | hadoop.hdfs.server.namenode.TestNamenodeRetryCache |
|   | hadoop.hdfs.TestDistributedFileSystemWithECFile |
|   | hadoop.hdfs.TestDecommissionWithStriped |
|   | hadoop.hdfs.TestReconstructStripedFileWithRandomECPolicy |
|   | hadoop.hdfs.TestErasureCodingPolicyWithSnapshot |
|   | hadoop.hdfs.TestDFSStripedOutputStream |
|   | hadoop.hdfs.server.balancer.TestBalancer |
|   | hadoop.hdfs.TestReadStripedFileWithDecoding |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | HDFS-13596 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12965317/HDFS-13596.004.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 8e71369d09f1 4.4.0-138-generic #164-Ubuntu SMP Tue Oct 2 17:16:02 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 73f43ac |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/26598/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/26598/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/26598/testReport/ |
| Max. process+thread count | 6220 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/26598/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



I'm really glad this has gained traction!  Otherwise 3.x hdfs was DOA...
{quote}  New properties will be ignored as I checked the code. There ls a length field in every edit log.
{quote}
I can't remember exact details but when I tried to take advantage of this behavior in ~2.6-2.7 it was completely broken so might double check.
 # The check for EC support should be in {{FSNamesystem}} methods, not {{NameNodeRpcServer}}, since there can be multiple entry points to the namesystem like webhdfs.
 # {{DFSUtil.isSupportedErasureCoding}} probably doesn't belong in {{DFSUtil}} since it's not something that should called outside of the NN.
 # In {{FSEditLogOp}}, please call the former method instead of duplicating the logic.
 # Super trivial, might rename new {{layoutVersion}} parameter in the write methods to {{logVersion}} to be consistent with the signatures for the read methods.
 # How about a {{FSNamesystem.checkErasureCodingSupported(String op)}} to avoid all the redundant check/throw code in the methods?
 # A test case is needed to prove the edits are correctly read/written.

Sorry for my mistake. Indeed length field is not behaving the way as my expectation. It just skip 4 bytes of checksum.
{quote}IOUtils.skipFully(in, 4 + 8); // skip length and txid
op.readFields(in, logVersion);
// skip over the checksum, which we validated above.
IOUtils.skipFully(in, CHECKSUM_LENGTH);{quote}
 

 

 

[~daryn] Thanks for you comments.Upload v005 patch. Could you please take a look?
{quote}
The check for EC support should be in FSNamesystem methods, not NameNodeRpcServer, since there can be multiple entry points to the namesystem like webhdfs.
{quote}
move check to FSNamesystem
{quote}
DFSUtil.isSupportedErasureCoding probably doesn't belong in DFSUtil since it's not something that should called outside of the NN.
{quote}
delete it from DFSUtil
{quote}
In FSEditLogOp, please call the former method instead of duplicating the logic.
{quote}
do not see duplicate logic
{quote}
Super trivial, might rename new layoutVersion parameter in the write methods to logVersion to be consistent with the signatures for the read methods.
{quote}
change layoutVertion to logVersion
{quote}
How about a FSNamesystem.checkErasureCodingSupported(String op) to avoid all the redundant check/throw code in the methods?
{quote}
add checkErasureCodingSupported in FSNamesystem
{quote}
A test case is needed to prove the edits are correctly read/written.
{quote}
add a test case, write op in lower version and read it in lower version.


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 22s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 18m 12s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 36s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 56s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 59s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 3 new + 774 unchanged - 24 fixed = 777 total (was 798) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 49s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}102m  2s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 32s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}160m 13s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | HDFS-13596 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12965536/HDFS-13596.005.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux c48141afaebf 3.13.0-153-generic #203-Ubuntu SMP Thu Jun 14 08:52:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 586826f |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/26612/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/26612/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/26612/testReport/ |
| Max. process+thread count | 2791 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/26612/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



Upload v006 patch, fix the checkstyle
TestDataNodeErasureCodingMetrics#testReconstructionBytesPartialGroup2 passed locally

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 29s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  5s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 35s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 59s{color} | {color:green} hadoop-hdfs-project/hadoop-hdfs: The patch generated 0 new + 774 unchanged - 24 fixed = 774 total (was 798) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 27s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 98m 16s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 34s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}155m 32s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped |
|   | hadoop.hdfs.server.namenode.TestNameNodeMXBean |
|   | hadoop.hdfs.server.datanode.TestBPOfferService |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | HDFS-13596 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12965548/HDFS-13596.006.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 24de378520b2 3.13.0-153-generic #203-Ubuntu SMP Thu Jun 14 08:52:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 586826f |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/26614/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/26614/testReport/ |
| Max. process+thread count | 3187 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/26614/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



Flaky Test. Failed Tests passed locally

Upload v007 patch
change 
{code}
if (flag.contains(CreateFlag.SHOULD_REPLICATE)
        || ecPolicyName != null) {
{code}
to 
{code}
boolean ecPolicyNameEmpty =
       org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName);
if (shouldReplicate || !ecPolicyNameEmpty) {
{code}

Test rolling upgrade passed from 2.7.0 to 3.3.0.

[~daryn] [~weichiu] Could you please have a look?

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  2m 25s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 22m 45s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 11s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 22s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 53s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 57s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  6s{color} | {color:green} hadoop-hdfs-project/hadoop-hdfs: The patch generated 0 new + 773 unchanged - 24 fixed = 773 total (was 797) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m  3s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 83m 29s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 32s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}152m 11s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.web.TestWebHdfsTimeouts |
|   | hadoop.hdfs.TestMaintenanceState |
|   | hadoop.hdfs.server.datanode.TestBPOfferService |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:bdbca0e |
| JIRA Issue | HDFS-13596 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12965713/HDFS-13596.007.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 91c73948f2ea 4.4.0-139-generic #165~14.04.1-Ubuntu SMP Wed Oct 31 10:55:11 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / abace70 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/26618/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/26618/testReport/ |
| Max. process+thread count | 3775 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/26618/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



after rollingUpgrade NN nodes to 3.x and keep DN 2.x ,  at this point, use 2.x client to read or write data to hdfs will failure

write failure sample:

{color:#d04437}19/04/16 15:21:42 INFO hdfs.DataStreamer: Exception in createBlockOutputStream{color}
 {color:#d04437}xxx org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , ack with firstBadLink as x.x.x.x:x{color}
 {color:#d04437} at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:134){color}
 {color:#d04437} at org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1823){color}
 {color:#d04437} at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1724){color}
 {color:#d04437} at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713){color}
 {color:#d04437}x.x.x.x 19/04/16 15:21:42 WARN hdfs.DataStreamer: Abandoning BP-1321128176-x-1552442036118:blk_1073742246_1422{color}
 {color:#d04437}.x.x.x 19/04/16 15:21:42 WARN hdfs.DataStreamer: Excluding datanode DatanodeInfoWithStorage[x.x.x.x:x,DS-63920a14-79b9-497a-b741-21bdf1401ad1,DISK]{color}
 {color:#d04437}19/04/16 15:21:42 INFO hdfs.DataStreamer: Exception in createBlockOutputStream{color}

[~zgw] could you be more specific? Did you rebuild HDFS with the latest patch that [~ferhui] provided? Thank you

Consider whether it would be simpler for {{FSEditLog#logOpenFile}} to put something (layout, boolean, etc) in the {{AddOp}} to indicate whether the EC field should be written.  Then we don't need to track and burrow the layout through the edit log, stream impls, double buf, etc.

If not, might consider adding this to significantly reduce patch size:
{code}
public void writeFields(DataOutputStream out, int logVersion) throws IOException {
  writeFields(out);
}
{code}

Test case should use a Feature enum prior to EC instead of hardcoding the number.  I only skimmed the patch (need to look at it more) but not sure the test case provides enough coverage.  It appears to assume the ability to read the edit means it was written "correctly" w/o the EC field.  If yes, that's a really bad assumption.



[~weichiu] Yes, I have merged the latest patch that Fei Hui provided and rebuild HDFS project, then replace hadoop-hdfs-3.1.1.jar

 

{color:#205081}*It seems like a common problem, I have encounter the same problem as well.*{color}

 

After rollingUpgrade NN nodes to 3.x and keep DN 2.x ,  at this point, use 2.x client to read or write data to hdfs will failure

write failure sample:

{color:#d04437}19/04/16 15:21:42 INFO hdfs.DataStreamer: Exception in createBlockOutputStream{color}
{color:#d04437}xxx org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , ack with firstBadLink as x.x.x.x:x{color}
{color:#d04437} at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:134){color}
{color:#d04437} at org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1823){color}
{color:#d04437} at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1724){color}
{color:#d04437} at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713){color}
{color:#d04437}x.x.x.x 19/04/16 15:21:42 WARN hdfs.DataStreamer: Abandoning BP-1321128176-x-1552442036118:blk_1073742246_1422{color}
{color:#d04437}.x.x.x 19/04/16 15:21:42 WARN hdfs.DataStreamer: Excluding datanode DatanodeInfoWithStorage[x.x.x.x:x,DS-63920a14-79b9-497a-b741-21bdf1401ad1,DISK]{color}
{color:#d04437}19/04/16 15:21:42 INFO hdfs.DataStreamer: Exception in createBlockOutputStream{color}

 

 

after rollingUpgrade NN and DN nodes to 3.2.0 ,  at this point, use 3.2.0 client run example pi job will failure

write failure sample:
{quote}19/04/17 11:45:00 INFO client.AHSProxy: Connecting to Application History server at slave01.jd.com/192.168.1.101:10200
 19/04/17 11:45:00 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /yarn/staging/hdfs/.staging/job_1555472525840_0001
 19/04/17 11:45:00 INFO mapreduce.JobSubmitter: Cleaning up the staging area /yarn/staging/hdfs/.staging/job_1555472525840_0001
 org.apache.hadoop.security.AccessControlException: setErasureCodingPolicy not supported.
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkErasureCodingSupported(FSNamesystem.java:7797)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setErasureCodingPolicy(FSNamesystem.java:7530)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setErasureCodingPolicy(NameNodeRpcServer.java:2171)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setErasureCodingPolicy(ClientNamenodeProtocolServerSideTranslatorPB.java:1598)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
 at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
 at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
 at org.apache.hadoop.hdfs.DFSClient.setErasureCodingPolicy(DFSClient.java:2748)
 at org.apache.hadoop.hdfs.DistributedFileSystem$65.doCall(DistributedFileSystem.java:2852)
 at org.apache.hadoop.hdfs.DistributedFileSystem$65.doCall(DistributedFileSystem.java:2849)
 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
 at org.apache.hadoop.hdfs.DistributedFileSystem.setErasureCodingPolicy(DistributedFileSystem.java:2867)
 at org.apache.hadoop.mapreduce.JobResourceUploader.disableErasureCodingForPath(JobResourceUploader.java:885)
 at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:176)
 at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:133)
 at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:99)
 at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:194)
 at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
 at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
 at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
 at org.apache.hadoop.examples.QuasiMonteCarlo.estimatePi(QuasiMonteCarlo.java:307)
 at org.apache.hadoop.examples.QuasiMonteCarlo.run(QuasiMonteCarlo.java:360)
 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
 at org.apache.hadoop.examples.QuasiMonteCarlo.main(QuasiMonteCarlo.java:368)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
 at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
 at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
 at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
 Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): setErasureCodingPolicy not supported.
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkErasureCodingSupported(FSNamesystem.java:7797)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setErasureCodingPolicy(FSNamesystem.java:7530)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setErasureCodingPolicy(NameNodeRpcServer.java:2171)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setErasureCodingPolicy(ClientNamenodeProtocolServerSideTranslatorPB.java:1598)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
 at org.apache.hadoop.ipc.Client.call(Client.java:1457)
 at org.apache.hadoop.ipc.Client.call(Client.java:1367)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
 at com.sun.proxy.$Proxy9.setErasureCodingPolicy(Unknown Source)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setErasureCodingPolicy(ClientNamenodeProtocolTranslatorPB.java:1603)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
 at com.sun.proxy.$Proxy10.setErasureCodingPolicy(Unknown Source)
 at org.apache.hadoop.hdfs.DFSClient.setErasureCodingPolicy(DFSClient.java:2746)
 ... 33 more
{quote}
I find class JobResourceUploader has a disableErasureCodingForPath method, which invoke setErasureCodingPolicy, when namenode invoke the method setErasureCodingPolicy the checkErasureCodingSupported was invoked, and throw this exception.

So I change the FSNamesystem.setErasureCodingPolicy() method add a if statement which ignore the 3 replica policy like follow:

 
{code:java}
if(!ErasureCodeConstants.REPLICATION_POLICY_NAME.equals(ecPolicyName)) {
    checkErasureCodingSupported(operationName);
 }
{code}
 

It work.

Not to add more noise, but this might be a good opportunity for us to learn a trick or two from our brethren in the HBase land.  HBase has this nice ability to upgrade to 3.0, but will not enable 3.0 features unless another command or setting is applied. We actually have a very similar situation here, we might have changes in Edit logs, but let us not allow that feature to be used until after the main step is completely done and we have some way of verifying that nothing is broken. Then you can enable the full 3.0 features once the full upgrade is done. (Thanks to [~ccondit] for educating me on how good HBase is in doing this, and letting me know that Ozone should probably learn from that experience).

 

if we do this, Rolling upgrade would be two steps, upgrade, then enable 3.0 features like EC.  Till enable call is done, HDFS will not allow 3.0 features like EC.

 

{quote}After rollingUpgrade NN nodes to 3.x and keep DN 2.x ,  at this point, use 2.x client to read or write data to hdfs will failure
{quote}
[~zgw] and [~maxigang] thanks for reporting token error. Looks HDFS-9807 and HDFS-6708 introduced two fields in BlockTokenIdenfier.AFAIK,Here namenode created token with all this new fileds and DataNode doesn't have these fields hence token verification is failing.

[~ferhui] and [~starphin], did you tried with security enabled ( mainly blockToken enabled)?

 

 

The same problem - HDFS-13236. 

I can repeat this problem with NN on all 3.x versions (with Kerberos too). I have special dedicated cluster for test and I can help with tests.

In my quest rollingUgrade (2.7 -> 3.x) was successfully only in case when there were no write operations to the cluster until finalize.

[~brahmareddy] Didn't try with security enabled.
[~maxigang] [~zgw] Could you please post your namenode and client configurations ? Thanks

I'm testing last patch on a cluster with Kerberos:
 # download sources from [https://github.com/apache/hadoop]
 # apply patch _HDFS-13596.007.patch_
 # build patched version of _hadoop-3.3.0-SNAPSHOT_
 # prepare running cluster (_hadoop-2.7.3_) to _rollingUpgrade_
 # run _hadoop-3.3.0-SNAPSHOT_ in _rollingUpgrade_ mode
 # upload test data to HDFS
 # restart NN and all is fine
 # stop cluster for rollback to hadoop-2.7.3 
 # hadoop-2.7.3 NN start fails with _ArrayIndexOutOfBoundsException_:
{code:java}
INFO   | jvm 1    | 2019/04/25 18:01:24 | STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'vinodkv' on 2016-08-18T01:01Z
INFO   | jvm 1    | 2019/04/25 18:01:24 | STARTUP_MSG:   java = 1.8.0_102
INFO   | jvm 1    | 2019/04/25 18:01:24 | ************************************************************/
INFO   | jvm 1    | 2019/04/25 18:01:24 | 2019-04-25 18:01:24,767  INFO [Thread-3] NameNode - registered UNIX signal handlers for [TERM, HUP, INT]
INFO   | jvm 1    | 2019/04/25 18:01:24 | 2019-04-25 18:01:24,769  INFO [Thread-3] NameNode - createNameNode [-rollingUpgrade, rollback]
{code}
{code:java}
INFO   | jvm 1    | 2019/04/25 18:01:30 | 2019-04-25 18:01:30,342 DEBUG [Thread-3] FSImage - Planning to load edit log stream: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@53468702
INFO   | jvm 1    | 2019/04/25 18:01:30 | 2019-04-25 18:01:30,342 DEBUG [Thread-3] FSImage - Planning to load edit log stream: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5e350e83
INFO   | jvm 1    | 2019/04/25 18:01:30 | 2019-04-25 18:01:30,342 DEBUG [Thread-3] FSImage - Planning to load edit log stream: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@cf86362
INFO   | jvm 1    | 2019/04/25 18:01:30 | 2019-04-25 18:01:30,342 DEBUG [Thread-3] FSImage - Planning to load edit log stream: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@840d683
INFO   | jvm 1    | 2019/04/25 18:01:30 | 2019-04-25 18:01:30,342  INFO [Thread-3] FSImage - Planning to load image: FSImageFile(file=/one/hadoop-data/dfs/current/fsimage_rollback_0000000000000000677, cpktTxId=0000000000000000677)
INFO   | jvm 1    | 2019/04/25 18:01:30 | Total time for which application threads were stopped: 0.0007557 seconds, Stopping threads took: 0.0001181 seconds
INFO   | jvm 1    | 2019/04/25 18:01:30 | 2019-04-25 18:01:30,377 ERROR [Thread-3] FSImage - Failed to load image from FSImageFile(file=/one/hadoop-data/dfs/current/fsimage_rollback_0000000000000000677, cpktTxId=0000000000000000677)
INFO   | jvm 1    | 2019/04/25 18:01:30 | java.lang.ArrayIndexOutOfBoundsException: 536870913
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadStringTableSection(FSImageFormatProtobuf.java:318)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:251)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:182)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat.java:226)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:963)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:947)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:746)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:677)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:294)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:976)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:585)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:645)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:812)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:796)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1493)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1559)
{code}
{code:java}
INFO   | jvm 1    | 2019/04/25 18:01:30 | 2019-04-25 18:01:30,726 ERROR [Thread-3] NameNode - Failed to start namenode.
INFO   | jvm 1    | 2019/04/25 18:01:30 | java.io.IOException: Failed to load FSImage file, see error(s) above for more info.
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:692)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:294)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:976)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:585)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:645)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:812)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:796)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1493)
INFO   | jvm 1    | 2019/04/25 18:01:30 |       at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1559)
INFO   | jvm 1    | 2019/04/25 18:01:30 | 2019-04-25 18:01:30,728  INFO [Thread-3] ExitUtil - Exiting with status 1
{code}

[~_ph] Thanks for your testing. There are two problems.

One is due to EC. Please see https://issues.apache.org/jira/browse/HDFS-14396 

Another is due to StringTable. It occurs downgrade from 3.3 to 2.7 and I guess the root cause is the commit as follow.
{code}
commit 8a41edb089fbdedc5e7d9a2aeec63d126afea49f

Author: Vinayakumar B <vinayakumarb@apache.org>

Date:   Mon Oct 15 15:48:26 2018 +0530



    Fix potential FSImage corruption. Contributed by Daryn Sharp.
{code}

StringTable change is incompatible. I revert this commit and It works.
I could not find JIRA ID because it was not within commit message

[~ferhui] thanks for your answer. Today I repeated test with _hadoop-trunk + HDFS-13596.007.patch + HDFS-14396.002.patch_ and rollingUpgrade with Rollback was successfully completed. 

PS: all time cluster working in Secure Mode with QJM

{quote}PS: all time cluster working in Secure Mode with QJM
{quote}
[~_ph] thanks for confirmation, did you test with block token enabled ("*dfs*.*block*.*access*.*token*.*enable")..? and* will you able to write/read after NN is upgraded..?
{quote} rollingUpgrade with Rollback was successfully completed.
{quote}
what about data loss and downtime..?

 

[~ferhui]
{quote}[~brahmareddy] Didn't try with security enabled.
{quote}
Please see [Secure mode|http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html] and enable the "*dfs*.*block*.*access*.*token*.*enable"* then write/read during upgrade.

 

[~brahmareddy] yes, test was with _dfs.block.access.token.enable=true_ and with full access to HDFS (read/write). After rollback (its possible only with downtime) all new data was lost and filesystem return to pre-update state.

[~_ph] see HDFS-14509 for more detailed. anyway we can discuss further there about this issue.

[~ferhui] thanks for working this jira. HDFS-13596.007.patch lgtm.

[~daryn] do you've any further comments..?

[~weichiu] do you've any comments on final patch..?

Thanks [~ferhui] for the patch. We are now testing rolling upgrades with the patch (+ HDFS-14396) and will feedback soon.

Would you reflect the Daryn's review comment? Probably it makes the patch more simple.
https://issues.apache.org/jira/browse/HDFS-13596?focusedCommentId=16819198&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16819198

Thanks for testing [~aajisaka]. Upload v008 patch. Reduce patch size as [~daryn] said
{code:java}
public void writeFields(DataOutputStream out, int logVersion) throws IOException {
  writeFields(out);
}
{code}
We have rolling upgraded to 3.2.0 from 2.7.2 and run stably for weeks.
We pick HDFS-13596,HDFS-14396 and revert the following commit
{code:java}
commit 8a41edb089fbdedc5e7d9a2aeec63d126afea49f

Author: Vinayakumar B <vinayakumarb@apache.org>

Date:   Mon Oct 15 15:48:26 2018 +0530



    Fix potential FSImage corruption. Contributed by Daryn Sharp.
{code}


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 50s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  6s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 34s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  8s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 47s{color} | {color:green} hadoop-hdfs-project/hadoop-hdfs: The patch generated 0 new + 796 unchanged - 1 fixed = 796 total (was 797) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 35s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}104m 30s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 33s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}163m 49s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.TestNameNodeMXBean |
|   | hadoop.hdfs.server.datanode.TestDirectoryScanner |
|   | hadoop.hdfs.server.namenode.ha.TestEditLogTailer |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=18.09.7 Server=18.09.7 Image:yetus/hadoop:bdbca0e53b4 |
| JIRA Issue | HDFS-13596 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12975238/HDFS-13596.008.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 049d5e21c68c 4.15.0-52-generic #56-Ubuntu SMP Tue Jun 4 22:49:08 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 7f1b76c |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_212 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/27263/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/27263/testReport/ |
| Max. process+thread count | 2958 (vs. ulimit of 5500) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/27263/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



Hi [~ferhui], may I ask a question. In patch-008 FSNamesystem.java line 2500,  we check the layout version when shouldReplicate is true.
{code:java}
2500	if (shouldReplicate || !ecPolicyNameEmpty) {
2501	  checkErasureCodingSupported("createWithEC");
2502	}
{code}
Why should we do the layout check when we want to start a replicate file?  Should we check the layout only when we are trying to start a ec file ? I mean something like:
{code:java}
if (!shouldReplicate && (
    !org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName)
        || FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip))) {
  checkErasureCodingSupported("createWithEC");
}{code}
 Looking forward to your answers.

[~LiJinglun] Thanks for your comments.Good Question! I made a mistake.
Move checkErasureCodingSupported to the right place.
We check it when shouldReplicate is false and ecPolicy is not empty.
{code:java}
        if (ecPolicy != null && (!ecPolicy.isReplicationPolicy())) {
          checkErasureCodingSupported("createWithEC");
          if (blockSize < ecPolicy.getCellSize()) {
            throw new IOException("Specified block size (" + blockSize
                + ") is less than the cell size (" + ecPolicy.getCellSize()
                +") of the erasure coding policy (" + ecPolicy + ").");
          }
{code}
Upload v009.patch


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  2m 21s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 20m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 56s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 29s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 14s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 49s{color} | {color:green} hadoop-hdfs-project/hadoop-hdfs: The patch generated 0 new + 796 unchanged - 1 fixed = 796 total (was 797) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 19s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 99m 59s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 30s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}162m 18s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.tools.TestDFSZKFailoverController |
|   | hadoop.hdfs.server.datanode.TestLargeBlockReport |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=18.09.7 Server=18.09.7 Image:yetus/hadoop:bdbca0e53b4 |
| JIRA Issue | HDFS-13596 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12976773/HDFS-13596.009.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 607a33003237 4.15.0-52-generic #56-Ubuntu SMP Tue Jun 4 22:49:08 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / d6697da |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_212 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/27411/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/27411/testReport/ |
| Max. process+thread count | 2837 (vs. ulimit of 5500) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/27411/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



LGTM, +1. I'll commit this tomorrow if there are no objections.

Given how much test this patch has been going through, I think the patch is ready to go.

I think it would make sense to update the "HDFS Rolling Upgrade" doc to call out the limitation of the approach -- if I understand the patch correctly, until HDFS finalizes upgrade, it won’t accept any erasure coding operations. Additionally, NameNode does not write out ec information in edit log, and does not check point with ec information in the new fsimage.

Additionally, what [~Malygin] and [~ferhui] found, that downgrade from 3.3.0 to 2.7.x can not complete successfully without reverting the commit "Fix potential FSImage corruption." Have folks tried rolling upgrade/downgrade between 2.8.x and 3.x? Does that work without reverting that commit? That commit is a critical fix and it is not acceptable to revert it for many users.

[~weichiu] I find the commit "Fix potential FSImage corruption."  in 2.8.6-snapshot ,2.9.2,2.9.3-snapshot, 3.0.4-snapshot, 3.1.1-snapshot, 3.2.0, 3.2.1-snapshot, 3.3.0-snapshot. 
With this fix downgrade from  3.0.0~3.0.3,3.1.0~3.1.1 to 2.8.0~2.8.5,2.9.0~2.9.1 will success.
By the way i think this issue resolve edit log incompatible problem and "Fix potential FSImage corruption." is string table incimpatible. we can file another jira to resolve it ?

Hi [~ferhui], is it really downgrade?
As [the rolling upgrade document|https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html#Downgrade_and_Rollback] says, downgrade is not supported between different NN/DN layout versions. Rollback is supported and [~kkori] and [~tasanuma] have tested that rolling upgrade from HDP2.6 to 3.3.0-snapshot and rollback from 3.3.0-snapshot to HDP 2.6.

[~aajisaka]
Rolling upgrade document says
{quote}
Note also that downgrade and rollback are possible only after a rolling upgrade is started and before the upgrade is terminated. An upgrade can be terminated by either finalize, downgrade or rollback. Therefore, it may not be possible to perform rollback after finalize or downgrade, or to perform downgrade after finalize.
{quote}
During rolling upgradefrom 2.6. to 3.3, and before finalize it is possible to downgrade from 3.3 to 2.6. But it is not possible to perform downgrade after finalize.

bq. But it is not possible to perform downgrade after finalize.

It is expected behavior I think. If there is any possibility to downgrade or rollback the cluster, you must not finalize it.

[~aajisaka] Additionally, you can check HDFS-8432

Thanks [~John Smith] for the information. I didn't know that feature because it not documented in the rolling upgrade doc.

bq. By the way i think this issue resolve edit log incompatible problem and "Fix potential FSImage corruption." is string table incimpatible. we can file another jira to resolve it ?

Now I'm +1 for filing another jira to resolve this issue. Thanks [~ferhui] and [~John Smith].

BTW, we need to upgrade the rolling upgrade document.

Filed HDFS-14753 to upgrade the upgrade doc :)

Thanx everyone for the work here.
Shouldn't {{checkErasureCodingSupported}} throw something like UnsupportedActionException instead ACE, ACE typically means the user performing the operation doesn't have permission.

[~ayushtkn] Thanks for your comments.
Upload v010 patch change ACE to UnsupportedActionException.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 24s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  1s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 33s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 55s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 50s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 55s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 50s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 49s{color} | {color:green} hadoop-hdfs-project/hadoop-hdfs: The patch generated 0 new + 797 unchanged - 1 fixed = 797 total (was 798) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 42s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 77m 51s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 35s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}132m  7s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.TestNameNodeMetadataConsistency |
|   | hadoop.hdfs.server.namenode.TestFsck |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.1 Server=19.03.1 Image:yetus/hadoop:bdbca0e |
| JIRA Issue | HDFS-13596 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12978049/HDFS-13596.010.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 2877da031a2a 4.4.0-157-generic #185-Ubuntu SMP Tue Jul 23 09:17:01 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 6244502 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_212 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/27580/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/27580/testReport/ |
| Max. process+thread count | 3836 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/27580/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



Failed Tests are unrelated.

[~ayushtkn] would you like to check again the patch?


+1 for  [^HDFS-13596.010.patch]

v10 LGTM +1

FAILURE: Integrated in Jenkins build Hadoop-trunk-Commit #17172 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/17172/])
HDFS-13596. NN restart fails after RollingUpgrade from 2.x to 3.x. (aajisaka: rev abc8fde4caea0e197568ee28392c46f1ce0d42e1)
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditsDoubleBuffer.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditsDoubleBuffer.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumOutputStream.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLog.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogBackupOutputStream.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/QJMTestUtil.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupJournalManager.java


Committed this to trunk, branch-3.2, and branch-3.1. Thanks [~ferhui] for the great effort, and thanks all who commented and reviewed on this jira.

Hi,   Found in my test downgrade from 3.1.3 to 2.7.2, namenode  successful  but datanode failed.  because  different DN layout versions. 

 
{code:java}
// code placeholder
2020-07-06 14:45:01,313 WARN org.apache.hadoop.hdfs.server.common.Storage: org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /data/hadoop/dfs. Reported: -57. Expecting = -56.
2020-07-06 14:45:01,315 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data/hadoop/dfs/in_use.lock acquired by nodename 21258@test-v03
2020-07-06 14:45:01,315 WARN org.apache.hadoop.hdfs.server.common.Storage: org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /data/hadoop/dfs. Reported: -57. Expecting = -56.
2020-07-06 14:45:01,315 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to test-v01/10.110.228.21:8020. Exiting.
java.io.IOException: All specified directories are failed to load.
        at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:478)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1358)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1323)
        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:317)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:802)
        at java.lang.Thread.run(Thread.java:748)
{code}
 

Does anyone know the reason?

Or, is there only one way to rollback datanode ？

Hi [~fengwu99]! Did you downgrade datanodes before namenodes?

Hi, [~_ph] ! Yes,  downgrade before namenodes and not finalize upgrade.  

Have you tested downgrade to 2.x from 3.x ?  Datanode downgrade successfully ?

[~fengwu99] yes, in my case (https://issues.apache.org/jira/browse/HDFS-13596?focusedCommentId=16826162&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16826162 + https://issues.apache.org/jira/browse/HDFS-13596?focusedCommentId=16826939&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16826939) downgrade to 2.7.3 was successfully from 3.1.2 and 3.3.0-SNAPSHOT versions.

[~_ph]   Thanks.

In your case, used rollback not downgrade .  Rollback will lost data,  I want  a way to roll downgrade do you think it works?

[~fengwu99] downgrade is not possible between major versions, because will be changing layout version (in your case 57/56 layout versions)

Note from official documentation:
{quote}
A newer release is downgradable to the pre-upgrade release only if both the namenode layout version and the datanode layout version are not changed between these two releases.
{quote}
URL: https://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html#Downgrade


[~_ph] ， At first I had the same view as you， when I saw this comment :https://issues.apache.org/jira/browse/HDFS-13596?focusedCommentId=16911102&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16911102

I tested downgrade  from hdfs 3.2.1 to 2.8.2  success,but 2.7 ~ 2.7.2 failed.

So, This can be understood as roll downgrading the current 3.x can only be downgraded to the 2.8+ , agree  ?

[~hanishakoneru]  [~ferhui]

[~fengwu99] You are right!  Failed is related to HDFS-8791.

hi [Hui Fei|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ferhui] ,I had the same problem recently，

I test Hadoop2.7.2 rolling upgrade Hadoop3.3.4 after the downgrade test，

Error reported when degrading Namenode startup：ArrayIndexOutOfBoundsException：536870913，

So I merged COMMIT in Hadoop2.7.2：Fix potential FSImage corruption(8a41edb089fbdedc5e7d9a2aeec63d126afea49f)，

However, the startup still failed with an error：NullPointerException，

The Owner and Group of the HDFS directory are null（The number of test data blocks is 68905183），

Later I found Hadoop2.7.2：

enum PermissionStatusFormat implements LongBitFormat.Enum {
    MODE(null, 16),
    GROUP(MODE.BITS, 24),
    USER(GROUP.BITS, 24);

And Hadoop3.3.4 and Fix potential FSImage corruption ：

enum PermissionStatusFormat implements LongBitFormat.Enum {
    MODE(null, 16),
    GROUP(MODE.BITS, 25),
    USER(GROUP.BITS, 23);

After I changed the GROUP and USER in Hadoop2.7.2 to 24, 24, the downgrade was successful。

This commit：https://github.com/lucasaytt/hadoop/commit/8a41edb089fbdedc5e7d9a2aeec63d126afea49f（Fix potential FSImage corruption），Can it be merged into Hadoop2.7.2?   Is there any hidden danger?

