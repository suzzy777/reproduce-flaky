One subtlety here: we probably shouldn't rely on the SparkConf having {{spark.hadoop.validateOutputSpecs}} set to {{false}}, since this is the SparkContext's {{conf}} and that context might be shared with other non-streaming jobs / tasks.  We also shouldn't mutate it, since, in general, mutating SparkConf is a serious anti-pattern (even in internal code).

Instead, we could add some plumbing so that every {{saveAs*}} RDD method accepts an optional parameter to disable output spec validation.  This solution also kind of messy, though, since it ends up touching a lot of code: if we don't change this everywhere, then there's the possibility that we'll miss some corner-case and re-introduce the bug.

Instead, we might be able to use DynamicVariable to dynamically bypass the output spec checking for jobs that are launched by the streaming scheduler.  This would have a somewhat minimal impact on the source code and would avoid merge-conflict hell when backporting this code, but might be hard to understand.  However, this might be nicer from a user-facing point-of-view since we wouldn't end up cluttering up the {{saveAs*}} methods with a {{bypassOutputSpecValidation}} boolean that's only used by streaming.

Alright, pushed an experimental version of the {{DynamicVariable}} approach as part of my streaming test flakiness PR: https://github.com/apache/spark/pull/3687.

Here's the actual commit: https://github.com/JoshRosen/spark/commit/3db335f01e01986c412ae2a1de6fbe6b8c3a7a32

I wonder whether there is a semantically cleaner way of doing this. In streaming we know which batches might have been already processed earlier because we store in the checkpoint the batches that were generated and queued. So upon recovery, the validation could be disabled only for those batches that were queue before failure, and not any of the subsequent batches. 
What do you think?


I thought about the "figure out whether the batch has already been processed" approach, but I was worried about various types of partial failure.  The processing of a batch might involve an arbitrary number of Spark jobs, so it seems like it could be possible that an entire {{saveAs*}} operation succeeds even though the batch itself is marked as a failure during a crash.  Also, I was worried about partial failure of the {{saveAs*}} call itself: is it possible for half of the partitions' saving to succeed before the crash such that the output directory is created but half-full?  Or is the output directory created only once the entire output operation has succeeded and committed (e.g. is success / failure atomic from a directory-existence point-of-view)?

That is a very good point. Lets brainstorm on this offline once I get to your PR.

User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3832

[~tdas],

After some more thought, I think that the "figure out which batch has already been processed" approach might actually be fine in those partial-failure cases, since there should be a record in the checkpoint that says whether a batch has successfully completed.  I think the crux of the problem is that a job might complete successfully but this might not be reflected in the checkpoint, so we'd _think_ that it was the first time that we've seen a batch even thought it was already processed.

I suppose that we could work around this if we used a write-ahead log to store a record before starting to process a batch, which would avoid this issue.  Can we do this with the existing WAL machinery?  If we already have this functionality, then I think we should use that.

I guess what we'd really want is something akin to a "batch attempt" number, but it doesn't look like the current WAL gives us this.

Lets keep this simple for now, just disable the validity checks for streaming by default. We can look into this later. 

[~joshrosen] Even if this resolved, there probably should be another JIRA opened regarding the spark.streaming.hadoop.validate , isnt it?

Yeah, let's open a second followup JIRA for that.

