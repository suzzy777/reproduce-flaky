some quick comments:
* this jenkins installation is literally 9 years old (IIRC it first put up in 2010 by matt massie)
* most spark builds are managed by jenkins job builder (JJB) configs, and that stuff is stored in a databricks github repo (this means the bash scripts referenced above are not copy-pasted)
* the ones that ARE NOT managed by JJB are the following:  SparkPullRequestBuilder, NewSparkPullRequestBuilder, all JDK11 builds, all ubuntu-testing builds, and finally testing-k8s-prb-make-spark-distribution-unified
* moving the JJB configs to the apache spark repo is a priority, but we need to figure out how to manage creds and secrets that are currently in the databricks/spark-jenkins-configurations repo
* we really need to upgrade jenkins to 2.0+, which will let us use pipeline builds (https://jenkins.io/doc/book/pipeline/)
* many many many of these builds were created long before i took over jenkins...  so even i don't have historical context for many of the testing decisions made
* this project will be very big, have many moving parts and dependencies, and will require a few people dedicated to making it work

OK, let's not boil the ocean. The only immediate need is Pyspark JDK 11 tests, but might find that simplifying or removing some stuff helps that goal.

Can we 'sever' the link to the private DB repo? that shouldn't be part of the loop here. What's the relationship... it syncs from scripts there? can we not do that and leave the Jenkins config as-is, simply calling run-tests in general?

That is, long-term I think we want to push the logic into run-tests and keep Jenkins simple.

oh yeah, i forgot the most important thing:

during this build/test/job audit we should *_dockerize everything_*

i don't think we can sever things right now from the JJB repo.  there are secrets stored for publishing artifacts.

i kinda think it's time to boil the ocean, tbh.  i am more than happy to BART over to SF and camp out w/some folks at databricks to help me push through this.  ;)

Just quick FYI, {{spark-master-test-maven-hadoop-3.2}} builder would need to install pyarrow and pandas identically like PR builder ... I believe this could be done separately in a separate ticket if it matters
cc [~yumwang] since you reached out to me about this. Once they are properly installed, those skipped tests will be executed properly (https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-3.2-jdk-11/326/testReport/org.apache.spark.sql/SQLQueryTestSuite/)

Some quick notes / braindump (I may write more later):
 * AFAIK _release_ publishing / artifact signing hasn't taken place on Jenkins for a while now (I'm not sure if we're still doing snapshot publishing there, though). Given this, we should delete unused publishing builders and their associated credentials (which I think have been rotated anyways). I'm _pretty_ sure this is technically feasible, but it's been a long time since I've last investigated. If we delete the publishing builders then it should be fairly straightforward to dump a snapshot of the JJB scripts into a public repo (sans-git-history, perhaps).
 * We should consider removing code / builders for old branches which will never be patched (such as {{branch-1.6}}). This may simplify the build scripts.
 * Strong +1 from me towards using Dockerized build container: a standard Docker environment would let us remove most of the legacy build cruft.
 ** IIRC Dockerization of these builds in AMPLab Jenkins was historically blocked by the old version of CentOS's Docker support: the Docker daemon would lock up / freeze if launching many PR builder jobs in parallel. This should be fixed  for the newer Ubuntu hosts, though.
 ** Alternatively, eventually porting all of this to Bazel and sourcing all languages' dependencies and toolchains from there instead from the local environment would sidestep a lot of these problems.
 * I think we may already have some mechanism which builds conda environments / virtualenvs for the PySpark packaging tests? Maybe that could be used for the regular PySpark tests as well?

hey everyone, i have to back-burner this issue for the next couple of weeks.  half of my team is out on vacation and for family issues and i'm sole support for 4 research labs right now.  also i threw my back out last week and have been dealing w/that.

wish me luck!  :\

Take care, Shane!

Can we move this out from the umbrella?

Meh, I think it's kind of important to declare JDK 11 support complete, to fully test Spark against JDK 11. However, right now it does seem to work when running Pyspark tests, and we'd re-test before a preview release of 3.0, etc. I am kind of neutral about whether this is essential to call this 'done', therefore.

I don't think we can call it a day.

My reason is that it's time to move toward the beyond on `building/running` phase.
As we know, JDK11 support has more issues. I want to drive the remaining issues into new sub-categories with more meaningful names in order to give a better verification direction to the community and to get more support on the remaining JDK11 issues.

The followings are my ideas.

1. Performance issues (I'll going to run all benchmark with JDK11 and report regressions if exists)
2. Integration test / release script / infra issue like this.

How do you think about this?

BTW, I'm running JDK11 build and testing including PySpark tests with all PyArrow testings.
It's slightly different from the community one, but for me, Spark code looks enough to build and run the tests.
If we have a UT failure, I will happily jump on that. Please let me know. :D

The meaning of JIRAs and umbrellas is already kind of nebulous, and I prefer smaller, actionable milestones. If the umbrella is about finishing the changes that are necessary to get a passing build on JDK 11, including Pyspark, then I think it's done, and I'd be fine to close it. We may have follow-on issues, but that is always true. And we should keep this open as we do need a Jenkins job to test what you are testing. But the umbrella is fine to close as far as I know.

Hi, All. I moved this issue to SPARK-29194 (JDK11 QA) umbrella.

Hi, All.
Can we restart this before `branch-3.0` cut because we need to duplicate all `master` Jenkins jobs during cutting branch?

FYI:  i will be OOO next week (mon-thur) with VERY limited availability until friday (when i need to create the branch-3.0 jobs).

and now to revisit this issue since it's been filed and address some points initially raised:



 
{code:java}
Narrowly, my suggestion is:
* Make the master Maven-based builds use dev/run-tests too, so that Pyspark tests are run. It's meant to support this, if AMPLAB_JENKINS_BUILD_TOOL is set to "maven". I'm not sure if we've tested this, then, if it's not used. We may need new Jenkins jobs to make sure it works. 
* Leave the Spark 2.x builds as-is as 'legacy'. 
{code}
 

re maven and dev/run-tests:  this will be super easy and i can probably get that done really quickly.  would dev/run-tests *replace* the mvn test block in the build script config?

re 2.x builds: easy.

 
{code:java}
Why also test with SBT? Maven is the build of reference and presumably one test job is enough? if it was because the Maven configs weren't running all the tests, and we can fix that, then are the SBT builds superfluous? Maybe keep one to verify SBT builds still work
{code}
i still am unsure why we have both, but would be more than happy to delete the SBT builds (esp if we have the maven test run dev/run-tests

 
{code:java}
Shouldn't the PR builder look more like the other Jenkins builds? maybe it needs to be special, a bit. But should all of them be using run-tests-jenkins?
{code}
 

for the most part, dev/run-tests-jenkins exists for pull request builds and posting results to PRs.  it also runs extra linting tests etc and acts mostly as a wrapper for dev/run-tests.  i'm nearly certain we can leave this as-is.

 
{code:java}
Looks like some cruft in the configs that has built up over time. Can we review/delete some? things like Java 7 home, hard-coding a Maven path. Perhaps standardizing on the simpler run-tests invocation does this?
{code}
i've actually been doing a lot of cleanup in the build configs.  i have a ways to go but things are MUCH cleaner.  

 

 

Thank you for update.

