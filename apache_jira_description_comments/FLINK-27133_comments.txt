Initial analysis for {{SerializationFrameworkMiniBenchmarks.serializerHeavyString}}:
* [build #189|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java8/189/console] (and previous runs): 235.725 ±   15.189  ops/ms
* [build #190|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java8/190/console]: 216.931 ±    1.231  ops/ms (c5d945a8ee..26aa543b3b)
* [build #191|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java8/191/console]: 201.851 ±    7.000  ops/ms (5f21d15a09..caa296b813)

It looks like the flush actually makes a difference. Test run [flink-benchmark-request #84|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/84/console] showed some increase in throughput again.

The previous test run [flink-benchmark-request #82|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/82/] didn't show an improvement after the LocalDataOutputStream changes (where the isClosed precondition check) were reverted

I focused on the following commits in my more recent analysis:
 * caa296b8 (\{{checkOpen}} invariant added in \{{LocalDataOutputStream}})
 * c50b0706 (removing {{flush}} from \{{FileSystemJobResultStore.createDirtyJobResultEntry}})
 * 26aa543b (DelegationTokenManager change)

* Build [#91|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/91/artifact/jmh-result.csv/*view*/], [#93|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/93/artifact/jmh-result.csv/*view*/], [#94|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/94/artifact/jmh-result.csv/*view*/]: caa296b8 (base) with 26aa543b being reverted
{code}
20:47:36    219.976 ±(99.9%) 15.001 ops/ms [Average]
20:47:36    (min, avg, max) = (199.255, 219.976, 254.392), stdev = 22.453
20:47:36    CI (99.9%): [204.975, 234.978] (assumes normal distribution)

15:37:21    219.255 ±(99.9%) 13.850 ops/ms [Average]
15:37:21    (min, avg, max) = (199.040, 219.255, 250.275), stdev = 20.730
15:37:21    CI (99.9%): [205.405, 233.105] (assumes normal distribution)

15:57:17    234.844 ±(99.9%) 14.785 ops/ms [Average]
15:57:17    (min, avg, max) = (200.617, 234.844, 253.508), stdev = 22.130
15:57:17    CI (99.9%): [220.059, 249.630] (assumes normal distribution)
{code}
* Build [#95|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/95/artifact/jmh-result.csv/*view*/], [#96|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/96/artifact/jmh-result.csv/*view*/], [#97|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/97/artifact/jmh-result.csv/*view*/]: Like #91 + caa296b8 being reverted
{code}
16:12:25    235.476 ±(99.9%) 14.475 ops/ms [Average]
16:12:25    (min, avg, max) = (203.669, 235.476, 254.322), stdev = 21.666
16:12:25    CI (99.9%): [221.001, 249.952] (assumes normal distribution)

16:27:27    235.626 ±(99.9%) 15.342 ops/ms [Average]
16:27:27    (min, avg, max) = (201.173, 235.626, 255.049), stdev = 22.963
16:27:27    CI (99.9%): [220.284, 250.967] (assumes normal distribution)

16:42:29    234.462 ±(99.9%) 15.133 ops/ms [Average]
16:42:29    (min, avg, max) = (199.156, 234.462, 253.515), stdev = 22.651
16:42:29    CI (99.9%): [219.328, 249.595] (assumes normal distribution)
{code}
* Build [#98|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/98/artifact/jmh-result.csv/*view*/], [#99|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/99/artifact/jmh-result.csv/*view*/], [#100|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/100/artifact/jmh-result.csv/*view*/]: Like #95 + c50b0706 being reverted
{code}
17:13:58    235.310 ±(99.9%) 14.886 ops/ms [Average]
17:13:58    (min, avg, max) = (202.351, 235.310, 254.051), stdev = 22.280
17:13:58    CI (99.9%): [220.424, 250.196] (assumes normal distribution)

17:29:06    219.856 ±(99.9%) 14.974 ops/ms [Average]
17:29:06    (min, avg, max) = (200.351, 219.856, 253.669), stdev = 22.413
17:29:06    CI (99.9%): [204.882, 234.830] (assumes normal distribution)

17:44:09    219.721 ±(99.9%) 14.996 ops/ms [Average]
17:44:09    (min, avg, max) = (196.237, 219.721, 253.804), stdev = 22.445
17:44:09    CI (99.9%): [204.725, 234.716] (assumes normal distribution)
{code}
* Build [#101|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/101/artifact/jmh-result.csv/*view*/], [#102|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/102/artifact/jmh-result.csv/*view*/], [#103|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/103/artifact/jmh-result.csv/*view*/]: Reset to cda343349f (pre Delegation Token Manager change; parent of 26aa543b)
{code}
18:29:35    235.985 ±(99.9%) 15.012 ops/ms [Average]
18:29:35    (min, avg, max) = (203.411, 235.985, 257.348), stdev = 22.469
18:29:35    CI (99.9%): [220.973, 250.996] (assumes normal distribution)

18:44:46    219.430 ±(99.9%) 15.044 ops/ms [Average]
18:44:46    (min, avg, max) = (192.279, 219.430, 252.936), stdev = 22.517
18:44:46    CI (99.9%): [204.386, 234.474] (assumes normal distribution)

18:59:43    253.396 ±(99.9%) 1.829 ops/ms [Average]
18:59:43    (min, avg, max) = (246.030, 253.396, 256.858), stdev = 2.737
18:59:43    CI (99.9%): [251.567, 255.225] (assumes normal distribution)
{code}
* Build [#104|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/104/artifact/jmh-result.csv/*view*/], [#105|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/105/artifact/jmh-result.csv/*view*/], [#106|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/106/artifact/jmh-result.csv/*view*/]: Like #101 with caa296b8 being added
{code}
19:42:59    249.811 ±(99.9%) 2.284 ops/ms [Average]
19:42:59    (min, avg, max) = (238.872, 249.811, 254.049), stdev = 3.418
19:42:59    CI (99.9%): [247.527, 252.095] (assumes normal distribution)

20:03:25    220.641 ±(99.9%) 15.517 ops/ms [Average]
20:03:25    (min, avg, max) = (199.993, 220.641, 255.706), stdev = 23.226
20:03:25    CI (99.9%): [205.123, 236.158] (assumes normal distribution)

20:18:30    220.226 ±(99.9%) 15.102 ops/ms [Average]
20:18:30    (min, avg, max) = (200.096, 220.226, 254.146), stdev = 22.604
20:18:30    CI (99.9%): [205.124, 235.328] (assumes normal distribution)
{code}

Finding from [~pnowojski]: This test was quite stable prior to 4a756059 in January (see {{git log 4a756059b724488021184f55073a50d72166b14a..00eed}}). It started to become flaky with the change set including the local working directory changes (FLINK-25402, FLINK-25436)

Looking at the history of this test, [~pnowojski] and I observed a few more things:
 * Test was stable until mid of January 2022 (FLINK-27165 was created to cover the investigation)
 * There's a big increase in performance beginning of March 2022: {{git log 4f233f86..d212b83f}}
 ** It's interesting to note that it contains adding the {{flush}} to the {{FileSystemJobResultStore#createDirtyJobResultEntry}} in {{281b8e74}} which was then reverted when the drop happened

I have to look into how the JMH runner works. The {{FileSystemJobResultStore#createDirtyJobResultEntry}} happens when the job reaches a globally-terminal state. Generally, it doesn't make sense from a logical point of few because the flush should be also be triggered by the {{OutputStream#close}} call that happens afterwards.

Other than that, the test results are quite flaky and, therefore, hard to interpret. My initial test runs didn't produce any conclusions. We came up with the conclusion that the test performance is kind of back to what it was before March 2022 and don't consider this a regression. But I'm tempted to add the flush anyway again and observe the test a bit longer...

I'm downgrading this issue from Blocker to Major because we're still on the same level as pre-March. But I will keep the issue open and do some additional test runs on the flush issue.

I looked into the changeset {{4f233f86..d212b83f}} to try to get an understanding of the performance improvement we observed there around March 10, 2022 (see [graph|http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=serializerHeavyString&extr=on&quarts=on&equid=off&env=2&revs=200]):
 * -d212b83fddb Revert "[hotfix][table-planner] Raw to Binary can fail if the user type ser/de fails"-
It's only a compilation error fix
 * ed5e6144441 FLINK-26306[state/changelog] Randomly offset materialization

 * -ce0da798f2a FLINK-26125[docs][table] Add new documentation for the CAST changes in 1.15. This closes #18813-
Documentation change

 * -cbfca3afae5 [hotfix][table-planner] Raw to Binary can fail if the user type ser/de fails-
Just added a not used method

 * -c5352fc5597 FLINK-26494[runtime] Adds log message to cleanup failure-
Log message extension in cleanup shouldn't have an impact on the performance

 * -6ea1782246c [hotfix][runtime] Adds check for consistency to avoid NullPointerException-
Only Precondition added

 * -f0fe63ee0ba [hotfix][runtime] Adds missing @ExtendWith to DefaultResourceCleanerTest-
Adds missing annotation in test class only

 * -254c918b3e6 FLINK-26550[checkpoint] Correct the information of checkpoint failure-
Fixes log message

 * -0318b260f71 FLINK-26450 Makes FileStateHandle.discardState fail if the file couldn't be deleted-
Makes FileStateHandle fail in case of delete returning {{false}} and the underlying state still exists (that should cause rather a decrease than a increase of performance)

 * -4907f5fb703 FLINK-26484[fs] Introduces Presto-specific FileSystem implementation that handles the recursive deletion-
Adds custom recursive deletion for PrestoFileSystem. Presto S3 FileSystem is not used in the test

 * -281b8e744dd FLINK-26555[runtime] Adds closing and flushing to OutputStream-
FileSystemJobResultStore is not used in the test (no HA enabled)

 * -22b025aaf37 FLINK-26557[docs] Extend CsvFormat documentation based on release-testing feedback-
docs change

[~roman]  Can {{ed5e6144441}} explain the increase in performance? That's the only commit I cannot explain out of the top of my head...

[~mapohl],  ed5e6144441 should only affect the setups with the changelog enabled. AFAIK, this is not the case with benchmarks.
And it's definetly not related to serialization.

Thanks for clarification. Hence, the jump around March 10, 2022 is not reasonable based on the commits. Same goes for the drop which is part of that issue based on the experiments I did. The test itself is quite flaky. FLINK-27165 is created as a follow-up to fix (or at least understand) the flakiness. I'm gonna close this issue as not-reproducible because current state is back to the levels we had before March 10, 2022.

Thanks for investigating this [~mapohl]. I also tend to think this must be some flaky performance gain on 10th March and loss a couple of weeks later. I would suspect that code was able to JIT optimise a bit differently. This or something similar could explain seemingly in explicable and conflicting benchmark results. +1 for closing as can not reproduce.

