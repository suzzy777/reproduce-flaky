some gaps found in [^org.apache.tez.test.TestRecovery-output.txt] which may be reduced:

7s
{code}
2020-05-10 12:39:59,406 INFO  [NM ContainerManager dispatcher] loghandler.NonAggregatingLogHandler (NonAggregatingLogHandler.java:handle(173)) - Scheduling Log Deletion for application: application_1589107183822_0001, with delay of 10800 seconds
2020-05-10 12:40:06,424 INFO  [Listener at MacBook-Pro.local/54645] containermanager.ContainerManagerImpl (ContainerManagerImpl.java:cleanUpApplicationsOnNMShutDown(762)) - Done waiting for Applications to be Finished. Still alive: [application_1589107183822_0001]
{code}

10s
{code}
2020-05-10 12:40:06,433 INFO  [IPC Server Responder] ipc.Server (Server.java:run(1466)) - Stopping IPC Server Responder
2020-05-10 12:40:16,451 INFO  [Listener at MacBook-Pro.local/54645] ipc.Server (Server.java:stop(3360)) - Stopping server on 54645
{code}

5s
{code}
2020-05-10 12:40:16,453 INFO  [Listener at MacBook-Pro.local/54645] nodemanager.NodeResourceMonitorImpl (NodeResourceMonitorImpl.java:isEnabled(85)) - Node Resource monitoring interval is <=0. org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl is disabled.
2020-05-10 12:40:21,475 WARN  [Listener at MacBook-Pro.local/54645] server.MiniYARNCluster (MiniYARNCluster.java:waitForAppMastersToFinish(526)) - Stopping RM while some app masters are still alive
{code}

this single case took 51s, but ~20s of it seemed to be "idle", maybe with some minicluster / yarn configuration would help
UPDATE: unfortunately most of the cases above happen only once per minicluster, so eliminating them doesn't reduce the overall runtime significantly

I have a starter patch you can use to help reduce the overall times for this test class. I'll look for it and upload it later today.

thanks [~jeagles], waiting for the starter patch
in the meantime, I was playing with parallelism, and I found that with fair scheduler I could have improved the overall testing time, please let me know if it makes sense

I modified testRecovery_OrderedWordCount to run all the cases (except the flaky from TEZ-4173), and I got the following result (scheduler, thread count):
{code}

CAPACITY SCHEDULER:

thread pool 10:
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 400.297 s - in org.apache.tez.test.TestRecovery

thread pool 5:
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 401.501 s - in org.apache.tez.test.TestRecovery

thread pool 3:
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 410.556 s - in org.apache.tez.test.TestRecovery

thread pool: 1
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 385.171 s - in org.apache.tez.test.TestRecovery



FAIR SCHEDULER:

thread pool 15:
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 332.778 s - in org.apache.tez.test.TestRecovery

thread pool 10:
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 320.876 s - in org.apache.tez.test.TestRecovery

thread pool 5:
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 281.534 s - in org.apache.tez.test.TestRecovery

thread pool 3:
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 269.571 s - in org.apache.tez.test.TestRecovery

thread pool 2:
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 253.058 s - in org.apache.tez.test.TestRecovery

thread pool: 1
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 388.268 s - in org.apache.tez.test.TestRecovery
{code}

tests use capacity scheduler by default, and after changing to fair scheduler I got a runtime minimum at 2-3 threads, important to add that this is an absolute zero config, I've only changed:
{code}
    conf.set(YarnConfiguration.RM_SCHEDULER, "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler");
{code}

I think the recovery tests are still valid by running AMs on the same minicluster in a parallel manner

thanks for the patch [~jeagles], it indeed improves further, with your conf the 3 threaded fair scheduled testRecovery_OrderedWordCount took: 230.083, 237.4, 232.466, 237.301 seconds in 4 consecutive runs (without the conf changes it was between 260-270s)
I'm sharing my patch merged with your configs soon

While testing with Fair Scheduler should be viable, I'm surprised it makes a difference like this. There must be some Capacity Scheduler configuration that delays scheduling to improve placement. I'm ok making this change as it is a quick way to large improvements. I would want the changes made globally in MiniTezCluster as likely that will give improvements in other testes as well.

okay, I refactored tez mini cluster and dfs cluster anyway into a wrapper class for convenience
(I'm about to commit TEZ-4173 in order to fix current issues first, and then sharing my patch here)
yeah, I think we should have your conf changes on tez mini cluster globally, and having fair scheduler only in TestRecovery until further investigation

[~jeagles]: could you please take a look at  [^TEZ-4149.01.patch]?
1. refactored dfs + tezmini clusters to a MiniClusterGroup wrapper
2. handled parallel runs with executors + concurrentunit (waiters take care of propagating assertions to testing main thread)
3. added your config

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 30s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} dupname {color} | {color:green}  0m  0s{color} | {color:green} No case conflicting files found. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 26s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 20s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 21s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} master passed {color} |
| {color:blue}0{color} | {color:blue} spotbugs {color} | {color:blue}  0m 44s{color} | {color:blue} Used deprecated FindBugs config; considering switching to SpotBugs. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 42s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 10s{color} | {color:orange} tez-tests: The patch generated 4 new + 60 unchanged - 4 fixed = 64 total (was 64) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  2s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 36m  0s{color} | {color:green} tez-tests in the patch passed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m  8s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 45m  4s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.8 Server=19.03.8 base: https://builds.apache.org/job/PreCommit-TEZ-Build/422/artifact/out/Dockerfile |
| JIRA Issue | TEZ-4149 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/13002709/TEZ-4149.01.patch |
| Optional Tests | dupname asflicense javac javadoc unit xml compile spotbugs findbugs checkstyle |
| uname | Linux de4857ea4f68 4.15.0-74-generic #84-Ubuntu SMP Thu Dec 19 08:06:28 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | personality/tez.sh |
| git revision | master / 4a97643 |
| Default Java | 1.8.0_252 |
| checkstyle | https://builds.apache.org/job/PreCommit-TEZ-Build/422/artifact/out/diff-checkstyle-tez-tests.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-TEZ-Build/422/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-TEZ-Build/422/artifact/out/patch-asflicense-problems.txt |
| Max. process+thread count | 1089 (vs. ulimit of 5500) |
| modules | C: tez-tests U: tez-tests |
| Console output | https://builds.apache.org/job/PreCommit-TEZ-Build/422/console |
| versions | git=2.7.4 maven=3.3.9 findbugs=3.0.1 |
| Powered by | Apache Yetus 0.11.1 https://yetus.apache.org |


This message was automatically generated.



[~abstractdog], sorry it's taken me a while to look at this patch.

- I think there is still value in being able to start hdfs and yarn separately, but I like that they are wrapped in a class like this. We can delay that decision until a test case needs one and not the other.
- What is the cost/benefit of using the runner factory as well at the Waiter class. It's a little messy, but may be worth it is limited situations like this.
- Let's fix the ASF license header on MiniClusterGroup

[~jeagles]: thanks for taking a look

_"I think there is still value in being able to start hdfs and yarn separately, but I like that they are wrapped in a class like this. We can delay that decision until a test case needs one and not the other."_
Yeah, I was thinking about that. There would be an optional "useHdfs(boolean)" kind of method before calling "start()" to tell the wrapper whether to hdfs or not (default: true). I'm including that into the next patch. I'm about to create a separate jira for adopting this wrapper to all unit tests, and then we can see if we need any additional APIs for the wrapper.

_"What is the cost/benefit of using the runner factory as well at the Waiter class. It's a little messy, but may be worth it is limited situations like this."_
I agree, it might look messy, both of them are used because this way I was able to share a common multithreaded runner logic (runWithShutDownConditions) between the 3 unit test methods in this class. Waiters cannot be eliminated as we assert inside the testing methods, and they run in subthreads from now on, we must not miss any assertion failures from those threads (if a subthread called the original Assert.assertXYZ, and failed, it would never propagate to the main thread, only through waiter.assertXYZ). Let me know if we can do this less mess, otherwise I would go on with this approach.

_"Let's fix the ASF license header on MiniClusterGroup"_
I'll take care of it in the next patch.

