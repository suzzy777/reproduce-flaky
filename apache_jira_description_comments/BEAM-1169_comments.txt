Alternatively, we may need the Runner to define how Metrics are matched, and the test just sets up the "correct" values. That would allow different runners/tests to impose reasonable restrictions.

This issue is P2 but has been unassigned without any comment for 60 days so it has been labeled "stale-P2". If this issue is still affecting you, we care! Please comment and remove the label. Otherwise, in 14 days the issue will be moved to P3.

Please see https://beam.apache.org/contribute/jira-priorities/ for a detailed explanation of what these priorities mean.


This issue was marked "stale-P2" and has not received a public comment in 14 days. It is now automatically moved to P3. If you are still affected by it, you can comment and move it back to P2.

This was resolved inÂ https://github.com/apache/beam/commit/4faa8feba822db000b4b42636408245422ed324d.

Was it? There's still a comment in the code pointing to this jira: https://github.com/apache/beam/blob/9ebdc8afcfd316333299c6929ad67ead8cf1ae4e/sdks/java/core/src/test/java/org/apache/beam/sdk/metrics/MetricsTest.java#L348

Also the test that links it, testAllAttemptedMetrics, is flaky (BEAM-7827), which seems related

Hello! Due to a bug in our Jira configuration, this issue had status:Resolved but resolution:Unresolved.

I am bulk editing these issues to have resolution:Fixed

If a different resolution is appropriate, please change it. To do this, click the "Resolve" button (you can do this even for closed issues) and set the Resolution field to the right value.

