Well I think it is high time we should handle these. As a first step I think yes we can disable all of these and raise JIRAs to fix them.
Once we have a green build, we can enforce a strict No-commit policy in case of a test failure.
Not sure, how much people will agree. But in the present case writing "Test failures look unrelated" has become a habit. :-) 

Thanks [~ayushtkn]
{quote}As a first step I think yes we can disable all of these and raise JIRAs to fix them.
{quote}
Those are the most frequent Junits:

       hadoop.hdfs.TestFileChecksum 
       hadoop.hdfs.TestFileChecksumCompositeCrc 
       hadoop.hdfs.server.datanode.TestBPOfferService 
       hadoop.hdfs.server.namenode.ha.TestHAAppend 
       hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped 
       hadoop.hdfs.server.namenode.ha.TestObserverNode 
       hadoop.hdfs.TestDFSOutputStream 
       hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks 
       hadoop.hdfs.server.federation.router.TestRouterRpcMultiDestination

 

Do you have suggestions for which tests should be turned off?

 

My main concern about disabling tests is that they may end up disabled for good; it is easy to ignore JIRAs.

I would do a big push for fixing those tests,
I'll take a pass on the list you have but feel free to tag me.

Thank you [~ahussein]. I think all the active Hadoop committers must read this description and take care of failing unit tests. I'll help to fix the failing tests. I suggest raising the priority to critical or blocker for all the JIRAs related to unit test failures to get attention.

For disabling tests, I have the same concern as Íñigo said.

Cc: [~stevel@apache.org] and [~tmarquardt] for Azure tests

I am very +1 for moving towards a no-commit policy on failed unit tests. If the unit test is bad, then fix it. If the unit test reveals a race/bug in the code, fix the code. But just ignoring them does basically no good for anything. 

Thank you guys for bringing the code repo back into shape.
I believe we need to apply those fixes backward on 3.x.

I think things have gone really bad as of now, The hdfs tests are failing like anything in almost every PR
https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2823/1/testReport/

We are again seeing a lot of OOM in daily builds:
https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/457/testReport/junit/org.apache.hadoop.hdfs.client.impl/TestBlockReaderLocal/testBlockReaderLocalByteBufferReadsNoChecksum/

I am not sure what happened as such recently, It looked good a month back as far as I remember, Did we bother the builds in any way. If anybody has some pointers on it?


Thanks [~ayushtkn],

Last time I checked the qbt-reports , there were only the dynamo test failing.
Do you know if the daily build runs all the unit tests? or only the modules with the delta commits?
If the latter, then the frequency of getting OOM depends on the commits (which modules were triggered everyday).

When there are a large number of unit tests to execute, then Some guesses:
* Too many threads are running. There could be some unit tests that make this problem worse.
* The entropy is not enough. this leads to threads stalling. See HADOOP-16810. I tried to set the JVM parameters for tests related to intermediate data encryption, but I don't think that this is enough.


[~ayushtkn] Yes, I submitted a ut yesterday and it passed, but there are a lot of other errors not related to my ut.

 

For Junit Test failure, it is a big trouble for developers, like the following example:

[pr-4349|https://github.com/apache/hadoop/pull/4349]
The test report shows that org.apache.hadoop.hdfs.server.namenode.ha.TestSeveralNameNodes.testCircularLinkedListWrites Failed.
{code:java}
 Error DetailsSome writers didn't complete in expected runtime! Current writer state:[Circular Writer:
  directory: /test-0
  target length: 50
  current item: 41
  done: false
, Circular Writer:
  directory: /test-1
  target length: 50
  current item: 39
  done: false
, Circular Writer:
  directory: /test-2
  target length: 50
  current item: 39
  done: false
] expected:<0> but was:<3> {code}
But I was surprised to find.

[https://github.com/apache/hadoop/pull/4339]

The test report shows that org.apache.hadoop.hdfs.server.namenode.ha.TestSeveralNameNodes.testCircularLinkedListWrites Success.All Tests
|[Test name|https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4339/4/testReport/org.apache.hadoop.hdfs.server.namenode.ha/TestSeveralNameNodes/#]|[Duration|https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4339/4/testReport/org.apache.hadoop.hdfs.server.namenode.ha/TestSeveralNameNodes/#]|[Status|https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4339/4/testReport/org.apache.hadoop.hdfs.server.namenode.ha/TestSeveralNameNodes/#]|
|[testCircularLinkedListWrites|https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4339/4/testReport/org.apache.hadoop.hdfs.server.namenode.ha/TestSeveralNameNodes/testCircularLinkedListWrites]|1 min 48 sec|Passed|

I now suspect that the junit test failure caused by the compile environment.

Is it possible that the compiled memory is too small, can you increase the memory used by some compilations?
{panel:title=patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt}
Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called? Command was /bin/sh -c cd /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4349/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs && {color:#ff0000}/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xmx2048m{color} -XX:+HeapDumpOnOutOfMemoryError -DminiClusterDedicatedDirs=true -jar /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4349/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs/target/surefire/surefirebooter3248290060089244263.jar /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4349/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs/target/surefire 2022-05-27T03-04-44_807-jvmRun2 surefire7444566443427356222tmp surefire_6685537493283452808462tmp Error occurred in starting fork, check output in log Process Exit Code: 1 Crashed tests:
{panel}
 

 

 

