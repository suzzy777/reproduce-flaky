Throwing intermittent tests I see in CI currently here.  

 
{code:java}
At-least 4 occurrences

[ERROR] Failures: 
[ERROR]   TestHoodieMultiTableDeltaStreamer.testMultiTableExecutionWithKafkaSource:168 expected: <true> but was: <false>

At-least 10 occurrences 

[ERROR] Failures: 
[ERROR]   TestRowDataToHoodieFunction.testRateLimit:72 should process at least 5 seconds ==> expected: <true> but was: <false>
 {code}

At least 2 occurrences 
{code:java}
8034 [ScalaTest-main-running-HoodieSparkSqlWriterSuite] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Started o.s.j.s.ServletContextHandler@4f622d2b{/metrics/json,null,AVAILABLE,@Spark}
- throw hoodie exception when there already exist a table with different name with Append Save mode
- test bulk insert dataset with datasource impl
- test insert dataset without precombine field
=====[ 2249 seconds still running ]=====
- test bulk insert dataset with datasource impl multiple rounds
- test basic HoodieSparkSqlWriter functionality with datasource insert for COPY_ON_WRITE
- test basic HoodieSparkSqlWriter functionality with datasource insert for MERGE_ON_READ
- test HoodieSparkSqlWriter functionality with datasource bootstrap for COPY_ON_WRITE
- test HoodieSparkSqlWriter functionality with datasource bootstrap for MERGE_ON_READ
- test schema evolution for COPY_ON_WRITE *** FAILED ***
  org.apache.spark.sql.AnalysisException: Intersect can only be performed on tables with the same number of columns, but the first table has 4 columns and the second table has 3 columns;;
'Intersect false
:- LogicalRDD [_row_key#1025, partition#1026, ts#1027L, new_field#1028], false
+- Project [_row_key#1047, partition#1048, ts#1049L]
   +- Project [_hoodie_file_name#1046, _row_key#1047, partition#1048, ts#1049L]
      +- Project [_hoodie_partition_path#1045, _hoodie_file_name#1046, _row_key#1047, partition#1048, ts#1049L]
         +- Project [_hoodie_record_key#1044, _hoodie_partition_path#1045, _hoodie_file_name#1046, _row_key#1047, partition#1048, ts#1049L]
            +- Project [_hoodie_commit_seqno#1043, _hoodie_record_key#1044, _hoodie_partition_path#1045, _hoodie_file_name#1046, _row_key#1047, partition#1048, ts#1049L]
               +- Relation[_hoodie_commit_time#1042,_hoodie_commit_seqno#1043,_hoodie_record_key#1044,_hoodie_partition_path#1045,_hoodie_file_name#1046,_row_key#1047,partition#1048,ts#1049L] parquet
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:43)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$12.apply(CheckAnalysis.scala:283)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$12.apply(CheckAnalysis.scala:280)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:280)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95) {code}

At least 2 times each
{code:java}
[ERROR] Failures: 1972[ERROR]   TestHoodieMergeOnReadTable.testRollbackWithDeltaAndCompactionCommitUsingFileList:727->testRollbackWithDeltaAndCompactionCommit:645 There files should have been rolled-back when rolling back commit 002 but are still remaining. Files: [hdfs://localhost:38052/tmp/junit1490151661555250871/dataset/2015/03/16/356a9385-7f25-4002-8c5a-27a6aa7fd838-0_1-20-25_001.parquet] ==> expected: <0> but was: <1> {code}
 
{code:java}
 [INFO] Results:8520[INFO] 8521[ERROR] Errors: 8522[ERROR]   TestHoodieDeltaStreamer.testUpsertsMORContinuousModeWithMultipleWriters:716->testUpsertsContinuousModeWithMultipleWriters:831->runJobsInParallel:940 » Execution{code}

{code:java}
[ERROR] Failures: 1970[ERROR]   TestCleaner.testKeepLatestCommits:1205 Must clean at least one old file ==> expected: <2> but was: <1> {code}

I tried to repro the follwoing tests by running them several times:

1. TestHoodieMergeOnReadTable.testRollbackWithDeltaAndCompactionCommitUsingFileList

2. TestHoodieDeltaStreamer.*testUpsertsMORContinuousModeWithMultipleWriters*

3. TestCleaner.testKeepLatestCommits

1 and 3 passed every time. 2 failed twice. In both the failures, deltaSteamerTestRunner timed out. After I increased the timeout from 240s to 360s, it passed.

cc: [~nishith29]

Couple more failures: 
{code:java}
https://travis-ci.com/github/apache/hudi/jobs/509789824

Tests run: 11, Failures: 7, Errors: 0, Skipped: 1, Time elapsed: 30.631 s <<< FAILURE! - in org.apache.hudi.sink.TestWriteMergeOnRead
[ERROR] org.apache.hudi.sink.TestWriteMergeOnRead.testInsertWithDeduplication  Time elapsed: 5.812 s  <<< FAILURE!
java.lang.AssertionError: Expected: is "[id1,par1,id1,Danny,23,4,par1]"     but: was "[]" at org.apache.hudi.sink.TestWriteMergeOnRead.checkWrittenData(TestWriteMergeOnRead.java:76)



https://travis-ci.com/github/apache/hudi/jobs/508670069

[ERROR] Tests run: 11, Failures: 0, Errors: 8, Skipped: 0, Time elapsed: 0.12 s <<< FAILURE! - in org.apache.hudi.keygen.TestTimestampBasedKeyGenerator
[ERROR] org.apache.hudi.keygen.TestTimestampBasedKeyGenerator.testTimestampBasedKeyGenerator  Time elapsed: 0.047 s  <<< ERROR!
{code}

-Starting from [this build|https://dev.azure.com/apache-hudi-ci-org/apache-hudi-ci/_build/results?buildId=204&view=results], the following tests are consistent failing in Azure CI-

-[ERROR] TestHoodieFileWriterFactory.testGetFileWriter:62 » UnsupportedOperation .orc f...-

-[ERROR] TestHoodieCompactor.testScheduleCompactionWithInflightInstant:155 » IllegalArgument-

Fixed in [https://dev.azure.com/apache-hudi-ci-org/apache-hudi-ci/_build/results?buildId=292&view=results] 

Seems to be a jar caching issue. cc [~vinoth]

First one seems related to ORC. 

Formatting using clusterid: testClusterID [ERROR] Tests run: 23, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 277.488 s <<< FAILURE! - in org.apache.hudi.table.TestHoodieMergeOnReadTable [ERROR] org.apache.hudi.table.TestHoodieMergeOnReadTable.testRollbackWithDeltaAndCompactionCommitUsingMarkers Time elapsed: 15.276 s <<< FAILURE! org.opentest4j.AssertionFailedError: There files should have been rolled-back when rolling back commit 002 but are still remaining. Files: [hdfs://localhost:45970/tmp/junit8832824241335171583/dataset/2016/03/15/e74c8c30-15e4-439c-a548-3b40021fe6cd-0_0-20-24_001.parquet] ==> expected: <0> but was: <1> at org.apache.hudi.table.TestHoodieMergeOnReadTable.testRollbackWithDeltaAndCompactionCommit(TestHoodieMergeOnReadTable.java:646) at org.apache.hudi.table.TestHoodieMergeOnReadTable.testRollbackWithDeltaAndCompactionCommitUsingMarkers(TestHoodieMergeOnReadTable.java:733) [INFO] [INFO] Results: [INFO] [ERROR] Failures: [ERROR] *TestHoodieMergeOnReadTable.testRollbackWithDeltaAndCompactionCommitUsingMarkers*:733->testRollbackWithDeltaAndCompactionCommit:646 There files should have been rolled-back when rolling back commit 002 but are still remaining. Files: [hdfs://localhost:45970/tmp/junit8832824241335171583/dataset/2016/03/15/e74c8c30-15e4-439c-a548-3b40021fe6cd-0_0-20-24_001.parquet] ==> expected: <0> but was: <1> [INFO] [ERROR] Tests run: 395, Failures: 1, Errors: 0, Skipped: 1

 

 

 

TestCleaner.testKeepLatestFileVersions:673 Must clean at least 1 file ==> expected: <2> but was: <1>

 

[ERROR] Tests run: 23, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 291.527 s <<< FAILURE! - in org.apache.hudi.table.TestHoodieMergeOnReadTable [ERROR] org.apache.hudi.table.TestHoodieMergeOnReadTable.testInsertsGeneratedIntoLogFilesRollbackUsingFileList Time elapsed: 0.341 s <<< ERROR! java.lang.NullPointerException at org.apache.hudi.table.TestHoodieMergeOnReadTable.init(TestHoodieMergeOnReadTable.java:118) at org.apache.hudi.table.TestHoodieMergeOnReadTable.init(TestHoodieMergeOnReadTable.java:135) [INFO] [INFO] Results: [INFO] [ERROR] Errors: [ERROR] TestHoodieMergeOnReadTable.init:135->init:118->HoodieClientTestHarness.initDFS:255 Â» NullPointer

[ERROR] Errors: [ERROR] TestHoodieClientMultiWriter.testMultiWriterWithAsyncTableServicesWithConflict:227 Â» Execution [INFO]

