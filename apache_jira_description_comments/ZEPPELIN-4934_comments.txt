[~dbanda] These jars are already in the classpath of spark process, do you mean you can not import then in pyspark ?

yes. the jars are in the classpath but not in the *python path.* for packages where the python code is in the jar it means we cannot import the python package in python. Spark (pyspark shell) solves this by adding all the jars to `spark.submit.pyfiles` after resolution;  then when `sc = SparkContext()` is called from python it automatically adds the jars to the python path.

I see [~dbanda] Thanks for the clarification.

Issue resolved by pull request 3838
[https://github.com/apache/zeppelin/pull/3838]

