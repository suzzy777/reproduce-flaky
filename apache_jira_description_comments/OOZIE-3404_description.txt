When we run spark in a cluster, we rely on the spark jars on hdfs. We don't deploy Spark on the cluster server. So running pySpark according to the Oozie documentation is not successful.
Â 
I found that when Hadoop is a 2.0+ version, although Oozie sets the {{SPARK_HOME}}  variable in {{mapred.child.env}} , the {{mapreduce.map.env}} variable is read first in Hadoop ([source code|https://github.com/apache/hadoop/blob/f95b390df2ca7d599f0ad82cf6e8d980469e7abb/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java#L45]) . So when we don't set {{SPARK_HOME}} env in {{mapreduce.map.env}} , pySpark doesn't work.