I should also point out this means that the timeouts don't get captured in the read timeout metric either due to the timeout occuring on the close for the PartitionIterator returned by StorageProxy:read where the timeouts are caught (see readRegular)

Any plans to fix this in the upcoming versions or atleast 4.0.x version? the error message is quite mis-leading.

[~stefan.miklosovic] is this still on your radar?

[~brandon.williams] Not really, honestly, I forgot this one completely. I can take a look, indeed, sometimes next week.

I am on it.

The proposed approach seems good to me but I had hard time to test this consistently. I tried the steps in the description of this ticket and sometimes I got the same response, sometimes I did not. Thinking about testing, I am not sure what test to write here. The options we have:

1) jvm dtest - this approach would be about setuping 2x3 cluster, inserting data, shutting down the node, removing the sstables of this node and starting it again, executing the query and watching its logs. I think that the step "removing of sstables" is not necessary because, I think, data dir of that node is automatically removed on the shutdown. I am not sure about the details and viability of this test approach yet.

2) same as 1 but it would be done in python dtests

3) Testing only RepairMergeListener and its close method. This would be very nice to do but what I noticed is that all inner classes in DataResolver (RepairMergeListener is inner class of DataResolver) are not static and they are all private. I can not just easilly test this class in isolation. I would need to rewrite it all to be static classes and so and this might have not-so-obvious consequences yet.

What I found interesting while I was testing this is that when I turned the node off, removed data, turned it on and listed the data dir of respective table, that SSTable was there again. How is this possible? Is not it like commit logs were flushed on the startup or something like that? I think we would need to remove commit logs too.

I added in-jvm dtest, in this branch.

The problem with that test is that it is flaky and I am not sure why. I tried to replicate same steps as [~cam1982] .

If exception is thrown, it can be either detected as part of shutdown process when cluster instance is being closed in try or it can be thrown right away. Sometimes it is not thrown at all and it just passes.

If you have any ideas [~cam1982] where this flakiness comes from that would be great.

https://github.com/apache/cassandra/pull/1683

[~stefan.miklosovic] do you have any estimate if this will be released anytime sooner? thank you

I do not plan to work on this. I am not sure how to move forward.

Let's give it another shot.

[~brandon.williams] this is what I have for 3.11. I think this is the max I can do here at the moment.

https://github.com/apache/cassandra/pull/1683

[~jaid] is this happening in 4.0 too? I am trying to figure that out. The logic around this stuff was rewritten there.

[https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/reads/repair/BlockingReadRepair.java#L82-L106]

Maybe [~benedict]  would know this as he was the last one who touched that code in CASSANDRA-14740.

branch: [https://github.com/apache/cassandra/pull/1683]
ci: [https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2304/]

I do not think that this is happening in 4.0+.

