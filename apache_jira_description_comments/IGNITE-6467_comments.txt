Seems there is second test failure related to this problem in CacheExchangeMergeTest.testDelayExchangeMessages(). 	
I've updated description. Or should we create separate issue for this failure?

Test history (88% success rate):
https://ci.ignite.apache.org/project.html?projectId=Ignite20Tests&testNameId=6645727674637282605&tab=testDetails

{noformat}
java.util.concurrent.ExecutionException: junit.framework.AssertionFailedError: Invalid value [node=distributed.CacheExchangeMergeTest14, client=true, order=13, cache=c9] expected:<3> but was:<null>
    at junit.framework.Assert.fail(Assert.java:57)
    at junit.framework.Assert.failNotEquals(Assert.java:329)
    at junit.framework.Assert.assertEquals(Assert.java:78)
    at junit.framework.TestCase.assertEquals(TestCase.java:244)
    at org.apache.ignite.internal.processors.cache.distributed.CacheExchangeMergeTest.checkNodeCaches(CacheExchangeMergeTest.java:1385)
    at org.apache.ignite.internal.processors.cache.distributed.CacheExchangeMergeTest.access$2500(CacheExchangeMergeTest.java:89)
{noformat}

Fixed simple bug in the test (it should not check data if tx failed, 81d04a5b61722bd62ee647ef46d3872bc71fbaaa). But locally got another failure: 

CacheException: class org.apache.ignite.IgniteCheckedException: Adding entry to partition that is concurrently evicted [grp=c6, part=554, shouldBeMoving=, belongs=true, topVer=AffinityTopologyVersion [topVer=11, minorTopVer=0], curTopVer=AffinityTopologyVersion [topVer=11, minorTopVer=1]]CacheException: class org.apache.ignite.IgniteCheckedException: Adding entry to partition that is concurrently evicted [grp=c6, part=554, shouldBeMoving=, belongs=true, topVer=AffinityTopologyVersion [topVer=11, minorTopVer=0], curTopVer=AffinityTopologyVersion [topVer=11, minorTopVer=1]]. 

 

Trying to add more simple test reproducing this.

For error with GridDhtInvalidPartitionException added more simple test (IgniteCacheClientNodeChangingTopologyTest.testPessimisticTx3), committed fix 7d4c46b9bfd502712143d6e05d40546a5c3420fb,

Again noticed failure on TeamCity with 'Invalid value ' error, need further investigate.

Debugged failure with ATOMIC cache (get after put returns null). Found out that problem caused by GridDhtPartitionTopologyImpl.updateRebalanceVersion, it is supposed that this method marks topology version as 'rebalanced' if all affinity nodes are owners and there are no others owners. But in this reproduce this scenario: servers A is owner, then affinity changes and another server B also becomes owner. Server A is still partition owner but GridDhtPartitionTopologyImpl.rebalanceFinished returns true, update in ATOMIC cache is not mapped to server A, but when server A tries to do get it still is owner and reads value locally.

It looks like sometimes  when GridDhtPartitionTopologyImpl.updateRebalanceVersion is called 'diffFromAffinity' is not correct (it does not match to current affinity version), and result of updateRebalanceVersion is not correct.

Fixed to skip GridDhtPartitionTopologyImpl.updateRebalanceVersion if is 'diffFromAffinityVersion' does not match to current affinity version.

