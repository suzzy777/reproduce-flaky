Saw it again. I took a brief look. The problem was that the connection to HMS was lost after the drop database commend was sent. HMS client however didn't know the command took effect and retried so it failed.
{noformat}
I0614 21:46:06.395942 14644 ClientCnxn.java:975] Opening socket connection to server localhost/0:0:0:
0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
W0614 21:46:06.396255 14644 ClientCnxn.java:1102] Session 0x0 for server null, unexpected error, clos
ing socket connection and attempting reconnect
Java exception follows:
java.net.ConnectException: Connection refused
  at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
  at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
  at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
  at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
W0614 21:46:06.548285 12859 RetryingMetaStoreClient.java:148] MetaStoreClient lost connection. Attemp
ting to reconnect.
Java exception follows:
org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out
  at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129)
  at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
  at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
  at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
  at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_drop_database(ThriftHiveMet
astore.java:733)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.drop_database(ThriftHiveMetastor
e.java:718)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:810)
  at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:101
)
  at com.sun.proxy.$Proxy5.dropDatabase(Unknown Source)
  at org.apache.impala.service.CatalogOpExecutor.dropDatabase(CatalogOpExecutor.java:1305)
  at org.apache.impala.service.CatalogOpExecutor.execDdlRequest(CatalogOpExecutor.java:300)
  at org.apache.impala.service.JniCatalog.execDdl(JniCatalog.java:146)
Caused by: java.net.SocketTimeoutException: Read timed out
  at java.net.SocketInputStream.socketRead0(Native Method)
  at java.net.SocketInputStream.read(SocketInputStream.java:152)
  at java.net.SocketInputStream.read(SocketInputStream.java:122)
  at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
  at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
  at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
  at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
  ... 16 more
I0614 21:46:06.550910 12859 HiveMetaStoreClient.java:547] Closed a connection to metastore, current c
onnections: 9
I0614 21:46:06.550966 12859 HiveMetaStoreClient.java:391] Trying to connect to metastore with URI thr
ift://localhost:9083
I0614 21:46:06.551259 12859 HiveMetaStoreClient.java:465] Opened a connection to metastore, current c
onnections: 10
I0614 21:46:06.551961 12859 HiveMetaStoreClient.java:518] Connected to metastore.
I0614 21:46:06.555757 12859 jni-util.cc:230] org.apache.impala.common.ImpalaRuntimeException: Error m
aking 'dropDatabase' RPC to Hive Metastore:
  at org.apache.impala.service.CatalogOpExecutor.dropDatabase(CatalogOpExecutor.java:1309)
  at org.apache.impala.service.CatalogOpExecutor.execDdlRequest(CatalogOpExecutor.java:300)
  at org.apache.impala.service.JniCatalog.execDdl(JniCatalog.java:146)
Caused by: NoSuchObjectException(message:test_fuzz_nested_types_67367717)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result$get_database_result
StandardScheme.read(ThriftHiveMetastore.java:16387)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result$get_database_result
StandardScheme.read(ThriftHiveMetastore.java:16364)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result.read(ThriftHiveMeta
store.java:16295)
  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database(ThriftHiveMeta
store.java:702)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database(ThriftHiveMetastore
.java:689)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1232)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:791)
  at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:101
)
  at com.sun.proxy.$Proxy5.dropDatabase(Unknown Source)
  at org.apache.impala.service.CatalogOpExecutor.dropDatabase(CatalogOpExecutor.java:1305)
  ... 2 more
{noformat}


Looking at the logs from the failed test (original post) and not the data-load failure, there are a number of issues with that run (from catalogd log):
{noformat}
java.lang.IllegalStateException at com.google.common.base.Preconditions.checkState(Preconditions.java:129) at org.apache.impala.common.FileSystemUtil.getDistributedFileSystem(FileSystemUtil.java:376) at org.apache.impala.catalog.CatalogServiceCatalog$CachePoolReader.run(CatalogServiceCatalog.java:317)
...
W0608 02:49:26.329107 727 RetryingMetaStoreClient.java:148] MetaStoreClient lost connection. Attempting to reconnect. Java exception follows: org.apache.thrift.transport.TTransportException at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table(ThriftHiveMetastore.java:1294) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table(ThriftHiveMetastore.java:1280) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1282)
...
E0608 02:51:10.309448 424 MetaStoreUtils.java:1350] Got exception: org.apache.thrift.transport.TTransportException null Java exception follows: org.apache.thrift.transport.TTransportException at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_all_databases(ThriftHiveMetastore.java:7\ 87) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_all_databases(ThriftHiveMetastore.java:775) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases(HiveMetaStoreClient.java:1105) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:101) at com.sun.proxy.$Proxy5.getAllDatabases(Unknown Source)
...
{noformat}
A lot of fs and metastore accesses were failing. From the hive log, looks like a few 'db not found' errors, but otherwise, looks like the db was created and accessed successfully. Here is what the hms log looks like around the time of the exception on the Impala side:
{noformat}
...
2018-06-08 05:49:26,101 INFO log.PerfLogger (PerfLogger.java:PerfLogBegin(127)) - <PERFLOG method=get_database from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
2018-06-08 05:49:26,101 INFO metastore.HiveMetaStore (HiveMetaStore.java:logInfo(836)) - 25: source:127.0.0.1 get_database:test_resolution_by_name_56b45511
2018-06-08 05:49:26,101 INFO HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(405)) - ugi=jenkins ip=127.0.0.1 cmd=source:127.0.0.1 get_database: test_resolution_by_name_56b45511
2018-06-08 05:49:26,101 INFO metastore.HiveMetaStore (HiveMetaStore.java:newRawStore(679)) - 25: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2018-06-08 05:49:26,103 INFO metastore.ObjectStore (ObjectStore.java:initialize(340)) - ObjectStore, initialize called
2018-06-08 05:49:26,109 INFO metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:<init>(136)) - Using direct SQL, underlying DB is OTHER
2018-06-08 05:49:26,109 INFO metastore.ObjectStore (ObjectStore.java:setConf(323)) - Initialized ObjectStore
2018-06-08 05:49:26,110 WARN metastore.ObjectStore (ObjectStore.java:getDatabase(628)) - Failed to get database test_resolution_by_name_56b45511, returning NoSuchObjectException
...{noformat}
Will look into the unique_database fixture to see if further retries make sense there. 

We have quite a few JIRAs caused by lost connection to namenode/HMS or even between impalads. I think we could try reducing the concurrency of the parallel tests and see how it goes.

All of the failures with this issue happen to be on S3. Are you noticing this across the board or more for some platforms?

Do you know where the knob for test concurrency lives?

- The other issues happen on non-S3 as well.
- https://github.com/apache/impala/blob/fd0ba0fd2c7631d48b4cce56e60f0b9f902cc446/tests/run-tests.py#L51

[~bharos92], can you please take a look and drive this jira to completion ?

Failed again:
{noformat}
19:18:01 ==================================== ERRORS ====================================
19:18:01  ERROR at teardown of TestMtDop.test_compute_stats[mt_dop: 2 | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: text/none] 
19:18:01 [gw1] linux2 -- Python 2.7.5 /data/jenkins/workspace/impala-cdh5-trunk-core-s3/repos/Impala/bin/../infra/python/env/bin/python
19:18:01 conftest.py:297: in cleanup
19:18:01     {'sync_ddl': sync_ddl})
19:18:01 common/impala_test_suite.py:523: in wrapper
19:18:01     return function(*args, **kwargs)
19:18:01 common/impala_test_suite.py:531: in execute_query_expect_success
19:18:01     result = cls.__execute_query(impalad_client, query, query_options, user)
19:18:01 common/impala_test_suite.py:621: in __execute_query
19:18:01     return impalad_client.execute(query, user=user)
19:18:01 common/impala_connection.py:160: in execute
19:18:01     return self.__beeswax_client.execute(sql_stmt, user=user)
19:18:01 beeswax/impala_beeswax.py:173: in execute
19:18:01     handle = self.__execute_query(query_string.strip(), user=user)
19:18:01 beeswax/impala_beeswax.py:339: in __execute_query
19:18:01     handle = self.execute_query_async(query_string, user=user)
19:18:01 beeswax/impala_beeswax.py:335: in execute_query_async
19:18:01     return self.__do_rpc(lambda: self.imp_service.query(query,))
19:18:01 beeswax/impala_beeswax.py:460: in __do_rpc
19:18:01     raise ImpalaBeeswaxException(self.__build_error_message(b), b)
19:18:01 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
19:18:01 E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
19:18:01 E    MESSAGE: ImpalaRuntimeException: Error making 'dropDatabase' RPC to Hive Metastore: 
19:18:01 E   CAUSED BY: NoSuchObjectException: test_compute_stats_2b9fb4b2
19:18:01 ---------------------------- Captured stderr setup -----------------------------
19:18:01 SET sync_ddl=False;
19:18:01 -- executing against localhost:21000
19:18:01 DROP DATABASE IF EXISTS `test_compute_stats_2b9fb4b2` CASCADE;
19:18:01 
19:18:01 SET sync_ddl=False;
19:18:01 -- executing against localhost:21000
19:18:01 CREATE DATABASE `test_compute_stats_2b9fb4b2`;
19:18:01 
19:18:01 MainThread: Created database "test_compute_stats_2b9fb4b2" for test ID "query_test/test_mt_dop.py::TestMtDop::()::test_compute_stats[mt_dop: 2 | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: text/none]"
19:18:01 ----------------------------- Captured stderr call -----------------------------
19:18:01 -- executing against localhost:21000
19:18:01 use functional;
19:18:01 
19:18:01 -- executing against localhost:21000
19:18:01 describe formatted alltypes;
19:18:01 
19:18:01 -- executing against localhost:21000
19:18:01 use functional;
19:18:01 
19:18:01 -- executing against localhost:21000
19:18:01 create external table test_compute_stats_2b9fb4b2.mt_dop like alltypes location 's3a://impala-test-uswest2-2/test-warehouse/alltypes';
19:18:01 
19:18:01 -- executing against localhost:21000
19:18:01 alter table test_compute_stats_2b9fb4b2.mt_dop recover partitions;
19:18:01 
19:18:01 SET mt_dop=2;
19:18:01 SET batch_size=0;
19:18:01 SET num_nodes=0;
19:18:01 SET disable_codegen_rows_threshold=0;
19:18:01 SET disable_codegen=False;
19:18:01 SET abort_on_error=1;
19:18:01 SET exec_single_node_rows_threshold=0;
19:18:01 -- executing against localhost:21000
19:18:01 compute stats test_compute_stats_2b9fb4b2.mt_dop;
19:18:01 
19:18:01 --------------------------- Captured stderr teardown ---------------------------
19:18:01 -- executing against localhost:21000
19:18:01 use default;
19:18:01 
19:18:01 SET sync_ddl=False;
19:18:01 -- executing against localhost:21000
19:18:01 DROP DATABASE `test_compute_stats_2b9fb4b2` CASCADE;
19:18:01 
19:18:01  ERROR at teardown of TestMtDop.test_compute_stats[mt_dop: 8 | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: kudu/none] 
19:18:01 [gw6] linux2 -- Python 2.7.5 /data/jenkins/workspace/impala-cdh5-trunk-core-s3/repos/Impala/bin/../infra/python/env/bin/python
19:18:01 conftest.py:297: in cleanup
19:18:01     {'sync_ddl': sync_ddl})
19:18:01 common/impala_test_suite.py:523: in wrapper
19:18:01     return function(*args, **kwargs)
19:18:01 common/impala_test_suite.py:531: in execute_query_expect_success
19:18:01     result = cls.__execute_query(impalad_client, query, query_options, user)
19:18:01 common/impala_test_suite.py:621: in __execute_query
19:18:01     return impalad_client.execute(query, user=user)
19:18:01 common/impala_connection.py:160: in execute
19:18:01     return self.__beeswax_client.execute(sql_stmt, user=user)
19:18:01 beeswax/impala_beeswax.py:173: in execute
19:18:01     handle = self.__execute_query(query_string.strip(), user=user)
19:18:01 beeswax/impala_beeswax.py:339: in __execute_query
19:18:01     handle = self.execute_query_async(query_string, user=user)
19:18:01 beeswax/impala_beeswax.py:335: in execute_query_async
19:18:01     return self.__do_rpc(lambda: self.imp_service.query(query,))
19:18:01 beeswax/impala_beeswax.py:460: in __do_rpc
19:18:01     raise ImpalaBeeswaxException(self.__build_error_message(b), b)
19:18:01 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
19:18:01 E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
19:18:01 E    MESSAGE: ImpalaRuntimeException: Error making 'dropDatabase' RPC to Hive Metastore: 
19:18:01 E   CAUSED BY: NoSuchObjectException: test_compute_stats_932a969c
{noformat}

Another occurrence:

{noformat}
conftest.py:319: in cleanup
    {'sync_ddl': sync_ddl})
common/impala_test_suite.py:691: in wrapper
    return function(*args, **kwargs)
common/impala_test_suite.py:699: in execute_query_expect_success
    result = cls.__execute_query(impalad_client, query, query_options, user)
common/impala_test_suite.py:793: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:180: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:187: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:362: in __execute_query
    handle = self.execute_query_async(query_string, user=user)
beeswax/impala_beeswax.py:356: in execute_query_async
    handle = self.__do_rpc(lambda: self.imp_service.query(query,))
beeswax/impala_beeswax.py:516: in __do_rpc
    raise ImpalaBeeswaxException(self.__build_error_message(b), b)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
E    MESSAGE: ImpalaRuntimeException: Error making 'dropDatabase' RPC to Hive Metastore: 
E   CAUSED BY: NoSuchObjectException: test_fuzz_uncompressed_parquet_e0e417c9
{noformat}

This has become more frequent, making it a P1. [~vihangk1] - I’m assigning this to you thinking you might have an idea what’s going on here; feel free to find another person or assign back to me if you're swamped.


{noformat}
W0610 11:28:56.681975 15747 RetryingMetaStoreClient.java:199] MetaStoreClient lost connection. Attempting to reconnect.
Java exception follows:
org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out
 at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129)
 at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
 at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
 at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
 at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
 at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_drop_database(ThriftHiveMetastore.java:801)
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.drop_database(ThriftHiveMetastore.java:786)
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1032)
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:968)
 at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:150)
 at com.sun.proxy.$Proxy17.dropDatabase(Unknown Source)
 at org.apache.impala.service.CatalogOpExecutor.dropDatabase(CatalogOpExecutor.java:1441)
 at org.apache.impala.service.CatalogOpExecutor.execDdlRequest(CatalogOpExecutor.java:336)
 at org.apache.impala.service.JniCatalog.execDdl(JniCatalog.java:162)
Caused by: java.net.SocketTimeoutException: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java:141)
 at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
 at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
 at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
 at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
 ... 17 more
I0610 11:28:56.682102 15747 HiveMetaStoreClient.java:628] Closed a connection to metastore, current connections: 9
I0610 11:28:56.682186 15747 HiveMetaStoreClient.java:472] Trying to connect to metastore with URI thrift://localhost:9083
I0610 11:28:56.682416 15747 HiveMetaStoreClient.java:546] Opened a connection to metastore, current connections: 10
I0610 11:28:56.688745 15747 HiveMetaStoreClient.java:599] Connected to metastore.
I0610 11:28:56.716217 15747 jni-util.cc:288] org.apache.impala.common.ImpalaRuntimeException: Error making 'dropDatabase' RPC to Hive Metastore:
 at org.apache.impala.service.CatalogOpExecutor.dropDatabase(CatalogOpExecutor.java:1446)
 at org.apache.impala.service.CatalogOpExecutor.execDdlRequest(CatalogOpExecutor.java:336)
 at org.apache.impala.service.JniCatalog.execDdl(JniCatalog.java:162)
Caused by: NoSuchObjectException(message:test_fuzz_uncompressed_parquet_e0e417c9)
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result$get_database_resultStandardScheme.read(ThriftHiveMetastore.java:27153)
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result$get_database_resultStandardScheme.read(ThriftHiveMetastore.java:27130)
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result.read(ThriftHiveMetastore.java:27061)
 at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database(ThriftHiveMetastore.java:770)
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database(ThriftHiveMetastore.java:757)
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:940)
 at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:150)
 at com.sun.proxy.$Proxy17.dropDatabase(Unknown Source)
 at org.apache.impala.service.CatalogOpExecutor.dropDatabase(CatalogOpExecutor.java:1441)
 ... 2 more 
{noformat}

It looks like the {{dropDatabase}} call to the HMS timed out and the {{RetryingHiveMetastoreClient}} tries to retry the operation but the retry fails with {{NoSuchObjectException}} since the first attempt succeeds on the server side.

On the metastore I see the following log:
{code}
2019-06-10T11:26:56,585  INFO [pool-8-thread-17] metastore.HiveMetaStore: 13: source:127.0.0.1 drop_database: test_fuzz_uncompressed_parquet_e0e417c9
.
.
.
2019-06-10T11:30:22,177  INFO [pool-8-thread-17] fs.TrashPolicyDefault: Moved: 's3a://impala-test-uswest2-2/test-warehouse/test_fuzz_uncompressed_parquet_e0e417c9.db' to trash at: s3a://impala-test-uswest2-2/user/jenkins/.Trash/Current/test-warehouse/test_fuzz_uncompressed_parquet_e0e417c9.db1560191216945
{code}

It takes ~3.5 min to drop this database on the HMS side. The client socket timeout defined in hive-site.xml is 2 min. I think we should increase the timeout value to reduce such flakiness.

{noformat}
      <property>
        <name>hive.metastore.client.connect.retry.delay</name>
        <value>0</value>
      </property>
      <property>
        <name>hive.metastore.client.socket.timeout</name>
        <value>120</value>
      </property>
{noformat}

Uploaded a patch which increases the metastore client timeout value to 10 min from 2 min to reduce such occurrences.

Commit e3117aa6645fec89ef587d6e57b38b981080cb03 in impala's branch refs/heads/master from Vihang Karajgaonkar
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=e3117aa ]

IMPALA-7154: Increase metastore client time out for tests

Some of the e2e tests create tables and databases on S3. HMS operations
like dropDatabaseCascade can take significant time (3-5 min) in such
tests. If the HMS Client times out, it retries for a configurable number
of times. Unfortunately, HMS client is not smart enough to understand
which operations should be retried. Eg, a drop database operation which
is timedout on the client side, should not be retried since the server
thread may have succeeded before the next retry. This would cause the
client to receive a NoSuchObjectException which would look like a flaky
test failure on the Impala side.

This patch increases the metastore client.socket.timeout for
the tests so that such flaky test failures are reduced.

Note:
1. The timeout value of 10 min was chosen based on looking at logs for
one such failure where the HMS API took about 3.5 min. Once this patch
is deployed we should continue to see if such failures reoccur and
readjust the client timeout value accordingly.

Change-Id: I253fc00618b28d8d6e626e1b6bf9c5c8290006d7
Reviewed-on: http://gerrit.cloudera.org:8080/13586
Reviewed-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


I am resolving this issue for now. Lets reopen this if we see this error again

