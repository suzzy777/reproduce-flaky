I think the test is flaky. It deletes the metadata files after creating the Iceberg table. The catalog update of adding the iceberg table could arrive to the coordinator earlier or later than the delete. The test fails in the former case.

If the catalog update arrives earlier than the files are deleted, coordinator has already loaded the metadata files so the SELECT won't fail. Note that when applying the catalog update of adding an Iceberg table, i.e. when deserializing the thrift Iceberg table, coordinator will also load the metadata files. The test depends on this step to failed. Expected logs:
{noformat}
I0712 19:59:10.272672  1593 ImpaladCatalog.java:230] Adding: TABLE:test_drop_corrupt_table_fb61cd07.corrupt_iceberg_tbl version: 1865 size: 3000
I0712 19:59:10.272737  1593 ImpaladCatalog.java:230] Adding: CATALOG_SERVICE_ID version: 1865 size: 60
I0712 19:59:10.340236  1593 CatalogUtil.java:300] Loading custom FileIO implementation: org.apache.iceberg.hadoop.HadoopFileIO
I0712 19:59:10.421828  1593 HiveMetaStoreClient.java:272] HMS client filtering is enabled.
I0712 19:59:10.422073  1593 HiveMetaStoreClient.java:720] Trying to connect to metastore with URI (thrift://localhost:9083) in binary transport mode
I0712 19:59:10.422343  1593 HiveMetaStoreClient.java:751] Opened a connection to metastore, URI (thrift://localhost:9083) current connections: 2
I0712 19:59:10.423260  1593 HiveMetaStoreClient.java:821] Connected to metastore.
I0712 19:59:10.423347  1593 RetryingMetaStoreClient.java:100] RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.metastore.HiveMetaStoreClient ugi=quanlong (auth:SIMPLE) retries=1 delay=1 lifetime=0
I0712 19:59:10.442678  1593 BaseMetastoreTableOperations.java:193] Refreshing table metadata from new version: hdfs://localhost:20500/test-warehouse/test_drop_corrupt_table_fb61cd07.db/corrupt_iceberg_tbl/metadata/00000-9ee5b3f7-3870-43fa-b26b-15a19d13b913.metadata.json
W0712 19:59:10.466976  1593 Table.java:544] The table corrupt_iceberg_tbl in database test_drop_corrupt_table_fb61cd07 could not be loaded.
Java exception follows:
org.apache.impala.catalog.IcebergTableLoadingException: Failed to load Iceberg table with id: test_drop_corrupt_table_fb61cd07.corrupt_iceberg_tbl
        at org.apache.impala.catalog.iceberg.IcebergHiveCatalog.loadTable(IcebergHiveCatalog.java:92)
        at org.apache.impala.util.IcebergUtil.loadTable(IcebergUtil.java:153)
        at org.apache.impala.util.IcebergUtil.loadTable(IcebergUtil.java:141)
        at org.apache.impala.catalog.IcebergTable.loadFromThrift(IcebergTable.java:469)
        at org.apache.impala.catalog.Table.fromThrift(Table.java:542)
        at org.apache.impala.catalog.ImpaladCatalog.addTable(ImpaladCatalog.java:477)
        at org.apache.impala.catalog.ImpaladCatalog.addCatalogObject(ImpaladCatalog.java:332)
        at org.apache.impala.catalog.ImpaladCatalog.updateCatalog(ImpaladCatalog.java:260)
        at org.apache.impala.service.FeCatalogManager$CatalogdImpl.updateCatalogCache(FeCatalogManager.java:114)
        at org.apache.impala.service.Frontend.updateCatalogCache(Frontend.java:559)
        at org.apache.impala.service.JniFrontend.updateCatalogCache(JniFrontend.java:186)
Caused by: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: ...{noformat}

To reproduce the issue, we can add a sleep before removing the files to let catalog update arrives first:
{code:java}
diff --git a/tests/query_test/test_iceberg.py b/tests/query_test/test_iceberg.py
index 08f9cff34..9323485f8 100644
--- a/tests/query_test/test_iceberg.py
+++ b/tests/query_test/test_iceberg.py
@@ -162,6 +162,8 @@ class TestIcebergTable(IcebergTestSuite):
     metadata_location = get_fs_path("""/test-warehouse/{0}.db/{1}/metadata""".format(
       unique_database, table))
     assert self.filesystem_client.exists(metadata_location)
+    from time import sleep
+    sleep(5)
     status = self.filesystem_client.delete_file_dir(metadata_location, True)
     assert status, "Delete failed with {0}".format(status)
     assert not self.filesystem_client.exists(metadata_location){code}

I think we should not expect the SELECT query to fail unless the table is invalidated so a metadata loading is triggered for the SELECT.

I've reproduced this with HDFS as well by running the test in a loop.

Thansk [~stigahuang] [~MikaelSmith] I will try to fix this soon

Commit 7a02b3b276a803cededd726abcc0aa0fd114e198 in impala's branch refs/heads/master from Andrew Sherman
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=7a02b3b27 ]

IMPALA-12049: Deflake test_drop_corrupt_table

Originally there were two flavors of the test_drop_corrupt_table test.
One version (test_drop_corrupt_table_with_invalidate) did
an 'invalidate metadata' as part of the test after deleting metadata
files, and the other version did not. The version without invalidation
was depending on the catalog update resulting from adding the iceberg
table arriving at the coordinator before the deletion. This happens a
lot of the time but it can't be guaranteed. Fix the test by removing
test_drop_corrupt_table. Simplify the code a little by inlining a
method, and rename the remaining test to be test_drop_corrupt_table.

Change-Id: I4cbdf5646ed20bb8333e5557ed43226de993b7dd
Reviewed-on: http://gerrit.cloudera.org:8080/20289
Reviewed-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


