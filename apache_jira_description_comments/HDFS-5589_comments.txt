Perhaps related, but there's an off-by-at-least-one error here when processing block reports. I have a 1 node cluster and added a cache directive with a repl of 3. Saw this log message:

{noformat}
13/12/04 17:51:39 WARN blockmanagement.CacheReplicationMonitor: We need 1 more replica(s) than actually exist to provide a cache replication of 3 for {blockId=1073741825, replication=3, mark=false}
{noformat}

When I bumped it to 4, it said 2, and at 2 it said 0. My guess is that the pending queue isn't getting cleared properly, leading to the single node getting double counted.

Patch attached. The cache/uncache bug was that we weren't clearing out blocks that weren't marked during the directive scan. So, an orphan block would retain the old mark and replication factor, and become cached again on the next rescan when the mark flipped to the old value.

I also incorporated HDFS-5507 (considering stale and capacity when caching), and also fixed another bug I found where we'd try to cache a block again on a node that already had it cached.


{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12621222/hdfs-5589-1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5816//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5816//console

This message is automatically generated.

Good work-- it's great to have finally found that bug.

{code}
+      DatanodeDescriptor datanode = chooseDatanodeForCaching(possibilities,
+          blockManager.getDatanodeManager().getStaleInterval());
+      assert (datanode != null);
{code}

Don't we need to handle the case where {{datanode == null}} here?  It seems like that will come up if, for example, we have no space available on any datanode.  It might be better to move the code from {{if (possibilities.isEmpty()) }} if statement into a new check for {{chooseDataNodeForCaching == null}}.  It would be nice to see for this case too-- just fill up the DN caches, or set them to 0...

When we're populating {{possibilities}}, we check the DNs for validity, including having enough remaining capacity, so I think this is technically right. I agree though that it reads poorly, so I'll refactor this, and also add a test that tries to cache some big files.

New patch rev. I refactored the CRM selection logic some, and also now account for pending cache commands while computing a DN's effective remaining capacity. I also include a new test for caching large files that exceed DN capacity.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12621425/hdfs-5589-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5822//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5822//console

This message is automatically generated.

+1.  Thanks, Andrew.

Test failure is unrelated and did not reproduce for me... TestHASafeMode seems to be flaky lately.

SUCCESS: Integrated in Hadoop-trunk-Commit #4961 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4961/])
HDFS-5589. Namenode loops caching and uncaching when data should be uncached. (awang via cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1555996)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java


SUCCESS: Integrated in Hadoop-Yarn-trunk #445 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/445/])
HDFS-5589. Namenode loops caching and uncaching when data should be uncached. (awang via cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1555996)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java


FAILURE: Integrated in Hadoop-Hdfs-trunk #1637 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1637/])
HDFS-5589. Namenode loops caching and uncaching when data should be uncached. (awang via cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1555996)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java


SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1662 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1662/])
HDFS-5589. Namenode loops caching and uncaching when data should be uncached. (awang via cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1555996)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java


Resolving as this was committed to trunk.

Closing tickets that are already part of a release.

