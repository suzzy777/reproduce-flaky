I set the kafka consumer/producer logs to TRACE so we can have a better understanding of the issue. 

Another run of the test discovered a similar issue (same configs for all consumers/producers/brokers as stated in bug description).

the attached file partial.log is an excerpt from the attached full log, it shows the relevant log entries for this scenario:
 * consumer-1 is assigned to all partitions at the beginning. we start by bringing down 1 broker.
the message "3" is sent to topic0 on offset 4 (key="3", value="3", offset=4).
 * it starts a transaction with test-producer-1, produces a message to topic1 (just echo the message, same key/value), and on AddOffsetsToTxnRequest it gets stuck for 15 seconds. at this point the broker has finished going down, and it comes back up.
 * the consumers are configured with *max.poll.latency=5000*, so they rebalance after 5 seconds, while consumer-1 is stuck without polling.
 * consumer-4 gets partition 3 now, and it receives the message on offset 4 since we didn't finish the transaction previously. it sends the message to topic1, commits the offset and the transaction using test-producer-2.
 * at this point we've handled offset 4 once.
 * some seconds after, test-producer-1 receives an error for AddOffsetsToTxnResponse, followed by a successful AddOffsetsToTxnResponse.
 * since there's no exception, it proceeds to complete the initial transaction, and succeeds. at this point we've handled [topic0 partition 3 offset 4] two times, breaking exactly-once semantics.

after talking with Matthias J Sax in the confluent slack, it seems that there was a mis-use of the api - transactional.ids were unique which caused dual processing.

