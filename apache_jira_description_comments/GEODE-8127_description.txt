The redis use of regions depends on the code that will modify the region that is storing redis data, to always execute on the primary. It thought it was accomplishing this by marking the function as "optimizeForWrite=true" and by routing the function to the node with the bucket using "withFilter(key)". This works most of the time. But in some cases the function executes on a redundant copy. It looks like what is happening is that at the time the function is dispatched it has one idea of who the primary is and sends the function to that node. But before it executes the primary moves from this node to another that is doing redundancy recovery. Then when our function finally does a "put" on the localDataSet it ends up being a remote operation that is sent to the other node.

If our redis function could get a lock that prevents the bucket primary status from changing (see BucketRegion doLockForPrimary) and then check to see if we are the primary (if not throw an exception that causes the function sender to retry  (see BucketMovedException) otherwise execute the function and at the end release the lock (see BucketRegion doUnlockForPrimary).

We could enable this with a new method added to Function (much like the existing isHA and optimizeForWrite). This new method could be executeOnPrimary and default to false (adding a default method to the Function interface will not cause backwards compatibility issues unless a current class that implements Function already had added a method named "executeOnPrimary").