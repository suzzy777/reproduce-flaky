I'm looking into this.

[~mkjellman] I've ran the following test more than 5k times since yesterday without a single failure: 

{code:java}
ant test -S -Dtest.name=org.apache.cassandra.cql3.ViewTest -Dtest.methods=testRegularColumnTimestampUpdates
{code}

I still have 20000 cycles test running (to be done in a few hours) to make sure I haven't missed anything. Is there more information about the failure you experienced or more details about your circumstances?

Thanks.




it's failing roughly 50% of the time when executed with circleci.. i've also seen it fail on jenkins and locally on a mac.

Could you send me a link? At the moment I have no information that would help to investigate this.

Thanks!

[~mkjellman] I've finished running additional 20k cycles for this specific method and for the whole org.apache.cassandra.cql3.ViewTest; all tests pass (I'm running locally on Linux and Mac). Are there any more links, pointers or details that I could use to investigate it further?

At the moment I can't reproduce it.

Thanks

[~alourie] hey, so sorry for the delayed reply.. i've been up to my eyeballs in the dtest pytest work along with all the other stuff and totally let this slip. I don't have a super great answer for you yet because I'm in the process of getting that story together... but maybe we can make this work :)

If you take a look at my C* fork, there is a CircleCI config:
https://github.com/mkjellman/cassandra/blob/trunk_circle/.circleci/config.yml

Create a free CircleCI account (if you don't have one yet) and register your C* fork on GitHub with CircleCI. Then, grab the above config and put it in a branch of trunk in your personal fork (you'll need to create a .circleci folder and put it in there. 

Starting at L47 of the config you'll need to switch things to use the free user config (i'm running under the assumption you don't have a paid CircleCI account here).

{code}
# Set env_settings, env_vars, and workflows/build_and_run_tests based on environment
env_settings: &env_settings
    # <<: *default_env_settings
    <<: *high_capacity_env_settings
env_vars: &env_vars
    # <<: *default_env_vars
    <<: *high_capacity_env_vars
workflows:
    version: 2
    # build_and_run_tests: *default_jobs
    build_and_run_tests: *with_dtest_jobs
{code}

comment out the instances of high_capacity_* and comment back in the default_* ones... and you might want to switch the workflows to only run the "default_jobs" which for right now will just build C* and run the unit tests.

This test fails about 50% of the time on CircleCI. Potentially it's exacerbated by running on Ubuntu? Another thing maybe worth trying is running the test via ant on ubuntu... The docker image I put together for CircleCI is available on DockerHub (config checked in to https://github.com/mkjellman/cassandra-test-docker) or you can grab it as kjellman/cassandra-test:0.1.3.

Another thing that we do is split up the unit tests across the total number of Circle containers available... based on historical runs it actually will try to distribute the tests that run in each container by time so you don't have a few containers with all the slow tests dragging the entire thing down. This means we use invoke the tests in each container via "ant testclasslist -Dtest.classlistfile=/path/to/unit/tests/to/run"... potentially maybe another test somewhere else doesn't clean up after itself and that causes testRegularColumnTimestampUpdates to fail? To be clear -- the splits across containers are on a per test method level -- not test class -- so you might have various methods of ViewTest run across different containers at the same time -- the results are all merged together by circle at the end to give one consolidated report for all the unit tests. none of the other unit tests on trunk have been flaky or failing when run via circle other than this test so I'm not sure I totally believe it's related to order it's run in or another test not cleaning up after itself -- also there are a lot of other asserts that are passing before the 2nd to last assert is hit (which is the one that's always failing -- and always failing with the same value of 1 instead of 2)...

hope all this helps get the ball rolling again... any hunches by just looking at the code? i don't really know the MV code very well... any chance there is a race between when the mv is completed building and available and when the assert is hit? maybe we need some kind of force blocking flush before we assert on those conditions? that's how we handle this in a lot of the other compaction related tests that check sstables on disk and row count...

[~mkjellman] Thanks Michael, that's exactly what I was looking for! I'll need to go over this, but it's a really good start. Will update you after I read this all and have anything additional :-)

[~mkjellman]

I've run the test on CircleCI with your configuration another 20 times and all pass successfully. So it might indeed be some kind of a racing condition in your environment.

The following is the code tested:

{code:java}
 updateView("UPDATE %s USING TIMESTAMP 1 SET c = ?, val = ? WHERE k = ?", 0, 0, 0);
 updateView("UPDATE %s USING TIMESTAMP 3 SET c = ? WHERE k = ?", 1, 0);
 updateView("UPDATE %s USING TIMESTAMP 2 SET val = ? WHERE k = ?", 1, 0);
 updateView("UPDATE %s USING TIMESTAMP 4 SET c = ? WHERE k = ?", 2, 0);
 updateView("UPDATE %s USING TIMESTAMP 3 SET val = ? WHERE k = ?", 2, 0);

 assertRows(execute("SELECT c, k, val FROM mv_rctstest"), row(2, 0, 2));
 assertRows(execute("SELECT c, k, val FROM mv_rctstest limit 1"), row(2, 0, 2));
{code}

This code works on the same data, as it updates the same row. It is possible that due to unclear timing peculiarities, the assert runs before the update had propagated to all the replicas; or that it runs even before the update has applied to the row on any replica. Even though that would be extremeley strange in the environment where the tests are run.

We could do something like you suggesting, adding  "getCurrentColumnFamilyStore().forceBlockingFlush();" before the asserts.

It seems that I can't reproduce this in my environment. Is there anything specific that your environment may have that mine doesn't?


[~mkjellman] bumping this one too.

[~mkjellman] this hasn't happened to me for a while. Do you reckon it still relevant?

[~mkjellman] This one is laying around for awhile. Do you still have issues in the tests? If not, maybe we should close this off.

[~alourie] I haven't seen this for a while, at least not on trunk. 3.0/3.11 are not completely green these days (about 5-10 consistently flaky). If this is not one of them, please go ahead and close. Thanks!

[~jasobrown] Just rerun this on trunk, 3.0 and 3.11, doesn't reproduce. Happy to close it off.

