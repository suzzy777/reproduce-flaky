Impala is not crashing. I think we are just returning an exception when the query is cancelled.

Dan, I think you made some changes here recently. We'd like a query that is explicitly cancelled to not return an error.

I can repro this very easily on vd0208.halxg.cloudera.com.

[~nong_impala_60e1], my cancellation change hasn't gone in yet (still out for review), but I can take a look at this one.

I've also disabled this on C4.

Dan, I'm not sure what's going on here. I would not rule out any possibilities, including weird cluster setup/env issues.

Where can I find an impalad.INFO corresponding to one of these failures? (we should get into the habit of attaching log files to bugs when they are filed - logs and other collateral can also come in handy down the road, e.g. for support to pattern match tickets with bugs already fixed in later releases).

Also, how do I run on vd0208.halxg.cloudera.com?  I'm not able to repro this on my own machine.

Ishaan and Lenni helped me with the previous questions. I repro'ed on vd0208 and will get the log myself.

The test is racy, and that's why we only see this on certain machines.  Note that the assert that is failing is after the delay, but *before* the cancelation call:

      sleep(vector.get_value('cancel_delay'))
      assert self.client.get_state(handle) != self.client.QUERY_STATES['EXCEPTION']
      cancel_result = self.client.cancel(handle)

When the test fails, it's because we've exceeded the memlimit sooner than 'cancel_delay' seconds.  Only some machines are fast enough to win that race, apparently.  If the cancel occurs first, then the test will pass okay.

(This was made more difficult to debug because returning Status::MEM_LIMIT_EXCEEDED (or any of the other static Status) do not result in any log file message with backtrace. I think I'll fix that to make things more debuggable in the future.)

The test case was added with the change below, but the commit message doesn't give reasoning for the memlimit 300:

commit a6666301fde07675804f41fad64362169b91b968
Author: Srinath Shankar <sshankar@cloudera.com>
Date:   Thu Jun 12 14:51:38 2014 -0700

    Check RuntimeState for cancellation in sorter.

Do we expect this query to succeed with memlimit 300?  Assuming not, I think we should remove the memlimit from the test case because it's inherently racy.

So Taras pointed out that the mem_limit is there to force spilling during sort. After the test is re-enabled, Nong said he'll look into re-enable the mem_limit to something reasonable so we get spilling but not Memory Limit Exceeded.

Running the query from the shell, here's the memory limit info:

{code}
[localhost.localdomain:21000] > select * from tpch_parquet.lineitem order by l_orderkey;
Query: select * from tpch_parquet.lineitem order by l_orderkey
WARNINGS: Memory limit exceeded


Backend 1:Memory Limit Exceeded
Query(9e4db2e9f45137f7:939fd867104a2994) Limit: memory limit exceeded. Limit=300.00 MB Consumption=300.06 MB
  Fragment 9e4db2e9f45137f7:939fd867104a2997: Consumption=300.06 MB
    UDFs: Consumption=0
    SORT_NODE (id=1): Consumption=192.27 MB
    HDFS_SCAN_NODE (id=0): Consumption=107.78 MB
    DataStreamSender: Consumption=4.00 KB
  Block Manager: Limit=200.00 MB Consumption=192.00 MB
WARNING: The following tables are missing relevant table and/or column statistics.
tpch_parquet.lineitem
{code}

commit e876c9e3f6ae4a6914c0d0976eb89dc45b8802b2
Author: Dan Hecht <dhecht@cloudera.com>
Date:   Tue Sep 16 16:23:44 2014 -0700

    IMPALA-1240: TestCancellationFullSort hits mem limit of 300MB with tpch_parquet

    The test sets a mem_limit of 300MB and does:

       delay(t)
       assert(query_state != EXCEPTION)
       cancel()

    If the mem_limit is hit before the delay completes, the assert will fail.  So,
    the test is racy.  It appears we are right on the 300 MB memory limit (on my
    machine, the query completes with peak mem usage in the high 200 MB).

    I'm not sure why the mem_limit dimension was added, but given that it makes
    the test racy and doesn't seem to add value, let's remove it and re-enable the
    test.

    Change-Id: I63767161bec6d7d6cd3de2e4563425297ffccb0d
    Reviewed-on: http://gerrit.sjc.cloudera.com:8080/4366
    Reviewed-by: Alex Behm <alex.behm@cloudera.com>
    Tested-by: jenkins

Over to you to determine a mem_limit that will induce spilling but never give Memory Limit Exceeded.

It seems the sort node is saying under the limit so spilling should be working. It's sort of random whether the rest of the query (the scans) can run in 100mb.

I might be misunderstanding this ticket, but it sounds like we still may not have coverage of spilling sort.

Is this still an issue?

I think this is likely another manifestation of how the sort node was broken when the buffered block mgr was shared between nodes.  We should fix up this test once we fix the sort node spilling.

I think this is probably trivial to fix now that we've fixed the sorter spilling bugs.



IMPALA-1240: add back spilling sort now that sorter is not flaky

This test coverage was removed because of flakiness.
Instead of mem_limit, use max_block_mgr_memory, which is more
deterministic since it isn't affected by scanner memory usage.
Now that the sorter is more stable, this test should reliably
succeed.

Change-Id: I88056db2eac3fa91bef85284a516be303f25333d
Reviewed-on: http://gerrit.cloudera.org:8080/3795
Reviewed-by: Tim Armstrong <tarmstrong@cloudera.com>
Tested-by: Internal Jenkins
---
M tests/query_test/test_cancellation.py
1 file changed, 19 insertions(+), 14 deletions(-)

Approvals:
  Internal Jenkins: Verified
  Tim Armstrong: Looks good to me, approved

