I'm unable to view the CI failure, and I looped this test for 48 hours without failure, so with nowhere to go, I give up.

{{So I got them to finally fail, I had to get down to a 2 core VM before they would:}}


{{test_rf_collapse_gossiping_property_file_snitch passed 1 out of the required 1 times. Success!}}
{{test_rf_expand_gossiping_property_file_snitch failed and was not selected for rerun.}}
{{ <class 'RuntimeError'>}}
{{ Ran out of time waiting for topology to change on node 0}}
{{ [<TracebackEntry /home/debian/code/cassandra-dtest/replication_test.py:392>, <TracebackEntry /home/debian/code/cassandra-dtest/replication_tes}}
{{t.py:583>, <TracebackEntry /home/debian/code/cassandra-dtest/replication_test.py:360>] }}
{{test_rf_collapse_property_file_snitch passed 1 out of the required 1 times. Success!}}
{{test_rf_expand_property_file_snitch failed and was not selected for rerun.}}
{{ <class 'RuntimeError'>}}
{{ Ran out of time waiting for topology to change on node 0}}
{{ [<TracebackEntry /home/debian/code/cassandra-dtest/replication_test.py:458>, <TracebackEntry /home/debian/code/cassandra-dtest/replication_tes}}
{{t.py:583>, <TracebackEntry /home/debian/code/cassandra-dtest/replication_test.py:360>] }}
{{test_cannot_restart_with_different_rack passed 1 out of the required 1 times. Success!}}
{{test_failed_snitch_update_gossiping_property_file_snitch passed 1 out of the required 1 times. Success! }}
{{test_failed_snitch_update_property_file_snitch passed 1 out of the required 1 times. Success!}}
{{test_switch_data_center_startup_fails passed 1 out of the required 1 times. Success!}}

Looks like the racks were assumed to be in the same order, adding a sort gets the tests to pass. I'm not sure why the order is changing sometimes, but in all cases the topology had changed and the comparison was just missing that fact.

fails

 

{{Status=Up/Down}}
 {{|/ State=Normal/Leaving/Joining/Moving}}
 {{-- Address Load Tokens Owns (effective) Host ID Rack}}
 {{UN 127.0.0.3 105.28 KiB 1 ? 41df89ae-0631-447c-9def-ff1fbff3b8a7 rack2}}
 {{UN 127.0.0.2 164.25 KiB 1 ? 47a49e4e-6964-4b00-b353-d3ab345f4aba rack1}}
 {{UN 127.0.0.1 128.46 KiB 1 ? b63ad12d-1d12-4e51-95ba-88ff61b252ea rack0}}

works

 

{\{16:05:35,630 replication_test DEBUG Datacenter: dc1 }}
 {{===============}}
 {{Status=Up/Down}}
 {{|/ State=Normal/Leaving/Joining/Moving}}
 {{-- Address Load Tokens Owns (effective) Host ID Rack}}
 {{UN 127.0.0.1 132.51 KiB 1 100.0% 8b244cb0-135d-4745-b498-86f46187348f rack0}}
 {{UN 127.0.0.2 164.36 KiB 1 100.0% 36f42f34-fe8c-4543-b6c9-6db12b93b660 rack1}}
 {{UN 127.0.0.3 192.66 KiB 1 100.0% 1224e768-db1f-4d56-a1c8-db9b764a46d5 rack2}}

 

 [pr|https://github.com/apache/cassandra-dtest/pull/58]

After running for awhile I did get a failure on a different test (not one of the one's listed) so that maybe worth another jira..but no flakey failures on the listed tests above after an overnight run.

||branch||circleci||jenkins dtest||
|[dtest_15605|https://github.com/apache/cassandra-dtest/pull/58]|[circleci|https://circleci.com/workflow-run/f23f13a9-bbdc-4764-8336-109517e137f1]|[!https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/3/badge/icon?style=flat-square!|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/3/]|

Note: no one seems able to access that circleci link…

Otherwise, [here|https://builds.apache.org/view/A-D/view/Cassandra%20trunk/job/Cassandra-trunk-dtest/lastSuccessfulBuild/testReport/replication_test/] is it easy to verify the flakiness of these tests.

Sorry, I hadn't realised those CircleCI workflow links weren't accessible. For posterity here's a dtest run that failed in that workflow: [j8_dtests-with-vnodes|https://circleci.com/gh/beobal/cassandra/1239] 

Committed as 981ac73a6174208adb3ec8de467e8558db9f0184

