[~paulo]I think I can take a look into this latter:D

This ticket is easy to do but I think it will not be optimal. Currently, upon every SELECT statement, we would need to load all snapshots all over again and again from disk because SnapshotManager is holding only to-be-expired ones.

[~maxwellguo] Thanks for your interest on this! Someone on my team was planning to work on this, do you mind if I reassign if you haven't started work on it yet?

[~smiklosovic] agreed, I think it's important to keep snapshots in memory to allow this to be more performant. Are you planning to do this in CASSANDRA-13338 ?

OK I have not start yet ，it ok to reassign to your team member .

[~maxwellguo] there is no our team. We are all one team. We just need to start to cache snapshots in SnapshotManager as well. We do that already but just for expiring snapshots. We need to remove that logic. If snapshots in memory and on disk get unsynchronized, there should be a way how to sync it again. I propose new flag in nodetool listsnaphot (like -r / --refresh). That would go to disk, by default it would read from SnapshotManager.

We can dedicate new ticket for this. After that is done, this ticket will be easy. 

{quote}If snapshots in memory and on disk get unsynchronized, there should be a way how to sync it again. I propose new flag in nodetool listsnaphot (like -r / --refresh). That would go to disk, by default it would read from SnapshotManager.
{quote}
I don't think snapshots in memory and disk should ever get unsynchronized, so I don't think we need such flag. We just need to ensure any created or removed snapshots are reflected in {{SnapshotManager}}.

The work of unifying snapshot management on {{SnapshotManager}} on [this branch|https://github.com/apache/cassandra/pull/1305] moves snapshots to memory, with tests. We just need to rebase the branch and remove stuff that was committed as part of CASSANDRA-17588.

bq. After that is done, this ticket will be easy.

I think these works are independent, but should be merged together. The virtual table will just read from SnapshotManager instead of reading from disk, but it should be transparent since the virtual table will just display a list of {{{}TableSnapshot{}}}.

bq. I don't think snapshots in memory and disk should ever get unsynchronized

Given how snapshots are managed today, I could see someone manually removing a snapshot from disk.

Yes, manual removal of a snapshot from disk by negligent or uninformed user, even done by accident, would make it unsynchronised.

We do not need to have that flag to refresh manually. We can refresh each 10 minutes, for example, just to be sure (or each hour?). That would still go to disk but so un-frequently it does not matter.

edit: however, what is funny is that if it was done periodically, by periodic refreshing, we would suddenly go to disk when we have not been touching that at all previously. So we would go to disk even more than before (only upon listing vs periodically).

bq. Yes, manual removal of a snapshot from disk by negligent or uninformed user, even done by accident, would make it unsynchronised.

We can add logic to check that the snapshot is present on disk before returning from {{SnapshotManager}}, I think this is done by the {{TableSnapshot.exists()}} method. We just need to remove it from {{SnapshotManager}} and maybe log a warning.

You mean like we would return it to listsnapshots output but we could add a column called "notes" and there it would be like "missing on disk"? 

_We can add logic to check that the snapshot is present on disk before returning from SnapshotManager_

so we would need to go to disk every time we need to list? For every snapshot to see if it is on disk or not? 

We could add a periodic refresher thread if we don't want to check existence when it's returned, but I don't expect the existence check on return to be very expensive (it just check the snapshot dir exists, and does not do traversal).

bq. You mean like we would return it to listsnapshots output but we could add a column called "notes" and there it would be like "missing on disk"?

{SnapshotManager.listSnapshots}} only returns a snapshot if {{exists() == true}}, otherwise it removes the phantom snapshot from {{SnapshotManager}}.

Interesting. I dont know ... going to disk kind of beats the whole purpose of not going there.

bq. so we would need to go to disk every time we need to list? For every snapshot to see if it is on disk or not?

File stat is very cheap since it's normally cached by the OS.

bq. Interesting. I dont know ... going to disk kind of beats the whole purpose of not going there.

The expensive part of going to disk is to traverse directories looking for snapshot, we will not be doing this.

Ok. Sounds good. Nice compromise.

Hi [~maxwellguo] we will no longer work on this so feel free to take it. This ticket can be done using the current {{listsnapshots}} implementation that loads snapshots from disk.

I will work on moving snapshots to memory in CASSANDRA-18111. Once that is done we can update the virtual table implementation to load snapshots from memory instead.

OK thx

[pr|https://github.com/apache/cassandra/pull/2117/]
[java8|https://app.circleci.com/pipelines/github/Maxwell-Guo/cassandra/399/workflows/afa49273-f53f-43e7-ad36-2baf0b635d15]
[java11|https://app.circleci.com/pipelines/github/Maxwell-Guo/cassandra/399/workflows/25e15960-41f0-4d09-83b9-90743a113063]

[~paulo][~smiklosovic]




Thanks. I have reviewed and requested more changes.

Thanks [~smiklosovic] I have left some responses to your reviewed comment.

java8 and java11 precommit are all green 
||Heading 1||Heading 2||
|pr for trunk|[trunk|https://github.com/apache/cassandra/pull/2117]|
|java8|[java8|https://app.circleci.com/pipelines/github/Maxwell-Guo/cassandra/415/workflows/e86c7bec-5c94-4ea8-91e9-1b2f00e04e49]|
|java11|[java11|https://app.circleci.com/pipelines/github/Maxwell-Guo/cassandra/415/workflows/3a2906c5-d2d0-4a77-8c02-324b04fee26b]|


[~paulo][~smiklosovic]I am so sorry that the old pr is discard due to an operation mistake by me and I feeel very sorry. And I have create a new one . 
[old pr link  that discard by me, but some conversations is left |https://github.com/apache/cassandra/pull/2117]
[the new pr that have been modify after resolved the conversations |https://github.com/apache/cassandra/pull/2205]
[java 8 precommit|https://app.circleci.com/pipelines/github/Maxwell-Guo/cassandra/417/workflows/440693db-00d0-4d86-aaf8-008d94f48ed4]
[java11 precommit |https://app.circleci.com/pipelines/github/Maxwell-Guo/cassandra/417/workflows/0b7c8599-2146-4f8f-83e7-347b60db5711]

java8 and java11 precommit ut are green

Hi [~maxwellguo] 

I created a PR against your branch in your repository here: [https://github.com/Maxwell-Guo/cassandra/pull/1]

I was not able to do any CQL select in CQL shell because types of the columns were not correct.

I also changed types of created_at and expires_at to CQL timestamp from text. If it is on timestamp, we can use CQL functions on that like "toDate" or "toUnixTimestamp" what was not possible before.

Please add these changes to your PR and we will run the CI.

Thanks

I merged your pr and also ran on a instance, thing seems to be all right . [~smiklosovic]

Thanks. The builds are running. I will let you know.

[https://github.com/apache/cassandra/pull/2205]

the squashed code I am building in CI is this:

[https://github.com/instaclustr/cassandra/tree/CASSANDRA-18102]

added test is flaky, run in multiplexer 500 times:

https://app.circleci.com/pipelines/github/instaclustr/cassandra/1969/workflows/65e26ddd-73ec-4e5d-ae12-89d6226c1876/jobs/10082/tests#failed-test-0

let me take a look

j1 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1972/workflows/21722109-cb38-4d57-80bb-f477037ab77
j8 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1972/workflows/47c41e84-ae79-436a-af20-c9a84275eca9

we need to ignore the order of these returned rows because it could be returned in different order each time.

https://github.com/instaclustr/cassandra/commit/c99f719f0f0f1cd2ec950b955dca3ef2e7f43cee

it is still flaky 

https://app.circleci.com/pipelines/github/instaclustr/cassandra/1972/workflows/21722109-cb38-4d57-80bb-f477037ab770/jobs/10408/tests#failed-test-0

very interesting. I do not have time for looking into that right now. Take a look if you want.

ok，let me take over this issue.

I have fixed this problem and it seems to be the the difference for changing Instant time to date . The fialed ut for java8/11 all passed. I will give this fixed pr latter .

I reworked the whole implementation a little bit. We need to retrieve these snapshots from snapshot loader.

j8 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1979/workflows/7fd6d250-bf81-4ede-88d9-400c22eb8260
j11 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1979/workflows/0b2ad3a2-5108-4ba5-947d-8cea232e7a15

PR https://github.com/instaclustr/cassandra/tree/CASSANDRA-18102

I am +1 on this branch and builds. Test was run 5000 times. 

[~paulo] would you take a look please and approve too?


{quote}I reworked the whole implementation a little bit. We need to retrieve these snapshots from snapshot loader.
{quote}
Nice, this looks cleaner.

One issue I found is that it was not possible to filter snapshots by size because they're in human readable string format, so I updated the {{size_on_disk}} and {{true_size}} to be long instead, to allow filtering by snapshot size:
{noformat}
cqlsh> SELECT * from system_views.snapshots WHERE keyspace_name = 'ks1' AND true_size > 1080;

 name | keyspace_name | table_name | created_at                      | ephemeral | expires_at                      | size_on_disk | true_size
------+---------------+------------+---------------------------------+-----------+---------------------------------+--------------+-----------
  ks1 |           ks1 |         t1 | 2023-03-19 17:24:39.236000+0000 |     False | 2023-03-20 03:24:39.236000+0000 |         6186 |      1082
  ks1 |           ks1 |         t2 | 2023-03-19 17:24:39.236000+0000 |     False | 2023-03-20 03:24:39.236000+0000 |         6186 |      1082
(2 rows)
{noformat}
Also, even though I suggested we use the {{id}} to identify a snapshot I actually think we should use {{name}} instead since this is the term used to describe snapshots on nodetool to avoid adding another term in addition to {{tag}} and {{{}name{}}}:
{noformat}
$ nodetool snapshot
Requested creating snapshot(s) for [ks1] with snapshot name [ks1.t1] and options {skipFlush=false}

$ nodetool listsnapshots
Snapshot Details:
Snapshot name              Keyspace name Column family name True size Size on disk Creation time            Expiration time
{noformat}
I made these changes on [this commit|https://github.com/pauloricardomg/cassandra/commit/1654048f2ee2cf89d5362d6e6172acc0d4f90d46].

CI (queued): [!https://ci-cassandra.apache.org/job/Cassandra-devbranch/2359/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/2359/pipeline]

I was discussing with Andres that we might add CQL functions taking longs and returning whatever size unit one wants. Like sizeof('true_size', 'MiB') would return it in MiB. So, I think that having just a simple long as a base to start with is fine. However, it might be quite confusing to parse that number "visually" to see what the size is, we have human-friendly output in nodetool already afaik but for this future flexibility in size representation it might be worth it (plus doing selects on it as in the above example).

Ive restarted it, there were some unused imports. Running here (1). 

Branch is instaclustr/CASSANDRA-18102

https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2360/

The build seems fairly OK.

bq. I was discussing with Andres that we might add CQL functions taking longs and returning whatever size unit one wants. Like sizeof('true_size', 'MiB') would return it in MiB.

This would be nice, is there a ticket for it yet?

bq. The build seems fairly OK.

+1, let's wait 24 hours to see if there's any feedback in the discuss thread and we can merge this tomorrow

+1

[~paulo] I ll merge it, no worries. My evening 20:00 CET.

sounds good, thanks!

Thanks [~paulo] and [~smiklosovic]

I ve run one more j8 pre-commit here https://app.circleci.com/pipelines/github/instaclustr/cassandra/2016/workflows/798fd5fd-7199-43a6-a030-bedf5333a538


