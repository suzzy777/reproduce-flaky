So I've been digging into this issue a bit today (mostly trying to see if I can get it to repro while running under strace). It looks like the buffers are getting flushed ok. How would people feel about catting unit-tests.log on build failure in jenkins so we have more info?

So I ran addPyFile and immeditetly used the result 100k times on my machine and couldn't repro it - so I think to make more progress it would be good to get the python test logs on failures in jenkins (unless there are hiding somewhere in plain sight I haven't noticed).

I also ran it again a few thousand times without strace just in case strace was causing the timing issue and didn't seem to be able repro it.

The logs should be available on the "Build Artifacts" for each build; unless the python logs end up somewhere else and are not being collected by jenkins.

So with a non logged in view I don't see it on https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/43800/ (for example), I remember there being build artifacts at one point - but I'm not sure if they aren't public anymore or I just don't remember where to look for them.

Hmm, interesting. I clicked on a few recent builds before I replied and I saw logs in those. Maybe the one above has been cleaned up or something?

Makes sense, I guess I'll wait for it to fail again and grab the build logs before they get cleaned up :)

Update: this is still an issue: https://spark-tests.appspot.com/tests/AddFileTests/test_add_py_file

I got the same error.Here is my unit-tests.log:

..E..........................................................................s............Ivy Default Cache set to: /home/jenkins/.ivy2/cache
The jars for the packages stored in: /home/jenkins/.ivy2/jars
file:/tmp/tmp22Y47d added as a remote repository with the name: repo-1
:: loading settings :: url = jar:file:/home/jenkins/.jenkins/workspace/V100R002C00_Spark_Core_UT/assembly/target/scala-2.10/spark-assembly-1.3.0-hadoop2.7.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
a#mylib added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found a#mylib;0.1 in repo-1
:: resolution report :: resolve 318ms :: artifacts dl 7ms
	:: modules in use:
	a#mylib;0.1 from repo-1 in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
	confs: [default]
	0 artifacts copied, 1 already retrieved (0kB/9ms)
.Ivy Default Cache set to: /home/jenkins/.ivy2/cache
The jars for the packages stored in: /home/jenkins/.ivy2/jars
file:/tmp/tmpW_7uje added as a remote repository with the name: repo-1
:: loading settings :: url = jar:file:/home/jenkins/.jenkins/workspace/V100R002C00_Spark_Core_UT/assembly/target/scala-2.10/spark-assembly-1.3.0-hadoop2.7.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
a#mylib added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found a#mylib;0.1 in repo-1
:: resolution report :: resolve 249ms :: artifacts dl 5ms
	:: modules in use:
	a#mylib;0.1 from repo-1 in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
	confs: [default]
	0 artifacts copied, 1 already retrieved (0kB/5ms)
..........
======================================================================
ERROR: test_add_py_file (__main__.AddFileTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/jenkins/.jenkins/workspace/V100R002C00_Spark_Core_UT/python/pyspark/tests.py", line 402, in test_add_py_file
    res = self.sc.parallelize(range(2)).map(func).first()
  File "pyspark/rdd.py", line 1301, in first
    rs = self.take(1)
  File "pyspark/rdd.py", line 1283, in take
    res = self.context.runJob(self, takeUpToNumLeft, p, True)
  File "pyspark/context.py", line 897, in runJob
    allowLocal)
  File "/home/jenkins/.jenkins/workspace/V100R002C00_Spark_Core_UT/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/home/jenkins/.jenkins/workspace/V100R002C00_Spark_Core_UT/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 1 times, most recent failure: Lost task 2.0 in stage 3.0 (TID 7, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "pyspark/worker.py", line 111, in main
    process()
  File "pyspark/worker.py", line 106, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "pyspark/rdd.py", line 1279, in takeUpToNumLeft
    yield next(iterator)
  File "/home/jenkins/.jenkins/workspace/V100R002C00_Spark_Core_UT/python/pyspark/tests.py", line 394, in func
    from userlibrary import UserClass
ImportError: cannot import name UserClass

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:167)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:208)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:126)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:71)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:222)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1285)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1276)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1275)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1275)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1469)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1430)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


----------------------------------------------------------------------
Ran 101 tests in 131.779s

FAILED (errors=1, skipped=1)
NOTE: Skipping SciPy tests as it does not seem to be installed
   Random listing order was used

Command exited with non-zero status 1
92.22user 4.84system 2:12.50elapsed 73%CPU (0avgtext+0avgdata 340784maxresident)k
1072inputs+92312outputs (0major+727455minor)pagefaults 0swaps


Haven't seen this in a long while, let's close it for now.

