Happened again [here|https://ci-cassandra.apache.org/job/Cassandra-4.0/53/testReport/junit/dtest.offline_tools_test/TestOfflineTools/test_sstableofflinerelevel/] and [here|https://ci-cassandra.apache.org/job/Cassandra-4.0/54/testReport/junit/dtest-offheap.offline_tools_test/TestOfflineTools/test_sstableofflinerelevel/]

{noformat}
Regression

dtest.offline_tools_test.TestOfflineTools.test_sstableofflinerelevel (from Cassandra dtests)
Failing for the past 1 build (Since Unstable#53 )
Took 3 min 28 sec.
Failed 1 times in the last 4 runs. Flakiness: 33%, Stability: 75%
Error Message

ccmlib.node.ToolError: Subprocess sstableoflinerelevel on keyspace1 : standard1 exited with non-zero status; exit status: 1;  stderr: WARN  11:30:58,846 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB WARN  11:30:58,852 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB WARN  11:30:58,852 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB WARN  11:30:58,852 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB WARN  11:30:58,852 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB WARN  11:30:58,852 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB Exception in thread "main" java.lang.RuntimeException: Failed to list files in /home/cassandra/cassandra/cassandra-dtest/tmp/dtest-y84fag44/test/node1/data0/keyspace1/standard1-b696b6e0baf011eb91ff8b04c35d7610  at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:77)  at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getFiles(LifecycleTransaction.java:604)  at org.apache.cassandra.db.Directories$SSTableLister.filter(Directories.java:892)  at org.apache.cassandra.db.Directories$SSTableLister.list(Directories.java:857)  at org.apache.cassandra.tools.SSTableOfflineRelevel.main(SSTableOfflineRelevel.java:104) Caused by: org.apache.cassandra.db.lifecycle.LogTransaction$CorruptTransactionLogException: Some records failed verification. See earlier in log for details.  at org.apache.cassandra.db.lifecycle.LogAwareFileLister.readTxnLog(LogAwareFileLister.java:136)  at org.apache.cassandra.db.lifecycle.LogAwareFileLister.classifyFiles(LogAwareFileLister.java:127)  at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)  at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)  at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)  at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)  at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)  at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)  at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)  at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)  at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)  at org.apache.cassandra.db.lifecycle.LogAwareFileLister.innerList(LogAwareFileLister.java:95)  at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:73)  ... 4 more

Stacktrace

self = <offline_tools_test.TestOfflineTools object at 0x7fd9db672a90>

    def test_sstableofflinerelevel(self):
        """
            Generate sstables of varying levels.
            Reset sstables to L0 with sstablelevelreset
            Run sstableofflinerelevel and ensure tables are promoted correctly
            Also test a variety of bad inputs including nonexistent keyspace and sstables
            @since 2.1.5
            @jira_ticket CASSANRDA-8031
            """
        cluster = self.cluster
        cluster.set_configuration_options(values={'compaction_throughput_mb_per_sec': 0})
        cluster.populate(1).start()
        node1 = cluster.nodelist()[0]
    
        # NOTE - As of now this does not return when it encounters Exception and causes test to hang, temporarily commented out
        # test by trying to run on nonexistent keyspace
        # cluster.stop(gently=False)
        # output, error, rc = node1.run_sstableofflinerelevel("keyspace1", "standard1", output=True)
        # assert "java.lang.IllegalArgumentException: Unknown keyspace/columnFamily keyspace1.standard1" in error
        # # this should return exit code 1
        # assert rc, 1 == msg=str(rc)
        # cluster.start()
    
        # now test by generating keyspace but not flushing sstables
    
        node1.stress(['write', 'n=1', 'no-warmup',
                      '-schema', 'replication(factor=1)',
                      '-col', 'n=FIXED(10)', 'SIZE=FIXED(1024)',
                      '-rate', 'threads=8'])
    
        cluster.stop(gently=False)
        try:
            output, error, _ = node1.run_sstableofflinerelevel("keyspace1", "standard1")
        except ToolError as e:
            assert re.search("No sstables to relevel for keyspace1.standard1", e.stdout)
            assert e.exit_status == 1, str(e.exit_status)
    
        # test by flushing (sstable should be level 0)
        cluster.start()
        session = self.patient_cql_connection(node1)
        logger.debug("Altering compaction strategy to LCS")
        session.execute("ALTER TABLE keyspace1.standard1 with compaction={'class': 'LeveledCompactionStrategy', 'sstable_size_in_mb':1, 'enabled':'false'};")
    
        node1.stress(['write', 'n=1K', 'no-warmup',
                      '-schema', 'replication(factor=1)',
                      '-col', 'n=FIXED(10)', 'SIZE=FIXED(1024)',
                      '-rate', 'threads=8'])
    
        node1.flush()
        cluster.stop()
    
        output, _, rc = node1.run_sstableofflinerelevel("keyspace1", "standard1")
        assert re.search("L0=1", output)
        assert rc == 0, str(rc)
    
        cluster.start()
        node1.nodetool('enableautocompaction keyspace1 standard1')
        # test by loading large amount data so we have multiple sstables
        # must write enough to create more than just L1 sstables
        keys = 8 * cluster.data_dir_count
        node1.stress(['write', 'n={0}K'.format(keys), 'no-warmup',
                      '-schema', 'replication(factor=1)',
                      '-col', 'n=FIXED(10)', 'SIZE=FIXED(1200)',
                      '-rate', 'threads=8'])
    
        node1.flush()
        logger.debug("Waiting for compactions to finish")
        self.wait_for_compactions(node1)
        logger.debug("Stopping node")
        cluster.stop()
        logger.debug("Done stopping node")
    
        # Let's reset all sstables to L0
        logger.debug("Getting initial levels")
        initial_levels = list(self.get_levels(node1.run_sstablemetadata(keyspace="keyspace1", column_families=["standard1"])))
        assert [] != initial_levels
        logger.debug('initial_levels:')
        logger.debug(initial_levels)
        logger.debug("Running sstablelevelreset")
        node1.run_sstablelevelreset("keyspace1", "standard1")
        logger.debug("Getting final levels")
        final_levels = list(self.get_levels(node1.run_sstablemetadata(keyspace="keyspace1", column_families=["standard1"])))
        assert [] != final_levels
        logger.debug('final levels:')
        logger.debug(final_levels)
    
        # let's make sure there was at least 3 levels (L0, L1 and L2)
        assert max(initial_levels) > 1
        # let's check all sstables are on L0 after sstablelevelreset
        assert max(final_levels) == 0
    
        # time to relevel sstables
        logger.debug("Getting initial levels")
        initial_levels = self.get_levels(node1.run_sstablemetadata(keyspace="keyspace1", column_families=["standard1"]))
        logger.debug("Running sstableofflinerelevel")
>       output, error, _ = node1.run_sstableofflinerelevel("keyspace1", "standard1")

offline_tools_test.py:204: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venv/src/ccm/ccmlib/node.py:1368: in run_sstableofflinerelevel
    return handle_external_tool_process(p, "sstableoflinerelevel on {} : {}".format(keyspace, cf))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

process = <subprocess.Popen object at 0x7fd9d9002100>
cmd_args = 'sstableoflinerelevel on keyspace1 : standard1'

    def handle_external_tool_process(process, cmd_args):
        out, err = process.communicate()
        if (out is not None) and isinstance(out, bytes):
            out = out.decode()
        if (err is not None) and isinstance(err, bytes):
            err = err.decode()
        rc = process.returncode
    
        if rc != 0:
>           raise ToolError(cmd_args, rc, out, err)
E           ccmlib.node.ToolError: Subprocess sstableoflinerelevel on keyspace1 : standard1 exited with non-zero status; exit status: 1; 
E           stderr: WARN  11:30:58,846 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB
E           WARN  11:30:58,852 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB
E           WARN  11:30:58,852 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB
E           WARN  11:30:58,852 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB
E           WARN  11:30:58,852 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB
E           WARN  11:30:58,852 Max sstable size of 1MB is configured for standard1.standard1.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB
E           Exception in thread "main" java.lang.RuntimeException: Failed to list files in /home/cassandra/cassandra/cassandra-dtest/tmp/dtest-y84fag44/test/node1/data0/keyspace1/standard1-b696b6e0baf011eb91ff8b04c35d7610
E           	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:77)
E           	at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getFiles(LifecycleTransaction.java:604)
E           	at org.apache.cassandra.db.Directories$SSTableLister.filter(Directories.java:892)
E           	at org.apache.cassandra.db.Directories$SSTableLister.list(Directories.java:857)
E           	at org.apache.cassandra.tools.SSTableOfflineRelevel.main(SSTableOfflineRelevel.java:104)
E           Caused by: org.apache.cassandra.db.lifecycle.LogTransaction$CorruptTransactionLogException: Some records failed verification. See earlier in log for details.
E           	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.readTxnLog(LogAwareFileLister.java:136)
E           	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.classifyFiles(LogAwareFileLister.java:127)
E           	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
E           	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
E           	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
E           	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
E           	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
E           	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
E           	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
E           	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
E           	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
E           	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.innerList(LogAwareFileLister.java:95)
E           	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:73)
E           	... 4 more

../venv/src/ccm/ccmlib/node.py:2303: ToolError
{noformat}


The error mentioned in the description and in the two tickets that this is a duplicate of is about the releveling itself, and I haven't been able to reproduce it in 4.0.

However, the error mentioned in the last comment is different. This new error consists on a validation error in the transaction log, thrown when running {{sstableoflinerelevel}}, at the moment of [listing the sstables|https://github.com/apache/cassandra/blob/cassandra-4.0.0/src/java/org/apache/cassandra/tools/SSTableOfflineRelevel.java#L104]. I haven't been able to reproduce this new type of failure in 3.0 and 3.11, although it can be reproduced in 4.0 with a few thousands of rounds in the multiplexer.

I suspect that somehow the corruption happens in the previous call to {{sstablelevelreset}}, so {{sstableoflinerelevel}} hits it before it actually starts the releveling. Indeed, the failure can be reproduced by just calling {{sstablelevelreset}} twice, without using {{sstableoflinerelevel}}.

I have been looking and playing a bit. Maybe there is some cross-test talk. I am seeing this failing almost in every other run but also errors on busy ips or node start collisions. Maybe you could make each test use a different KS...

[~bereng] the error running {{sstableoflinerelevel}} can be found running the test in isolation, although with a very low flakiness. Indeed I'm only able to reproduce it in the multiplexer. I think it goes away by running a {{sstableutil --cleanup}} after calling {{sstablelevelreset}}, [this way|https://github.com/adelapena/cassandra-dtest/commit/e3c2ea8d148329c6452ee74c20515a4857383649], but I'm not sure whether it would be masking a deeper problem in {{sstablelevelreset}}. How do you get those errors on busy ips or node start collisions? Is it running the entire {{offline_tools_test.py}} locally?

Sorry I didn't make myself clear. In the jenkins CI you can see in runs ip/node collisions, I was not referring specifically to this test. Just wondering if some env issue was causing this.

It seems the problem is caused by {{sstablelevelreset}} editing the metadata of temporary tables. When this happens the next call to {{Directories#sstableLister}} finds the validation problem. This call happens in the following call to {{sstableoflinerelevel}}, and it would also happen if we called {{sstablelevelreset}} again. The problem goes away if we modify {{sstablelevelreset}} to skip temporary tables, as it's done in [the proposed PR|https://github.com/apache/cassandra/pull/1058].

Additionally, [the PR for dtests|https://github.com/apache/cassandra-dtest/pull/144] cleans up the outstanding transactions with {{sstableutil}} just because the dtest doesn't have a way to get the levels of only the non-temporary sstables since it uses {{sstablemetadata}}, which AFAIK doesn't have a way to skip the temporary sstables.

I'm not familiarized with the {{lifecycle}} package so I'm not sure whether skipping the temporary sstables when resetting the levels is right, or whether the validation error that happens after changing the metadata is caused by a deeper problem. [~marcuse] [~stefania_alborghetti] I think you might be more familiarized with the related areas of the codebase, would you mind taking a look?

Here is a CircleCI round including 2k multiplexed runs of the failing tests:
* [j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/573/workflows/0f933cad-d639-4a80-8e93-cebbbb1d0dca]
* [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/573/workflows/2149e1ff-1dc8-4816-8678-a864722a228d]

{quote} 
 I'm not familiarized with the lifecycle package so I'm not sure whether skipping the temporary sstables when resetting the levels is right, or whether the validation error that happens after changing the metadata is caused by a deeper problem.
{quote}
I would need to see the full reason why the transaction rejected a record and I wasn't able to find a full failure, but it must have failed the checksum verification because the metadata file is changed by the standalone tools, {{sstablelevelreset}} in our case.

The transaction is checking if anything has tampered with a file guarded by it. This is done by {{LogFile.verify()}} and would also prevent a main Cassandra process from starting up. This is because there is some automated cleanup done on startup when {{LogTransaction.removeUnfinishedLeftovers()}} is called. Since we don't want to mistakenly delete files restored by users for example, we check using a checksum which is calculated from the files that existed when the transaction record was created. There are more checks but this is the main one and the one that I believe must have failed.

So if anything changes any of these files, temporary or permanent, the transaction detects it. These two standalone tools change the sstable metadata and hence probably triggered it.

I think it's reasonable to change {{sstablelevelreset}} to skip temporary files, because if the transaction did not complete, it's as if these files never existed. However, I don't think this is sufficient to fix the problem, because changing the old existing metadata files could also trigger a checksum error. So I may be wrong, but it seems to me that the real fix is to use the cleanup utility in the test, before running {{sstablelevelreset}} so that there are no left over transactions.

If these two tools are likely to be used directly from users when the process is offline, as they seem to be, I believe that they should cleanup leftover transactions first, or at least issue a warning if there are any. Otherwise the main process may refuse to start for the same reason explained above. To cleanup leftovers we can simply call {{LifecycleTransaction.removeUnfinishedLeftovers(cfs)}} from the tool itself, before doing any work. We should consider a follow up to do this, or fix this directly in this ticket. If we fix this here, then we don't need to do this in the test.

So you can either merge what you have and open a follow up, or add {{LifecycleTransaction.removeUnfinishedLeftovers(cfs)}}, as well as kipping the temporary files (which seems more correct to me), and see if this fixes it without changing the test.

Thanks a lot for the detailed explanation, it's really helpful.

I have moved the cleanup of the transaction log from the dtest to the {{sstablelevelreset}} and {{sstableoflinerelevel}} tools as suggested. I have also modified the dtests for those tools to start the cluster after editing the sstables, so we can be sure that the process can start after that edition. Here are the CI runs, including 1000 multiplexer runs:
||branch||CircleCI||
|3.0|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/588/workflows/28ac873f-0480-4372-9924-bcbfb9da899c]|
|3.11|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/587/workflows/a06ab77f-33b4-4d2e-8e4c-f2fd9ab5fa39]|
|4.0.0|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/585/workflows/a53b89a3-2a50-4b69-a20b-3d674f991730], [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/585/workflows/d563226b-68b0-49fd-8d3a-80e5cfb7d8ad]|
|4.0|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/581/workflows/4baaa9a0-572f-4b45-81ca-3919d699d8ed], [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/581/workflows/66f1afd2-503b-4fc1-bc16-33d775d13373]|
|trunk|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/586/workflows/9267e775-a113-46c3-80a3-872999a434d8], [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/586/workflows/9c8f8253-7642-4573-b774-9941973511a4]|

Please note that the patch addresses the problem described in Berenger's comment, but the failure about assertions on the number of levels mentioned in the description is a different problem that is not addressed by this patch. However that failure only affects 3.0 and 3.11. I think we should open a followup ticket for that particular problem, which has been around for a while, and keep this one focused on the transaction log problem that affects all branches, including 4.0.0.

Also, now that we know what's the cause of the problem, there is the question on whether this ticket should be a blocker for 4.0.0. The problem with the transaction log seems to happen very rarely and, even if it happens, it doesn't seem to have very dramatic consequences, since there is no damage to the sstables and operators can use {{sstableutil}} to cleanup the outstanding transactions. 

Having read the explanations I think [~stefania]'s opinion is the one that counts here but here are my 2cts:
* +1 to a separate 3.x ticket for the specific problem to those versions
* +1 to the current code if those {{sstableverify}} failures existed before the PR
* +1 to _not_ making this a 4.0.0 blocker as we have {{sstableutil}} as a workaround

wdyt?

The plan proposed above makes sense to me.

I also looked at the PR for trunk and it LGTM.

Great job [~adelapena] and [~bereng].

{quote}+1 to the current code if those {{sstableverify}} failures existed before the PR
{quote}
The failure in {{sstableverify}} can be seen in [this old Jenkins run|https://ci-cassandra.apache.org/job/Cassandra-3.0/lastCompletedBuild/testReport/dtest-novnode.offline_tools_test/TestOfflineTools/test_sstableverify/]. I have also started a [run in the multiplexer|https://app.circleci.com/pipelines/github/adelapena/cassandra/591/workflows/f42cc6fe-8ceb-4a04-ac45-10311e707e5a] without the patch. It hasn't finished yet but the {{sstableverify}} can also be found in the logs. Also, I think that the proposed changes in {{sstablelevelreset}} and {{sstableoflinerelevel}} are not in the path of that test. So I think it's safe to say that the failure is not related.
{quote}+1 to not making this a 4.0.0 blocker as we have sstableutil as a workaround
{quote}
Being the patch ready to commit, I'm not sure whether we should commit it to 4.0.0 or not. The two changed offline tools seem quite isolated so it doesn't seem that this is going to introduce a great risk of slowing down the release, but at this point any risk seems very undesirable. [~bereng] should we just exclude {{cassandra-4.0.0}} from the commit? wdyt?

[~stefania] thanks again for helping with this.

{quote}+1 to a separate 3.x ticket for the specific problem to those versions
{quote}
About that one, I think that only happens in 3.0, and 3.11 is also unaffected. The failure can be seen in [this 3.0 run|https://app.circleci.com/pipelines/github/adelapena/cassandra/589/workflows/374d302c-26a8-486c-a615-205a365f51cc/jobs/5699], but not in [this other 3.11 run|https://app.circleci.com/pipelines/github/adelapena/cassandra/590/workflows/7ec24e4b-d3c8-48cd-ab16-d292b9e048c2/jobs/5702]. I wrongly thought that I had seen the failure in 3.11 but it was just a timeout.

{quote}Berenguer Blasi should we just exclude cassandra-4.0.0 from the commit? wdyt?{quote}

Yep I already mentioned I wouldn't merge into 4.0.0 this late in the game imo, apologies if I was not clear.

Ok, I've changed the fix version. I guess that we will just ignore the occasional test failure if it appears in 4.0.0.

Committed to 3.0 as [e97b8fba1180b0326c28d5223453be5cd7a5d483|https://github.com/apache/cassandra/commit/e97b8fba1180b0326c28d5223453be5cd7a5d483] and merged up to [3.11|https://github.com/apache/cassandra/commit/d844b4b40607489b2b946fe8ecaff379e68a9f05], [4.0|https://github.com/apache/cassandra/commit/2573b087260930189dbf2314b7a164a77d285152] and [trunk|https://github.com/apache/cassandra/commit/c3a468c6691a127b3d611270edea0b218bedc3b5], skipping [4.0.0|https://github.com/apache/cassandra/commit/2493ad15dd617ce37308bba02de555bbaf1fa511]. 

Dtest changes committed as [84ea158737f42efb1bcc4853c496476fc4c91b66|https://github.com/apache/cassandra-dtest/commit/84ea158737f42efb1bcc4853c496476fc4c91b66].

Thanks for the reviews.

The followup ticket for the assertion error in 3.0 is CASSANDRA-16748

