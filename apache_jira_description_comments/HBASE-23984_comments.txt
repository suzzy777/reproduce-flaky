 !Screen Shot 2020-03-17 at 9.46.49 PM.png! 

This one keeps failing... about 30% of the time. It happens pretty frequently in local runs too.

Studying this some, it can happen in other test teardowns in other tests -- locally at least -- and there is another form of this failure that happens where the problem happens when AssignRegionHandler runs during cluster shutdown and the shutdown sequence is out of the usual ordering (the TestMasterAbortAndRSGotKilled issue is when UnassignRegionHandler runs at cluster shutdown). In both cases, the close that goes with the unassign or the open that goes w/ the assign, they can fail in a not usual way whe the cluster is going down and the sequencing strays from the usual. When this happens, the exception complaining we can't close/open because server aborting happens WITHOUT clearing the region entry from the RegionServer regionsInTransitionInRS Map and so we are destined to circle the waitOnAllRegionsToClose loop until the test timesout.

Looking at the handler code in UnassignRegionHandler and in AssignRegionHandler, they need a bit of restructuring so the clearing of the Region from regionsInTransitionInRS happens even on error. CloseRegionHandler does this already. Code in HRegionServer#closeRegion duplicates much of what is in CloseRegionHandler. Code in UnassignRegionHandler needs to close a Region and again dupes CloseRegionHandler.

Attached patch addresses the non-clearing of this.regionsInTransitionInRS in the respective Handlers in a finally block making shutdown succeed more often and it cleans up some of the duplication.

I think the behavior is correct? Once master is down then we can not finish the move, so I think this is a test problem, we should try to fix the test, not messing up the currect code. The logic in UnassignRegionHandler has been greatly simplified so I do not want to see that we pull it back to the old CloseRegionHandler...

Which behavior is correct? Not the bit where URH and ARH exit w/o clearning up regionsInTransitionInRS? Maybe you are referring to question in the patch around how the subject-line test ends? If so, ok. I'll update the test comment. The logic in URH duplicates CRH in all but one line except it missed regionsInTransitionInRS cleanup whereas CRH was doing right thing.

If there are exceptions thrown from the process method then we will abort the whole region server, so whether or not to remove an entry from the regionsInTransitionInRS does not matter, or you even do not know if you should remove it from regionsInTransitionInRS(maybe we need to prevent further transition on the region to prevent further damage to the system). If we hang in shutdown then it is the problem for the shutdown method, not the problem for UnassignRegionHandler.

Thanks [~zhangduo].

bq. ....so whether or not to remove an entry from the regionsInTransitionInRS does not matter...

But it does. If we don't remove ourselves from regionsInTransitionInRS, even though we are aborting, the RS won't go down. It gets stuck in the RS#waitOnAllRegionsToClose loop.

bq.  If we hang in shutdown then it is the problem for the shutdown method, not the problem for UnassignRegionHandler.

Nah. The shutdown method looks at online regions and regionsInTransitionInRS. Trying to have shutdown do compensation for a Handler that does not do proper state handling will complicate logic needlessly in an already involved process.

I've changed my approach. I made a new patch. It is the minimal changes needed to fix the two scenarios that can cause tests to hang on exit. I need this committed to make progress on test cleanup. In this new patch I do not address code duplication and cleanup to try and make all Handlers look and act about the same. This latter effort has brought on an interesting discussion. I created HBASE-24015 yesterday because I figured we'd see a reaction like the one we are having here. We can carry on there? Meantime I'd like to get the fix committed. Thanks.

New PR is https://github.com/apache/hbase/pull/1311. I seem to have accidentally closed the old one. I'll take on the helpful feedback over in our new issue, HBASE-24105 but will do innocuous changes and feedback in a sub-issue just filed against this task.

Attached last version of ambitious patch. Sub-issue adds commentary, and innocuous changes from the ambitious patch in particular folding in some feedback gotten up on the PR.

HBASE-24026 is suggestion that came of the PR review.

Pushed on branch-2.3, branch-2, and master (didn't mark this 2.4.0 as per comment by Duo up on dev list... until there is a 2.3.0 and an issue is in both, then its 2.3.0, not 2.4.0).

Thanks for the reviews. Linked follow-ons to this issue.

Results for branch branch-2.3
	[build #3 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.3/3/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.3/3//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.3/3//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.3/3//JDK8_Nightly_Build_Report_(Hadoop3)/]


(x) {color:red}-1 jdk11 hadoop3 checks{color}
-- For more information [see jdk11 report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.3/3//JDK11_Nightly_Build_Report/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(x) {color:red}-1 client integration test{color}
--Failed when running client tests on top of Hadoop 2. [see log for details|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.3/3//artifact/output-integration/hadoop-2.log]. (note that this means we didn't run on Hadoop 3)


Results for branch master
	[build #1674 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/1674/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(x) {color:red}-1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/1674//General_Nightly_Build_Report/]




(/) {color:green}+1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1674//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1674//JDK8_Nightly_Build_Report_(Hadoop3)/]


(x) {color:red}-1 jdk11 hadoop3 checks{color}
-- For more information [see jdk11 report|https://builds.apache.org/job/HBase%20Nightly/job/master/1674//JDK11_Nightly_Build_Report/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(x) {color:red}-1 client integration test{color}
--Failed when running client tests on top of Hadoop 2. [see log for details|https://builds.apache.org/job/HBase%20Nightly/job/master/1674//artifact/output-integration/hadoop-2.log]. (note that this means we didn't run on Hadoop 3)


Results for branch branch-2
	[build #2560 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2560/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2560//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2560//JDK8_Nightly_Build_Report_(Hadoop2)/]


(/) {color:green}+1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2560//JDK8_Nightly_Build_Report_(Hadoop3)/]


(x) {color:red}-1 jdk11 hadoop3 checks{color}
-- For more information [see jdk11 report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2560//JDK11_Nightly_Build_Report/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(x) {color:red}-1 client integration test{color}
--Failed when running client tests on top of Hadoop 2. [see log for details|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2560//artifact/output-integration/hadoop-2.log]. (note that this means we didn't run on Hadoop 3)


[~ndimiduk]... Duo noted that if a patch goes into branch-2.3 and will make the 2.3.0 release, then its fix version should be 2.3.0 and NOT 2.4.0.  2.4.0 would have stuff that is in 2.4.0 only or 2.4.0 and 2.3.x.

bq. Duo noted that if a patch goes into branch-2.3 and will make the 2.3.0 release, then its fix version should be 2.3.0 and NOT 2.4.0. 2.4.0 would have stuff that is in 2.4.0 only or 2.4.0 and 2.3.x.

ACK. Will need to take a JIRA cleanup pass. Would be good to get this kind of logic coded into the release audit script.

I meet this problem on branch-2.2 too. This case happened because the DelayCloseCP. The event execute order is:
 # Close regiong. But because the DelayCloseCP, it will close after 10 seconds.
 # Finish ut and shutdown cluster.
 # Shutdown master.
 # Shutdown RS. Call waitOnAllRegionsToClose method. But abortRequested is false now.
 # Close region and failed because master is down and report master error. Then abort RegionServer and set abortRequested to ture.
 # waitOnAllRegionsToClose hanged because the online regions cannot be empty.

 

waitOnAllRegionsToClose(final boolean abort) already consider the abort case but the problem is abortRequested is false when call this method. I thought the fix should be that keep to check the abortRequested in waitOnAllRegionsToClose method internal.

 

