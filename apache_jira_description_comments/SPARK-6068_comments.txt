This kind of failure seems very unlikely in practice.  For the test suite, it would be reasonable to use a few fixed random seeds as in some other test suites: [https://github.com/apache/spark/blob/master/mllib/src/test/scala/org/apache/spark/mllib/clustering/GaussianMixtureSuite.scala#L40]

If this is really an issue in practice, then (1) seems best.  However, I would vote against it as not worth it for realistic settings, especially since kmeans Parallel is only used for initialization.

Unit tests should never fail, so something must be done.

Changing the test to provide a magic seed value ties the test to the implementation.

Fixing the implementation is trivial.

Sent from my iPhone



Yes, unit tests should not be flaky.

True, fixed seeds are a bit of a hack but have worked pretty well so far.

That would be great if you fixed the implementation to prevent low-likelihood failures.

I've tried providing pull requests to Spark without success. However, you can find my fixes here: 

https://github.com/derrickburns/generalized-kmeans-clustering

Sent from my iPhone



Has the test failed or is this theoretical?
Fixing the implementation to guarantee this contract is ideal, if there's no real downside. 

Something that fails once in a blue, blue moon due to random state isn't inherently a problem, so I would not delete the test over it, no. The alternative is usually to always test the same set of random states, with a fixed seed (where that is even possible), which isn't great either. Regular failure makes it an unuseful test though. Hopefully a moot point.

Derrick what PR are you having trouble with -- the big-bang multi-JIRA PR that's been going on for ages? targeted bite-size fixes to existing code here are much easier to get in. I hope you'll offer some changes for some (others) of the many JIRAs you've opened here. A lot look useful.

Not theoretical. The unit test failed for me in my project (branch of Spark 1.1 mllib). I spent 20 minutes stepping through the code before I realized that there was no (new) problem with my implementation of KMeansParallel but instead was a pre-existing problem. I confirmed via visual inspection that the problem still exists in a recent branch of the Spark.  

In this case the fix is simple: during kmeans parallel maintain a count of the number of centers actually added for each run. Terminate the iterations of the loop that add 2k points when both the step limit has been reached AND the minimum number of k points have been made centers. 

With this change, the high probability case will be  unaffected (I.e. no extra iterations) while the low probability case that gets hit in the unit test will be covered as well.

I've abandoned the "big bang multi-JIRA PR" that I submitted. It was too much at one time, as you point out. However, that is what the Spark clusterer requires in order to offer the flexibility that I demonstrate in my branch....  I put a lot of time into the re-architecture including many many hours of large scale testing. Perhaps others can benefit from that by forking or using an upcoming release of my branch. 

Sent from my iPhone



Is this something for which a PR can easily be created then? it sounds like you're saying you have fixed it in your copy and that bit still resembles the original code here. Or if you'll point me at it I can try to extract the change.

On a broader question, IMHO:

I think you are ultimately creating a fairly different implementation in order to get in your improvements, and it's quite hard to propose a radical change to the implementation here. Especially if it changes the API or behaviors people are using. It's a shame that any library can only reasonably contain one implementation of a thing, but of course, nobody said MLlib is supposed to have everything or every bell and whistle, and we can and should be able to drop in other implementations in a Spark program as we like.

Overhauls are possible at inflection points in a project lifecycle, and there is of course the 'pipelines' API rewrite going on now, note. I don't have any view on how realistic it is to drop in your work as the new implementation there.

Failing that, I wonder if a lot of the improvements you've suggested here require a substantial rewrite, and won't realistically happen on the current impl? For those, I might suggested withdrawing the existing PRs / JIRAs and instead leave one placeholder JIRA summarizing the key features you'd like to see in a future rewrite, and track it as a feature request. It's up to your judgment but I suggest it since you say you abandoned the open PR.

It would still be good to get in any smaller clear-win changes that can be 'back-ported'.

Thanks Sean for your thoughtful comments!

The main requirement that drove my initial effort was to generalize the distance function used to include the provably largest class of distance functions for which the core algorithm works. This is the class of Bregman Divergences. 

Unfortunately, the current Spark implementation uses knowledge of the specific distance function in many places. Reversing that would result in code that is more general, just as efficient, much easier to read, and easier to prove correct. Alas, it would also touch many lines of code.

The other changes that I have made can be easily layered on the base change. They are largely independent. One could make those changes to either code base, (as one such change was recently implemented). 

However, I do not want to invest in supporting a code base that lacks my driving feature need.  Despite that, I report the issues that I find and fix that are shared in both implementations so that others may at least be aware of them. 

I think that my alternative implementation demonstrates that one can introduce my desired features with minimal impact to the user visible API, so this is not an API/backward compatibility issue like the new pipelines architecture. 

I'm happy to maintain a separate implementation and make it publicly available, particularly since my application requires a different distance function. Next week, I plan to release a version, if I can figure out how to do that easily. :)


Sent from my iPhone



Yes, it seems like too much change to the existing version. From https://github.com/apache/spark/pull/2634 it seems like there are just some differences of opinion about what's worth doing and how. I think the only way forward would be to propose integration what you've done for the new version in the {{.ml}} package, because it's not clear the existing PR isn't going to proceed.

I'm hoping to just drive a resolution to what is almost one big issue rather than leave it hanging. I'm looking at the ~8 JIRAs for k-means you created:
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20reporter%20%3D%20%22Derrick%20Burns%22%20AND%20resolution%20%3D%20Unresolved

I assume a couple (like this one) are 'back-portable' from your work to the existing impl. Can we zap those and close them with a PR? This would be great and I'd like to help get those quick wins in.

The rest sound like interdependent aspects of one proposal: create a new k-means implementation with different design and properties X / Y / Z, and use it in the new pipelines API. (I can't say whether this would be accepted or not but that's what's on the table). I'd rather coherently collect that rather than have it live in pieces in JIRA, esp. since I'm getting the sense these remaining pieces won't otherwise move forward.

[~derrickburns]  I'm sorry about how it can take a long time to get a PR into Spark, but sending small PRs with one PR per JIRA helps a lot.  For a reviewer to say "LGTM," they need to fully understand and be prepared to "own" the code, which makes reviewing large patches *much* harder.  I've spent a lot of time breaking my patches into smaller pieces.

Looking over your JIRAs, the changes all sound useful.  It also seems like the most important change for you (supporting general Bregman divergences) could potentially be added in spark.ml or spark.mllib without making breaking changes.  Since there is no distance metric parameter currently, adding one based on a Bregman divergence API should be possible.  However, but it's pretty hard to figure out exactly what changes are needed because of the many issues being addressed in your big k-means PR.  A smaller PR would help a lot.

I hope it will prove worthwhile for you to help get these improvements into MLlib, piece by piece.  I don't think they will all require waiting for the spark.ml API, but if you do want to make major API changes, then this would be time to design the new API for the spark.ml package.
* [SPARK-6001] might require an API change since it would return a model which could not be serialized.  Perhaps it could follow a similar pattern as LDA, which returns a DistributedLDAModel (with info about the training dataset topic distributions), which in turn can be converted into a LocalLDAModel (which stores model parameters locally and drops the training dataset info).

