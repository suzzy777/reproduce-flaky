Reported failure of magic committer block uploads as pending upload ID is unknown. Likely cause: it's been aborted by another job

# Make it possible to turn off cleanup of pending uploads in magic committer
# log more about uploads being deleted in committers
# and upload ID in the S3aBlockOutputStream errors

There are other concurrency issues when you look close, see SPARK-33230

* magic committer uses app attempt ID as path under __magic; if there are duplicate then they will conflict
* staging committer local temp dir uses app attempt id

Fix will be to have a job UUID which for spark will be picked up from the SPARK-33230 changes, (option to self-generate in job setup for hadoop 3.3.1+ older spark builds); fall back to app-attempt *unless that fallback has been disabled*

MR: configure to use app attempt ID
Spark: configure to fail job setup if app attempt ID is the source of a job uuid