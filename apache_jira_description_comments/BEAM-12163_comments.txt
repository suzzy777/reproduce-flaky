I couldn't find anything suspicious in the commits before the first failure.

There was a grpcio release (1.37.0) ~coincident with the first failure, but I checked on it and this was not used in the first failing run.

I diffed the dependencies between the first failure and last success (https://github.com/apache/beam/runs/2274621182?check_suite_focus=true) and found:
{code}
‚ùØ diff good bad
12c12
< docker==4.4.4
---
> docker==5.0.0
57c57
< SQLAlchemy==1.4.5
---
> SQLAlchemy==1.4.6
{code}

SQLAlchemy is probably innocuous, but I bet docker 5.0.0 release could cause this. I'll try setting docker <5

Hm that's looking like a less likely culprit.. we don't actually rely on docker in the python SDK, that dependency is coming in through testcontainers

I don't think docker is it. Here's a failure with FutureTimeoutError and docker 4.4.4: https://github.com/apache/beam/runs/2274114120?check_suite_focus=true

Earliest failure like this I can find on the master branch is here: https://github.com/apache/beam/runs/2254979254?check_suite_focus=true

These failures only seem to occur on Ubuntu, perhaps related to ubuntu-latest going to 20.04 like BEAM-12104? That happened on March 18 though.

Pinning to ubuntu-18.04 just made every suite fail: https://github.com/apache/beam/runs/2337218340?check_suite_focus=true

The only other potential culprit I found was an update to httplib2, not sure how that could cause this though.

Reverting httplib2 didn't help either: https://github.com/apache/beam/actions/runs/749360732

Currently running Python tests on 2.28.0 release branch: https://github.com/apache/beam/actions/runs/753552301

This should rule out/confirm if the flakiness is due to a change in the repo, or some external change. 

https://github.com/apache/beam/runs/2357140594?check_suite_focus=true run on the 2.28.0 release branch still failed with FutureTimeoutErrors. This rules out any change in the Beam repo.

Could be due to something about the GHA executors, or a dependency update

Saw a similar error on Jenkins just now: https://ci-beam.apache.org/job/beam_PreCommit_Python_Commit/18301/

Still an issue? [~yichi] - could you take a look at this?

Yeah I think this is still happening, e.g. https://github.com/apache/beam/actions/runs/817666764

This issue is assigned but has not received an update in 30 days so it has been labeled "stale-assigned". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.

This issue was marked "stale-assigned" and has not received a public comment in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.

Hi [~bhulette], how did you reproduced this error? Which other dependencies should be tried?

Hi [~Mike Hernandez] I was never able to reproduce the error locally, I only ever saw it on GitHub Actions checks (it might happen on Jenkins checks too, but more rarely?). I was reproducing the error by opening a PR with a proposed fix and seeing if any of the "Python tests" runs failed with this error.

I don't have any other ideas on what dependencies should be tried. I'm less convinced that this is a dependency issue now.

I couldn't reproduce the error by opening PRs. I also checked through the PRs that were opened in June that are still open and couldn't find any of them that has this error. Is there another way to check if the error is still happening?

You could search for failures on the master branch with the query [workflow:"Python Tests" branch:master is:failure|https://github.com/apache/beam/actions?query=workflow%3A%22Python+Tests%22+branch%3Amaster+is%3Afailure]. These are all very likely flakes.

I didn't look through them to see if any are due to this exact error though.

The same error can be reproduced locally although it fails in another classes of portable_runner_test.py. Here are the [results.|https://docs.google.com/document/d/1Pqd0-vuYHSjLr6yQvfGwiK3NcYypT-WrHfjCbP-Xob4/edit?usp=sharing]

Hi [~altay], has anybody else taken a look into this or a similar issue that can give some pointers?

[~heejong], looks like you worked on a grpc issue previously. Do you have any pointers?

[~lcwik] Can this be related to BEAM-10158?

The earliest test detected failure was Apr 2 which is almost a year after Jun 1st when BEAM-10158 was merged. Other then BEAM-10158 being about a thread pool, [~kileys] is there anything else that pointed you to it?
If not I would look for the issue elsewhere.

Mainly looked for the last changes to grpc. Do you have any pointers for [~Mike Hernandez] to debug this issue?

I can only suggest checking the logs and seeing if it is an ordering issue in the test where the server isn't yet ready and because the test is doing a lot or many are running concurrently that the timeout triggers when connecting.

We could test with a much larger timeout to see if that's the case.

This issue is assigned but has not received an update in 30 days so it has been labeled "stale-assigned". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.

This issue was marked "stale-assigned" and has not received a public comment in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.

This Jira ticket has a pull request attached to it, but is still open. Did the pull request resolve the issue? If so, could you please mark it resolved? This will help the project have a clear view of its open issues.

The issue is still unresolved, all the PRs were tests that are already closed.

This issue has been migrated to https://github.com/apache/beam/issues/20974

