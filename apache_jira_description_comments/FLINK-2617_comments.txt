(I think that the subject is in fact critical because it happens really often in my cluster as soon as parallelism is > 250)

I agree, this is critical.

The reason for this exception is that the Hadoop code assumes only one thread is ever working with the configuration. That is the MapReduce model where each task is in a dedicated JVM.
In Flink, multiple tasks can run in one JVM, which goes a bit against Hadoop's assumptions.

We need to lock the {{open()}} method of all MapReduce compatibility functions (on a static object) in order to prevent this.

A short term mitigation may be to change your setup to run more TaskManagers, each with one slot - that way, you have one task per JVM as well.

Yes, that could be a fix.
But I think it would be good to know, why that happens because it might bite us at other places as well. Maybe we need to guard all calls to Hadoop classes by locks.
I looked into the code and it is not obvious. All HadoopInputFormats (Flink's wrapper) are configured with different Configuration objects. 

Which version of HCatalog are you using?
(I'm asking because my argument list of the  HCatInputFormat.setInput() differs from yours)

hive-hcatalog-core-0.14.0.jar

GitHub user fhueske opened a pull request:

    https://github.com/apache/flink/pull/1111

    [FLINK-2617] Added static mutexes for configure, open, close HadoopIOFormats

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fhueske/flink hadoopConcurrent

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/1111.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1111
    
----
commit ff8fe027de2cac3c28fe5d81bd5351af3fcefb28
Author: Fabian Hueske <fhueske@apache.org>
Date:   2015-09-09T12:32:21Z

    [FLINK-2617] Added static mutexes for configure, open, close HadoopFormats

----


Hi [~ArnaudL], I opened a PR against the 0.10 SNAPSHOT master that synchronizes open() calls in Flink's {{HadoopInputFormat}}.
Can you check if that solves the problem with the HCatInputFormat? I can also port the fix to 0.9 if necessary.

Btw. I did not find the reason for the exception, but I verified that Flink is separate config objects in its code. So I assume, the root cause is somewhere hidden in the HCat code.

Github user rmetzger commented on the pull request:

    https://github.com/apache/flink/pull/1111#issuecomment-138928137
  
    +1 Looks good to merge


Github user fhueske commented on the pull request:

    https://github.com/apache/flink/pull/1111#issuecomment-139189798
  
    will merge this


Github user asfgit closed the pull request at:

    https://github.com/apache/flink/pull/1111


Fixed for 0.10 with 8754352ff53cd1ab621d6c97f7e5baac369b5c28
Fixed for 0.9 with d656fc33a28dbcc286723b1f7413e297b1407add

Hi, today (11-09)  I've tested the 0.10 nightly build with success, thank you very much.
However, I did not find the patch in the 0.9 snapshot ; is it normal?

Hi Arnaud,

Thanks for verifying the fix! Where did you look for the 0.9 patch? 
We are pushing fixes for minor version 0.9 to the [release-0.9|https://github.com/apache/flink/tree/release-0.9] branch and the patch for this issue is in.

Well, I've searched apache snapshot repository for 0.9-SNAPSHOT and I've looked at http://stratosphere-bin.s3.amazonaws.com/flink-0.9-SNAPSHOT-bin-hadoop2.tgz. I was trying to avoid building it from the sources until I can get the time to configure my env.

I see. 
SNAPSHOT builds are only uploaded if all tests pass. The last build failed due to some flaky tests. 
I will push an empty commit to trigger another build. *fingersCrossed* that this build passes.

Hi [~ArnaudL], a new 0.9-SNAPSHOT build was just uploaded to S3.

