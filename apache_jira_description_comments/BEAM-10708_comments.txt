[~emukang], [~rohdesam] could one of you take a look at this? Is it possible to get a copy of the pipeline without writing to proto using {{copy.deepcopy}} or similar?

We could also look into making it so you can read the proto back when it includes cross-language transforms. It seems reasonable to expect python to re-create the original ExternalTransform instance in this case

CC: [~robertwb]

This issue is P2 but has been unassigned without any comment for 60 days so it has been labeled "stale-P2". If this issue is still affecting you, we care! Please comment and remove the label. Otherwise, in 14 days the issue will be moved to P3.

Please see https://beam.apache.org/contribute/jira-priorities/ for a detailed explanation of what these priorities mean.


This issue was marked "stale-P2" and has not received a public comment in 14 days. It is now automatically moved to P3. If you are still affected by it, you can comment and move it back to P2.

As of today (2021-01-12), when a pipeline including a SqlTransform is executed with InteractiveRunner() or invoked in `from_runner_api(to_runner_api())`, an example failure stack trace can be found as following:

{code:python}
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-9-ae3e3b2a5b98> in <module>
----> 1 ib.show(pcoll)

~/beam/sdks/python/apache_beam/runners/interactive/utils.py in run_within_progress_indicator(*args, **kwargs)
    226   def run_within_progress_indicator(*args, **kwargs):
    227     with ProgressIndicator('Processing...', 'Done.'):
--> 228       return func(*args, **kwargs)
    229 
    230   return run_within_progress_indicator

~/beam/sdks/python/apache_beam/runners/interactive/interactive_beam.py in show(*pcolls, **configs)
    484   recording_manager = ie.current_env().get_recording_manager(
    485       user_pipeline, create_if_absent=True)
--> 486   recording = recording_manager.record(pcolls, max_n=n, max_duration=duration)
    487 
    488   # Catch a KeyboardInterrupt to gracefully cancel the recording and

~/beam/sdks/python/apache_beam/runners/interactive/recording_manager.py in record(self, pcolls, max_n, max_duration)
    420     # arbitrary variables.
    421     self._watch(pcolls)
--> 422     pipeline_instrument = pi.PipelineInstrument(self.user_pipeline)
    423     self.record_pipeline()
    424 

~/beam/sdks/python/apache_beam/runners/interactive/pipeline_instrument.py in __init__(self, pipeline, options)
    113     # proto is stable. The snapshot of pipeline will not be mutated within this
    114     # module and can be used to recover original pipeline if needed.
--> 115     self._pipeline_snap = beam.pipeline.Pipeline.from_runner_api(
    116         pipeline.to_runner_api(use_fake_coders=True), pipeline.runner, options)
    117     ie.current_env().add_derived_pipeline(self._pipeline, self._pipeline_snap)

~/beam/sdks/python/apache_beam/pipeline.py in from_runner_api(proto, runner, options, return_context)
    900     if proto.root_transform_ids:
    901       root_transform_id, = proto.root_transform_ids
--> 902       p.transforms_stack = [context.transforms.get_by_id(root_transform_id)]
    903     else:
    904       p.transforms_stack = [AppliedPTransform(None, None, '', None)]

~/beam/sdks/python/apache_beam/runners/pipeline_context.py in get_by_id(self, id)
    113     # type: (str) -> PortableObjectT
    114     if id not in self._id_to_obj:
--> 115       self._id_to_obj[id] = self._obj_type.from_runner_api(
    116           self._id_to_proto[id], self._pipeline_context)
    117     return self._id_to_obj[id]

~/beam/sdks/python/apache_beam/pipeline.py in from_runner_api(proto, context)
   1250     result.parts = []
   1251     for transform_id in proto.subtransforms:
-> 1252       part = context.transforms.get_by_id(transform_id)
   1253       part.parent = result
   1254       result.parts.append(part)

~/beam/sdks/python/apache_beam/runners/pipeline_context.py in get_by_id(self, id)
    113     # type: (str) -> PortableObjectT
    114     if id not in self._id_to_obj:
--> 115       self._id_to_obj[id] = self._obj_type.from_runner_api(
    116           self._id_to_proto[id], self._pipeline_context)
    117     return self._id_to_obj[id]

~/beam/sdks/python/apache_beam/pipeline.py in from_runner_api(proto, context)
   1250     result.parts = []
   1251     for transform_id in proto.subtransforms:
-> 1252       part = context.transforms.get_by_id(transform_id)
   1253       part.parent = result
   1254       result.parts.append(part)

~/beam/sdks/python/apache_beam/runners/pipeline_context.py in get_by_id(self, id)
    113     # type: (str) -> PortableObjectT
    114     if id not in self._id_to_obj:
--> 115       self._id_to_obj[id] = self._obj_type.from_runner_api(
    116           self._id_to_proto[id], self._pipeline_context)
    117     return self._id_to_obj[id]

~/beam/sdks/python/apache_beam/pipeline.py in from_runner_api(proto, context)
   1250     result.parts = []
   1251     for transform_id in proto.subtransforms:
-> 1252       part = context.transforms.get_by_id(transform_id)
   1253       part.parent = result
   1254       result.parts.append(part)

~/beam/sdks/python/apache_beam/runners/pipeline_context.py in get_by_id(self, id)
    113     # type: (str) -> PortableObjectT
    114     if id not in self._id_to_obj:
--> 115       self._id_to_obj[id] = self._obj_type.from_runner_api(
    116           self._id_to_proto[id], self._pipeline_context)
    117     return self._id_to_obj[id]

~/beam/sdks/python/apache_beam/pipeline.py in from_runner_api(proto, context)
   1250     result.parts = []
   1251     for transform_id in proto.subtransforms:
-> 1252       part = context.transforms.get_by_id(transform_id)
   1253       part.parent = result
   1254       result.parts.append(part)

~/beam/sdks/python/apache_beam/runners/pipeline_context.py in get_by_id(self, id)
    113     # type: (str) -> PortableObjectT
    114     if id not in self._id_to_obj:
--> 115       self._id_to_obj[id] = self._obj_type.from_runner_api(
    116           self._id_to_proto[id], self._pipeline_context)
    117     return self._id_to_obj[id]

~/beam/sdks/python/apache_beam/pipeline.py in from_runner_api(proto, context)
   1250     result.parts = []
   1251     for transform_id in proto.subtransforms:
-> 1252       part = context.transforms.get_by_id(transform_id)
   1253       part.parent = result
   1254       result.parts.append(part)

~/beam/sdks/python/apache_beam/runners/pipeline_context.py in get_by_id(self, id)
    113     # type: (str) -> PortableObjectT
    114     if id not in self._id_to_obj:
--> 115       self._id_to_obj[id] = self._obj_type.from_runner_api(
    116           self._id_to_proto[id], self._pipeline_context)
    117     return self._id_to_obj[id]

~/beam/sdks/python/apache_beam/pipeline.py in from_runner_api(proto, context)
   1227     ]
   1228 
-> 1229     transform = ptransform.PTransform.from_runner_api(proto, context)
   1230     # Ordering is important here.
   1231     # TODO(BEAM-9635): use key, value pairs instead of depending on tags with

~/beam/sdks/python/apache_beam/transforms/ptransform.py in from_runner_api(cls, proto, context)
    728     parameter_type, constructor = cls._known_urns[proto.spec.urn]
    729 
--> 730     return constructor(
    731         proto,
    732         proto_utils.parse_Bytes(proto.spec.payload, parameter_type),

~/beam/sdks/python/apache_beam/transforms/core.py in from_runner_api_parameter(unused_ptransform, pardo_payload, context)
   1417   def from_runner_api_parameter(unused_ptransform, pardo_payload, context):
   1418     fn, args, kwargs, si_tags_and_types, windowing = pickler.loads(
-> 1419         DoFnInfo.from_runner_api(
   1420             pardo_payload.do_fn, context).serialized_dofn_data())
   1421     if si_tags_and_types:

~/beam/sdks/python/apache_beam/transforms/core.py in from_runner_api(spec, unused_context)
   1491       return StatelessDoFnInfo(spec.urn)
   1492     else:
-> 1493       raise ValueError('Unexpected DoFn type: %s' % spec.urn)
   1494 
   1495   @staticmethod

ValueError: Unexpected DoFn type: beam:dofn:javasdk:0.1
{code}


You cannot run 'from_runner_api()' on runner API proto definitions that contain external transforms since you won't be able to create a Python object representation for some external constructs (for example, Java DoFns). I got rid of this for regular pipelines (for Dataflow) but possibly somehow you are still running into this for interactive runner.

Yes the interactive runner has a separate runner API roundtrip to create a copy of the pipeline: https://github.com/apache/beam/blob/dce1eb83b8d5137c56ac58568820c24bd8fda526/sdks/python/apache_beam/runners/interactive/pipeline_fragment.py#L120

In an offline conversation I suggested that Ning try using copy.deepcopy instead of the proto roundtrip to make a copy.



Thanks for the comments, Cham and Brian! If `from_runner_api` is not a possible solution to copy pipelines, the InteractiveRunner has to either copy the pipeline objects explicitly or make changes directly on the proto before each iteration of execution.

This issue is assigned but has not received an update in 30 days so it has been labeled "stale-assigned". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.

This issue was marked "stale-assigned" and has not received a public comment in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.

[~ningk]Â any updates on this ?

[~chamikara] Thanks for asking, the first PR#14368 that initiated a new augmented_pipeline module to replace pipeline_instrument has been merged. I'll be working on this after finishing some other higher priority tasks. 
There are some known users who depend on the old modules that uses runner api roundtrips, I'll work with them to roll out the new module. We might need to keep the old modules as deprecated for a while.

This issue is assigned but has not received an update in 30 days so it has been labeled "stale-assigned". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.

An update: I'm currently working on BEAM-12506 that could help resolve the issue.

This issue was marked "stale-assigned" and has not received a public comment in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.

Is this resolved now that https://github.com/apache/beam/pull/15368 is in?

Yes, customers could use the beam_sql magic instead of the SqlTransform directly with the InteractiveRunner.

For streaming, I'll additionally support TestStream in FnApiRunner.

Thanks. That's great to know.

Â 

Is the fix somehow SQL only or will cross-language transforms work for InteractiveRunner in general now ?

Â 

Also is there an integration test somewhere for InteractiveRunner + Python SQL transform ?

It's a workaround for SQL only. There is no integration test for this. Underlying it, it builds a normal pipeline executing SqlTransform with a DirectRunner. If that's covered, there is no need to add another integration test.

> It's a workaround for SQL only. There is no integration test for this. Underlying it, it builds a normal pipeline executing SqlTransform with a DirectRunner. If that's covered, there is no need to add another integration test.

That should be covered in XVR Direct (https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct). Although I don't think that actually verifies with the runner configured for streaming.

I think Cham's question was around the proto roundtrip that originally broke multi-language pipelines on the interactive runner. Does InteractiveRunner no longer make a roundtrip to the runner API proto, or is there some other fix specific to SQL?

*For streaming,  a summary of the current status*:

For the 2 DirectRunner implementations:
FnApiRunner
* Support SqlTransform
* Does not support TestStream
* Does not support NativeSources such as ReadFromPubSub

BundleBasedDirectRunner
* Support TestStream and ReadFromPubSub
* Does not support SqlTransform

SqlTransform is not supported by DirectRunner for streaming pipelines anyway.

I'm going to make FnApiRunner support TestStream so that
* Support SqlTransform
* Support TestStream/NativeSources such as ReadFromPubSub (locally an InteractiveRunner uses TestStream to replace all ReadFromPubSubs)
* Not to be deprecated and xLang friendly.


*The other topic*:

The workaround usage for SQL can be demonstrated as:


{code:python}
p = beam.Pipeline(ir.InteractiveRunner())

class Person(typing.NamedTuple):
    id: int
    name: str

persons = (p | beam.Create(range(10))
           | beam.Map(lambda x: Person(id=x, name=names.get_full_name())).with_output_types(Person))

persons_2 = (p | beam.Create(range(5,15))
              | beam.Map(lambda x: Person(id=x, name=names.get_full_name())).with_output_types(Person)) 

%%beam_sql persons_id_conflict  # This is the output variable name in the __main__ module.
SELECT * FROM persons JOIN persons_2 USING (id)
{code}

The output is a PCollection. Note there is no need from the user to explicitly apply a SqlTransform nor build the input PCollection dict.

Under the hood, InteractiveRunner has no change (still uses roundtrips to duplicate pipelines without external transforms).
For SQL usages, the beam_sql ipython magic takes care of implicitly building a pipeline with appropriate SqlTransform and inputs.





Thanks for the update [~ningk]. It's great to know that the InteractiveRunner now supports the SQL transform.

Is there a plan to move out of Runner API from/to round trip in general ? I think it happens to work for most transforms in Python SDK by accident and there's no guarantee that a given transform will support this in a given environment (and cross-language transforms definitely do not support this).

Â 

cc: [~robertwb]

Yes, we could deprecate the runner api roundtrip usage in Interactive Beam by moving the DirectRunner usage completely to its FnApiRunner implementation.

We can use this ticket to track the progress. I could work on this next quarter.

Once FnApiRunner supports TestStream, streaming pipelines defined with InteractiveRunner (with or without sql transforms) can be executed.

I see, so the round-trip occurs only when using the "BundleBasedDirectRunner" ? Agree that In that case moving to "FnApiRunner" should be the correct path. Also agree that we should keep this Jira open till cross-language transforms are supported in general for the InteractiveRunner.

Thanks for the update.Â 

Thanks for asking. Switching to "FnApiRunner" makes the deprecation of runner api roundtrip feasible.

Our roundtrip *does nothing but making copies of pipelines*. {color:#505F79}It's something useful (to avoid corrupting the __main__ module in the REPL env) in the interactive scenario where the user applies-transforms-then-inspect-output one by one, but not needed for scenarios where the user creates-pipelines-then-execute one by one (or any other non-interactive use cases. Deep copying pipelines is not a feature because it's not needed. So we could only use runner api roundtrip in the early days.){color}

To deprecate it, instead of making copies of pipelines, we make copies of runner api protos.
Theoretically, runner api is the SDK-and-runner-independent definition of a Beam pipeline. Every runner implementation should be able to accept them for execution.
For DirectRunner, the right approach is to use its FnApiRunner implementation that is implemented to accept a runner api through "run_via_runner_api".

I hope DataflowRunner could also support "run_via_runner_api" to truly support mixing matching SDKs and runners envisioned [here|https://docs.google.com/document/d/1XYzb1Fnt2sam7u2MsGFaZp-2qSIGxUn66VLer-bcXAk/edit#heading=h.p6lvszfbmyj6]. But to productionize a pipeline from notebooks to dataflow, we could have other workaround. Making copies of pipelines is not needed.

Dataflow supports "run_via_runner_api" via portable job submission.

(y) Thanks! That's good to know!

This issue is assigned but has not received an update in 30 days so it has been labeled "stale-assigned". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.

This issue was marked "stale-assigned" and has not received a public comment in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.

This Jira ticket has a pull request attached to it, but is still open. Did the pull request resolve the issue? If so, could you please mark it resolved? This will help the project have a clear view of its open issues.

Use beam_sql magic in notebooks to work with SqlTransform and InteractiveRunner.

I don't think we should close this. The beam_sql magic is a great workaround for a specific (important) use-case, but we should still track the underlying issue. There are many other external transforms and it would be nice for them all to work with the InteractiveRunner.

It would also be nice to fail more gracefully as an intermediate step.

Just seconding Brian's comment.Â Â 

Â 

I am a corporate developer stuck behind a firewall, with business users trying to evaluate Beam.Â  The first thing we want to try is loading up csvs and running SQL on them offline, and outputting a file (might sound pedestrian, but this is where we are coming from).Â 

Â 

Of course, you could ask why offline, but getting a project approved, and setup, and a bucket is actually a weeks long process in a bank, if you haven't done it before.Â 

Hi [~nnamnielk], yes, I completely agree with that! We still have a long way to rebuild InteractiveRunner so that it can work with external transforms. For the time being, could you please check whether the `beam_sql` magic works for your use cases? An example can be found and [here|https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/interactive/examples/Run%20Beam%20SQL%20with%20beam_sql%20magic.ipynb] and [here|https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#beam_sql_and_beam_sql_magic].

This issue has been migrated to https://github.com/apache/beam/issues/20526

