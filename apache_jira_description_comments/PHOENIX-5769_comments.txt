It's not just precommit, the post-commit jobs are just as bad, and I see the similar random failures locally as well.

Trying to reduce the number of running ITs, to test the theory that this is caused by memory/resource starvation.

I have also removed the 6GB -Xmx setting in the Jenkins project, as 7x6GB + overhead can possibly overload the build hosts (that I recon have 16GB of RAM).

This is all just poking in the dark, but we can't get it fixed without trying out solutions.

OK, so it looks like the build hosts have 48GB, but have up two executors running on them, so we can count on having ~24GB of memory available. (provided that the other job doesn't hog the memory, though that does not seem to be the case, we fail as much when we are the only job, as we do when we are sharing the host)

I have added lscpu to the startup script, because I couldn't figure out how many and how fast cores we can work with.

My approach of editing the test script to test fixes was bogus, as the tests scripts always run from master. 


So I started changing config in the Jenkins precommit job directly. It is not ideal, but it's not like I can make it worse than it is now.

OK, so the build machines have two Xeon 5620 processors from 2010, with four hyperthreaded cores each. 

According to some benchmarks I dug up these cores have roughly half of the single-core performance of a contemporary core.

In light of this, increasing test parallelism is unlikely to help much (especially considering that we are not supposed to completely nuke the build hosts), and increasing the timeout seems necessary. I have bumped the timeout limit from 300 to 400 minutes. 

I found the multiple regions issue as well on some of the builds and filed a Jira for that : [https://issues.apache.org/jira/projects/PHOENIX/issues/PHOENIX-5779]

Commented on the Jira my initial investigation as well.  [~dbwong] [~stoty] [~ckulkarni]

So I took a look at the patch but I didn't see the corresponding timeout is that in the jenkins configuration I don't have access to I assume?

Some more observations based on [https://builds.apache.org/job/PreCommit-PHOENIX-Build/3691]:
 * The jenkins build hosts are about thrice as slow running the test suite as a modern machine (ryzen 2700x + NVME SSD). Running a single query takes ~13s vs 4.5s.
 * mvn verify for the phoenix-core subproject with the current settings takes about 4.5 hours, so the 5 hour default limit was really too optimistic, considering that we have other subprojects, and downloads and Yetus stuff also takes ~20 minutes until the mvn verify gets started. 
 * However, the timeouts look super strange. It's as if Jenkins has simply disconnected from maven, and didn't get any maven output after 22:20 until after 00:10, when it aborted the build. The SplitSystemCatalogTests have actually run, as Jenkins processes the output files, as can be seen later in the log. 

{noformat}

22:14:01 [INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 217.56 s - in org.apache.phoenix.tool.ParameterizedPhoenixCanaryToolIT
22:15:10 [WARNING] Tests run: 26, Failures: 0, Errors: 0, Skipped: 4, Time elapsed: 322.952 s - in org.apache.phoenix.schema.stats.NamespaceDisabledStatsCollectorIT
22:15:40 [WARNING] Tests run: 39, Failures: 0, Errors: 0, Skipped: 6, Time elapsed: 328.538 s - in org.apache.phoenix.schema.stats.NonTxStatsCollectorIT
22:15:51 [WARNING] Tests run: 26, Failures: 0, Errors: 0, Skipped: 4, Time elapsed: 363.015 s - in org.apache.phoenix.schema.stats.NamespaceEnabledStatsCollectorIT
22:20:05 [WARNING] Tests run: 78, Failures: 0, Errors: 0, Skipped: 12, Time elapsed: 530.679 s - in org.apache.phoenix.schema.stats.TxStatsCollectorIT
00:10:34 Build timed out (after 400 minutes). Marking the build as aborted.
00:10:35 Build was aborted
00:10:35 Archiving artifacts
00:10:38 [INFO] 
00:10:38 [INFO] Results:
00:10:38 [INFO] 
00:10:38 [WARNING] Tests run: 1079, Failures: 0, Errors: 0, Skipped: 65
00:10:38 [INFO] 
00:10:38 [INFO] 
00:10:38 [INFO] --- maven-failsafe-plugin:2.22.0:integration-test (SplitSystemCatalogTests) @ phoenix-core ---
00:10:38 [INFO] 
00:10:38 [INFO] -------------------------------------------------------
00:10:38 [INFO]  T E S T S
00:10:38 [INFO] -------------------------------------------------------

{noformat}

* We also seem to have some interference between miniClusters started by different tests, but failsafe helpfully scrubs that information that may help us track that down. I've opened PHOENIX-5814 for that problem


 

Looking at https://builds.apache.org/job/PreCommit-PHOENIX-Build/3690/artifact/phoenix-core/target/failsafe-reports/
It is apparent that some output files are not closed until the test is aborted.

{noformat}
org.apache.phoenix.end2end.MigrateSystemTablesToSystemNamespaceIT.txt	31-Mar-2020 18:35:53	381 B	 view
org.apache.phoenix.end2end.MigrateSystemTablesToSystemNamespaceIT-output.txt	31-Mar-2020 21:40:36	13.81 MB
{noformat}

Bafflingly, these tests run successfully, and I could not found any indication of problem in the output files either.
I suspect that these tests leak some miniClusters/threads, as most of them diverge from the standard cluser setup/teardown procedure.

@dbwong . Yes it is. 

Actually, those settings seem to be somewhat of a red herring, as the VM options set there only apply to the main maven process, and everything interesting is happening in the separate VMs started by failsafe, which have their own VM settings.

It is also overridden by setting the property from the Jenkins script. (though I have commented that out in Jenkins now)

The patches that I add here are mostly for testing theories about why the tests fail, and are not confirmed to be fixes for now.

I could reproduce the hang in the test suite locally.

Whether it is a problem with the hanging test, or we simply run out of some resource at this point needs further investigation:

{noformat}
"main" #1 prio=5 os_prio=0 tid=0x00007f6ad004b000 nid=0x55aef waiting on condition [0x00007f6ad66c9000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.waitProcedureResult(HBaseAdmin.java:3557)
        at org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.get(HBaseAdmin.java:3487)
        at org.apache.hadoop.hbase.client.HBaseAdmin.get(HBaseAdmin.java:2197)
        at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:642)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated(ConnectionQueryServicesImpl.java:1339)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:1801)
        at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:2960)
        at org.apache.phoenix.schema.MetaDataClient.createTable(MetaDataClient.java:1094)
        at org.apache.phoenix.compile.CreateTableCompiler$CreateTableMutationPlan.execute(CreateTableCompiler.java:383)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:415)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:397)
        at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:396)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:384)
        at org.apache.phoenix.jdbc.PhoenixStatement.execute(PhoenixStatement.java:1886)
        at org.apache.phoenix.end2end.IndexToolTimeRangeIT.createTableAndIndex(IndexToolTimeRangeIT.java:72)
        at org.apache.phoenix.end2end.IndexToolTimeRangeIT.setup(IndexToolTimeRangeIT.java:60)
        - locked <0x0000000083006708> (a java.lang.Class for org.apache.phoenix.end2end.IndexToolTimeRangeIT)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
        at org.junit.rules.RunRules.evaluate(RunRules.java:20)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.junit.runners.Suite.runChild(Suite.java:128)
        at org.junit.runners.Suite.runChild(Suite.java:27)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
        at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:383)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:344)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125){noformat}


Opened PHOENIX-5816 for the test hang problem.

{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12998546/PHOENIX-5769.master.v4.patch
  against master branch at commit 54f2c93176ce57a077769a6f9740345c1da3ac45.
  ATTACHMENT ID: 12998546

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 0 new or modified tests.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-PHOENIX-Build/3703//testReport/
Console output: https://builds.apache.org/job/PreCommit-PHOENIX-Build/3703//console

This message is automatically generated.

The hanging test issue should be fixed.

OK, so now we are back to having flaky tests that sometimes fail, instead of having tests that always fail. 
This is in fact an improvement.

I have added console timestamps, coloring, and host info printouts to the _4.x _and _master_ post-commit jobs as well.

A bit off-topic, but I can see the post-commit matrix jobs getting killed in the last stretch of the tests after 300 minutes.
Bumping the timeout by an hour, to 360 minutes.
Setting the same 360 minutes on the precommit jobs.

The infra issues are mostly fixed, but the flaky tests remain.
I don't think I'll have the bandwidth to work on this in the next few weeks.

Thanks for all your work [~stoty].  I'll see if i can spend any time next week and I know [~neha.gupta] may be looking at these as well.

