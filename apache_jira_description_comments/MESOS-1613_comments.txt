From the log it's actually failing because it couldn't find the healthy_state set when the task is killed, which is added in 169ec89.
Both Apache Jenkins and on my Ubuntu they both passes, so I'm not really sure how this is failing as it should get picked up.

How does the Twitter CI run? Does it do a make clean and re clone before running it again?

FWICT, the scheduler only received 4 updates. It didn't receive the 4th TASK_RUNNING update and 5th TASK_KILLED updates. I don't think it has to do with the healthy_state being set or not. This is indeed surprising.

I'll disable the test for now on HEAD. But I'll help you setup a build on Jenkins that runs this test repeatedly so we can track this down with more debugging. 

Disabled on HEAD.

Updated "mesos-tnachen" Jenkins build (which watches the "health_check" branch of yours) as follows:

GLOG_v=1 MESOS_VERBOSE=1 make distcheck GTEST_SHUFFLE=1 GTEST_FILTER="*ConsecutiveFailures*" GTEST_REPEAT=100 GTEST_BREAK_ON_FAILURE=1

Lets hope we can repro. Also, feel free to add debugging info in your branch and push upstream to your repo.

Hmm at least that's what it shows in the log in the ticket, just above the expected call exception there is a expect call that is checking for the task kill message healthy field?

[~vinod@twitter.com] Vinod I have a reviewboard out already, I saw you commented earlier but can you try it or if it looks good commit it?

Thanks!

Can you change the ticket's status to "Reviewable" and also paste the review link in the comment?

commit 656b0e075c79e03cf6937bbe7302424768729aa2
Author: Timothy Chen <tnachen@apache.org>
Date:   Wed Aug 6 11:34:03 2014 -0700

    Re-enabled HealthCheckTest.ConsecutiveFailures test.
    
    The test originally was flaky because the time to process the number
    of consecutive checks configured exceeds the task itself, so the task
    finished but the number of expected task health check didn't match.
    
    Review: https://reviews.apache.org/r/23772


Looks like it's still flaky:

{noformat}
Changes

Summary

Made ephemeral ports a resource and killed private resources. (details)
Do not send ephemeral_ports resource to frameworks. (details)
Create static mesos library. (details)
Re-enabled HealthCheckTest.ConsecutiveFailures test. (details)
Merge resourcesRecovered and resourcesUnused. (details)
Added executor metrics for slave. (details)

[ RUN      ] HealthCheckTest.ConsecutiveFailures
Using temporary directory '/tmp/HealthCheckTest_ConsecutiveFailures_fBrAEu'
I0806 15:06:59.268267  9210 leveldb.cpp:176] Opened db in 29.926087ms
I0806 15:06:59.275971  9210 leveldb.cpp:183] Compacted db in 7.40006ms
I0806 15:06:59.276254  9210 leveldb.cpp:198] Created db iterator in 7678ns
I0806 15:06:59.276741  9210 leveldb.cpp:204] Seeked to beginning of db in 2076ns
I0806 15:06:59.277034  9210 leveldb.cpp:273] Iterated through 0 keys in the db in 1908ns
I0806 15:06:59.277307  9210 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0806 15:06:59.277868  9233 recover.cpp:425] Starting replica recovery
I0806 15:06:59.277946  9233 recover.cpp:451] Replica is in EMPTY status
I0806 15:06:59.278240  9233 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0806 15:06:59.278296  9233 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0806 15:06:59.278391  9233 recover.cpp:542] Updating replica status to STARTING
I0806 15:06:59.282282  9234 master.cpp:287] Master 20140806-150659-16842879-60888-9210 (lucid) started on 127.0.1.1:60888
I0806 15:06:59.282316  9234 master.cpp:324] Master only allowing authenticated frameworks to register
I0806 15:06:59.282322  9234 master.cpp:329] Master only allowing authenticated slaves to register
I0806 15:06:59.282330  9234 credentials.hpp:36] Loading credentials for authentication from '/tmp/HealthCheckTest_ConsecutiveFailures_fBrAEu/credentials'
I0806 15:06:59.282508  9234 master.cpp:358] Authorization enabled
I0806 15:06:59.283121  9234 hierarchical_allocator_process.hpp:296] Initializing hierarchical allocator process with master : master@127.0.1.1:60888
I0806 15:06:59.283174  9234 master.cpp:121] No whitelist given. Advertising offers for all slaves
I0806 15:06:59.283413  9234 master.cpp:1127] The newly elected leader is master@127.0.1.1:60888 with id 20140806-150659-16842879-60888-9210
I0806 15:06:59.283429  9234 master.cpp:1140] Elected as the leading master!
I0806 15:06:59.283435  9234 master.cpp:958] Recovering from registrar
I0806 15:06:59.283491  9234 registrar.cpp:313] Recovering registrar
I0806 15:06:59.284046  9233 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.600113ms
I0806 15:06:59.284080  9233 replica.cpp:320] Persisted replica status to STARTING
I0806 15:06:59.284226  9233 recover.cpp:451] Replica is in STARTING status
I0806 15:06:59.284580  9233 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0806 15:06:59.284643  9233 recover.cpp:188] Received a recover response from a replica in STARTING status
I0806 15:06:59.284747  9233 recover.cpp:542] Updating replica status to VOTING
I0806 15:06:59.289934  9233 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.119357ms
I0806 15:06:59.290256  9233 replica.cpp:320] Persisted replica status to VOTING
I0806 15:06:59.290876  9237 recover.cpp:556] Successfully joined the Paxos group
I0806 15:06:59.291131  9232 recover.cpp:440] Recover process terminated
I0806 15:06:59.300732  9236 log.cpp:656] Attempting to start the writer
I0806 15:06:59.301061  9236 replica.cpp:474] Replica received implicit promise request with proposal 1
I0806 15:06:59.306172  9236 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.070193ms
I0806 15:06:59.306229  9236 replica.cpp:342] Persisted promised to 1
I0806 15:06:59.306747  9236 coordinator.cpp:230] Coordinator attemping to fill missing position
I0806 15:06:59.307143  9236 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0806 15:06:59.309715  9236 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 2.521311ms
I0806 15:06:59.310199  9236 replica.cpp:676] Persisted action at 0
I0806 15:06:59.320276  9234 replica.cpp:508] Replica received write request for position 0
I0806 15:06:59.320335  9234 leveldb.cpp:438] Reading position from leveldb took 26656ns
I0806 15:06:59.325726  9234 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.358479ms
I0806 15:06:59.325781  9234 replica.cpp:676] Persisted action at 0
I0806 15:06:59.325999  9234 replica.cpp:655] Replica received learned notice for position 0
I0806 15:06:59.328487  9234 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 2.458504ms
I0806 15:06:59.328887  9234 replica.cpp:676] Persisted action at 0
I0806 15:06:59.329144  9234 replica.cpp:661] Replica learned NOP action at position 0
I0806 15:06:59.340193  9236 log.cpp:672] Writer started with ending position 0
I0806 15:06:59.340481  9236 leveldb.cpp:438] Reading position from leveldb took 30496ns
I0806 15:06:59.341670  9236 registrar.cpp:346] Successfully fetched the registry (0B)
I0806 15:06:59.341696  9236 registrar.cpp:422] Attempting to update the 'registry'
I0806 15:06:59.342859  9236 log.cpp:680] Attempting to append 117 bytes to the log
I0806 15:06:59.342912  9236 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0806 15:06:59.343128  9236 replica.cpp:508] Replica received write request for position 1
I0806 15:06:59.350430  9236 leveldb.cpp:343] Persisting action (134 bytes) to leveldb took 7.261692ms
I0806 15:06:59.350914  9236 replica.cpp:676] Persisted action at 1
I0806 15:06:59.360164  9234 replica.cpp:655] Replica received learned notice for position 1
I0806 15:06:59.365715  9234 leveldb.cpp:343] Persisting action (136 bytes) to leveldb took 5.492077ms
I0806 15:06:59.365774  9234 replica.cpp:676] Persisted action at 1
I0806 15:06:59.365784  9234 replica.cpp:661] Replica learned APPEND action at position 1
I0806 15:06:59.366268  9234 registrar.cpp:479] Successfully updated 'registry'
I0806 15:06:59.366322  9234 registrar.cpp:372] Successfully recovered registrar
I0806 15:06:59.366368  9234 log.cpp:699] Attempting to truncate the log to 1
I0806 15:06:59.366452  9234 master.cpp:985] Recovered 0 slaves from the Registry (81B) ; allowing 10mins for slaves to re-register
I0806 15:06:59.366508  9234 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0806 15:06:59.367105  9232 replica.cpp:508] Replica received write request for position 2
I0806 15:06:59.369526  9232 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 2.392686ms
I0806 15:06:59.369560  9232 replica.cpp:676] Persisted action at 2
I0806 15:06:59.369807  9232 replica.cpp:655] Replica received learned notice for position 2
I0806 15:06:59.377322  9232 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 7.463724ms
I0806 15:06:59.377411  9232 leveldb.cpp:401] Deleting ~1 keys from leveldb took 35552ns
I0806 15:06:59.377426  9232 replica.cpp:676] Persisted action at 2
I0806 15:06:59.377435  9232 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0806 15:06:59.378401  9210 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0806 15:06:59.381873  9236 slave.cpp:167] Slave started on 96)@127.0.1.1:60888
I0806 15:06:59.381899  9236 credentials.hpp:84] Loading credential for authentication from '/tmp/HealthCheckTest_ConsecutiveFailures_jnWV1D/credential'
I0806 15:06:59.381995  9236 slave.cpp:265] Slave using credential for: test-principal
I0806 15:06:59.382102  9236 slave.cpp:278] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0806 15:06:59.382159  9236 slave.cpp:306] Slave hostname: lucid
I0806 15:06:59.382169  9236 slave.cpp:307] Slave checkpoint: false
I0806 15:06:59.382702  9236 state.cpp:33] Recovering state from '/tmp/HealthCheckTest_ConsecutiveFailures_jnWV1D/meta'
I0806 15:06:59.382812  9236 status_update_manager.cpp:193] Recovering status update manager
I0806 15:06:59.382863  9236 containerizer.cpp:287] Recovering containerizer
I0806 15:06:59.383121  9236 slave.cpp:3175] Finished recovery
I0806 15:06:59.383474  9236 slave.cpp:589] New master detected at master@127.0.1.1:60888
I0806 15:06:59.383501  9236 slave.cpp:663] Authenticating with master master@127.0.1.1:60888
I0806 15:06:59.383543  9236 slave.cpp:636] Detecting new master
I0806 15:06:59.383576  9236 status_update_manager.cpp:167] New master detected at master@127.0.1.1:60888
I0806 15:06:59.383646  9236 authenticatee.hpp:128] Creating new client SASL connection
I0806 15:06:59.383748  9236 master.cpp:3545] Authenticating slave(96)@127.0.1.1:60888
I0806 15:06:59.383833  9236 authenticator.hpp:156] Creating new server SASL connection
I0806 15:06:59.383906  9236 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0806 15:06:59.383926  9236 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0806 15:06:59.383949  9236 authenticator.hpp:262] Received SASL authentication start
I0806 15:06:59.383987  9236 authenticator.hpp:384] Authentication requires more steps
I0806 15:06:59.384011  9236 authenticatee.hpp:265] Received SASL authentication step
I0806 15:06:59.384042  9236 authenticator.hpp:290] Received SASL authentication step
I0806 15:06:59.384055  9236 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0806 15:06:59.384062  9236 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0806 15:06:59.384069  9236 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0806 15:06:59.384076  9236 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0806 15:06:59.384081  9236 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0806 15:06:59.384086  9236 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0806 15:06:59.384096  9236 authenticator.hpp:376] Authentication success
I0806 15:06:59.384119  9236 authenticatee.hpp:305] Authentication success
I0806 15:06:59.384138  9236 master.cpp:3585] Successfully authenticated principal 'test-principal' at slave(96)@127.0.1.1:60888
I0806 15:06:59.384209  9236 slave.cpp:720] Successfully authenticated with master master@127.0.1.1:60888
I0806 15:06:59.384249  9236 slave.cpp:971] Will retry registration in 2.692473ms if necessary
I0806 15:06:59.384337  9233 master.cpp:2767] Registering slave at slave(96)@127.0.1.1:60888 (lucid) with id 20140806-150659-16842879-60888-9210-0
I0806 15:06:59.384434  9233 registrar.cpp:422] Attempting to update the 'registry'
I0806 15:06:59.385726  9231 log.cpp:680] Attempting to append 287 bytes to the log
I0806 15:06:59.385782  9231 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0806 15:06:59.385999  9231 replica.cpp:508] Replica received write request for position 3
I0806 15:06:59.388566  9231 leveldb.cpp:343] Persisting action (306 bytes) to leveldb took 2.532556ms
I0806 15:06:59.388595  9231 replica.cpp:676] Persisted action at 3
I0806 15:06:59.388803  9231 replica.cpp:655] Replica received learned notice for position 3
I0806 15:06:59.389863  9233 slave.cpp:971] Will retry registration in 39.249021ms if necessary
I0806 15:06:59.389991  9233 master.cpp:2755] Ignoring register slave message from slave(96)@127.0.1.1:60888 (lucid) as admission is already in progress
I0806 15:06:59.391331  9210 sched.cpp:139] Version: 0.20.0
I0806 15:06:59.391752  9235 sched.cpp:235] New master detected at master@127.0.1.1:60888
I0806 15:06:59.391777  9235 sched.cpp:285] Authenticating with master master@127.0.1.1:60888
I0806 15:06:59.391863  9235 authenticatee.hpp:128] Creating new client SASL connection
I0806 15:06:59.391944  9235 master.cpp:3545] Authenticating scheduler-c7f52aa8-e390-465a-aad0-7e440c08940a@127.0.1.1:60888
I0806 15:06:59.392031  9235 authenticator.hpp:156] Creating new server SASL connection
I0806 15:06:59.392089  9235 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0806 15:06:59.392102  9235 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0806 15:06:59.392122  9235 authenticator.hpp:262] Received SASL authentication start
I0806 15:06:59.392155  9235 authenticator.hpp:384] Authentication requires more steps
I0806 15:06:59.392179  9235 authenticatee.hpp:265] Received SASL authentication step
I0806 15:06:59.392213  9235 authenticator.hpp:290] Received SASL authentication step
I0806 15:06:59.392230  9235 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0806 15:06:59.392242  9235 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0806 15:06:59.392258  9235 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0806 15:06:59.392273  9235 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0806 15:06:59.392310  9235 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0806 15:06:59.392323  9235 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0806 15:06:59.392343  9235 authenticator.hpp:376] Authentication success
I0806 15:06:59.392387  9235 authenticatee.hpp:305] Authentication success
I0806 15:06:59.392452  9235 master.cpp:3585] Successfully authenticated principal 'test-principal' at scheduler-c7f52aa8-e390-465a-aad0-7e440c08940a@127.0.1.1:60888
I0806 15:06:59.392570  9235 sched.cpp:359] Successfully authenticated with master master@127.0.1.1:60888
I0806 15:06:59.392585  9235 sched.cpp:478] Sending registration request to master@127.0.1.1:60888
I0806 15:06:59.392628  9235 master.cpp:1246] Received registration request from scheduler-c7f52aa8-e390-465a-aad0-7e440c08940a@127.0.1.1:60888
I0806 15:06:59.392645  9235 master.cpp:1206] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0806 15:06:59.392765  9235 master.cpp:1305] Registering framework 20140806-150659-16842879-60888-9210-0000 at scheduler-c7f52aa8-e390-465a-aad0-7e440c08940a@127.0.1.1:60888
I0806 15:06:59.392875  9235 sched.cpp:409] Framework registered with 20140806-150659-16842879-60888-9210-0000
I0806 15:06:59.392906  9235 sched.cpp:423] Scheduler::registered took 16129ns
I0806 15:06:59.392942  9235 hierarchical_allocator_process.hpp:326] Added framework 20140806-150659-16842879-60888-9210-0000
I0806 15:06:59.392951  9235 hierarchical_allocator_process.hpp:688] No resources available to allocate!
I0806 15:06:59.392956  9235 hierarchical_allocator_process.hpp:650] Performed allocation for 0 slaves in 5598ns
I0806 15:06:59.399893  9231 leveldb.cpp:343] Persisting action (308 bytes) to leveldb took 11.054696ms
I0806 15:06:59.399946  9231 replica.cpp:676] Persisted action at 3
I0806 15:06:59.399956  9231 replica.cpp:661] Replica learned APPEND action at position 3
I0806 15:06:59.400360  9231 registrar.cpp:479] Successfully updated 'registry'
I0806 15:06:59.400446  9231 log.cpp:699] Attempting to truncate the log to 3
I0806 15:06:59.400507  9231 master.cpp:2807] Registered slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid)
I0806 15:06:59.400524  9231 master.cpp:4018] Adding slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0806 15:06:59.400640  9231 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0806 15:06:59.400725  9231 slave.cpp:754] Registered with master master@127.0.1.1:60888; given slave ID 20140806-150659-16842879-60888-9210-0
I0806 15:06:59.400805  9231 hierarchical_allocator_process.hpp:439] Added slave 20140806-150659-16842879-60888-9210-0 (lucid) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0806 15:06:59.400858  9231 hierarchical_allocator_process.hpp:714] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140806-150659-16842879-60888-9210-0 to framework 20140806-150659-16842879-60888-9210-0000
I0806 15:06:59.400949  9231 hierarchical_allocator_process.hpp:670] Performed allocation for slave 20140806-150659-16842879-60888-9210-0 in 114359ns
I0806 15:06:59.401000  9231 slave.cpp:2324] Received ping from slave-observer(80)@127.0.1.1:60888
I0806 15:06:59.401064  9231 master.hpp:816] Adding offer 20140806-150659-16842879-60888-9210-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140806-150659-16842879-60888-9210-0 (lucid)
I0806 15:06:59.401111  9231 master.cpp:3492] Sending 1 offers to framework 20140806-150659-16842879-60888-9210-0000
I0806 15:06:59.401257  9231 sched.cpp:546] Scheduler::resourceOffers took 20565ns
I0806 15:06:59.401422  9231 replica.cpp:508] Replica received write request for position 4
I0806 15:06:59.402755  9234 master.hpp:826] Removing offer 20140806-150659-16842879-60888-9210-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140806-150659-16842879-60888-9210-0 (lucid)
I0806 15:06:59.402817  9234 master.cpp:2130] Processing reply for offers: [ 20140806-150659-16842879-60888-9210-0 ] on slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid) for framework 20140806-150659-16842879-60888-9210-0000
I0806 15:06:59.402847  9234 master.cpp:2216] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
I0806 15:06:59.403132  9234 master.hpp:788] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140806-150659-16842879-60888-9210-0 (lucid)
I0806 15:06:59.403168  9234 master.cpp:2282] Launching task 1 of framework 20140806-150659-16842879-60888-9210-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid)
I0806 15:06:59.403311  9234 slave.cpp:1002] Got assigned task 1 for framework 20140806-150659-16842879-60888-9210-0000
I0806 15:06:59.403586  9234 slave.cpp:1112] Launching task 1 for framework 20140806-150659-16842879-60888-9210-0000
I0806 15:06:59.405397  9234 slave.cpp:1222] Queuing task '1' for executor 1 of framework '20140806-150659-16842879-60888-9210-0000
I0806 15:06:59.405566  9234 containerizer.cpp:427] Starting container '64988b19-cec6-4228-80e5-6e80a271bb8a' for executor '1' of framework '20140806-150659-16842879-60888-9210-0000'
I0806 15:06:59.405800  9234 slave.cpp:543] Successfully attached file '/tmp/HealthCheckTest_ConsecutiveFailures_jnWV1D/slaves/20140806-150659-16842879-60888-9210-0/frameworks/20140806-150659-16842879-60888-9210-0000/executors/1/runs/64988b19-cec6-4228-80e5-6e80a271bb8a'
I0806 15:06:59.410970  9231 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 9.489363ms
I0806 15:06:59.411043  9231 replica.cpp:676] Persisted action at 4
I0806 15:06:59.411597  9231 replica.cpp:655] Replica received learned notice for position 4
I0806 15:06:59.412961  9234 launcher.cpp:137] Forked child with pid '12834' for container '64988b19-cec6-4228-80e5-6e80a271bb8a'
I0806 15:06:59.413980  9234 containerizer.cpp:537] Fetching URIs for container '64988b19-cec6-4228-80e5-6e80a271bb8a' using command '/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src/mesos-fetcher'
I0806 15:06:59.425834  9231 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 14.188783ms
I0806 15:06:59.425940  9231 leveldb.cpp:401] Deleting ~2 keys from leveldb took 49144ns
I0806 15:06:59.425961  9231 replica.cpp:676] Persisted action at 4
I0806 15:06:59.425974  9231 replica.cpp:661] Replica learned TRUNCATE action at position 4
E0806 15:07:00.245468  9231 slave.cpp:2471] Container '64988b19-cec6-4228-80e5-6e80a271bb8a' for executor '1' of framework '20140806-150659-16842879-60888-9210-0000' failed to start: TaskInfo/ExecutorInfo not supported
I0806 15:07:00.301451  9231 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 41375ns
I0806 15:07:00.611855  9231 slave.cpp:1733] Got registration for executor '1' of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:00.612200  9231 slave.cpp:1852] Flushing queued task 1 for executor '1' of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:00.612619  9238 process.cpp:1098] Socket closed while receiving
I0806 15:07:00.617409  9231 slave.cpp:2087] Handling status update TASK_RUNNING (UUID: d3e56bf8-179e-4d3b-bfe2-f78b824968ae) for task 1 of framework 20140806-150659-16842879-60888-9210-0000 from executor(1)@127.0.1.1:52559
I0806 15:07:00.617502  9231 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: d3e56bf8-179e-4d3b-bfe2-f78b824968ae) for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:00.617517  9231 status_update_manager.cpp:499] Creating StatusUpdate stream for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:00.617617  9231 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: d3e56bf8-179e-4d3b-bfe2-f78b824968ae) for task 1 of framework 20140806-150659-16842879-60888-9210-0000 to master@127.0.1.1:60888
I0806 15:07:00.617749  9231 master.cpp:3136] Forwarding status update TASK_RUNNING (UUID: d3e56bf8-179e-4d3b-bfe2-f78b824968ae) for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:00.617787  9231 master.cpp:3102] Status update TASK_RUNNING (UUID: d3e56bf8-179e-4d3b-bfe2-f78b824968ae) for task 1 of framework 20140806-150659-16842879-60888-9210-0000 from slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid)
I0806 15:07:00.617817  9231 slave.cpp:2245] Status update manager successfully handled status update TASK_RUNNING (UUID: d3e56bf8-179e-4d3b-bfe2-f78b824968ae) for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:00.617827  9231 slave.cpp:2251] Sending acknowledgement for status update TASK_RUNNING (UUID: d3e56bf8-179e-4d3b-bfe2-f78b824968ae) for task 1 of framework 20140806-150659-16842879-60888-9210-0000 to executor(1)@127.0.1.1:52559
I0806 15:07:00.618036  9231 sched.cpp:637] Scheduler::statusUpdate took 33324ns
I0806 15:07:00.618113  9231 master.cpp:2617] Forwarding status update acknowledgement d3e56bf8-179e-4d3b-bfe2-f78b824968ae for task 1 of framework 20140806-150659-16842879-60888-9210-0000 to slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid)
I0806 15:07:00.618216  9231 status_update_manager.cpp:398] Received status update acknowledgement (UUID: d3e56bf8-179e-4d3b-bfe2-f78b824968ae) for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:00.618271  9231 slave.cpp:1673] Status update manager successfully handled status update acknowledgement (UUID: d3e56bf8-179e-4d3b-bfe2-f78b824968ae) for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:00.618535  9238 process.cpp:1098] Socket closed while receiving
2014-08-06 15:07:00,854:9210(0x2b245964a700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36839] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0806 15:07:01.330152  9230 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 43958ns
I0806 15:07:01.677136  9230 slave.cpp:2087] Handling status update TASK_RUNNING (UUID: 0b3404a9-3e62-4780-b2bb-9a6c1c47dc27) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 from executor(1)@127.0.1.1:52559
I0806 15:07:01.677273  9230 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 0b3404a9-3e62-4780-b2bb-9a6c1c47dc27) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:01.677304  9230 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 0b3404a9-3e62-4780-b2bb-9a6c1c47dc27) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 to master@127.0.1.1:60888
I0806 15:07:01.677428  9230 master.cpp:3136] Forwarding status update TASK_RUNNING (UUID: 0b3404a9-3e62-4780-b2bb-9a6c1c47dc27) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:01.677466  9230 master.cpp:3102] Status update TASK_RUNNING (UUID: 0b3404a9-3e62-4780-b2bb-9a6c1c47dc27) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 from slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid)
I0806 15:07:01.677496  9230 slave.cpp:2245] Status update manager successfully handled status update TASK_RUNNING (UUID: 0b3404a9-3e62-4780-b2bb-9a6c1c47dc27) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:01.677505  9230 slave.cpp:2251] Sending acknowledgement for status update TASK_RUNNING (UUID: 0b3404a9-3e62-4780-b2bb-9a6c1c47dc27) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 to executor(1)@127.0.1.1:52559
I0806 15:07:01.677682  9230 sched.cpp:637] Scheduler::statusUpdate took 22626ns
I0806 15:07:01.677749  9230 master.cpp:2617] Forwarding status update acknowledgement 0b3404a9-3e62-4780-b2bb-9a6c1c47dc27 for task 1 of framework 20140806-150659-16842879-60888-9210-0000 to slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid)
I0806 15:07:01.677850  9230 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 0b3404a9-3e62-4780-b2bb-9a6c1c47dc27) for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:01.677902  9230 slave.cpp:1673] Status update manager successfully handled status update acknowledgement (UUID: 0b3404a9-3e62-4780-b2bb-9a6c1c47dc27) for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:01.678419  9238 process.cpp:1098] Socket closed while receiving
I0806 15:07:02.330945  9233 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 68481ns
I0806 15:07:02.683594  9233 slave.cpp:2087] Handling status update TASK_RUNNING (UUID: 77ba4fce-7925-43af-abeb-3cdb53a0fb75) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 from executor(1)@127.0.1.1:52559
I0806 15:07:02.683812  9233 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 77ba4fce-7925-43af-abeb-3cdb53a0fb75) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:02.683856  9233 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 77ba4fce-7925-43af-abeb-3cdb53a0fb75) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 to master@127.0.1.1:60888
I0806 15:07:02.684006  9233 master.cpp:3136] Forwarding status update TASK_RUNNING (UUID: 77ba4fce-7925-43af-abeb-3cdb53a0fb75) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:02.684054  9233 master.cpp:3102] Status update TASK_RUNNING (UUID: 77ba4fce-7925-43af-abeb-3cdb53a0fb75) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 from slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid)
I0806 15:07:02.684089  9233 slave.cpp:2245] Status update manager successfully handled status update TASK_RUNNING (UUID: 77ba4fce-7925-43af-abeb-3cdb53a0fb75) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:02.684098  9233 slave.cpp:2251] Sending acknowledgement for status update TASK_RUNNING (UUID: 77ba4fce-7925-43af-abeb-3cdb53a0fb75) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 to executor(1)@127.0.1.1:52559
I0806 15:07:02.684299  9233 sched.cpp:637] Scheduler::statusUpdate took 40572ns
I0806 15:07:02.684434  9233 master.cpp:2617] Forwarding status update acknowledgement 77ba4fce-7925-43af-abeb-3cdb53a0fb75 for task 1 of framework 20140806-150659-16842879-60888-9210-0000 to slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid)
I0806 15:07:02.684790  9238 process.cpp:1098] Socket closed while receiving
I0806 15:07:02.684916  9232 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 77ba4fce-7925-43af-abeb-3cdb53a0fb75) for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:02.684984  9232 slave.cpp:1673] Status update manager successfully handled status update acknowledgement (UUID: 77ba4fce-7925-43af-abeb-3cdb53a0fb75) for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:03.340453  9235 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 44308ns
I0806 15:07:03.692570  9235 slave.cpp:2087] Handling status update TASK_RUNNING (UUID: e83bd7a9-a974-443e-9292-9b38ee0c8a8c) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 from executor(1)@127.0.1.1:52559
I0806 15:07:03.692721  9235 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: e83bd7a9-a974-443e-9292-9b38ee0c8a8c) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:03.692755  9235 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: e83bd7a9-a974-443e-9292-9b38ee0c8a8c) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 to master@127.0.1.1:60888
I0806 15:07:03.692875  9235 master.cpp:3136] Forwarding status update TASK_RUNNING (UUID: e83bd7a9-a974-443e-9292-9b38ee0c8a8c) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:03.692911  9235 master.cpp:3102] Status update TASK_RUNNING (UUID: e83bd7a9-a974-443e-9292-9b38ee0c8a8c) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 from slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid)
I0806 15:07:03.692942  9235 slave.cpp:2245] Status update manager successfully handled status update TASK_RUNNING (UUID: e83bd7a9-a974-443e-9292-9b38ee0c8a8c) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:03.692952  9235 slave.cpp:2251] Sending acknowledgement for status update TASK_RUNNING (UUID: e83bd7a9-a974-443e-9292-9b38ee0c8a8c) for task 1 in health state unhealthy of framework 20140806-150659-16842879-60888-9210-0000 to executor(1)@127.0.1.1:52559
I0806 15:07:03.693111  9235 sched.cpp:637] Scheduler::statusUpdate took 23054ns
I0806 15:07:03.693174  9235 master.cpp:2617] Forwarding status update acknowledgement e83bd7a9-a974-443e-9292-9b38ee0c8a8c for task 1 of framework 20140806-150659-16842879-60888-9210-0000 to slave 20140806-150659-16842879-60888-9210-0 at slave(96)@127.0.1.1:60888 (lucid)
I0806 15:07:03.693272  9235 status_update_manager.cpp:398] Received status update acknowledgement (UUID: e83bd7a9-a974-443e-9292-9b38ee0c8a8c) for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:03.693323  9235 slave.cpp:1673] Status update manager successfully handled status update acknowledgement (UUID: e83bd7a9-a974-443e-9292-9b38ee0c8a8c) for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:03.693723  9238 process.cpp:1098] Socket closed while receiving
2014-08-06 15:07:04,190:9210(0x2b245964a700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36839] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0806 15:07:04.289959  9234 master.cpp:121] No whitelist given. Advertising offers for all slaves
I0806 15:07:04.353030  9234 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 75401ns
I0806 15:07:05.360039  9230 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 39386ns
I0806 15:07:06.371474  9233 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 125156ns
I0806 15:07:07.380746  9233 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 38124ns
2014-08-06 15:07:07,527:9210(0x2b245964a700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36839] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0806 15:07:08.391015  9235 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 72442ns
I0806 15:07:09.299963  9235 master.cpp:121] No whitelist given. Advertising offers for all slaves
I0806 15:07:09.404326  9237 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 44315ns
I0806 15:07:10.411216  9236 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 80194ns
2014-08-06 15:07:10,864:9210(0x2b245964a700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36839] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0806 15:07:11.420480  9236 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 55269ns
I0806 15:07:12.806651  9236 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 107843ns
tests/health_check_tests.cpp:336: Failure
Failed to wait 10secs for status4
tests/health_check_tests.cpp:311: Failure
Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
         Expected: to be called 6 times
           Actual: called 4 times - unsatisfied and active
I0806 15:07:13.794618  9232 master.cpp:715] Framework 20140806-150659-16842879-60888-9210-0000 disconnected
I0806 15:07:13.794680  9232 master.cpp:1586] Deactivating framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:13.794806  9232 master.cpp:737] Giving framework 20140806-150659-16842879-60888-9210-0000 0ns to failover
I0806 15:07:13.795145  9232 hierarchical_allocator_process.hpp:402] Deactivated framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:13.795831  9236 master.cpp:3372] Framework failover timeout, removing framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:13.795866  9236 master.cpp:3868] Removing framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:13.796226  9236 master.hpp:806] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140806-150659-16842879-60888-9210-0 (lucid)
W0806 15:07:13.796470  9236 master.cpp:4311] Removing task 1 of framework 20140806-150659-16842879-60888-9210-0000 and slave 20140806-150659-16842879-60888-9210-0 in non-terminal state TASK_RUNNING
I0806 15:07:13.796775  9236 slave.cpp:1406] Asked to shut down framework 20140806-150659-16842879-60888-9210-0000 by master@127.0.1.1:60888
I0806 15:07:13.796835  9236 slave.cpp:1431] Shutting down framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:13.796895  9236 slave.cpp:2855] Shutting down executor '1' of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:13.805369  9235 hierarchical_allocator_process.hpp:560] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20140806-150659-16842879-60888-9210-0 from framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:13.805450  9235 hierarchical_allocator_process.hpp:591] Framework 20140806-150659-16842879-60888-9210-0000 filtered slave 20140806-150659-16842879-60888-9210-0 for 5secs
I0806 15:07:13.805552  9235 hierarchical_allocator_process.hpp:357] Removed framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:13.806340  9234 master.cpp:624] Master terminating
I0806 15:07:13.813288  9238 process.cpp:1098] Socket closed while receiving
I0806 15:07:13.813813  9234 hierarchical_allocator_process.hpp:650] Performed allocation for 1 slaves in 25871ns
I0806 15:07:13.819965  9236 slave.cpp:2356] master@127.0.1.1:60888 exited
W0806 15:07:13.820013  9236 slave.cpp:2359] Master disconnected! Waiting for a new master to be elected
I0806 15:07:13.832304  9235 containerizer.cpp:909] Destroying container '64988b19-cec6-4228-80e5-6e80a271bb8a'
I0806 15:07:13.902968  9238 process.cpp:1037] Socket closed while receiving
2014-08-06 15:07:14,199:9210(0x2b245964a700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36839] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0806 15:07:14.370625  9234 containerizer.cpp:1025] Executor for container '64988b19-cec6-4228-80e5-6e80a271bb8a' has exited
I0806 15:07:14.371238  9234 slave.cpp:2573] Executor '1' of framework 20140806-150659-16842879-60888-9210-0000 terminated with signal Killed
I0806 15:07:14.371309  9234 slave.cpp:2709] Cleaning up executor '1' of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:14.371554  9234 slave.cpp:2784] Cleaning up framework 20140806-150659-16842879-60888-9210-0000
E0806 15:07:14.371716  9234 slave.cpp:2843] Failed to unmonitor container for executor 1 of framework 20140806-150659-16842879-60888-9210-0000: Not monitored
I0806 15:07:14.371762  9234 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_ConsecutiveFailures_jnWV1D/slaves/20140806-150659-16842879-60888-9210-0/frameworks/20140806-150659-16842879-60888-9210-0000/executors/1/runs/64988b19-cec6-4228-80e5-6e80a271bb8a' for gc 6.99999570068444days in the future
I0806 15:07:14.371861  9234 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_ConsecutiveFailures_jnWV1D/slaves/20140806-150659-16842879-60888-9210-0/frameworks/20140806-150659-16842879-60888-9210-0000/executors/1' for gc 6.99999569990815days in the future
I0806 15:07:14.371892  9234 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_ConsecutiveFailures_jnWV1D/slaves/20140806-150659-16842879-60888-9210-0/frameworks/20140806-150659-16842879-60888-9210-0000' for gc 6.99999569887704days in the future
I0806 15:07:14.371935  9234 status_update_manager.cpp:282] Closing status update streams for framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:14.371968  9234 status_update_manager.cpp:530] Cleaning up status update stream for task 1 of framework 20140806-150659-16842879-60888-9210-0000
I0806 15:07:14.375880  9210 slave.cpp:466] Slave terminating
[  FAILED  ] HealthCheckTest.ConsecutiveFailures (15139 ms)
{noformat}

Does the CI machine leave slave/task logs after tests failure? It is quite hard to know how come there is nothing going on for a long period of time.

And is this in the Apache CI or Twitter CI machines?

So far only Twitter CI is exposing this flakiness. I've pasted the full logs in the comment above, are you looking for logging from the command executor? We may want to investigate wiring up the tests to expose them in the output to make this easier for you to debug.

Yes I will need the task logs and need your help to get output from Twitter machines as I can't repro this.
I'll try to log the task output in the tests to help debug this.

[~tnachen] it's failing on ASF CI as well, can you triage or disable in the interim?

E.g.
https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2299/consoleFull

Chatted with [~tnachen] on IRC. Decided to temporarily disabled the test. He is going to send a review out with more instrumentation to the test to find the root cause.

{code}
commit 7814140991217209494f5ec0f266e1d7b0f1d897
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Thu Aug 7 20:50:14 2014 -0700

    Temporarily disabled HealthcheckTest.ConsecutiveFailures test due to
    MESOS-1613.
{code}



Added output to stdout and stderr in my branch, hopefully to see what's going on

For posterity, I also wasn't able to reproduce this by just running in repetition. However, when I ran one {{openssl speed}} for each core on my laptop in order to induce load, I could reproduce easily. We probably want to direct folks to try this when they are having trouble reproducing something flaky from CI.

I will post a fix through MESOS-4106.

Re-enabled the test:

{noformat}
commit 7aa7957fb9a5bce5a9d8ae5a2560d1bde97d1274
Author: Benjamin Mahler <benjamin.mahler@gmail.com>
Date:   Wed Dec 9 17:43:27 2015 -0800

    Fixed a message dropping bug in the health checker.

    Much like in the command executor, we need to sleep after we send
    the final message in the health checker. Otherwise, we may exit
    before libprocess is able to finish sending the message over the
    local network.

    This led to the following issues:
    https://issues.apache.org/jira/browse/MESOS-1613
    https://issues.apache.org/jira/browse/MESOS-4106

    Review: https://reviews.apache.org/r/41178
{noformat}

