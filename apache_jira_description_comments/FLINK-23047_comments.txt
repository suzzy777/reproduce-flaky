https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21791&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=13994

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21934&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361&l=15090

{{CassandraConnectorITCase.createTable}} fails with a similar exception. https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21962&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14549

{{CassandraConnectorITCase.testDataPersistenceUponMissedNotify}} on 1.12:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22540&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=13647

CassandraConnectorITCase.testScalingUp
CassandraConnectorITCase.testIdealCircumstances
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23142&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407&l=13846

CassandraConnectorITCase.testScalingDown
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23344&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=14083

CassandraConnectorITCase.createTable
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23606&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14252

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23759&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=14035

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23877&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=14431

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23956&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=13647

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27229&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11032]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27357&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11433

Generally speaking, this error comes with load. I've had it already on another ASF project with embedded Cassandra and I fixed it with retries.
 

[~chesnay] can you assign me this ticket ?

So you mean that we are querying Cassandra too often?

The embedded cassandra cluster has always being a massive resource hog. I remember way back when it was added that it would freeze my entire machine If I ran it locally.

Maybe we should look into upgrading our cassandra dependencies first?

[~trohrmann] no I mean that the ITest servers might be overloaded some times.

[~chesnay] There are alternatives as discussed [here|https://issues.apache.org/jira/browse/FLINK-22775?focusedCommentId=17446552&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17446552]. Achilles project works fine in Apache Beam tests and is ASF V2 licenced. Of course upgrade is to be done as well

Do you guys want that we migrate the Cassandra daemon in the tests to Achilles ? It is true that there are a lot of flakiness in this test suite

[~echauchot] I'm not familiar with Achilles, but given that we're using Testcontainers more and more, would that also be an option? https://www.testcontainers.org/modules/databases/cassandra/

I'd prefer testcontainers as well.

[~MartijnVisser] yes I proposed testContainers in the other ticket [here|https://issues.apache.org/jira/browse/FLINK-22775?focusedCommentId=17446552&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17446552] so it is definitely an option.  I opened FLINK-25147 for migration to testContainers. That being said, testContainers relies on the official cassandra docker image with the following conf: XMx 1024M and JMX disabled. It may not consume less resources but at least it could improve isolation compared to the current situation and also it has the advantage of not adding an unknown tool. If you want I can tackle the migration.

[~chesnay] can you please:
 * assign me to FLINK-25147
 *  unassign me from FLINK-23047
 *  unassign me from FLINK-24844
 *  for FLINK-22775 leave the assignment as I need to monitor if lowering replica expectancy fixed the flakiness.
 * When testContainers PR is merged we'll see if the flakiness tickets above can be closed.

Done.

[~chesnay] thx

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28017&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11106]

OOM really  ? [~gaoyunhaii] thx for pointing out, I guess [migrating to test containers|https://issues.apache.org/jira/browse/FLINK-25147] will help

As can be seen in the umbrella ticket https://issues.apache.org/jira/browse/FLINK-25147 that Cassandra tests are now using the testcontainers. We're going to monitor the Cassandra tests until the new year and if no new occurences are popping up, we are closing this (and the other subtasks) as resolved

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28447&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d

Can we disable the test until the issue is resolved?

[~echauchot] WDYT?

Thanks David for pointing out, it's nice to work with you again ! That is exactly the monitoring I was referring to. [~MartijnVisser] We are in the case I mentioned that migration to testContainers is not enough to erase all flakiness. So, I agree, we could disable this test. In the meanTime, I can introduce retrials on connection because this error regularly happens no matter the Cassandra backend we use: I saw that using [Achilles test backend|https://github.com/doanduyhai/Achilles], cassandra daemon and testContainers. So I opened [this ticket|https://issues.apache.org/jira/browse/FLINK-25415]

Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28502&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=12056

+1 for disabling this test and re-enabling it after being fixed. [~echauchot] do you want to open a PR for it (with backports for 1.14 and 1.13)?

Hi Till, sure! PR ongoing but I did not have time to finish before leaving. I'm on a train I'm off until monday. I'll submit on monday

Very thanks [~echauchot] for taking care this issue! 

Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14352

1.13: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=13475

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11924

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=11527

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28602&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28603&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=12943

I disabled this flaky test and backported to 1.13 and 1.14:
https://github.com/apache/flink/pull/18206
https://github.com/apache/flink/pull/18205
https://github.com/apache/flink/pull/18204

I'm preparing a PR to [enable retrials|https://issues.apache.org/jira/browse/FLINK-25415] to fix the flakiness issues

Test disabled via

master: 509530bc79c8b9acce0bc86805956d624fc3f0fa
release-1.14: 457c42fe36b32c8436f6310a825b7f8ec1e8a2e0
release-1.13: 4f81fc60eabe772b2c05f5eff9fea6b01e1e0d28

Bumping the priority of this ticket to blocker in order to not forget about this disabled test.

[~trohrmann]I just submitted [the PR for retrials|https://github.com/apache/flink/pull/18211] as all the failures above are linked to NoHostAvailableException. But I sumitted the PR as part of the retrial ticket (FLINK-25415) and not the current ticket because it is more relevant. AFAICT there were several cassandra flakiness:
- the NoHostAvailableException: should be corrected with retrial above: FLINK-25415 (https://github.com/apache/flink/pull/18211)
- the wait for replica timeout: was closed by one of my previous PRs: FLINK-22775 (https://github.com/apache/flink/pull/17849)
- the table already exists exception: might be closed by the migration to testContainers FLINK-25147 (https://github.com/apache/flink/pull/18115) + keyspace drop FLINK-25415 (https://github.com/apache/flink/pull/18211)


Thanks for the PRs [~echauchot]. Could you maybe link the PRs for the individual points you mentioned. I don't know what the "above PR" is, because it refers to keyspace drop whereas one of the PRs adds retries. Due to this, I don't really know when this ticket can be closed.

[~trohrmann] sure ! I updated my comment above to add the links to the PRs

I think all the PRs are now merged. Do you think that we can now close this ticket [~echauchot]?

[~trohrmann] Yes all the PRs are merged (thanks for that) but I'd prefer to monitor flakiness for a week on all the tickets that depend upon [cassandra test container & retrials (FLINK-25147)|https://issues.apache.org/jira/browse/FLINK-25147] , especially _NoHostAvailableException_ and _table already exists_ error.  Then if no flakiness we could close all of them

[~echauchot] I think that's fine, I have reduced the priority to a Critical one and I propose that we close it after this week if no issues are occurring anymore.

Sounds like a good plan [~echauchot] :-) Thanks for working so much on hardening the Cassandra tests. This is super helpful for the community!

My pleasure ! Thanks guys for the reviews / feedback /merge ...

[~trohrmann] [~MartijnVisser] I just checked the status of ci connectors tests on master and so far there was no failure since the last PR was merged on Thursday.  Let's wait until Friday to have to a full business week with less holidays and more load to see if all this tickets were indeed fixed.

[~echauchot] Sounds good!

Hi guys, FYI, I'm checking now the history of failures and I'll will give the status for deciding on closing this ticket. Stay tuned !

I see no failure since my last Cassandra PR on retrials (https://github.com/apache/flink/pull/18211) was merged on dec 29th, see report on https://dev.azure.com/apache-flink/apache-flink/_test/analytics?definitionId=1&contextType=build

Please also remember that this PR was also backported to 1.13 and 1.14

[~trohrmann] I think you can close all the cassandra failure tickets that are linked in https://issues.apache.org/jira/browse/FLINK-25147, meaning:
- https://issues.apache.org/jira/browse/FLINK-23047
- https://issues.apache.org/jira/browse/FLINK-24844
- https://issues.apache.org/jira/browse/FLINK-25165
- https://issues.apache.org/jira/browse/FLINK-22739

Cassandra flakiness seems to be over ! :)

Fixed via FLINK-25147.

Great news [~echauchot]! Thanks a lot for your help with this issue :-)

I'm always glad to help !

