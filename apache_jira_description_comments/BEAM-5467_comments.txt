Anecdotally, tasks are failing because of segfault with following error 
04:18:38 Segmentation fault (core dumped) 04:18:38 04:18:38 > Task :beam-sdks-python:flinkCompatibilityMatrixStreaming FAILED 04:18:38 :beam-sdks-python:flinkCompatibilityMatrixStreaming (Thread[Task worker for ':' Thread 6,5,main]) completed. Took 1 mins 13.28 secs. 04:18:38 04:18:38 FAILURE: Build completed with 2 failures. 04:18:38 04:18:38 1: Task failed with an exception. 04:18:38 ----------- 04:18:38 * Where: 04:18:38 Build file '/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_VR_Flink/src/sdks/python/build.gradle' line: 340 04:18:38 04:18:38 * What went wrong: 04:18:38 Execution failed for task ':beam-sdks-python:flinkCompatibilityMatrixBatch'. 04:18:38 > Process 'command 'sh'' finished with non-zero exit value 139 04:18:38 04:18:38 * Try: 04:18:38 Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output. Run with --scan to get full insights. 04:18:38 ==============================================================================

[~angoenka] should we try to turn off the parallel execution?

I also think we should move the following to a distinct task in sdks/python/build.gradle:
{code:java}
tasks(':beam-sdks-python:flinkCompatibilityMatrixBatch')
tasks(':beam-sdks-python:flinkCompatibilityMatrixStreaming'){code}
 

 

I verified that they get executed sequentially so that should not be a problem. 
 
:beam-sdks-python:flinkCompatibilityMatrixBatchFAILED
Started: 5m 27.699s
Duration: 4m 0.393s
 
:beam-sdks-python:flinkCompatibilityMatrixStreamingFAILED
Started: 9m 28.093s
Duration: 1m 13.280s

The test pass consistently on local jenkins setup.

Yes, and as you had also noticed earlier, they frequently fail in CI with:


 Segmentation fault (core dumped)

[https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/]

[~swegner] do you have an idea what the cause could be or where to look for next level of detail? Perhaps memory settings?

  

[~thw] offhand this doesn't look familiar to me. Can you link to a Jenkin job run / Gradle build scan with more details?

[~swegner] here is an example. (It was necessary to download the full log to find the error message.)

[https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/305/consoleText]
{code:java}
# Thread: <Thread(Thread-10, started daemon 139873855989504)>
Segmentation fault (core dumped)

> Task :beam-sdks-python:flinkCompatibilityMatrixBatch FAILED

...


BUILD FAILED in 11m 6s
59 actionable tasks: 54 executed, 4 from cache, 1 up-to-date

Publishing build scan...
https://gradle.com/s/f2u3q2obrgaqu

Build step 'Invoke Gradle script' changed build result to FAILURE
Build step 'Invoke Gradle script' marked build as failure
Sending e-mails to: commits@beam.apache.org
Finished: FAILURE
{code}
 

Thanks for the additional context. I'm not an expert on diagnosing memory issues, but here's what I can pull out of there:

* The build scan shows [some stats on memory usage|https://scans.gradle.com/s/f2u3q2obrgaqu/performance/build#memory], and for this build I see "PS Eden Space" of 1.36/1.36 GB (99.5%). I would deduce that the JVM ran out of allotted memory causing the segfault.
* The [infrastructure tab|https://scans.gradle.com/s/f2u3q2obrgaqu#infrastructure] shows the "Max JVM memory heap size" for the job: 3824 MB
* In the [timeline|https://scans.gradle.com/s/f2u3q2obrgaqu/timeline] I can see that the task that failed was {{:beam-sdks-python:flinkCompatibilityMatrixBatch}}. Nothing was running concurrently as part of the build, so either this task ate up the entire heap space, or some previous task is leaking memory.

My recommendation would be to work towards getting a local repro so that you can attach a memory profiler and validate potential fixes. The Jenkins job shows the full command-line used to launch the job, including JVM memory configuration:

{{gradlew --info --continue --max-workers=12 -Dorg.gradle.jvmargs=-Xms2g -Dorg.gradle.jvmargs=-Xmx4g :beam-sdks-python:flinkCompatibilityMatrixBatch :beam-sdks-python:flinkCompatibilityMatrixStreaming}}

[~swegner] thanks for taking a look.

There have been a couple changes and the equivalent new command to run is:

 
{code:java}
./gradlew --info --continue --max-workers=12 -Dorg.gradle.jvmargs=-Xms2g -Dorg.gradle.jvmargs=-Xmx4g :beam-sdks-python:flinkCompatibilityMatrixDocker{code}
Above works fine on macOS (running the task locally has been reliable for a while now).

Anyone who might investigate this further, note that this build also forks a job server process, which may contribute to the issue.

The "Segmentation fault" errors interestingly occur after tests have succeeded, sometimes in the same run for both streaming and batch. Perhaps it is related to how the job server is terminated?

Related thread: https://lists.apache.org/thread.html/f1aad2d80977cb2f208e546033ad4f8136a70b1702a51f5881363cb1@%3Cdev.beam.apache.org%3E

 

Is this bug a the same as https://issues.apache.org/jira/browse/BEAM-5916 ?

Not exactly as in 5916, we had a test case which got stuck for 30 sec. After which the gradle task failed because of this issue. 

We need to fix this issue before moving to another flakyness in the :beam-sdks-python:flinkCompatibilityMatrixBatchDOCKER

 

Looks like the same issue to me, if the seg fault occurs after all tests have already executed.

The test case failed before the seg fault happened after the test case failed which suggest that this might require separate investigation.

In the regular seg fault failure, all test case pass but the gradle task fails. However, it is possible that failing testcase might have somehow caused by the segfault.

We should revisit https://issues.apache.org/jira/browse/BEAM-5916 after seg fault issue is fixed.

 

Can we already bisect this to a particular commit? This would be helpful for debugging.

I did a little more investigation here: [https://github.com/apache/beam/pull/6937]

The seg faults happen after the test run is complete (regardless if there are test failures or not). Looks like the Python process fails just before it exits.

The specific test case failure is indeed a separate issue, but also non-deterministic.
{code:java}
======================================================================
ERROR: test_pardo_unfusable_side_inputs (__main__.FlinkRunnerTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "apache_beam/runners/portability/fn_api_runner_test.py", line 254, in test_pardo_unfusable_side_inputs
    equal_to([('a', 'a'), ('a', 'b'), ('b', 'a'), ('b', 'b')]))
  File "apache_beam/pipeline.py", line 425, in __exit__
    self.run().wait_until_finish()
==================== Timed out after 30 seconds. ===================={code}
 

 

beam_PostCommit_Python_VR_Flink is perma red since yesterday. Seems to be the same error.

[https://builds.apache.org/view/A-D/view/Beam/job/beam_PostCommit_Python_VR_Flink/674/]

[https://scans.gradle.com/s/y4sqyzd4zozh6/failure?openFailures=WzBd&openStackTraces=WzJd#top=0]

 
Process 'command 'sh'' finished with non-zero exit value 1
[Hide exception|https://scans.gradle.com/s/y4sqyzd4zozh6/failure?openFailures=WzBd&openStackTraces=WzJd#top=0]
org.gradle.api.tasks.TaskExecutionException
: 
Execution failed for task ':beam-sdks-python:flinkCompatibilityMatrixBatchDOCKER'.Open stacktrace
 
Caused by: 
org.gradle.process.internal.ExecException
: 
Process 'command 'sh'' finished with non-zero exit value 1Close stacktrace
at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:395)
at org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:37)
 

 

 

 

The culprit is actually [https://github.com/apache/beam/pull/6959] 

[~robertwb] [~udim] please take a look

 

[https://github.com/apache/beam/pull/6999] fixes this

 

Seems like this is still occasionally flaky. Could we close this and break this into more precise issues for individual tests?

There are fewer failures now. Following flake shows up in most recent failed runs: ERROR: test_assert_that (__main__.FlinkRunnerTest)

I would suggest we get a clean baseline and then close this?


+1

Got here via BEAM-5916, which is marked a duplicate of this. Seeing more failures then passes.
https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/1167/
https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/1166/
https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/1165/
https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/1164/
https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/1162/
https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/1160/
https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/1157/
https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/1154/


Thanks for the build logs [~apilloud]. The {{test_assert_that}} test failures could be caused by BEAM-6019. Will take a look.

The culprit is now {{test_pardo_unfusable_side_inputs}}. Not seeing any failures of the {{test_assert_that}} anymore.

The issue is that the test contains two pipelines which each take about 17 seconds to run on Flink. The default timeout of 30 seconds is too low.

Closing because tests are back to normal: https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/

{quote}The issue is that the test contains two pipelines which each take about 17 seconds to run on Flink. The default timeout of 30 seconds is too low.{quote}

[~mxm] do you think the test timeout should be increased to reduce future flakiness?

[~swegner] Already done in the last PR before I closed this: https://github.com/apache/beam/pull/7376

