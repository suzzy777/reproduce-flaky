If I understand this correctly: Seems like this is introduced in https://issues.apache.org/jira/browse/KAFKA-14024, which originated from https://issues.apache.org/jira/browse/KAFKA-13310.  I think the cause of the flakiness/duplication is, the consumer is busy waiting for the prior async commit to complete (in order to complete the rebalance process), while fetching new data.  After the async complete finished, the partition gets revoked, and the fetch progress will be lost, and eventually causes duplicated consumption.

A few comments:
 # Do we want to continue to fetch, while waiting for the async commit to complete? I believe this is the expectation of the new poll API.
 # If we don't want to block consumer from fetching, then we will need to continue to commit asynchronously.  I see this could be problematic, as the consumer could stuck in the poll loop while busy catching up with committing the fetched data, and never complete the rebalance process.

Kind of originated from this commit: https://github.com/apache/kafka/pull/12349/files

[~pnee] , thanks for the analysis. Yes, we forgot about during the following poll, the offset might advance while we're waiting for the old async offset commit completion.

Actually, while checking the code, even if we don't do the change for KAFKA-14024,and KAFKA-13310, (that is, changing sync commit to async commit) the issue will still happen, just not that easily. The issue is, in the consumer#poll process, we do onJoinPrepare (i.e. commit the offset), and then fetch new records. I'm thinking we should have a way to terminate poll process to avoid it keep fetching new records and return.

 

Maybe in `KafkaConsumer#updateAssignmentMetadataIfNeeded`, we passed in a parameter to allow the `onJoinPrepare` method to change the flag to notify if we need to terminate the poll and not to fetch records. WDYT?

cc [~guozhang] [~dajac]  [~aiquestion]

Thanks Luke, per your suggestion, could you elaborate more about the reason to terminate the poll?

I've got a few questions to clarify here:
 # I don't think we need to pause the fetch if the previous async commit (autocommit) hasn't yet go through, for the normal situation (not rebalancing)? Because as long as we are sending out the commit, I think we could tentatively assume the acked data has been committed. Am I right?
 # I think we only need to pause the fetch, if there's a rebalance process taking place, because it only waits for the current in-flight commit, then revoke the partition.  Once the partition is revoked, I don't think we can do anything about the uncommitted data.

And because this regression was caused by the "rebalancing internal state" (pardon me if the words use is confusing), do you think it might be worth exposing the rebalance internal states? and perhaps adding a state to represent the current rebalancing progress, to prevent more fetching from happening during onJoinPrepare?

[~pnee] Thanks for reporting this. While reviewing KAFKA-13310 I have realized this, but as Luke said this is not a new regression (we would potentially have duplicates even before this, since as we commit sync, and if the commit fails, we still log a warning and move forward with the revocation, in which case we would also have duplicates), I suggested we add a TODO there indicating it's sub-optimal but is allowed under at least once semantics.

I think in the long run, as we move the rebalancing related procedure all to the background thread, this would no longer be an issue since between the time background thread received an response telling it to start rebalancing (of which, the first step is to potentially revoking partitions in `onJoinPrepare`), and the time after the auto commit has been completed, the background thread could simply mark those revoking partitions as "not retrievable" so that calling thread's `poll` calls would not return any more data for those partitions. Right?

If that's the case, then we only need to consider before that comes, what we should do with this. Like I said, the behaviors before are 1) we commit sync, and even if it fails we still move forward, which would cause duplicates, or 2) we commit async so that `poll` timeout could be respected, but we would still potentially return data for those revoking partitions. I'm thinking what about just taking the middle ground: we still commit async, while at the same time mark those revoking partitions as "not retrievable" to not return any more data, note this would still not forbid duplicates completely, but would basically take us to where we were in the likelihood of the duplicates. And then we rely on the threading remodeling (there's a WIP page that Philip would be sending out soon) to completely resolve this issue.

[~pnee] 
 # I don't think we need to pause the fetch if the previous async commit (autocommit) hasn't yet go through, for the normal situation (not rebalancing)? Because as long as we are sending out the commit, I think we could tentatively assume the acked data has been committed. Am I right?

 --> correct. for normal situation (not rebalancing), we don't pause anything
 # I think we only need to pause the fetch, if there's a rebalance process taking place, because it only waits for the current in-flight commit, then revoke the partition.  Once the partition is revoked, I don't think we can do anything about the uncommitted data.

--> correct.

 

[~guozhang] , thanks for the suggestion.

> I suggested we add a TODO there indicating it's sub-optimal but is allowed under at least once semantics.

Agree!

> we still commit async, while at the same time mark those revoking partitions as "not retrievable" to not return any more data

Sounds good to me!

 

From Philip:

> And because this regression was caused by the "rebalancing internal state" (pardon me if the words use is confusing), do you think it might be worth exposing the rebalance internal states? and perhaps adding a state to represent the current rebalancing progress, to prevent more fetching from happening during onJoinPrepare?

I think we can just `pause` the SubscriptionState of the partitions that we're going to revoked. From the javadoc:
{code:java}
/**
 * Suspend fetching from the requested partitions. Future calls to {@link #poll(Duration)} will not return
 * any records from these partitions until they have been resumed using {@link #resume(Collection)}.
 * Note that this method does not affect partition subscription. In particular, it does not cause a group
 * rebalance when automatic assignment is used.
 *
 * Note: Rebalance will not preserve the pause/resume state.
 * @param partitions The partitions which should be paused
 * @throws IllegalStateException if any of the provided partitions are not currently assigned to this consumer
 */
@Override
public void pause(Collection<TopicPartition> partitions) {{code}
 

I think that's what we want, right?

Thanks Luke and GW, it looks like we could just pause it, but I'll test it out to see if that does what we want... I'll get back to you guys soon. :)

[~showuon] and [~guozhang] - I think pausing should probably work, and it's also kind of convenient because the partition revocation will unpausing these partition automatically.  Let me know if you think the draft is ok, I'll add tests later on: [https://github.com/apache/kafka/pull/12603]

 

Though a few questions here:
 # Should we consider the difference between cooperative and eager protocol.  Because, cooperative doesn't revoke all partitions.  However, I worry that the subscription might change during the onJoinPrepare, so I meant there could be edge cases we need to handle here.
 # I believe this only applies to autocommit enabled.  I think for non-autocommit case, user should handle the offset during the revocation, so we are good there?

# Should we consider the difference between cooperative and eager protocol.  Because, cooperative doesn't revoke all partitions.  However, I worry that the subscription might change during the onJoinPrepare, so I meant there could be edge cases we need to handle here.

--> I think we should consider the difference between cooperative and eager protocol, because one of the purpose for cooperative rebalance is to allow "non-revoking" partitions can keep processing during rebalance. About the edge case, I think that's fine because in the your PR, we'll check and pause the partitions each time we enter onJoinPrepare, right? So, even if there's subscription change while we're waiting commitAsync, we can pause the updated subscription partitions in onJoinPrepare each time. Besides, that's really rare. WDYT?
 # I believe this only applies to autocommit enabled.  I think for non-autocommit case, user should handle the offset during the revocation, so we are good there?

--> Yes, we only need to worry about autocmmit enabled case

Thanks Philip, and regarding your two questions above I agree with [~showuon]'s thoughts as well. Especially for 1), I think even if subscriptions changed in between consecutive onJoinPrepare, as long as they will not change the assigned partitions (i.e. as long as `assignFromSubscribed()` has not called) I think we are fine, since the returned records depend on that assigned partitions.

To clarify, this was introduced in 3.2.1 (not 3.2.0), correct?

Also, this is currently marked as a blocker. Is there a crisp description of the regression?

[~ijuma] - I think that's right, according to the [release notes|https://downloads.apache.org/kafka/3.2.1/RELEASE_NOTES.html] (I see 10424 there).  I can add the description but I don't really know where, do you mean by updating the description/title of this ticket?

>  Also, this is currently marked as a blocker. Is there a crisp description of the regression?

Prior to revocation, eager rebalance strategies will attempt to auto-commit offsets before revoking partitions and joining the rebalance. Originally this logic was synchronous, which meant there was no opportunity for additional data to be returned before the revocation completed. This changed when we introduced asynchronous offset commit logic. Any progress made between the time the asynchronous offset commit was sent and the revocation completed would be lost. This results in duplicate consumption.

