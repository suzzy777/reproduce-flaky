cc [~squito] is it related with the 2GB limitation change?

I'm still looking at this, but so far haven't figured it out -- I'll share my observations anyhow in case it gives anybody else any ideas.

I haven't been able to reproduce this, despite running it locally hundreds of times, nor even in a handful of tries in the PR builder with some extra debugging turned on (https://github.com/apache/spark/pull/22460).

Every case I've looked at has *exactly* this checksum error: {{1651574976 != 1165629262}}.  It also seems to only be exactly this one case "caching on disk, replicated (encryption = on) (with replication as stream)", despite all the other variants touching the same code just in different combinations.

The failures I've looked at seem to be when task 1 on executor 1 is simultaneously pulling broadcast data from executor 0, and executor 0 is pushing the replicated RDD to executor 1 from task 0.

User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22496

ok I think I figured this out -- the problem is that TorrentBroadcast disposes of the bytebuffers after it adds them to the memorystore.  That would have been a no-op if they were regular byte buffers, but now they are memory-mapped from a file, and so they might contain garbage after getting disposed.

I should have a real PR soon, just want to confirm my understanding of a few things.

User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22511

User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22511

As it might be useful in the future, lemme add a few more details about what was going wrong and some lessons learned for debugging things like this.

I noticed that all the failures were when a broadcast block was fetched from another executor, not from the driver.  I made some hacks to the internals of spark to reproduce that scenario, but the failure still wasn't happening regularly just by reproducing that sequence of events, though it did become far more common with that, so I could reproduce locally (though it was still infrequent enough that I had to run tests for ~30 min to reproduce).

I kept tweaking logging to get more info, but it seemed like some changes were making the failures even less likely (might have been my imagination).  Somehow, I eventually got the failures to be relatively common, every 10 iterations or so, and I observed that (a) the broadcast block was received correctly on the first executor, but when there was a failure, by the time that executor sent the block, the block was already garbage on that same executor and (b) the bytes it was sending always looked like {{0000000000000000000000000000000000000000000000000070112801000000001004000000000000000000000000000020eb24010000000000000100000000000000000000000000000000000000000000000000000000000000000000000000f0f41d01...}}

it turns out that buffer.dispose() was always called before the block got sent, but the buffer doesn't actually get cleaned immediately.  That byte sequence may be "breadcrumbs" left from the cleaner, perhaps as a way to tell the buffer had been cleaned, instead of just being 0s -- though googling didn't turn up a reference to anything.

encryption was not directly involved, I guess it just made the error more likely as everything is slower with encryption.  And in fact it led me astray for a while, as other changes that have gone in since make this error impossible with encryption, as there were no mapped buffers with encryption.


User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22546

User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/22546

Issue resolved by pull request 22546
[https://github.com/apache/spark/pull/22546]

Unfortunately I messed up the testing of this slightly -- after this change, replication of large blocks no longer works, though it'll work after a config change.  I filed SPARK-25704 to fix properly.

Will this problem potentially hitting Spark 2.3.1 as well? I have a new cluster at this version and always hitting corrupt remote block when 1 specific table involved.Â 

[~magnusfa] handling blocks > 2GB was broken in many ways before 2.4.  You'll need to upgrade to 2.4.0 to get it work.  There are no known issues with large blocks in 2.4.0 (as far as I know, anyway).

