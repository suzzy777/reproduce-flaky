GitHub user clohfink opened a pull request:

    https://github.com/apache/cassandra/pull/132

    Reduce default num_tokens for CASSANDRA-13701

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/clohfink/cassandra 13701

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/132.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #132
    
----
commit 87039ed693aff57d0eb59a5e55f2f65bf03fef33
Author: Chris Lohfink <clohfink@users.noreply.github.com>
Date:   2017-07-19T23:15:36Z

    Reduce default num_tokens for CASSANDRA-13701

----


[~cnlwsu] Thanks and agree with you, but we'll probably get some opinions on this. Regardless, can you add in:
- an update the NEWS.txt with a couple of sentences summarizing with ticket numbers
- a fix to the comment in {{cassandra.yaml}} right above {{num_tokens}} to point to http://cassandra.apache.org/doc/latest/operating/index.html instead of that dead wiki page 

Should be blocked by CASSANDRA-13348


I don't think it's as well known as you think that you should reduce num_tokens, and also the token allocation algorithm isn't that well known either. I don't think we should be reducing the num_tokens default unless we also enforce the new allocation algorithm by default. If people start using 16 tokens with random allocation they are going to have a really fun time once N > RF. Sure, it can be fixed by turning on the algorithm before adding nodes, but you need to know that ahead of time.

bq.  I don't think we should be reducing the num_tokens default unless we also enforce the new allocation algorithm by default

Excellent point [~KurtG]. Is there a reason why we would not want to do this?

Right now the new algorithm double assigns tokens, so that's at least one reason 

Adding datacenters with the new algorithm requires some additional configuration.  We would need to make users aware of that trade-off when using that algorithm and the benefits of fewer token ranges per node.  It's talked about [here|http://docs.datastax.com/en/dse/5.1/dse-dev/datastax_enterprise/config/configVnodes.html] but we should make it clearer in the apache docs as well.  We can point to those in the comments around vnode tokens.

So it would be nice to add some more information [here|http://cassandra.apache.org/doc/latest/configuration/cassandra_config_file.html#allocate-tokens-for-keyspace] and then perhaps in [here|http://cassandra.apache.org/doc/latest/operating/topo_changes.html] with an additional section about adding a datacenter.

And Jeff: good point about the token allocation - would be good to track that down before making the new algorithm the default.  However even still I think even with the old algorithm we could at the very least halve the number of default vnode ranges.

Obviously the bug Jeff mentioned, but also we probably need to do some work on making configuration a bit more straightforward, as just enforcing it will make startup of a new cluster slightly more complicated.

At the moment you won't be able to start a node if the keyspace you specify doesn't exist. This is a kind of chicken and egg problem for seed nodes, which I believe is currently solved by seed nodes using random allocation, at which point you can create the keyspace and add the keyspace to the yaml for any new nodes.

Honestly this is probably a little bit convoluted for new users. DSE has obviously realised this as they have changed their yaml property to actually specify the RF, rather than the keyspace, which means you can specify this before creating the keyspace. Kind of works but not sure if it's the best choice.

IMO we'll need to come up with some novel way so that you can have it so you can configure and start a multi node cluster in a straightforward manner. I'd say it's reasonable that all nodes should be able to have the same underlying configuration, i.e, the minimum set of yaml properties is the same for all nodes. This would make config management for clusters much simpler rather than having to be aware of this, and special case seed nodes.

[CASSANDRA-14557 |https://issues.apache.org/jira/browse/CASSANDRA-14557] adds default_keyspace_rf to yaml that could possibly serve the purpose [~KurtG] is referring to, in the above comment.

Possibly, but it's doubling up on meanings for a single property. I can see a case where your default is not necessarily the same RF as your biggest keyspace (which is what you want to set the allocation algorithm to), and it's not exactly clear how it would work with multi-dc and different RF's per DC. I think Datastax did it right by moving to specifying the RF including multi-dc in the yaml.

With CASSANDRA-15260 committed we have the option of assigning it as default value (RF=3) and moving ahead with this ticket.

Any takers?

As discussed in [this thread|https://lists.apache.org/thread.html/r164d8a4143551b5ef774734afdce0e6666f31a0e461d71276f8446be%40%3Cdev.cassandra.apache.org%3E] the community decided on a default of 16 for now.  I'll assign to myself and put in some release notes about it.  At the same time I'll add some documentation for the topic.

[~mshuler] What would we need to do to update testing to 16 so that it coincides with the new defaults?

This is set here in the {{cassandra-builds}} repo:

{{(master)mshuler@hana:~/git/cassandra-builds$ git grep NUM_TOKENS}}
{{build-scripts/cassandra-dtest-pytest.sh:export NUM_TOKENS="32"}}
{{build-scripts/cassandra-dtest.sh:export NUM_TOKENS="32"}}

Also found these in the {{cassandra}} repo - one for pytest and circleci has a bunch:

{{(trunk)mshuler@hana:~/git/cassandra$ git grep NUM_TOKENS=}}
{{pylib/cassandra-cqlsh-tests.sh:export NUM_TOKENS="32"}}
{{(trunk)mshuler@hana:~/git/cassandra$ git grep num-tokens= .circleci/ | wc -l}}
{{24}}

Pull request: https://github.com/apache/cassandra/pull/663

Can we also standardize the tests to use the default values - that is, from 32 to the new defaults (16 {{num_tokens}} with {{allocate_tokens_for_local_replication_factor=3}} uncommented).

Changed the tests and circle configs here: https://github.com/driftx/cassandra/tree/CASSANDRA-13701

CI: https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/198/
https://app.circleci.com/pipelines/github/driftx/cassandra/48/workflows/885af1d3-10a7-4fd3-aa2a-54370a102ba9

Tests need to also be updated here: https://github.com/apache/cassandra-builds/blob/master/build-scripts/cassandra-dtest-pytest.sh#L19
Everything else looks good (though small nit in the NEWS.txt, see comment on commit in Brandon's fork+branch).

Cassandra-builds patch: https://github.com/apache/cassandra-builds/compare/master...thelastpickle:mck/13701--num_tokens_16
New dtest CI run: https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-dtest/213

Heaps of failures in that^ dtest run.

The failures look legit.
For example https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-dtest/213/testReport/junit/dtest.materialized_views_test/TestMaterializedViews/test_insert_during_range_movement_rf1/

Thanks to [~dimitarndimitrov] for pointing this out.
Dimitar also pointed this out earlier on the ML [here|https://lists.apache.org/thread.html/rbf96711ce1679d8efd1882b215cdb9ac2067bb7e5e5903466601eeef%40%3Cdev.cassandra.apache.org%3E].

Maybe the simplest thing to do is compromise at 32 instead of 16, since the tests have been running at 32 forever?

So if it's a matter of like Dimitar says, making the bootstraps sequential, then that isn't strictly an error as much as an unfortunate side effect of the new algorithm with dtest parallelism.  So there appears to be two paths forward:

1) Use the randomized algorithm both in tests and in the defaults with a higher num_tokens count
2) Change the dtests with bootstrapping/joining to be sequential with the new defaults

Is it possible to start by trying option 2 and see where that gets us in terms of dtest runtimes and errors?  I don't want to go down a rabbit hole but it would be nice to quantify the trade-offs.

I'm finally getting things going with dtests on a server (after spending hours trying to run them on my laptop).  One of the failures with the num_tokens update with bootstrap.py just needed to have a little more time.sleep - from 5 to 10 seconds.  I'm going through all of them to see if I can fix them in some way and will then see if I can work with someone to get an updated set of dtests running on the jenkins server.

I started down this road but don't think I can get through fixing all of the dtests.  On this line in bootstrap-test.py I changed the time.sleep to 10 seconds and it appears to solve that problem - https://github.com/apache/cassandra-dtest/blob/master/bootstrap_test.py#L485

However there were many tests with replace_address that I'm not sure about.  I don't know how or why replace address would be affected by the new token allocation algorithm.  Dimitar said something about parallel bootstrap but I don't see that - sometimes no_wait or wait_other_notice is true or false so I thought it was that, but perhaps someone more familiar with ccm could see.

I'm sorry - I really want this to get in for the release but I don't have the time to dedicate to learning dtest at a deeper level to fix all of these in time.

[~jeromatron], I'm picking this up.

Initial observation is that on the test_simultaneous_bootstrap test, node2 manages to bootstrap before node3 gets a chance to get kicked off.
I'll go through the Cassandra code paths of bootstrap in order to understand how the new token allocation algorithm impacts us here.

Will send an update soon.

Quick update:

I was able to make the *bootstrap_test.py::TestBootstrap::test_simultaneous_bootstrap* pass with this branch.

The test assumes that both starting nodes will see each other when they check for endpoint collision. But if the nodes start at exactly the same time (or roughly), then they can both perform the check while none of them is gossiping yet, meaning only node1 is part of the ring, which allows them to get tokens and start bootstrapping.

Since there's a 30s pause, waiting for gossip to settle, adding a 10s pause between node2 and node3 startup allows us to "luckily" avoid the race condition.

The code is not bulletproof to this scenario though. 
I still wonder why this is only happening with the new token allocation algorithm. Furthermore, tests are executed with num_tokens = 1, which makes it fairly fast to pick a token.
It seems like the orchestration is different between the random token allocation and the rf based allocation which makes the race condition more obvious.

I'll check the other failing tests tomorrow to see if we're dealing with the same problems. 

bq. then they can both perform the check while none of them is gossiping yet

Just fyi, this is a known limitation, collision checks are best effort since as you've pointed out, there will always be a small window where you can manage to avoid it.

Thanks [~brandon.williams], 

that's valuable information and I can move on to fixing the other tests.

I've identified several issues today:
 * ccm uses a hardcoded 30s timeout when waiting for events (like nodes to start) which doesn't work with the additional wait times that come with the new token allocation algorithm. Fix is [here|https://github.com/riptano/ccm/commit/8a91a5aa49473211863a1fb7a980206e5222ce5d].
 * ccm starts all nodes at the same time when cluster.start() is invoked, which creates clashes when the new token allocation algorithm is used and makes some tests flaky. Starting them sequentially using [this fix|https://github.com/riptano/ccm/commit/e6e4abcff375debde8195104c5cffd1cecb8d6cf], allowed all the bootstrap dtests to pass.
* [~jeromatron]'s branch is missing some commits in the current trunk that fix other failing dtests. Rebasing it over trunk is necessary to get them all to pass
* Adding a few seconds of sleep in [bootstrap_test.py::TestBootstrap::test_simultaneous_bootstrap|https://github.com/adejanovski/cassandra-dtest/blob/master/bootstrap_test.py#L769-L771] allows the test to pass.

I'm currently rerunning all dtests with the various fixes to see if I still get failures. I'll follow up on monday and hopefully push PRs to ccm and cassandra-dtests that will allow the patch to be applied (there are conflicts though so a rebase will be necessary).

A follow up discussion and ticket will probably be necessary because the new token allocation algorithm and concurrent bootstraps aren't working nicely together.

CI run [here|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/247/pipeline/258/]. 
[~adejanovski], could you check if any of [these failures|https://ci-cassandra.apache.org/job/Cassandra-devbranch/247/testReport/] are related?

Based on just the one run (though there was no pipelines running in ci-cassandra.a.o at the time), the total runtime for devbranch tests (trunk based) has gone from ~16hrs to ~27hrs. But, due to parallelisation, the pipeline run times have only increased from ~2hrs to ~2:20 hours. This is not ideal but I think it's worth pushing fixing/further-improving dtest performance to out-of-scope and a separate ticket.

New CI run with some additional adjustements in timings [here|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/248/tests].

The last failing test is fixed in trunk by [this commit|https://github.com/apache/cassandra/commit/c94ececec0fcd87459858370396d6cd586853787]. It's unrelated to this ticket.

I've squashed the commits in [my cassandra-dtests branch|https://github.com/adejanovski/cassandra-dtest/tree/CASSANDRA-13701], but I still need to drop the commit that points to [the patched version of ccm|https://github.com/adejanovski/ccm/tree/CASSANDRA-13701].

Let's wait for the conversation to settle in the ASF Slack before moving on here.
Maybe we should re-run CI again to see if we have some flaky tests that would be related to this ticket?

I ran [~brandon.williams]'s [patch | https://github.com/driftx/cassandra/tree/CASSANDRA-13701] with [~adejanovski]'s [dtest | https://github.com/adejanovski/cassandra-dtest/tree/CASSANDRA-13701] and [ccm | https://github.com/adejanovski/ccm/tree/CASSANDRA-13701] patches with MID res in CircleCI [here | https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/308/workflows/27370106-78be-478d-8138-3add0f46102f].

Dtests run durations changed as follows:

* with vnodes - 22 to 43 minutes
* without vnodes - 18 to 28 minutes  

Linked the follow-on ticket for improving dtest runtime.

I was able to improve runtime of vnode dtests by around 50% on my local machine by [making CCM start nodes in parallel|https://github.com/pauloricardomg/ccm/commit/3b21db1a46b596c2b4850c076e035b5251d7dc39] with a new flag {{-Dcassandra.init.wait_for_live_members}}.

[This flag|https://github.com/pauloricardomg/cassandra/commit/d03956b088e0f408ade607c55182619d593c8519] makes the node wait until a specified number of nodes is live *and* part of the ring before proceeding with bootstrap. This ensures the processes are started in parallel but tokens are assigned sequentially. So the first node is started with {{-Dcassandra.init.wait_for_live_members=0}}, the second node with {{-Dcassandra.init.wait_for_live_members=1}}, the third node with {{-Dcassandra.init.wait_for_live_members=2}} and so on.

A bit hacky but seems to improve runtimes significantly since we can parallelize a big chunk of the startup time. I'm running this on a very slow machine so we might get nicer improvements on a better CI machines.

The good news is that on the non-vnode case the tokens are assigned manually via CCM so we don't need to make nodes start sequentially so the runtimes on the non-vnode case are unchanged.

[~e.dimitrova] would you (or someone with CI access) mind re-running the tests above with the branches below to see how the runtimes look with this change?
 * [cassandra|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-13701]
 * [dtest|https://github.com/pauloricardomg/cassandra-dtest/tree/CASSANDRA-13701]
 * [ccm|https://github.com/pauloricardomg/ccm/tree/CASSANDRA-13701]

(cc [~mck] since this is related to CASSANDRA-16079)

[~pauloricardomg], yeah that comment, this work, is under CASSANDRA-16079. Looks good, will kick off a build.



Final patches (rebased):
 - cassandra [mck/trunk_13701|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_13701]
 - cassandra-builds [mck/13701--num_tokens_16|https://github.com/apache/cassandra-builds/compare/trunk...thelastpickle:mck/13701--num_tokens_16]

Committed [3cfc8502b82ba88da6ffc69fdad476f7fa0819ca|https://github.com/apache/cassandra/commit/3cfc8502b82ba88da6ffc69fdad476f7fa0819ca] and [6ff05f088ccab9a2376d2b83f9ef66a800b0c787|https://github.com/apache/cassandra-builds/commit/6ff05f088ccab9a2376d2b83f9ef66a800b0c787]

