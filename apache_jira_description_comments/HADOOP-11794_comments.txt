Won't changing the unit break hftp?


A new option to distcp could trigger parallel-block copy. It cannot be used with hftp.

Hello,
    I wish to contribute to this issue but I am new to this project.Can you guys give some tips for where to start from

Is anyone working on this feature?


I wanna work on this, hey nikhil .. would you like to discuss

I'm working on this feature right now. Already done writing the code. Testing now.

By default, distcp.copy.by.chunk is set to true in the configuration.  The user can set it to false to use the original distcp. But the type of destination will be checked afterward. distcp.copy.by.chunk will remain true only if the destination file system is the distributed file system.




chop files into chunks before copy, and stitch them back after copy.

chop files into chunks before copy and then stitch them back after copy

-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12474807/MAPREDUCE-2257.patch
  against trunk revision 1082703.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    -1 findbugs.  The patch appears to cause Findbugs (version 1.3.9) to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:


    -1 contrib tests.  The patch failed contrib unit tests.

    -1 system test framework.  The patch failed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/148//testReport/
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/148//console

This message is automatically generated.

-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12474806/MAPREDUCE-2257.patch
  against trunk revision 1082703.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2256 javac compiler warnings (more than the trunk's current 2244 warnings).

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/147//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/147//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/147//console

This message is automatically generated.

imported the missing package

-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12475059/MAPREDUCE-2257.patch
  against trunk revision 1087098.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2256 javac compiler warnings (more than the trunk's current 2244 warnings).

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/150//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/150//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/150//console

This message is automatically generated.

Need to fix the bug in the concat method before using the parallel distcp

fix the findbug warning

>By default, distcp.copy.by.chunk is set to true in the configuration. The user can set it to >false to use the original distcp. But the type of destination will be checked afterward. >distcp.copy.by.chunk will remain true only if the destination file system is the distributed >file system.

This needs to get added to the release notes.

-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12475126/MAPREDUCE-2257.patch
  against trunk revision 1087098.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2256 javac compiler warnings (more than the trunk's current 2244 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/152//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/152//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/152//console

This message is automatically generated.

The failure of the contrib test is not related to the new distcp.

The class FileChunkPair is not really a pair, right? It stores 5 fields.

Can't we somehow unify the if/else in copy()? At least doCopyFile() could use doCopyFileChunks().

FileChunkPair is still src/dst file pairs but with the other 3 fields telling the starting point and offset for the file chunk pairs
Also I merged doCopyFile() and doCopyFileChunks(), now we only have one doCopyFile method.


-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12476634/MAPREDUCE-2257.patch
  against trunk revision 1094093.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2256 javac compiler warnings (more than the trunk's current 2244 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/173//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/173//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/173//console

This message is automatically generated.

    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:59: warning: [deprecation] org.apache.hadoop.mapred.FileSplit in org.apache.hadoop.mapred has been deprecated
    [javac] import org.apache.hadoop.mapred.FileSplit;
    [javac]                                ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:60: warning: [deprecation] org.apache.hadoop.mapred.InputFormat in org.apache.hadoop.mapred has been deprecated
    [javac] import org.apache.hadoop.mapred.InputFormat;
    [javac]                                ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:61: warning: [deprecation] org.apache.hadoop.mapred.InputSplit in org.apache.hadoop.mapred has been deprecated
    [javac] import org.apache.hadoop.mapred.InputSplit;
    [javac]                                ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:63: warning: [deprecation] org.apache.hadoop.mapred.JobClient in org.apache.hadoop.mapred has been deprecated
    [javac] import org.apache.hadoop.mapred.JobClient;
    [javac]                                ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:64: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac] import org.apache.hadoop.mapred.JobConf;
    [javac]                                ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:66: warning: [deprecation] org.apache.hadoop.mapred.Mapper in org.apache.hadoop.mapred has been deprecated
    [javac] import org.apache.hadoop.mapred.Mapper;
    [javac]                                ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:211: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]   private JobConf conf;
    [javac]           ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:738: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]   private static void checkSrcPath(JobConf jobConf, List<Path> srcPaths)
    [javac]                                    ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:831: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]   static private void finalize(Configuration conf, JobConf jobconf,
    [javac]                                                    ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:1096: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]   private static int setMapCount(long totalBytes, JobConf job)
    [javac]                                                   ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:1120: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]   private static JobConf createJobConf(Configuration conf) {
    [javac]                  ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:1148: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]   private static void setReplication(Configuration conf, JobConf jobConf,
    [javac]                                                          ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:1190: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]   static boolean setup(Configuration conf, JobConf jobConf,
    [javac]                                            ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:1562: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]       FileSystem jobfs, Path jobdir, JobConf jobconf, Configuration conf
    [javac]                                      ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:257: warning: [deprecation] org.apache.hadoop.mapred.InputFormat in org.apache.hadoop.mapred has been deprecated
    [javac]   static class CopyInputFormat implements InputFormat<Text, Text> {
    [javac]                                           ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:265: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]     public InputSplit[] getSplits(JobConf job, int numSplits)
    [javac]                                   ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:265: warning: [deprecation] org.apache.hadoop.mapred.InputSplit in org.apache.hadoop.mapred has been deprecated
    [javac]     public InputSplit[] getSplits(JobConf job, int numSplits)
    [javac]            ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:316: warning: [deprecation] org.apache.hadoop.mapred.InputSplit in org.apache.hadoop.mapred has been deprecated
    [javac]     public RecordReader<Text, Text> getRecordReader(InputSplit split,
    [javac]                                                     ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:317: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]         JobConf job, Reporter reporter) throws IOException {
    [javac]         ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:326: warning: [deprecation] org.apache.hadoop.mapred.Mapper in org.apache.hadoop.mapred has been deprecated
    [javac]       implements Mapper<LongWritable, FilePair, WritableComparable<?>, Text> {
    [javac]                  ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:337: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]     private JobConf job;
    [javac]             ^
    [javac] /data/users/rosieli/hadoop_jira/hadoop-mapred-trunk/src/tools/org/apache/hadoop/tools/DistCp.java:617: warning: [deprecation] org.apache.hadoop.mapred.JobConf in org.apache.hadoop.mapred has been deprecated
    [javac]     public void configure(JobConf job)
For the warning added, they are all from using deprecated class.

Shouldn't you change your code to use the class that replaced the deprecated one? :)

the original code was using the deprecated one......like the JobConf, InputSplit....

Maybe it's time to change it to non-deprecated classes.

made change to methods that use deprecated classes.

+1
Patch looks good. Just make sure it passes the QA test. Hadoop QA doesn't seem to have picked up the latest version.

Canceling the patch as it is rather old, and does not apply to trunk any longer.  Dhruba, this patch looks like it has a lot of promise to speed things up during a distcp of large files.  If you no longer want to work on this patch please indicate it so that someone else can pick it up.  If you do want to work on it I would be happy to review it and commit it after your upmerge.

This patch really speeded up distcp. However I am unable to work on this at present. If somebody can take over, that will be great, otherwise I will get back to this one sometime soon.

I'll take a look. I already have a patch that accomplishes the bulk of this. The finishing touches remain.

I'll post a patch shortly.

Thanks for taking this up Mithun!

[~mithun] - I know its been a while, but are you still working on this?

Since HDFS-222 is getting some attention, I feel it would be good to have this as an inbuilt usage of the same (and since Dhruba has already mentioned it is a great improvement to DistCp).

Sorry, I haven't been able to spare the time yet. I'll try make the time, shortly.

since distcp has distcp2, is there a patch exits for distcp2 to copy blocks in parallel?

Hi [~mithun], thanks for your earlier work here. Wonder if you will continue to work on this issue? If not, I'm interested in taking it on. Thanks.
 


[~yzhangal]: Thank you, sir. Please do. Hive has kept me busy enough not to devote time here. I'd be happy to review your work.

I had a patch a couple of years ago which split files on block-boundaries, copied them over, and then stitched them together using {{DistributedFileSystem.concat()}} in a reduce-step. If I can find the patch, I'll ping it to you, but it's not terribly hard to do this from scratch. The prototype had very promising performance.

I look forward to your solution.

Thanks [~mithun]. There is a patch currently attached to this jira, is that the one you referred to in your last comment?


Sorry, no. That's likely [~dhruba]'s work, which might have been based on the DistCp-v1 code. We'll need new code for the DistCp-v2 code (i.e. my rewrite from MAPREDUCE-2765).

Apologies if you've already thought this through. One would need to change the {{DynamicInputFormat#createSplits()}} implementation, which currently looks thus:

{code:java:borderStyle=solid:title=DynamicInputFormat.java}
  private List<InputSplit> createSplits(JobContext jobContext,
                                        List<DynamicInputChunk> chunks)
          throws IOException {
    int numMaps = getNumMapTasks(jobContext.getConfiguration());

    final int nSplits = Math.min(numMaps, chunks.size());
    List<InputSplit> splits = new ArrayList<InputSplit>(nSplits);
    
    for (int i=0; i< nSplits; ++i) {
      TaskID taskId = new TaskID(jobContext.getJobID(), TaskType.MAP, i);
      chunks.get(i).assignTo(taskId);
      splits.add(new FileSplit(chunks.get(i).getPath(), 0,
          // Setting non-zero length for FileSplit size, to avoid a possible
          // future when 0-sized file-splits are considered "empty" and skipped
          // over.
          getMinRecordsPerChunk(jobContext.getConfiguration()),
          null));
    }
    DistCpUtils.publish(jobContext.getConfiguration(),
                        CONF_LABEL_NUM_SPLITS, splits.size());
    return splits;
  }
{code}

You'll need to create a {{FileSplit}} per file-block (by first examining the file's block-size). The mappers will now need to emit something like {{(relativePathForOriginalSourceFile, targetLocation_with_block_number)}}. By keying on the relative-source-paths (+ expected number of blocks), you can get all the target-block-locations to hit the same reducer, where you can stitch them together. 

Good luck. :]

Thanks for offering to pick this up Yongjun Zhang!

Thanks [~mithun] and [~dhruba]!  

There will be some complexity with regard to block size since we now support variable size block (introduced by the append feature). We might need ask NN for the size of all blocks a file has, and avoid have the split boundary at the middle of a block. Another possibility is, to split the block into two if it happens (since now we support multiple size block), I have not looked deeper at this yet.

And I'm thinking we could have one FileSplit per multiple file-blocks, we can make it as an input option to distcp.

Thanks again.


BTW guys, I was thinking (some ideas are inspired by the patch attached to this jira):

1. when creating file listing which used to have one entry like <srcFile, CopyListingFileStatus> for file srcFile, we can create multiple entries for the same file, each entry representing a chunk of the file. And include <offset, chunkLength> as two new members of class CopyListingFileStatus.

2.  we can do some change to the following code in UniformSizeInputFormat.java (probably enabled with a command line switch). In the code below. Right now  we check whether including a file would exceed the bytesPerSplit, and decide whether to include the split. Instead of check the file length, we check the chunk length. If including the chunk make exceed the bytesPerSplit, then we don't include chunk in the split. Otherwise, we include.

We could probably introduce a new ChunkUniformSizeInputFormat class instead of modifying the current one.

{code}
  private List<InputSplit> getSplits(Configuration configuration, int numSplits,
                                     long totalSizeBytes) throws IOException {
    List<InputSplit> splits = new ArrayList<InputSplit>(numSplits);
    long nBytesPerSplit = (long) Math.ceil(totalSizeBytes * 1.0 / numSplits);

    CopyListingFileStatus srcFileStatus = new CopyListingFileStatus();
    Text srcRelPath = new Text();
    long currentSplitSize = 0;
    long lastSplitStart = 0;
    long lastPosition = 0;

    final Path listingFilePath = getListingFilePath(configuration);

    if (LOG.isDebugEnabled()) {
      LOG.debug("Average bytes per map: " + nBytesPerSplit +
          ", Number of maps: " + numSplits + ", total size: " + totalSizeBytes);
    }
    SequenceFile.Reader reader=null;
    try {
      reader = getListingFileReader(configuration);
      while (reader.next(srcRelPath, srcFileStatus)) {
        // If adding the current file would cause the bytes per map to exceed
        // limit. Add the current file to new split
        if (currentSplitSize + srcFileStatus.getLen() > nBytesPerSplit && lastPosition != 0) {
          FileSplit split = new FileSplit(listingFilePath, lastSplitStart,
              lastPosition - lastSplitStart, null);
          if (LOG.isDebugEnabled()) {
            LOG.debug ("Creating split : " + split + ", bytes in split: " + currentSplitSize);
          }
          splits.add(split);
          lastSplitStart = lastPosition;
          currentSplitSize = 0;
        }
        currentSplitSize += srcFileStatus.getLen();
        lastPosition = reader.getPosition();
      }
      if (lastPosition > lastSplitStart) {
        FileSplit split = new FileSplit(listingFilePath, lastSplitStart,
            lastPosition - lastSplitStart, null);
        if (LOG.isDebugEnabled()) {
          LOG.info ("Creating split : " + split + ", bytes in split: " + currentSplitSize);
        }
        splits.add(split);
      }

    } finally {
      IOUtils.closeStream(reader);
    }

    return splits;
  }
{code}

3. The CopyMapper is changed to copy each chunk of the big file, with the chunk offset included in the temporary target file name.

4. Change to CopyCommitter to load the copylisting file and iterate through, and stitch the segments of the same file to the target file. 
This work is not quite distributed. A more elegant solution would be to use reducers to do the stitching as Mithun suggested.

Wonder if this makes sense to you guys.

Thanks.

We probably don't need ChunkUniformSizeInputFormat, and just use UniformSizeInputFormat (When we break large file into chunks, it make the split more uniform), when a file doesn't need to be break into chunks, there is a single entry for it in the fileListing, and we make the entry's chunkLength the same as its file length.  

I was thinking that for the initial implementation, we can just change the CopyCommitter, as I described in last comment, instead of introducing a reducer stage for distcp.

Welcome to comment. Thanks.




Looking at it more, I think we should apply "breaking file into chunks" to both UniformSizeInputFormat and DynamicInputFormat  as an improvement to both strategies, and enable it by command line option initially. Thanks.



HI [~mithun],

Some more thinking to share.

When I commented earlier about  "include <offset, chunkLength> as two new members of class CopyListingFileStatus.", I was thinking the offset, chunkLength at bytes level, inspired by your suggestion "You'll need to create a FileSplit per file-block ", I think we can make them blocks.

That is, we can split the file into chunks, each chunk contains multiple blocks. The chunk is represented as a block range <bgnIdx, numBlocks>, where bgnIdx is the block index of the first block of the chunk, and numBlocks is  the number of blocks in the chunk. A degenerated case is what you suggested: one file-block per split. But I'm making it more flexible here, such that we can support variable number blocks per split.

I'd make the number of blocks per split as a distcp parameter. For a give distcp run, the number of blocks in a split is fixed as specified by the parameter, except for  the last split of a file, which might contain fewer blocks. BTW, introduced by "append" feature, a same file may contain blocks of different size, thus it's not always true that each split will be same size in bytes.

We need some new client-namenode API protocol to get back the locatedBlocks for the specified block range, so the CopyMapper can work on the given block range (possible there will be other application need the similar API). I will create a jira about it.

BTW, I had quite some fun with distcp, but I did not know who is the author of distcp v2, until working on this jira. Appreciate your excellent work!

Thanks.


[~yzhangal],

bq. Appreciate your excellent work!
You're too kind. :]

bq. But I'm making it more flexible here, such that we can support variable number blocks per split.
I agree with the principle of what you're suggesting. Combining multiple splits into a larger split (based on size) is a problem that {{CombineFileInputFormat}} provides a solution for. Do you think we can use {{CombineFileInputFormat}} to combine block-level splits into a larger split?

bq. We need some new client-namenode API protocol to get back the locatedBlocks for the specified block range...
Hmm... Do we? DistCp copies whole files (even if at a split level). Since we can retrieve located blocks for all blocks in the file, shouldn't that be enough? We could group locatedBlocks by block-id. Perhaps I'm missing something.


Thanks [~mithun]!

Not sure about {{CombineFileINputFormat}}, but I will take a look.

{quote}
Hmm... Do we? DistCp copies whole files (even if at a split level). Since we can retrieve located blocks for all blocks in the file, shouldn't that be enough? We could group locatedBlocks by block-id. Perhaps I'm missing something.
{quote}

Sorry I was not clear. This jira is to avoid copying a large single file within one mapper. What's in my mind is to break  large file into block ranges (by a new distcp command line arg), such as (0, 10), (10, 20), ...(100, 4), each entry here is a pair (starting block index, and number of blocks) here, all entries for the same file except the last entry have same number of blocks.  So we could assign the entries of the same file to different mappers (to work in parallel). In order to do this, we can have the API I described to fetch back block locations for the block range. My argument is that fetching all block locations for a file is not as efficient as fetching only the block range the mapper is assigned to work on.

Do you agree that the API would help based on my explanation here? I have done a prototype of the API to fetch block locations of a block range, will try to post it after the holiday. I think there may be other applications that need this kind of API too.

Thanks.



bq. My argument is that fetching all block locations for a file is not as efficient as fetching only the block range the mapper is assigned to work on.

Thank you for explaining. Let me see if I can phrase my questions more clearly than before:

# Would it make sense to include the block-locations within the splits, at the time of split-calculation, instead of the block-ranges? If yes, then we can make do with the API we already have, by fetching locatedBlocks for all files, and grouping them among the DistCp splits. (It is indeed possible that keeping ranges, and using your proposed API on the map-side might be faster. But those map-side calls might possibly also exert more parallel load on the name-node, depending on the number of maps.)

# Naive question: Why do we need to identify locatedBlocks? Don't HDFS files have uniformly sized blocks (within a file)? As such, aren't the block-boundaries implicit (i.e. from {{blockId*blockSize}} to {{(blockId+1)*(blockSize) - 1}})? Can't we simply copy that range of bytes into a new file (and stitch the new files in reduce)?

HI [~mithun],

For your question 2, it's because we now support variable block length, see
https://issues.apache.org/jira/browse/HDFS-3689?focusedCommentId=14277548&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14277548

For question 1, I'm worried that it will take much longer time to prepare the copy listing if we need to get block locations for each file. That's why I think it's easier to defer this to mapper. As you pointed out, indeed this will incur more communication work between mapper and NN, but since it's split among mappers, and the call to NN would be scattered in the lifespan of distcp job, it should be easier then when we prepare copy listing. Plus, we may make mapper cache some block locations if multiple block-ranges of the same file are assigned to the same mapper.

Thanks.




Ah, I finally see. That makes complete sense. Thank you for the pointer to the JIRA.

Also, {{CombineFileInputFormat}} might work with {{UniformSizeInputFormat}}, but it might not with {{DynamicInputFormat}}. Maybe combining a configurable number of blocks (ranges) into splits would be easier to work with.

I see what you're doing, and I agree.

Thanks much [~mithun], I will resume working on this after holiday, wish you a nice one!
 

[~yzhangal] 

Hey, I was wondering, if you're still om this issue?

Yes [~mmukhi_2]. Sorry for the delay incurred due to a critical issue. I will update here soon.

Thanks.


No worries, just wanted to check in. :)

Sorry for the long delay, attaching patch rev 001. 

With this patch, we can pass -chunksize <x> to distcp,  to tell distcp to split large files into chunks, each containing a number of blocks specified by this new parameter, except the last chunk of a file may be smaller.  CopyMapper will treat each chunk as a single file so the chunks can be copied in parallel; And the CopyCommitter concat the parts into one target file.

With this switch, we will enable preserving block size, disable the randomization of entries in the sequence file, disable append feature. We could do further optimization as follow-ups.

Any review is very welcome!

Thanks a lot.

In addition, thanks [~weichiu], [~xiaochen] for assisting in an initial draft we did a while back, and the three of us will be contributers of this jira.



| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 12s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 18s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 19s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 16s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 15s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 13s{color} | {color:orange} hadoop-tools/hadoop-distcp: The patch generated 33 new + 230 unchanged - 11 fixed = 263 total (was 241) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 16s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m  9s{color} | {color:red} hadoop-tools_hadoop-distcp generated 3 new + 49 unchanged - 0 fixed = 52 total (was 49) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 10m 47s{color} | {color:red} hadoop-distcp in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 28m 46s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.tools.mapred.TestCopyCommitter |
|   | hadoop.tools.TestOptionsParser |
|   | hadoop.tools.TestDistCpSystem |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12848419/HADOOP-11794.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux bc131b55cd5f 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 5d8b80e |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11473/artifact/patchprocess/diff-checkstyle-hadoop-tools_hadoop-distcp.txt |
| javadoc | https://builds.apache.org/job/PreCommit-HADOOP-Build/11473/artifact/patchprocess/diff-javadoc-javadoc-hadoop-tools_hadoop-distcp.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11473/artifact/patchprocess/patch-unit-hadoop-tools_hadoop-distcp.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11473/testReport/ |
| modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11473/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Patch rev 002 to fix test failures.


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 22s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m  6s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 24s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 47s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 42s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 17s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 18s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 17s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 11m 38s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 51s{color} | {color:orange} root: The patch generated 26 new + 374 unchanged - 11 fixed = 400 total (was 385) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m  7s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 24s{color} | {color:red} hadoop-tools_hadoop-distcp generated 3 new + 49 unchanged - 0 fixed = 52 total (was 49) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 71m  4s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 11m 46s{color} | {color:green} hadoop-distcp in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 37s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}166m 53s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Timed out junit tests | org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12848700/HADOOP-11794.002.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux f93789aa4d43 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / ccf2d66 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11489/artifact/patchprocess/diff-checkstyle-root.txt |
| javadoc | https://builds.apache.org/job/PreCommit-HADOOP-Build/11489/artifact/patchprocess/diff-javadoc-javadoc-hadoop-tools_hadoop-distcp.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11489/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11489/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-distcp U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11489/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



HI [~mithun],

Thanks much for the earlier discussion. Would you please help doing a review of the patch when convenient?

Thanks and best regards.



Latest patch looks pretty good to me. Just a few small comments from me:

# "randomdize" -> "randomize": {{// When splitLargeFile is enabled, we don't randomdize the copylist}}
# In two places you have basically "if (LOG.isDebugEnabled) { LOG.warn(...); }" You should do {{LOG.debug(...)}} in these places, and perhaps also make these debug messages a little more helpful instead of just "add1", which would require someone to read the source code to understand.
# I think this log message is a little misleading:
{code}
+  CHUNK_SIZE("",
+      new Option("chunksize", true, "Size of chunk in number of blocks when " +
+          "splitting large files into chunks to copy in parallel")),
{code}
Assuming I'm reading the code correctly, the way a file is determined to be "large" in this context is just if it has more blocks than the configured chunk size. This log message also seems to imply that there might be some other configuration option to enable/disable splitting large files at all. I think better text would be something like "If set to a positive value, files with more blocks than this value will be split at their block boundaries during transfer, and reassembled on the destination cluster. By default, files will be transmitted in their entirety without splitting."
# Rather than suppressing the checkstyle warnings, recommend implementing the builder pattern for the {{CopyListingFileStatus}} constructors. That should make things quite a bit clearer.
# There are a handful of lines that are changed that I think are just whitespace, but not a big deal.

HI [~atm],

Thanks a lot for the review! All very good comments!

I uploaded rev 003 to address all of them. In addition, I also added couple of new changes:
1, make sure the target cluster is DistributedFileSystem, otherwise ignore the switch -chunksize with warning message
2. added documentation

Would you please help taking a look again?

Thanks.


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 14s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 14s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 24s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 26s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 11m  9s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 43s{color} | {color:orange} root: The patch generated 26 new + 374 unchanged - 11 fixed = 400 total (was 385) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 49s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 24s{color} | {color:red} hadoop-tools_hadoop-distcp generated 3 new + 49 unchanged - 0 fixed = 52 total (was 49) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 70m 26s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 12m  1s{color} | {color:green} hadoop-distcp in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 37s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}162m 37s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Timed out junit tests | org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12850159/HADOOP-11794.003.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 33eac87c7a0f 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 87852b6 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11537/artifact/patchprocess/diff-checkstyle-root.txt |
| javadoc | https://builds.apache.org/job/PreCommit-HADOOP-Build/11537/artifact/patchprocess/diff-javadoc-javadoc-hadoop-tools_hadoop-distcp.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11537/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11537/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-distcp U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11537/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Wow, this is really good work. (I'm continually astonished at how much DistCp has been improved upon and added to.)
Please forgive me, my DistCp-ese is a little rusty. I have a couple of minor questions:
# In {{DistCpUtils::toCopyListingFileStatus()}}, the javadoc says it {{"Converts a list of FileStatus to a list CopyListingFileStatus"}}. The method does not take a {{List<FileStatus>}}. Shall we remove {{"list of"}}?
# Could we rephrase the doc to {{"Converts a `FileStatus` a list of `CopyListingFileStatus`. Returns either one CopyListingFileStatus per chunk of file-blocks (if file-size exceeds chunk-size), or one CopyListingFileStatus for the entire file (if file-size is too small to split)."}}?
# {{DistCpUtils::toCopyListingFileStatus()}} handles heterogeneous block-sizes via {{DFSClient.getBlockLocations()}}, but only if {{fileStatus.getLen() > fileStatus.getBlockSize()*chunkSize}}. Is it possible for an HDFS file with {{fileStatus.getBlockSize() == 256M}} to be composed entirely of tiny blocks (say 32MB)? Could we have a situation where a splittable file (with small blocks) ends up unsplit, because {{fileStatus.getBlockSize() >> effectiveBlockSize}}?
# I wonder if {{chunksize}} might be confused to be the "chunk-length in bytes" (like {{CopyListingFileStatus.chunkLength}}). I could be wrong, but would {{blocksPerChunk}} be less ambiguous? (Please ignore if this is too pervasive.)
# Nitpick: {{CopyListingFileStatus.toString()}} uses String concatenation inside a call to {{StringBuilder.apend()}}. (It was that way well before this patch. :/) Shall we replace this with a chain of {{.append()}} calls?
# In {{CopyCommitter::concatFileChunks()}}, could we please add additional logging for what files/chunks are being merged?

Thanks so much for working on this, [~yzhangal]. :]

Hi [~mithun],

Thanks you so much for the review and all the good comments!

I just uploaded rev 004 to address all of them.

* To answer your question in 3, to avoid the extra RPC call to get all blocks of a file by ONLY calling the RPC when the file size is bigger than {{blockSize * blocksPerChunk}}, and then check if the number of blocks is bigger than {{blocksPerChunk}}. So it's possible that a file with many small blocks are not split. But I think that should be ok, because the patch here intend to deal with really large file, and variable size blocks are infrequent, this check maybe reasonably good.  However, in the future, we could still improve it if necessary.
* About 6. the logging is already done in the method {{mergeFileChunks}}, when debug logging is enabled.

In addition, I also added one more condition to check if the source FS is DistributedFileSystem, otherwise, the file won't be splitted too.

Wonder if you could take a look at the new patch.

Thanks a lot.




| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 24s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 13s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 48s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 10s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 17s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 12m 31s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 51s{color} | {color:orange} root: The patch generated 27 new + 374 unchanged - 11 fixed = 401 total (was 385) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m  7s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 26s{color} | {color:red} hadoop-distcp in the patch failed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 73m 18s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 11m 57s{color} | {color:green} hadoop-distcp in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 38s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}170m 13s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation |
|   | hadoop.hdfs.TestMissingBlocksAlert |
|   | hadoop.hdfs.server.datanode.checker.TestThrottledAsyncChecker |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12850378/HADOOP-11794.004.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 270130d7026c 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / bec9b7a |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11552/artifact/patchprocess/diff-checkstyle-root.txt |
| javadoc | https://builds.apache.org/job/PreCommit-HADOOP-Build/11552/artifact/patchprocess/patch-javadoc-hadoop-tools_hadoop-distcp.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11552/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11552/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-distcp U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11552/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



rev 005 to address checkstyle and javadoc issues.


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 21s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 56s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 53s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 31s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 14s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 17s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 15m 39s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  9s{color} | {color:orange} root: The patch generated 1 new + 376 unchanged - 11 fixed = 377 total (was 387) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 23s{color} | {color:green} hadoop-tools_hadoop-distcp generated 0 new + 48 unchanged - 1 fixed = 48 total (was 49) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 76m  6s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 12m 18s{color} | {color:green} hadoop-distcp in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 50s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}181m 37s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.TestDecommissioningStatus |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12850464/HADOOP-11794.005.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 69a1c5342d85 3.13.0-105-generic #152-Ubuntu SMP Fri Dec 2 15:37:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / b6f290d |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11554/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11554/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11554/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-distcp U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11554/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 13s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 14s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 17m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 42s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 22s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 30s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 17s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 21s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 17s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 13m 17s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 42s{color} | {color:orange} root: The patch generated 1 new + 375 unchanged - 11 fixed = 376 total (was 386) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 48s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} hadoop-tools_hadoop-distcp generated 0 new + 48 unchanged - 1 fixed = 48 total (was 49) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 71m  5s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 11m 47s{color} | {color:green} hadoop-distcp in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 42s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}169m 47s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.TestStartup |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12850464/HADOOP-11794.005.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 19fc42a38ba4 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / b6f290d |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11555/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11555/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11555/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-distcp U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11555/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



rev 6 to address checkstyle issue, some misc changes
* Improved exception message
* reorder condition checking when deciding whether to split file for better performance
* add two more "__" to the chunk file names



| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 14s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 21s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 12s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 10s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 17s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 16s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 12m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 54s{color} | {color:green} root: The patch generated 0 new + 375 unchanged - 11 fixed = 375 total (was 386) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 55s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} hadoop-tools_hadoop-distcp generated 0 new + 48 unchanged - 1 fixed = 48 total (was 49) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 71m 44s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 12m 20s{color} | {color:green} hadoop-distcp in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 44s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}169m 12s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12850883/HADOOP-11794.006.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux b24de0651763 3.13.0-107-generic #154-Ubuntu SMP Tue Dec 20 09:57:27 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / e023584 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11577/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11577/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-distcp U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11577/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Rev 007:

1. Better -update handling, only copy files that need to be updated
2. Better error handling: if user enable ignore error, then continue after error
3. Do sanity checking in CopyCommitter, to ensure the chunk files are continuous for a given file
4. Corrected implementation of file comparison in unit test, to handle variable size blocks in a file.
5. Added some additional unit tests.

Thanks.


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 19s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 17s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m  6s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m  8s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 26s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 33s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  7s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 18s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 21s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 12m  1s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 42s{color} | {color:orange} root: The patch generated 5 new + 374 unchanged - 12 fixed = 379 total (was 386) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 43s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 47s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} hadoop-tools_hadoop-distcp generated 0 new + 48 unchanged - 1 fixed = 48 total (was 49) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 77m 52s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 11m 59s{color} | {color:green} hadoop-distcp in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 45s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}172m 27s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.TestDFSRSDefault10x4StripedOutputStreamWithFailure |
|   | hadoop.hdfs.TestDistributedFileSystem |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure160 |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12851564/HADOOP-11794.007.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux f551dea4e114 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 2007e0c |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11596/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11596/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11596/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-distcp U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11596/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



very useful improvement

Thanks for the positive feedback [~Lu Tao]!

Hi [~mithun] and [~atm], would you please help taking a look at the latest patch to see if all your comments are addressed?

Thanks.


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 17s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 49s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 11m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 29s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 11s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 16s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 57s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m  8s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  0m 45s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  0m 16s{color} | {color:red} hadoop-distcp in the patch failed. {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 10m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 10m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 49s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} mvnsite {color} | {color:red}  0m 22s{color} | {color:red} hadoop-distcp in the patch failed. {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 18s{color} | {color:red} hadoop-distcp in the patch failed. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 46s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 17s{color} | {color:green} hadoop-tools_hadoop-distcp generated 0 new + 48 unchanged - 1 fixed = 48 total (was 49) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 64m  6s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  0m 25s{color} | {color:red} hadoop-distcp in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 30s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}143m  6s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.web.TestWebHdfsTimeouts |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12851564/HADOOP-11794.007.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 92355ce8312a 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 694e680 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| mvninstall | https://builds.apache.org/job/PreCommit-HADOOP-Build/11702/artifact/patchprocess/patch-mvninstall-hadoop-hdfs-project_hadoop-hdfs.txt |
| mvninstall | https://builds.apache.org/job/PreCommit-HADOOP-Build/11702/artifact/patchprocess/patch-mvninstall-hadoop-tools_hadoop-distcp.txt |
| mvnsite | https://builds.apache.org/job/PreCommit-HADOOP-Build/11702/artifact/patchprocess/patch-mvnsite-hadoop-tools_hadoop-distcp.txt |
| findbugs | https://builds.apache.org/job/PreCommit-HADOOP-Build/11702/artifact/patchprocess/patch-findbugs-hadoop-tools_hadoop-distcp.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11702/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11702/artifact/patchprocess/patch-unit-hadoop-tools_hadoop-distcp.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11702/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-distcp U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11702/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Looks like something changed in the build side, same patch rev7 was almost clean in last run (except for checkstyle, and a few unrelated test failure), but it different errors above.

Uploaded new rev8 which has a fix for totalBytesToCopy.
 

| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 16s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 14s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 26s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 20s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  7s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 11m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 50s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} hadoop-tools_hadoop-distcp generated 0 new + 48 unchanged - 1 fixed = 48 total (was 49) {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 64m 16s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 11m 21s{color} | {color:green} hadoop-distcp in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 39s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}156m  6s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12854375/HADOOP-11794.008.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 8341f7763e65 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 132f758 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11706/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-distcp U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11706/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



[~yzhangal] Can you provide a github link to make review easier?  I am very interested.



Thanks for being interested [~zshao], 

Wonder if you would consider trying patch viewer https://chrome.google.com/webstore/detail/git-patch-viewer/hkoggakcdopbgnaeeidcmopfekipkleg?hl=en-US ? It works nicely in browser, you click on the patch and it show the changes made by the patch side by side.

When patch viewer is not sufficient, I usually review patch by applying the patch and doing "git difftool" with a graphical viewer. I saw most folks upload patches to jira and use this way to view. 

Hope that helps, 


# this is an opportunity to switch distcp over to using the slf4j logger class; existing logging can be left alone, but all new logs can switch to the inline logging
# What does "YJD ls before distcp" in tests mean?
# {{TestDistCpSystem}} does a cleanup in {{testDistcpLargeFile}} as the last operation in a successful test run. Does it still cleanup on a failure? If not, what is the final state of the call & does it matter
# in the s3a tests we now have a -Pscale profile for scalable tests, and can set file sizes. It might be nice to have here, but it's a complex piece of work: not really justifiable except as a bigger set of scale tests


I was trying to evaluate your patch with ADLS:
Tried the bits on a HDInsight 3.5 cluster (this comes with hadoop 2.7)
Observed following compatibility issues:
     a.	You are checking for instance of *DistributedFileSystem* in many places and all other *FileSystem* implementations dont implement *DistributedFileSystem*
          i.	Could this be changed to something more compatible with other *FileSystem* implementations?
     b.	You are using the new *DFSUtilClient*, which makes DistCp incompatible with older versions of Hadoop
         i.	Can this be changed to be backward compatible?
If the compatibility issues are addressed, the DistCp with your feature would be available for other *FileSystem* implementations and also would be backward compatible.

Thanks much for reviewing and trying [~stevel@apache.org] and [~omkarksa]!

{quote}
this is an opportunity to switch distcp over to using the slf4j logger class; existing logging can be left alone, but all new logs can switch to the inline logging
{quote}
Since this jira has been going on for long, I hope we can address logger issue as a separate follow-up jira.

{quote}
What does "YJD ls before distcp" in tests mean?
{quote}
Good catch, I forgot to drop some debugging stuff in test code. will in next rev.

{quote}
Does it still cleanup on a failure? If not, what is the final state of the call & does it matter
{quote}
It does not really matter since the test failed, but cleaning it up would be ok too. 

{quote}
in the s3a tests we now have a -Pscale profile for scalable tests, and can set file sizes. It might be nice to have here, but it's a complex piece of work: not really justifiable except as a bigger set of scale tests
{quote}
Scale test is a good thing to do, the unit of the patch mostly focus on functionality.

{quote}
5.      Observed following compatibility issues:
a.      You are checking for instance of DistributedFileSystem in many places and all other FileSystem implementations dont implement DistributedFileSystem
                                                    i.     Could this be changed to something more compatible with other implementations of FileSystem?
{quote}
The main reason of checking DistributedFileSystem is the support of getBlockLocations, and concat feature. I'm not sure whether we can assume other File System support that.
  
{quote}
b.      You are using the new DFSUtilClient, which makes DistCp incompatible with older versions of Hadoop
                                                    i.     Can this be changed to be backward compatible
{quote}
The current patch is for trunk where client and server code are separated. When we backport this change to other version of hadoop, we can make the change accordingly, for example, to use DFSUtil. 

{quote}
6.      If the compatibility issues are addressed, the new DistCp with your feature would be available for other FileSystem implementations as well as backward compatible.
a.      I was able to make little modifications to your patch and got it working with ADLS.
{quote}
Good work there! Glad to hear that it works for you with little modifications. I think we can probably commit this patch first, and then do other work as improvement jiras.

Thanks again!
 



  



{quote}
The main reason of checking DistributedFileSystem is the support of getBlockLocations, and concat feature. I'm not sure whether we can assume other File System support that.
{quote}
The *getFileBlockLocations* and *concat* are APIs that are part of *FileSystem.java* from [hadoop v1.2.1|https://hadoop.apache.org/docs/r1.2.1/api/index.html]
{quote}
The current patch is for trunk where client and server code are separated. When we backport this change to other version of hadoop, we can make the change accordingly, for example, to use DFSUtil. 
{quote}
You could just use the default constructor that would internally get the NNAddress:
{code}
final DFSClient dfs = new DFSClient(conf);
{code}

Thanks [~omkarksa].

I chose the other method because the following one is marked as Deprecated. Not sure why it was, it seems a convenient API to exist.
{code}
  /**
   * Same as this(NameNode.getNNAddress(conf), conf);
   * @see #DFSClient(InetSocketAddress, Configuration)
   * @deprecated Deprecated at 0.21
   */
  @Deprecated
  public DFSClient(Configuration conf) throws IOException {
    this(DFSUtilClient.getNNAddress(conf), conf);
  }
{code}

About other file systems, I did not get to test various file systems with this patch (except for DistributedFileSystem), we could follow-up with new jira to relax the file system requirement, and add corresponding tests for the corresponding file system?

Hi [~atm], I will address the most recent comments since you reviewed last time. Would you please take a look at rev8 to see if you have additional comments?

Thanks a lot.


Thanks for the clarification [~yzhangal].
About this ...
{quote}
About other file systems, I did not get to test various file systems with this patch (except for DistributedFileSystem), we could follow-up with new jira to relax the file system requirement, and add corresponding tests for the corresponding file system?
{quote}
Is there any reason *not* to use *FileSystem.concat* & *FileSystem.getFileBlockLocations* ?

bq. Is there any reason not to use FileSystem.concat & FileSystem.getFileBlockLocations ?

{{FileSystem.getFileBlockLocations }} is something filesystems have to implement this otherwise basic client code fails; if they don't have locality they tend just to say "localhost" and 1 block.

Concat though, barely implemented. As the [FS spec says|http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-project-dist/hadoop-common/filesystem/filesystem.html#void_concatPath_p_Path_sources], "This is a little-used operation currently implemented only by HDFS"

Looking for subclasses of FileSyste.concat() It looks like only: hdfs, webhdfs, httpfs, filterfilesystem do lt. 

supporting webhdfs would be really good, as its the one recommended for cross-hadoop version distcp, and long-haul.

For now, how about have it it check for HDFS and webhdfs and rejects if anything else.





One of the problems we have here is there's no API for impls to declare what they do; HADOOP-9565 has discussed this, but it's stalled. As it is, no way to determine if an FS implements a feature unless probed.

There is always the option of doing that: sending in an invalid concat() request and differentiating between: UnsupportedException and any other response, then assuming that the "any other response" exception means that it is implemented, but that the arguments were invalid. concat("/", new Path[0]) should be enough.

Omkar, are you planning to do a new concat? Because it might be that for different filesystems, there are better things to do.
For S3, we could attempt to do multipart PUT operations in parallel, though that would be somewhat complicated by the fact you need to know the request ID before any part of the operation begins. If you were doing the upload from a single machine, the block output stream writes data in blocks now anyway.

I don't know about other object stores, but we can and should think about how best to support them, even assuming their parallel upload mechanisms are similarly unique. It may be that the code needs to be worked to support different partitioning & scheduling for different endpoints. FWIW, I've been contemplating what it would take to do one in Spark, because that might let me get away with starting the upload before even the listing has finished, and reschedule work where there is capacity, rather than decide up front how to break things up. Supporting parallelised chunk upload wasn't something I'd considered though. An extra complication.

[~steve_l] concat is implemented by ADLS backend as a constant operation.

Thanks [~steve_l], [~jzhuge],

Yes, this could be one way to do it. Let me see if there is any other way.
{quote}
There is always the option of doing that: sending in an invalid concat() request and differentiating between: UnsupportedException and any other response, then assuming that the "any other response" exception means that it is implemented, but that the arguments were invalid. concat("/", new Path[0]) should be enough.
{quote}

That's right ADLS supports both *concat* and *getFileBlockLocations* and as commented before in this JIRA, I was able to get this patch working with ADLS with these changes and one more change.

I'm less worried about GFBL: everything at least makes something up there. If concat works in ADLS then it should be supported.

Given this feature only turns on if blocksperchunk is set, then maybe we should just say "if you enable that, your destination had better support concat". That could be added in the --help usage. the {{concat}} call can be added by a catch UnsupportedException handler, which tells the caller to stop it.

The test for this should go alongside/inside AbstractContractDistCpTest then, so that it can be tested in all filesystems which support concat 


Hi [~omkarksa], [~steve_l] [~jzhuge],

Thanks for the good discussion here. All are very good comments!

I personally prefer a step by step approach, this jira can be a foundation to add new additional support and improvement, such as webhdfs, ADLs. It's easier to manage that way. The new jira will not only include the code change (even if it's minimum), but also some unit tests. 

Omkar, what about we create a follow-up jira for ADLs, and we work together on the jira after this jira is in?

Steve, I will also create a follow-up jira for webhdfs.

Thanks.





the thing is: if we remove the checks for FS type, we don't increase code complexity, it's simplified: one less check. Just have the catch of the exception go to logging something

I agree with [~stevel@apache.org]. If the FileSystem doesn't support concat, then allowing the job to fail is a reasonable "foundation".

Hi [~steve_l], [~chris.douglas],

Thanks for the feedback.

I think if we know the job will fail, we want it to fail sooner rather than later. That's why I had the DistributedFileSystem check in the beginning of distcp.
Imagine if we run the job half way and found it doesn't work, we not only wasted computing power, but also possibly left the cluster in an inconsistent state. 

So I think we should check if file system support getBlockLocations and concat in the very beginning of distcp. That in my opinion, can be done as a follow-up jira, because supporting a different filesystem involves not only the check here, but also new unit tests for the corresponding file systems, and system level testing.  Does this make sense to you?

Thanks.







[~omkarksa], can you post your patch?

I see your point, [~yzhangal], but shouldn't the cleanup/rollback code handle the inconsistency? Moreover, doesn't distcp also use append to support sync, without first verifying that the destination FS supports it? Wasted cycles are unlikely; this doesn't work inconsistently, it fails 100% of the time for unambiguous reasons. Surely someone would test this option before trying it on a significant deployment.

To fail before submission, this could use {{concat}} during job setup if enabled e.g., parallelize the scan and concatenate the result for the input file [1]. More generally, distcp could add a phase to job setup that tests that the options are consistent with the capabilities of the src/dst FileSystems, but _that_ would be an extension.

The early check makes sense, but false positives are worse than false negatives, here.

[1] Unfortunately, SequenceFile (format for distcp) would need some modifications to make that straightforward. There's an option to omit the header if the file already exists, but not one that explicitly and independently suppresses it.

Had a discussion with [~atm] (thanks ATM), we now agreed on an approach that

- relax the file system checking,
- adding the doc as [~steve_l] commented, stating concat need to be supported when the feature is turned on
- creating follow-up jiras to add unit tests and validation for ADLS and other file systems
- for file systems that don't support getBlockLocations or concat,  create jiras to let them fail the run sooner.

Please be aware of the situation I pointed out in my last comment, that if user enabled this feature for file systems that don't support concat, distcp may fail in the middle of a run and make the file system into an inconsistent state.

Does that sound good to all?

Thanks.



HI [~chris.douglas],

Sorry did not see your last post before I did my previous comment and I had to be offline for some time. 

In a case that source support getBlockLocations and target doesn't support concat, the current patch would split a file and copy them into chunk files at target, then at commit stage we will find concat doesn't work, thus the target is polluted (inconsistent state). At this point, distcp may have been running for very long time. To remedy that, I will add a "concat" check at the same place where I check DistributedFileSystem and catch UnsupportedOperationException (as [~stevel@apache.org] suggested). This will address the last item in my previous comment.

Thanks.


bq. the current patch would split a file and copy them into chunk files at target, then at commit stage we will find concat doesn't work, thus the target is polluted (inconsistent state)

Doesn't this imply that FileSystems that support concat can also be left in an inconsistent state? If the concat operation fails, the job is killed/fails/dies, etc. then distcp cleanup should remove partial work. If a FileSystem doesn't support concat, shouldn't that failure follow the same path?

Either way, +1 on the approach you outline. Thanks for considering the feedback and updating the patch, [~yzhangal]. Looking forward to this enhancement; it's been a long time coming!

Thanks [~chris.douglas].

Uploaded rev9 as outlined. Would all reviewers please take a look? thanks a lot!

Would appreciate if you could help testing ADLS with this version, [~omkarksa]!

With this patch:
- removed DistributedFileSystem checking
- in order to enable the feature, the source FS need to implements getBlockLocations and the target FS implements concat
- check concat support at the beginning of distcp, throw exception when -blocksperchunk is passed and concat is not supported 

{quote}
Doesn't this imply that FileSystems that support concat can also be left in an inconsistent state? If the concat operation fails, the job is killed/fails/dies, etc. then distcp cleanup should remove partial work. If a FileSystem doesn't support concat, shouldn't that failure follow the same path?
{quote}
Yes, indeed. However, if we run the same job again successfully, it will clean up the temporary chunk files. We could clean up the chunk files when concat fails if we really want. However, given concat is supported, if concat failed, we need to know why it failed, keeping the chunk files would help debugging. If the files are good, we have the potential of concat them manually.  If distcp failed in the middle for other reason,  the source and target will be different anyways.

Thanks.


[~yzhangal] Thanks for reconsidering the comments and making the required changes for FlieSystem compatibility.
[~steve_l], [~jzhuge], [~atm], [~chris.douglas] Thanks for providing clarity and a way ahead.

{quote}
Omkar Aradhya K S, can you post your patch?
{quote}
I made only the following changes in my patch to get it working with ADLS on hadoop 2.7:
# Remove all the checks for *DistributedFileSystem*
# Use {code}fs.concat{code} instead of {code}dstdistfs.concat{code}
# Use {code}fs.getFileBlockLocations{code} instead of {code}dfs.getBlockLocations{code}
# Use (now deprecated) {code}final DFSClient dfs = new DFSClient(conf);{code} instead of {code}final DFSClient dfs = new DFSClient(DFSUtilClient.getNNAddress(conf), conf);{code}

[~yzhangal] Once the new patch with all above changes is checked in, we need to back port it to older versions of hadoop, which will be addressed by new JIRAs?

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 16s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 14s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 18s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 22m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 22s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 10s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 17s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m 49s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red} 15m 49s{color} | {color:red} root generated 1 new + 777 unchanged - 0 fixed = 778 total (was 777) {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  1s{color} | {color:orange} root: The patch generated 5 new + 377 unchanged - 12 fixed = 382 total (was 389) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 25s{color} | {color:green} hadoop-tools_hadoop-distcp generated 0 new + 48 unchanged - 1 fixed = 48 total (was 49) {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 63m 52s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 12m 50s{color} | {color:green} hadoop-distcp in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 41s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}171m 55s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12860084/HADOOP-11794.009.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 314982f8157f 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 59d6925 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| javac | https://builds.apache.org/job/PreCommit-HADOOP-Build/11893/artifact/patchprocess/diff-compile-javac-root.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11893/artifact/patchprocess/diff-checkstyle-root.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11893/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-distcp U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11893/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



I like the direction here. I undertstand while a fail-fast is good: otherwise it's the workers failing and the problem with reporting.That's something to consider later. For now: Caller had better know about the destination.

Looking forward to seeing this in, and thanks to everyone engaged in testing it

Omkar: if ADL doesn't implement the distcp contract test, you might want to follow up this patch with a distcp test that forces the use of the concat operation.

Thanks much all!

[~omkarksa], 

All the changes except one you need was already in rev9. I will add it in next rev.

All:
About
{code}
  /**
   * Same as this(NameNode.getNNAddress(conf), conf);
   * @see #DFSClient(InetSocketAddress, Configuration)
   * @deprecated Deprecated at 0.21
   */
  @Deprecated
  public DFSClient(Configuration conf) throws IOException {
    this(DFSUtilClient.getNNAddress(conf), conf);
  }
{code}
I am not sure why this API is marked deprecated, it seems a good convenient API to exist. Wonder if we should remove the deprecation?

Thanks.


Uploaded rev10 to address remaining stuff. 

About the API mentioned in my last comment, it turned out we don't need to use it so let's leave it as is.

Thanks.


bq. Once the new patch with all above changes is checked in, we need to back port it to older versions of hadoop, which will be addressed by new JIRAs?

This is usually handled in the same ticket, and by cherry-picking the patch. Backporting doesn't usually warrant a new JIRA unless the implementation is significantly different.

bq. I am not sure why this API is marked deprecated, it seems a good convenient API to exist. Wonder if we should remove the deprecation?

If it was deprecated in 0.21, evidently we're not serious about removing it.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 17s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 18s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m 45s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 22m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  7s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 30s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 38s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 12s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 18s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 17m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 17m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 23s{color} | {color:green} root: The patch generated 0 new + 377 unchanged - 13 fixed = 377 total (was 390) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 56s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 27s{color} | {color:green} hadoop-tools_hadoop-distcp generated 0 new + 48 unchanged - 1 fixed = 48 total (was 49) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 76m 13s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 13m 46s{color} | {color:green} hadoop-distcp in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 44s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}192m 12s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure000 |
|   | hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12860188/HADOOP-11794.010.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 5de3607cd1d0 3.13.0-105-generic #152-Ubuntu SMP Fri Dec 2 15:37:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 595f62e |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11898/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11898/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-distcp U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11898/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



The failed tests succeeded in local run. They look flaky and are not relevant to the change here.



{quote}
This is usually handled in the same ticket, and by cherry-picking the patch. Backporting doesn't usually warrant a new JIRA unless the implementation is significantly different.
{quote}
[~chris.douglas], [~yzhangal] Thanks for all the clarifications.

+1 overall, though the DistCp docs currently claim:
{noformat}
Both the source and the target FileSystem must be DistributedFileSystem
{noformat}

With the relaxed check, this could read "The target FileSystem must support the {{FileSystem#concat}} operation".

[~omkarksa], you may want to verify the latest patch works for ADLS.

Thanks [~chris.douglas], the doc was already addressed in rev10. Maybe you were looking at the -rdiff section which is not relevant to the patch here? Thanks.


Hi [~omkarksa],

I'd really appreciate if you could try out rev10 and confirm that it works with ADLS. 

Thanks  a lot.

bq. Maybe you were looking at the -rdiff section which is not relevant to the patch here?
You're right. Sorry, I misread the diff.

No problem and thanks for confirming [~chris.douglas].


Hi Yongjun,

I am on vacation till tomorrow.
Would it be late if I review it tomorrow?
If you are held up because om me, I can reach home and try it today itself. Please let me know.

Regards,
Omkar



No problem to test it out after you are back from vacation [~omkarksa], many thanks!



[~yzhangal] I have tested the patch with ADLS and it works without any changes. Thanks.

Thank you so much and great to hear  [~omkarksa]!

Hi [~chris.douglas] [~atm], your recent review indicates a very close +1, would you please help taking a look at the latest rev10 to see it all looks good? If so, we can get the patch in and follow-up with additional work if necessary. Other folks are welcome to review too. Thanks!



I skimmed the patch, looks reasonable. +1

Thanks for all the followup, [~yzhangal].

Thanks a lot [~chris.douglas]! will commit soon.


SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11505 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11505/])
HADOOP-11794. Enable distcp to copy blocks in parallel. Contributed by (yzhang: rev 064c8b25eca9bc825dc07a54d9147d65c9290a03)
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListingFileStatus.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* (edit) hadoop-tools/hadoop-distcp/src/site/markdown/DistCp.md.vm
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* (edit) hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* (edit) hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java
* (edit) hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java


I just committed to trunk. Will work on branch-2 version asap (tried and see quite some conflicts).

Many thanks to many people! [~dhruba] for reporting the issue, [~rosieli] for the very initial patch  (MAPREDUCE-2257), [~weichiu] and [~xiaochen] for the assistance when I worked on the initial patch of HADOOP-11794, [~mithun], [~atm], [~steve_l], [~chris.douglas] for the review, [~omkarksa] for reviewing and testing with ADLS, [~andrew.wang] and [~jzhuge] for the discussion!



SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11506 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11506/])
Revert "HADOOP-11794. Enable distcp to copy blocks in parallel. (yzhang: rev 144f1cf76527e6c75aec77ef683a898580f3cc8d)
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* (edit) hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* (edit) hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* (edit) hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListingFileStatus.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java
* (edit) hadoop-tools/hadoop-distcp/src/site/markdown/DistCp.md.vm
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
HADOOP-11794. Enable distcp to copy blocks in parallel. Contributed by (yzhang: rev bf3fb585aaf2b179836e139c041fc87920a3c886)
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* (edit) hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* (edit) hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListingFileStatus.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* (edit) hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* (edit) hadoop-tools/hadoop-distcp/src/site/markdown/DistCp.md.vm
* (edit) hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java


[~yzhangal] Thanks for re-considering the suggestions and re-doing the patch to accommodate all FileSystem implementations.
{quote}
I just committed to trunk. Will work on branch-2 version asap (tried and see quite some conflicts).
{quote}
Could you please elaborate on how you plan to proceed with backporting?


Welcome [~omkarksa], I will be working on backporting to other branches asap. Do you have specific expectations?

Just to clarify, I did not mean not to consider supporting other file system, rather, I was suggesting working on that as a separate jira. Your help on the testing out ADLS, together with [~steve_l]'s suggestion about checking concat support (UnsupportedException) made it easier for us to relax the file system constraint in this jira. So thank you guys again!

BTW, Steve still has an item for you to follow-up here:-)

https://issues.apache.org/jira/browse/HADOOP-11794?focusedCommentId=15938217&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15938217

Thanks.





{quote}
BTW, Steve still has an item for you to follow-up here

https://issues.apache.org/jira/browse/HADOOP-11794?focusedCommentId=15938217&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15938217
{quote}
[~yzhangal] Sorry for the late reply. Thanks for pointing this out. I almost missed this!

{quote}
Omkar: if ADL doesn't implement the distcp contract test, you might want to follow up this patch with a distcp test that forces the use of the concat operation.
{quote}
[~steve_l] I will look into this.


Welcome and thanks [~omkarksa].

BTW, I have a branch-2 version of HADOOP-11794, for which I backported HADOOP-13626 to branch-2 to make things cleaner. Hi [~chris.douglas], do we have any concern not to have HADOOP-13626 in branch-2 earlier? if not, I will commit it (it's a clean one), then post the branch-2 version here.

Thanks.


HI [~chris.douglas],

Thanks for confirming that HADOOP-13626 can be put into branch-2 and I have just committed it. Now uploaded branch-2 patch for this jira here. Resolved some misc conflicts, would you please help taking a look?  

Hi [~omkarksa], wonder if you could help run this branch-2 patch on ADLS too if possible?

Thanks a lot!



| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  8s{color} | {color:red} HADOOP-11794 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HADOOP-11794 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12862192/HADOOP-11794.010.branch2.patch |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12035/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



{quote}
Hi Omkar Aradhya K S, wonder if you could help run this branch-2 patch on ADLS too if possible?
{quote}
[~yzhangal] Sure, I will finish testing this by early next week.

{quote}
Yongjun Zhang Sure, I will finish testing this by early next week.
{quote}
[~yzhangal] I was able to do some basic tests and it works! Thanks for the patch.

The branch-2 is *2.9.0*. However, will this patch work on older versions like *2.2.x*?

Omkar: what version are you looking at? We could talk about a backport to 2.8.1, but given it's a feature I don't see it being pulled back any earlier

[~steve_l] I was able to test the bits with HDI 3.3, which is *2.7.1*. 
However, I was wondering if we can go as back as *2.5.x*/*2.2.x*?

bq. Steve Loughran I was able to test the bits with HDI 3.3, which is 2.7.1. 

good. Something related to backporting that can be handled.

bq.  if we can go as back as 2.5.x/2.2.x?

Sorry, we don't go near that, especially for a new feature. 2.7 is pretty much the limit for backports, unless someone using 2.6 needs something. Twitter do, which is why they lead the 2.6.x releases.

Thanks much [~omkarksa] and [~steve_l].

Sorry I was out for a few days. I just committed to branch-2.




Thanks. We'll probably pick this up with a 2.9 release. Exciting!

Updated branch-2 patch HADOOP-11794.010.branch2.002.patch to fix TestDistCpOptions failure.



Hi guys, I noticed that the patch for branch2 targets to version 2.9.x and we have the same requirement for version 2.8.x. I tried to backport the patch to branch-2.8.2 and found most of the code is compatible. I filed a new [issue|https://issues.apache.org/jira/browse/HADOOP-14866] and the patch is available. Please take a look if you are interested.

Hi [~yzhangal], I have a doubt that whether there is a bug in RetriableFileCopyCommand.copyBytes( ) when enable blocksperchunk .


{code:java}
 int bytesRead = readBytes(inStream, buf, sourceOffset);
 while (bytesRead >= 0) {
 if (chunkLength > 0 &&
     (totalBytesRead + bytesRead) >= chunkLength) {
   bytesRead = (int)(chunkLength - totalBytesRead);
   finished = true;
 }
 totalBytesRead += bytesRead;
 //action == FileAction.APPEND always false when blocksperchunk >= 1,
 //the sourceOffset will never been change
 //readBytes(inStream, buf, sourceOffset) will always start from the same position 'sourceOffset'
 //so the buf[] always read same data, and result in wrong split copy finally
 if (action == FileAction.APPEND) {
   sourceOffset += bytesRead;
 }
 outStream.write(buf, 0, bytesRead);
 updateContextStatus(totalBytesRead, context, source2);
 if (finished) {
   break;
 }
 bytesRead = readBytes(inStream, buf, sourceOffset);
 }
{code}


I merge this patch to branch *cdh5.10*, and when I set blocksperchunk >= 1, the copy of data will be different form source.
 I debug and find this problem, and when I modify the condition as follow, it works.
{code:java}
if (action == FileAction.APPEND || (source2.isSplit() && sourceOffset > 0)) {
  sourceOffset += bytesRead;
}{code}


I am not sure whether it is a bug or just caused by version, please look at it if possible, thanks.



Hi[~yzhangal]
After using the blocksperchunk to copy a single file in HDFS the checksum is not matching. Is this excepted?



If I try to use DistCp without blocks per chunk the checksum of the file matches. Am I missing something here?

[~sahilnagpal]: is this HADOOP-16049 ? if not: file a new JIRA giving the exact hadoop version you are encountering this problem on

Yes,HADOOP-16049is the issue I am encountering as well. Thanks for pointing this out [~stevel@apache.org]. I see that this is fixed in branch-2 and branch-2.9. I will try to use branch-2.9.

