Seems to be dup of HBASE-6690

@Ted marked 6690 as a dup of this since this stayed open :)

Here's the link to the overall history (definitely flapping):
* https://builds.apache.org/job/HBase-TRUNK/3293/testReport/junit/org.apache.hadoop.hbase.backup.example/TestZooKeeperTableArchiveClient/testMultipleTables/history/

And the most recent failure:
* https://builds.apache.org/job/HBase-TRUNK/3299/testReport/org.apache.hadoop.hbase.backup.example/TestZooKeeperTableArchiveClient/testMultipleTables/

Attaching patch. Dramatically simplified the test to ensure the same code path is reused between each test as much as seemed reasonable. 

Ran consecutively 20x on my local machine without seeing a failure. Also, seemed to be a bit faster than the original test, so that's nice.

Committed to trunk after trying it locally.  Thanks for the patch Jesse.

Integrated in HBase-TRUNK #3312 (See [https://builds.apache.org/job/HBase-TRUNK/3312/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps (Revision 1381852)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java


I reverted the patch.  This fail of the trunk build prompted by this commit is suspicious given its a fail in TestZooKeeperTableArchiveClient: 

https://builds.apache.org/view/G-L/view/HBase/job/HBase-TRUNK/3312/testReport/junit/org.apache.hadoop.hbase.backup.example/TestZooKeeperTableArchiveClient/testArchivingOnSingleTable/

What you think Jesse?  Related to your change or another failure mode not yet handled?

See if hadoopqa will run it now.

Looks like a missed failure mode. Taking a look... thanks stack!

Thanks for taking a look J.

Integrated in HBase-TRUNK #3313 (See [https://builds.apache.org/job/HBase-TRUNK/3313/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps; REVERT (Revision 1381877)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java


Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #164 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/164/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps; REVERT (Revision 1381877)
HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps (Revision 1381852)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java

stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java


New version - this time we ignore it if we can't compact the files and just make sure there are some files that have been archived. 

I was trying both in the original and the previously attached patch to ensure that we had enough files before trying the compaction, but its just easier to try-and-fail than be defensive about it (though slightly more brittle, its unlikely to change anytime soon...and much simpler to reason about code-wise).

Also ran it 10x locally (and another 10x before rebasing onto trunk) without issue, though that clearly doesn't mean all that much.

-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12544304/hbase-6707-v1.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause mvn compile goal to fail.

    -1 findbugs.  The patch appears to cause Findbugs (version 1.3.9) to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

     -1 core tests.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.io.hfile.TestForceCacheImportantBlocks
                  org.apache.hadoop.hbase.master.TestSplitLogManager

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/2820//testReport/
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/2820//console

This message is automatically generated.

In it goes again.  Thanks for the patch Jesse (I tried it locally and it passed).

Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #177 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/177/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps (Revision 1385388)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java


Integrated in HBase-TRUNK #3340 (See [https://builds.apache.org/job/HBase-TRUNK/3340/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps (Revision 1385388)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java


Reopening again.  I reverted the patch because I got this fail on trunk build: https://builds.apache.org/view/G-L/view/HBase/job/HBase-TRUNK/3342/  If you get a chance, take a looksee mighty Jesse.  Thanks.

@stack yeah, I'll take a look. This is getting really frustrating :-/

[~jesse_yates] Deep breaths.

Integrated in HBase-TRUNK #3343 (See [https://builds.apache.org/job/HBase-TRUNK/3343/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps; REVERT AGAIN (Revision 1386748)

     Result = SUCCESS
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java


Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #178 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/178/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps; REVERT AGAIN (Revision 1386748)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java


Attaching a completely reworked version. 

I got sick of having to deal with all the uncertainty in how the region was doing things (compactions and flushes never seemed to be exactly right, causing the flapping).  Instead, I ended up mocking out basically all the important parts around the HRegion and using those to actually run an HRegion independently of anything else, with full support of flushes and compactions (though no splitting). I'm a lot more sure that this won't flap.

This is a bit on the larger side, so it might warrant an spin on RB, but I'll leave that up to reviewers. 

Oh, and the changed test passed 20x locally (not that that means much with this test, but still pretty good).

Pull MockRegionServer up to top level in test now its used by master package and top-level package.  It was in master package because that was only place it was needed until now.

I will hold my nose when I commit stuff like this:

{code}
+  /**
+   * Exposed for testing!
+   */
+  public CompactSplitThread getCompactSplitThread() {
+    return this.compactSplitThread;
+  }
{code}

Rather than comment out, remove:

{code}
-    UTIL.createTable(TABLE_NAME, TEST_FAM);
+    // UTIL.createTable(TABLE_NAME, TEST_FAM);
{code}

That mocked hregion looks like it could be generally useful.  Nice.

Good on you Jesse.



Nice effort, Jesse.

bq. compactions and flushes never seemed to be exactly right, causing the flapping
Can you be more specific about the above ? If you spot incorrect behavior, we'd better fix it :-)

A far more conservative patch than the last one. 

Thanks Ted for calling me out on mocking out ALL THE THINGS. Definitely a more brittle approach (the logic was that the core compaction/split stuff doesn't change all that often :) to just get a couple region to pump out a few hfiles and compactions.

This new version just creates a simple HRegion, 2x writes a single key to create an hfile via flushing and does a compaction to force hfile archiving. Then it does the real checking with the LongTermHFileArchiver.

This is technically a smaller scope than the original test, but its ok because (1) it tests real hfiles that follow the standard archival path, (2) uses a real HFileCleaner to run the cleaner.

As long as the HFileCleaner and the rest of the system works correctly (which is why we have other tests), then this test is sufficient to cover the validity of the example classes.

Seem reasonable?

bq. compactions and flushes never seemed to be exactly right, causing the flapping

Meant there that you can end up with a race condition where you load and flush the region, then try and trigger a compaction via the Store interface which then fails because it doesn't have enough store files because the region compacted them away from under you. The problem was the 'correct' way to orchestrate a region and compactions wasn't obvious - just took a bit more poking around beyond the obvious methods.

Thanks for the quick turn around, Jesse.

bq. The problem was the 'correct' way to orchestrate a region and compactions wasn't obvious
This is important for future test cases. Mind logging a JIRA ?

bq. 2x writes a single key to create an hfile via flushing and does a compaction to force hfile archiving
I think writing rows with different row keys is desirable. The closer the test case is to real usage, the higher confidence we have for this feature.

Putting up patch on review board would facilitate review comments.

[~stack] bq. I will hold my nose when I commit stuff like this:

Wouldn't be necessary if we roll in HBASE-5456 ;)

bq. I think writing rows with different row keys is desirable. The closer the test case is to real usage, the higher confidence we have for this feature.

Why? The real use case here is to not delete hfiles based on the zookeeper initiated update the the cleaner.  Seems like it covers it. I'd argue that it even covers the case even if I didn't use an hregion, but just put files in the archive in the right directories - this way is just gravy (and little future proofing).


bq. Putting up patch on review board would facilitate review comments.

Added review here: https://reviews.apache.org/r/7344/

bq. This is important for future test cases. Mind logging a JIRA ?

What are you thinking for this? More information in the book or updating the javadocs?

Updating javadocs or enriching hbase book would be good for the 'correct' way to orchestrate a region and compactions.

Now there seems to be another dimension to consider: TestZooKeeperTableArchiveClient is failing in trunk (maybe due to switch to jdk 1.7)
How would we know that the fix really works after it gets checked in ?

[~tedyu@apache.org] why would that change anything? It was already flaky, so we know the fix works when the test passes...?

bq. so we know the fix works when the test passes
TestZooKeeperTableArchiveClient has been failing in trunk builds 3397 and 3398, but not in build #3396 where jdk 1.6 was used.

Yeah, but the test didn't always passing so we try to fix it, and then if it flaps try fixing it again. The nature of flappers is such that you can' be sure that it will always pass anyways (see the above comments on this thread), so its more of a wait and see. However, if the test is always failing and then we fix it such that its not, that's a net win.

bq. if the test is always failing and then we fix it such that its not, that's a net win.
I am still trying to install jdk 1.7 on my MacBook.
It would be nice to confirm that this fix makes TestZooKeeperTableArchiveClient pass on jdk 1.7 reliably, before integration.

Thanks

bq. It would be nice to confirm that this fix makes TestZooKeeperTableArchiveClient pass on jdk 1.7 reliably, before integration.

+1 Wouldn't HadoopQA ensure that?

It's open to discussion.
If QA run returns with 196 test failures (the current number of failures on trunk), how do we interpret the test results ?

[~jesse_yates] Mind updating patch for trunk?  It seems to have rotted.  I want to commit it because flakey TestZooKeeperTableArchiveClient is messing up clean unit test build on jdk7.  Thanks boss.

As of build #3400, there were 198 test failures.
TestZooKeeperTableArchiveClient has been failing in the last 4 builds.

Attaching new version with Ted's comments addressed on trunk. Ran 10x locally without issue.

Thanks for the quick response, Jesse.
My laptop is in Corp IT department.

I will go over latest patch tomorrow.

that was 10x with jdk6 - looks like all the failures on jenkins are due to connection loss issues due to ZK in the mini-cluster setup. Getting the same issue locally and for other tests that also use ZK.

bq. Getting the same issue locally and for other tests that also use ZK.
Isn't the above a blocker for this JIRA in terms of passing trunk build ?

[~tedyu@apache.org] yup. I'm looking into it now... do you know of another jira for that issue?

bq. Isn't the above a blocker for this JIRA in terms of passing trunk build ?

Nope.  The zk failures seem to be related to jdk7.

I'll take a look at those.  Don't worry Jesse.


[~jesse_yates] This test is jinxed.  Ran v4 here on linux:

{code}
Running org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient



Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 412.066 sec <<< FAILURE!

Results :

Failed tests:   testMultipleTables(org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient): Archived HFiles (hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam) should have gotten deleted, but didn't, remaining files:[hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/6d6af62bad2d453fb389ac55a92bd9d1, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/6e1c742116e04935835898773d817e8d, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/7250d611f0474075af64b06892526c43, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/766fe140ea944257ad6986a1ec98fe5f, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/78b4db0bc6bc4196bde9ba6e2af9d5c6, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/7db62f8705824a0eb92321228f90099b, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/8555f37b352042bc95430bae2ad1e4f1, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/86017a9d5e854963805cf6a538e958e9, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/86468e76f1004f418a71e2377b0daa92, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/8757d514111347a9bab1c62cbdeeda7c, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/8d0adef159bb4a919344bbc711e7b072, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/90112e511a58407d984e5eb82a895aff, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/90fc97bd0ae448d88835d4c0f478d561, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/962e398c7f3549dcbf2b11544bff2244, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/98c79c1bab9d40c3bb57cf5b107446d5, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/9b40e32e48de4cb7ab16177839dd355a, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/9ca1e31e01554a8c9b8b58adb5a99046, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/9e0b166d8fed4026a1dcbde892f31587, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/a08c7abcb5d2422e9d6a7426165ab604, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/a1a16ee6f1444f0a93f2df91f4becab2, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/a48ea330d5c44fa1b0191ed87da184c7, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/a8c1a75b93244e6f8cc5730b7d2445c6, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/ad791e416d134068b511c1c0501c7bd5, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/ae26c632b8004a7aa93df6f727bb183e, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/b5a3bf0023ef486c9d2289cdfde0e5e3, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/b8ab2f1aa34c4214b484ec6bff012d5b, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/ba1b5ab89f95497f8bb4bf33a0f59a10, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/bce5552f319442a6b27499eff22622b0, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/bdc657c0faea4fbbb79af222981b1fb2, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/bf8071d03fc945eb98f75c1e613b2aef, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/c26b96112d27407c88b709238117a5ee, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/c415ddc0bc6e4c8b973d5771d3c35117, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/c75a0900ae424c70af2e700bb2d72efd, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/c7cd4713af164dde9d5165dc9a409577, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/c9ad36722fd546c3b528c1d083cc3e46, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/ccdfdd36cd1e46c6b18506c63cab9b7d, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/d10c25325df54e008d22ff8147c5acda, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/d1f465564c4941d89e2cdf070957000e, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/d3361a1edac94717b75282916eb934e4, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/d3c09b7e7a7b420bb741e5e970127b98, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/d843419369a94546bfcb9be971cf1e01, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/db5f4b6b3fae4f07ba40b9d078f5e2d8, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/dba3f13c5f2a429a990860153453b93d, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/dc10994b7f7042889ed8b00160e3e0ad, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/dcd46e2ba90745caa4a07e824067259a, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/e807aecb4103472da0ffc4722822ac04, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/ef7f6a46612c4188a4a3acf0e3cd5abe, hdfs://localhost:39252/user/stack/hbase/.archive/otherTable/3d3f71c42883a0da891ee06abd92422f/fam/fcbd1f87a1ab465598a840c9f71b5e16]

Tests run: 3, Failures: 1, Errors: 0, Skipped: 0


And ....




-------------------------------------------------------
Running org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient







Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 316.773 sec <<< FAILURE!

Results :

Tests in error: 
  testMultipleTables(org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient): otherTable,,1349149111188.263ba836fbabdb1c12c43f7a2fe6f856. is closing

Tests run: 3, Failures: 0, Errors: 1, Skipped: 0
{code}



[~stack] are you sure you ran v4? I can't seem to find where that output would even occur in the patch...

@Jesse:
Do you mind updating review request on review board so that I can see the changes between the two patches ?

Thanks

@Ted - done. review can be found here: https://reviews.apache.org/r/7344/

[~jesse_yates] I must have done something wrong.  Sorry about that.  Seems fine running it in a loop here locally.  Waiting on Ted's review before committing.

The gist of this JIRA is about making sure that HFiles for table not configured to endure table drop should be deleted.
I had a question w.r.t. the above which I noted on review board.

I am sorry Jesse: HBASE-5699 has gone in and patch v4 is stale now.

Once you provide rebased patch, I will run the test and confirm that the fix works.

Thanks for your patience.

@Ted - which jira? That one's still open... I want to make sure my changes make sense.

Pardon me. Looks like Stack entered wrong JIRA:
{code}
r1393126 | stack | 2012-10-02 12:29:19 -0700 (Tue, 02 Oct 2012) | 1 line

HBASE-5699 Refactor HLog into an interface
{code}
What he integrated was HBASE-5937

[~tedyu@apache.org][~stack] looks like stack accidentally rolled in the latest version of this patch with HBASE-5937. So feel free to test it Ted(?) since its in. We can always revert if not good. Only thing missing from the patch to trunk is removal of CheckedArchivingHFileCleaner.java

I backed out hbase-6707 bits that made it in as part of an overcommit on HBASE-5937.

Let me just commit this.  I tested it locally already so you don't have to Ted.

Committed patch to trunk.  Thanks for the patch Jesse.  If we have to back it out again, I'll come looking for you.

bq. Committed patch to trunk....

Ted, it looked to me like Jesse answered your question up in RB.

Integrated in HBase-TRUNK #3409 (See [https://builds.apache.org/job/HBase-TRUNK/3409/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps (Revision 1393222)
HBASE-5937 Refactor HLog into an interface; REMOVE HBASE-6707 POLLUTION/OVERCOMMIT (Revision 1393221)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/CheckedArchivingHFileCleaner.java

stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java


Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #204 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/204/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps (Revision 1393222)
HBASE-5937 Refactor HLog into an interface; REMOVE HBASE-6707 POLLUTION/OVERCOMMIT (Revision 1393221)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/CheckedArchivingHFileCleaner.java

stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java


nope, flapping again. Looks like we are getting closer. See errors in builds #2997 (https://builds.apache.org/job/PreCommit-HBASE-Build/2997/testReport/org.apache.hadoop.hbase.backup.example/TestZooKeeperTableArchiveClient/testMultipleTables/) and #2995 (https://builds.apache.org/job/PreCommit-HBASE-Build/2995/testReport/org.apache.hadoop.hbase.backup.example/TestZooKeeperTableArchiveClient/testMultipleTables/). I think we can fix it with an addendum though - give me a little bit (done today though).

I miss counted the number of files looked at in the archiver, allowing the test to progress when it hasn't actually seen all the files, allowing the expected to get out of sync with the cleaner. Working on patch.

Attaching addendum to:
* fix count of files to check via the cleaner (which correlates to when we continue the test and run checks on existing files)
* adding more debug output
* cleaning up impl of the longtermarchiver

Reopen so that we can run addendum through QA.

Ran patch 20x locally without seeing any weird output or failures.

Can you clarify whether you were using jdk 1.6 or jdk 1.7 ?
I think both jdk should be used: 1.6 is used by Hadoop QA, 1.7 is used by trunk build.
{code}
+      // if its a directory, then it can be deleted
+      if (!fs.isFile(file)) return true;
...
-      // if its a directory with stuff in it, don't delete
{code}
The above change deviates from original assumption. Please explain why a directory can be deleted regardless of whether it has files under it.

bq. I think both jdk should be used: 1.6 is used by Hadoop QA, 1.7 is used by trunk build.

No.  What are you trying to do?  Stop all forward motion on the project by making up arbitrary rules for devs to follow?  If it passes hadoopqa, that is enough up to this. If you want hadoopqa to do both jdk6 and jdk7, that is another matter (and how about providing a patch).  Don't be putting friction on fellas.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12547654/hbase-6707-v4-addendum.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 83 warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 5 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.client.TestFromClientSideWithCoprocessor
                  org.apache.hadoop.hbase.master.TestSplitLogManager

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/3001//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3001//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3001//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3001//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3001//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3001//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3001//console

This message is automatically generated.

{code}
+    CountDownLatch finished = setupCleanerWatching(delegate, cleaners, files.size() + 3);
{code}
Why the addition of 3 above ?

bq.  If it passes hadoopqa, that is enough up to this. 

I think that's reasonable. If we want to support both 1.6 and 1.7 that should be part of the official test process. Wanna file a ticket Ted?

bq. Why the addition of 3 above ?

So it checks each of the directories on the way down - tablename/region/family - hence the plus three. 

The way CleanerChore works is that it looks at the main directory, and then if it has files under it, attempts to see if those files are deleteable. If they aren't deletable, it skips checking the directory and continues on to the next directory to check. If all the files are deletable, then if checks to see if the directory is deletable.

bq. The above change deviates from original assumption. Please explain why a directory can be deleted regardless of whether it has files under it.

Therefore, we can always consider directories deletable because there is nothing special about a directory, but only the files under the directory. Perhaps that was a flaw in the design, but we should file another ticket to change that behavior such that we only check for files and delete directories when there are no files for that directory.

Further note, for the original quote, that we need only have +3 for the non-archived table's directories, but not an extra +3 (which would make a total of +6) for the archived table because we don't attempt to delete the directory containing hfiles that are retained.

bq. If we want to support both 1.6 and 1.7 that should be part of the official test process. Wanna file a ticket Ted?
Done. I logged HBASE-6944

bq. Perhaps that was a flaw in the design, but we should file another ticket to change that behavior such that we only check for files and delete directories when there are no files for that directory.
Want to file a ticket Jesse ?

bq. only check for files and delete directories when there are no files for that directory.
Done - HBASE-6949

Are we happy with the latest addendum?

I think this hunk is not needed:
{code}
@@ -50,16 +50,14 @@ public class LongTermArchivingHFileCleaner extends BaseHFileCleanerDelegate {
   @Override
   public boolean isFileDeletable(Path file) {
     try {
+      // if its a directory, then it can be deleted
+      if (!fs.isFile(file)) return true;

+      // check to see if
       FileStatus[] deleteStatus = FSUtils.listStatus(this.fs, file, null);
       // if the file doesn't exist, then it can be deleted (but should never
       // happen since deleted files shouldn't get passed in)
       if (deleteStatus == null) return true;
-      // if its a directory with stuff in it, don't delete
-      if (deleteStatus.length > 1) return false;
-
-      // if its an empty directory, we can definitely delete
-      if (deleteStatus[0].isDir()) return true;

       // otherwise, we need to check the file's table and see its being archived
       Path family = file.getParent();
{code}
I placed breakpoint on the following line in the debugger:
{code}
+      if (!fs.isFile(file)) return true;
{code}
file turned out to be empty every time when it was a directory.

Without the above hunk, test still passed in a loop (on jdk 1.7).

bq. file turned out to be empty every time when it was a directory.

What do you mean by this? What are you proposing the method should look like? The intention was to always delete directories (which can later be removed by HBASE-6949), but that it does need to check if the table for those hfiles is being archived.

If the first hunk is needed by HBASE-6949, it can go into that patch.

I tried the addendum with and without the first hunk and the test failed on jdk 1.6 at the same spot:
{code}
testMultipleTables(org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient)  Time elapsed: 1.207 sec  <<< ERROR!
java.lang.NullPointerException
  at org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables(TestZooKeeperTableArchiveClient.java:259)
{code}
I can provide test output if you need it.

[~tedyu@apache.org] what do you mean by first and second hunk? Can you please provide the code? Thanks!

The first hunk is from line 5 to line 24 in the addendum.

I didn't mention second hunk.

6707-v4-addendum.txt is my version of addendum.

bq. If the first hunk is needed by HBASE-6949, it can go into that patch.

That can be removed in HBASE-6949 - it just depends on which order things are applied.

bq. I didn't mention second hunk.

Oops.

bq. I tried the addendum with and without the first hunk and the test failed on jdk 1.6 at the same spot:

I'm trying the test on jdk 1.6 with the first hunk applied (and not) and its working fine. 

Your comment above would imply that the test isn't working either way, so your patch would then not be correct, unless I'm missing something? 




My addendum fixes NullPointerException.
But it failed again on jdk 1.6 with:
{code}
testMultipleTables(org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient)  Time elapsed: 1.186 sec  <<< FAILURE!
java.lang.AssertionError: Not all archived files for the primary table were retained. expected:<2> but was:<0>
  at org.junit.Assert.fail(Assert.java:93)
  at org.junit.Assert.failNotEquals(Assert.java:647)
  at org.junit.Assert.assertEquals(Assert.java:128)
  at org.junit.Assert.assertEquals(Assert.java:472)
  at org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables(TestZooKeeperTableArchiveClient.java:266){code}
On jdk 1.7 I didn't see test failure.

I tried with the first hunk applied (and not) on jdk 1.6, same error.
The test output is for the test run without first hunk.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12547830/testZooKeeperTableArchiveClient-output.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 149 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3007//console

This message is automatically generated.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12547825/6707-v4-addendum.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 81 warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 5 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.regionserver.TestColumnSeeking

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/3006//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3006//console

This message is automatically generated.

Hmmm, I think part of this complexity is due to how the cleaner chore is handling directories. We should consider doing  HBASE-6949 before the addendum. Then the LongTermHFileArchiver becomes really simple and easy to verify.

I'm surprised that the Ted's addendum fixed the failure - the chore already handles deleting directories so the logic should always preserve the directory, Line 224  - CleanerChore:
{code}
(!this.fs.delete(filePath, false)) 
{code}

even if it has files in it. 

With the recent changes to trunk (HBASE-6949) my earlier addendum (v4-addenum.patch) has become correct since the cleaner the always removes directories now. The extra check is a precaution, but (1) should never be used and (2) doesn't affect correctness even if it somehow does get passed.

[~jesse_yates] So what to do here now Jesse to resolve the issue?

[~stack] i'd say commit v4-addendum.patch. 

Committed v4.  Lets see how it does.

Integrated in HBase-TRUNK #3494 (See [https://builds.apache.org/job/HBase-TRUNK/3494/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps; ADDENDUM v4 (Revision 1403098)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java


Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #243 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/243/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps; ADDENDUM v4 (Revision 1403098)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java


Well, at least it didn't fail immediately. (cross-fingers and knocks on wood).

Integrated in HBase-TRUNK #3496 (See [https://builds.apache.org/job/HBase-TRUNK/3496/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps; ADDENDUM v4 -- I FORGOT THIS BIT (Revision 1403539)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java


Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #244 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/244/])
    HBASE-6707 TEST org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables flaps; ADDENDUM v4 -- I FORGOT THIS BIT (Revision 1403539)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java


Stack committed this to trunk and the backport was also successful on its own ticket.

Marking closed.

