Starting part 1 of this -- identifying hard vs. soft state in the master and documented this. 

Initial design document: https://docs.google.com/a/cloudera.com/document/d/1RLiR1vzv-R4AeZ2mZDQfEtdpZkz1PT10PU7w0w2J9mY/edit?usp=sharing

Updated the README: http://gerrit.sjc.cloudera.com:8080/#/c/3744/

Couple things in flight. Waiting on JD to finish the stuff which allows a single host to have multiple RPC services exposed.

Update from Alex:
- got the permanent UUID discovery stuff working (how to discover UUIDs before the quorum starts up)
- implementing common TabletPeerLookupInterface (maybe rename to TabletHostInterface later)
- now working on the interface to specify the host/ports of the master hosts at startup (command line flags)
- also working on making this stuff testable inside a minicluster. (half a patch done, needs to be rebased and integrated)

goal for this week (little stretch):
- want to have working multi-master systables, passing unit tests, demoable, but will still be in review.
- realistic to commit the permanent UUID stuff this week


Update this week: Permanent UUID stuff ready to go in. Working on integration tests and doc.

Plan for this week:
- Adding test coverage for starting multimaster minicluster, and verification that metadata replicates between masters
- Documenting/breaking out the later milestones in the design doc - maybe we can adjust some milestones in a bit tighter

Alex is demoing this today. Some notes:

- need to file a JIRA for web UI support
- currently, the tservers only heartbeat to a single master (which has to be the leader)
- in the process of the demo, there seemed to be some cases where the master was segfaulting. Alex, can you file a JIRA for that issue?
- hit some kind of issue with log-dump crashing
-- should also use a nicer "todebugstring" when printing the data associated with INSERTs, etc in log-dump

another bug to be addressed: master needs to handle bad status when writing to systables

One thing missing from this jira is the scoping of automated failover (or what the design doc calls "unattended" failover). Is it a stretch goal for M5 or will it be addressed in the following milestone? Either way we should clarify it in the description, and in the latter case we should open a roadmap ticket for "Public beta roadmap'.

[~jdcryans] It depends on leader election being there -- I am going to spend some time tomorrow on updating the docs and will make sure this is addressed as part of the docs and has a JIRA for it.


Alex was in iceland last week eating smelly fish.

- there's a review out for the replicated systables stuff (probably will need a rebase)
- amended the design doc for some unattended failover bits - sketched out and waiting for folks to review it
- after the above stuff is committed, will work on the UI this week (and then changes to client)

- last week was waiting for review comments, addressed them and rebased, should be ready to go. (todd to review)
- had discussions with David
- learning about client, trying to figure out how this integrates with the multi-master
-- starting with C++ client only (easier for testing)
- starting to nail down metrics and web UI (diff out in the next few days)

- Adar to help with reviews on the above

- High priority item: figuring out what to do when replication fails (eg trying a write on the wrong master)

Alex's update:
- spent some time back and forth on client design stuff
- committed the replica catalog mgr
- final diff ready for a web ui (and related plumbing for stuff which will be used by the client)
-- the masters knowing the web ui ports for other masters, etc

This week:
- working on client failing over from master to master
- design issues around what to do when the replicate fails (handle gracefully instead of crashing)
- getting web ui patch committed

- committed RPCs to list masters
- two patches up for client:
-- setting up config for where the masters are
-- in the initial connection, tries to find the master
-- master rejects RPCs if it is not the leader
-- working on making the data path (tablet lookups, etc) fault tolerant to find the right master
- also some revs on the design doc
- Alex will do an internal demo on 10/27


Alex's update:
- got feedback on some of last week's reviews
- need to add more tests and address some feedback from Adar
- need to go back and do some design for how to handle tablet servers, and look at how CreateTable works and will work with failover
- his washing machine broke :(

- diff(s) outstanding for multi-master client (awaiting review)
- today/tomorrow working on lookup path fault tolerance for the client (kudos to adar for general client rpc stuff)
- focusing on tablet server stuff for the rest of the week


- did a demo last week
- committed: client initialized w/ multiple masters
- put up a diff to make locating the master asynchronous (for fault tolerance and rediscovering the current master for GetTabletLocations)
-- doing the lookups in parallel put off for now, but we need to fix this for realistic fault tolerance
- this week:
-- making GetTableSchema fault tolerant
-- starting to look at tablet server, working on design for what to do with soft state, etc.

Java client?
- haven't looked at it yet
- need to learn how it works, then integrate it there
- original design doc didn't take into account the java client for milestones (underestimated)
-- will estimate 11/21 for java client to be equivalent to C++ client. Also testing of it and testing with YCSB. (may need to revise the estimate based on looking at the work required)
--- some work on the test infrastructure will be required to start multiple masters, kill servers, etc.

- last week:
-- committed fault tolerant GetTableSchema
-- made some tests less flaky (needed some hooks to set timeouts, etc)
-- started working on tablet server support: WIP where TS can register with multiple masters. if one goes down and another becomes the leader, the TS will start sending heartbeats + reports to the other one. lots of copy-pasta between client and TS, need to figure out a way to share them. (maybe a generic registration PB).
-- Mike is "volunteering" to work with Alex on cleaning up the above copy-pasted code around finding leaders in QuorumPBs, etc.

- this week:
-- writing up a design doc explaining the way that TS interacts with leader change
-- the above-mentioned cleanup (working with Mike)

- java client:
-- haven't started yet (prioritizing the harder problems, will work on this once the other stuff's done. assuming it should be pretty mechanical)

alex's update:
- bunch of client stuff committed last week
- committed support for initting TS with multiple masters
- master still has to be restarted manually

Had a design discussion on Friday
- have to change the way that master stores metadata - needs to be only a single tablet, so that it is at all times either just a leader or a replica, not mixed (eg right now tablets and tables system tablets could be in different roles)
- david working on a callback for txn completion

This week:
- go back to the client and make the master determination be in parallel so that if a leader fails, it's a shorter pause
- handle create table and delete (currently only the lookup path is handled)
- in between reviews, starting to look at java client
- if time, start working on master using a single tablet
-- and looking at which hard state needs to be cached (comprehensive design doc)

edit: alex's patch also heartbeats to multiple masters, not just inits

- patch up for review which makes client talk to the masters in parallel to find which master is leader
- when the catalog manager is initialized, it finds its role from the consensus quorum instead of from the command line flag
- WIP single-tablet master: unified tablets list and table list into one with a schema (type, id, protobuf)
-- need to rename some classes and collapse inheritance hierarchy now given there is only one systable
- WIP hacky patch which wires everything up to use leader elections and initialize the catalog manager once it gets elected to a leader

Java client: no time to dive into it yet (been working on above)

- Have a demo ready for unattended failover
- integration test for it (client doesn't use the meta cache)
- more C++ client fixes
- configuration changes to make it easier
- hoping to finalize the diff today
- this week: create and alter table are not fault tolerant for the master, fix trickiness related to the parallel RPCs

Few more things:

* The demo also showed tablet server heartbeating to different masters as leader election happens

This week:

* RPC fixes, fix the issue where we don't handle consensus failures (may be hard to test, but I can reproduce it in unit test runs occasionally)

- The diff which Alex demoed last week is under review (awaiting Todd's review, but all rebased and ready to go)
- Alex working on the Java stuff along with some assistance from JD to lean about the Java side of things.
- Need to add fault tolerance and some testing for create table on replicated masters.

After committing the Java stuff, we should change the integration test cluster to run multi-master. Alex and JD will look at that in SF on Friday.

- last week, committed unattended failover on C++ side
- started working on the Java
- C++ side, started working on create/alter table. found some issues, mostly around testability, but had to fix some client code too
-- timeout on OpenTable (Adar reviewing)
-- a couple cases where the client thinks the table failed, but did actually get created, then a retry gets "already exists"
-- has a specific solution for it (checking that the schema's the one we wanted to create), but we need a general replay cache solution

upcoming:
- once the java stuff works, need to deploy multi-master on our integration clusters


Alex:
- debugged/solved some issues with fault-tolerant create table
- demoed a client creating tables in a loop while doing master failovers
- need to address some code review comments from Adar

- may need an async create-table at some point in the future
- still need to do delete/alter fault tolerance

- spent some time working on java client lookup path, but not ready yet. Will demo next week (YCSB)


Alex:
- Figured out the issue with create table. mike and alex workign together on reviewing/committing
- Working on Java in between review interations
- C++ alter table is in progress, but waiting on the create table stuff to get in before pushing
- Improved tests a bit to be faster


Alex:
- Got the master java stuff committed, yay!
- Fix issues seen on YCSB run (tight retry loop, some weird message that should not be printed out).
- We're still using only 1 master on the test clusters, will work on this with JD.
- Pushed a small patch for hooks for testing.
- Fixed a TSAN issue.
- Focus: getting everything stabilized (re-enabling tests, add more, try to get on ITBLL and maybe kill masters along with TS) and alter table (making sure it goes only to the master leader, abort if it fails, doesn't leave state behind, make it not more dangerous than it is right now). Will talk to David and Mike.

Alex:
- It's working but unstable, needs a lot more attention to stabilize, add more tests.
- We should disable tests that are too flaky until this work is re-assigned, likely next week since we'll all be back.

Alex isn't with the company anymore, so we need to find a new owner on this.

This is probably lower priority than some of the other gaping holes we have, so will re-assign this when folks have more bandwidth after finishing pending projects

We should mark the multi-master support as experimental for public beta.

I've started working on parts of this.

[~adar] - I started trying to play with multi-master on a cluster today. one thing I'm not sure if it's tracked yet: how can someone convert from a single master to a multi-master setup? Seems like we might need some startup mode to do the conversion (eg starting the other masters with a '--bootstrap-from' flag or something like that?) This same process might be usable to "repair" a master that had to be rebuilt.

bq. one thing I'm not sure if it's tracked yet: how can someone convert from a single master to a multi-master setup?

Good thinking; I hadn't thought about that at all. I filed KUDU-1474 to track that work and linked it back into this JIRA.

[~adar]: I'm doing a triage of open "critical" JIRAs. Mind if we close this one and just use the couple remaining subtasks to track the remaining work? Given they've been open for 6+ months they must not be that critical, and I think the replicated master can be considered supported/finished enough to close this umbrella JIRA

bq. Mind if we close this one and just use the couple remaining subtasks to track the remaining work?

Works for me.


