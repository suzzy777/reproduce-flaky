cc [~bryanc]

About the issues happened in arm instance SPARK-29205, finally, we increase the timeout and the batch time, see [https://github.com/theopenlab/spark/pull/27/files#diff-f7e50078760ce2d40f35e4c3b9112227]  and then the tests pass.

Did you see test failures consistently and the commit fixed the test consistently? Or did you run the test 100 times or so? Increasing timeout and batch time sounds me as still relying on timing, so we may need to run bunch of times to ensure it is no longer flaky.

The tests specified in -SPARK-29205- failed every time when testing in arm instance, and after increasing the timeout and batch time they success, but we didn't test 100 times, just several times.  I have no idea about the batchDuration of StreamingContext setting, is there a principle? 

Shall we increase the time a bit more if it was verified that it helps the flakiness?

[~hyukjin.kwon], we took test to increase default timeout to 120s and batchDuration to 3s, I will check if we need to increase the time a bit more.

[~hyukjin.kwon]，we took test and seems these failed tests depends on the performance of the instance, the tests maybe pass once for several times test. But it seems everything is ok if we increase default timeout to 120s and batchDuration to 3s.

Can you make a PR to increase the timeout?

[~hyukjin.kwon], the failure is releated with the performance of arm instance, now we donate an arm instance to AMPLab and we havn't build the python job yet, and we plan to donate some higher performance arm instances to AMPLab next month, if the issues happen again then I will create a pr to fix this. Thanks a lot.

Still happening on master (3.1.0-SNAPSHOT)

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/121232

