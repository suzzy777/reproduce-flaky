Just got a [fresh run failure|https://github.com/smengcl/hadoop-ozone/runs/6012168314] (after 3 runs, first 2 runs passed, failed on the 3rd) from my test [branch|https://github.com/smengcl/hadoop-ozone/commits/HDDS-6546] based on the Apr 12 master branch (plus one [debugging print|https://github.com/smengcl/hadoop-ozone/commit/389c7af67a6644fdb2c84f7bdf9e592cf3fea303#diff-031bc2d52ad1aeca1089ae32b0ba13381c05b5d30276294e7a359c6eb95277c0R396] , which wasn't triggered, and some GH action yaml mod to trigger only the relevant test run). The same issue can be observed in {{./upgrade/1.1.0-1.2.0/docker-1.2.0-finalized.log}}

 [^acceptance-misc-run-6012168314.zip] 

So I was able to "set a breakpoint" at the failure to pause the upgrade test run (so the cluster doesn't get destroyed when a test case failed, but the services are still running) after maybe 20 runs 5 min each, then I tried the exact same {{key put}} command in scm container (after ~9 hours. I let it run overnight). And it looks like SCM fixes itself during the time (but I have not found any signs in the SCM logs that this pipeline map is fixed, maybe in the trace/debug log).

The manual key put is successful. I also can't find the problemsome pipeline ID in Recon Web UI.

{code:title=Time in UTC. SCM throwing PipelineNotFoundException from PipelineStateMap.getPipeline, the proof of a repro}
scm_1    | 2022-04-14 06:15:33,270 [IPC Server handler 0 on default port 9863] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 1.
scm_1    | 2022-04-14 06:15:33,271 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 1 to 1001.
scm_1    | 2022-04-14 06:15:33,274 [IPC Server handler 0 on default port 9863] ERROR block.BlockManagerImpl: Pipeline Machine count is zero.
scm_1    | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=10d41c84-82d8-4973-b57a-7002a391f096 not found
scm_1    | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:157)
scm_1    | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerImpl.getPipeline(PipelineStateManagerImpl.java:137)
scm_1    | 	at jdk.internal.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
scm_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm_1    | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeLocal(SCMHAInvocationHandler.java:83)
scm_1    | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:68)
scm_1    | 	at com.sun.proxy.$Proxy16.getPipeline(Unknown Source)
scm_1    | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.getPipeline(PipelineManagerImpl.java:212)
scm_1    | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.newBlock(BlockManagerImpl.java:200)
scm_1    | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:180)
scm_1    | 	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:194)
scm_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:180)
scm_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:130)
scm_1    | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:112)
scm_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13937)
scm_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
scm_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
scm_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
scm_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
scm_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
scm_1    | 2022-04-14 06:18:28,187 [ReplicationMonitor] INFO container.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
{code}

{code:title=Attempting a manual repro but the pipeline seems to be fixed}
$ pwd
${HOME}/repo/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade
$ docker-compose -f compose/ha/docker-compose.yaml exec scm bash
bash-4.2$ ozone sh bucket list /new2-volume/
[ {
  "metadata" : { },
  "volumeName" : "new2-volume",
  "name" : "new2-bucket",
  "storageType" : "DISK",
  "versioning" : false,
  "usedBytes" : 0,
  "usedNamespace" : 0,
  "creationTime" : "2022-04-14T06:15:32.154Z",
  "modificationTime" : "2022-04-14T06:15:32.154Z",
  "quotaInBytes" : -1,
  "quotaInNamespace" : -1,
  "bucketLayout" : "OBJECT_STORE",
  "owner" : "hadoop",
  "link" : false
} ]
bash-4.2$ ozone sh key list /new2-volume/new2-bucket/
[ ]
bash-4.2$ ozone sh key put /new2-volume/new2-bucket/new2-key /opt/hadoop/NOTICE.txt
bash-4.2$ ozone sh key list /new2-volume/new2-bucket/
[ {
  "volumeName" : "new2-volume",
  "bucketName" : "new2-bucket",
  "name" : "new2-key",
  "dataSize" : 17526,
  "creationTime" : "2022-04-14T16:17:27.052Z",
  "modificationTime" : "2022-04-14T16:17:28.217Z",
  "replicationConfig" : {
    "replicationFactor" : "THREE",
    "requiredNodes" : 3,
    "replicationType" : "RATIS"
  },
  "replicationFactor" : 3,
  "replicationType" : "RATIS"
} ]
{code}

So I've enabled trace logging in docker-compose.yaml by adding
{code}
      OZONE_OPTS: -Dhadoop.root.logger=TRACE,console
      OZONE_DAEMON_ROOT_LOGGER: TRACE,RFA
{code}
under SCM service.

Soon after I trigger the test loop I hit another bug? This time not PipelineNotFoundException from getPipeline but 2 out of 3 DNs are down, thus unable to form a RATIS 3 pipeline.

{code:title=Same robot test result failure}
==============================================================================
Generate :: Generate data
==============================================================================
Create a volume and bucket                                            | PASS |
------------------------------------------------------------------------------
Create key                                                            | FAIL |
255 != 0
------------------------------------------------------------------------------
Generate :: Generate data                                             | FAIL |
2 tests, 1 passed, 1 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-4.xml
Press any key to resume .. # <-- Note I added this in testlib.sh to pause the test run when it fails
{code}

{code:title=Same client error. Reproducible}
bash-4.2$ ozone admin safemode status
SCM is out of safe mode.
bash-4.2$ ozone sh key put /new2-volume/new2-bucket/new2-key /opt/hadoop/NOTICE.txt
INTERNAL_ERROR Allocated 0 blocks. Requested 1 blocks
{code}

{code:title=But (seemingly) different root cause}
scm_1    | 2022-04-14 18:44:44,887 [IPC Server handler 9 on default port 9863] TRACE protocol.ScmBlockLocationProtocolServerSideTranslatorPB: [service=BlockLocationProtocol] [type=AllocateScmBlock] request is received: <json>cmdType: AllocateScmBlock\ntraceID: ""\nversion: 1\nallocateScmBlockRequest {\n  size: 268435456\n  numBlocks: 1\n  type: RATIS\n  factor: THREE\n  owner: "om3"\n  excludeList {\n  }\n}\n</json>
scm_1    | 2022-04-14 18:44:44,889 [IPC Server handler 9 on default port 9863] DEBUG server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList {datanodes = [], containerIds = [], pipelineIds = []}
scm_1    | 2022-04-14 18:44:44,889 [IPC Server handler 9 on default port 9863] TRACE block.BlockManagerImpl: Size : 268435456 , replicationConfig: RATIS/THREE
scm_1    | 2022-04-14 18:44:44,889 [IPC Server handler 9 on default port 9863] TRACE ha.SCMHAInvocationHandler: Invoking method public abstract java.util.List org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipelines(org.apache.hadoop.hdds.client.ReplicationConfig,org.apache.hadoop.hdds.scm.pipeline.Pipeline$PipelineState,java.util.Collection,java.util.Collection) on target org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerImpl@173373b4 with arguments [RATIS/THREE, OPEN, [], []]
scm_1    | 2022-04-14 18:44:44,889 [IPC Server handler 9 on default port 9863] DEBUG ha.SCMHAInvocationHandler: Call: public abstract java.util.List org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipelines(org.apache.hadoop.hdds.client.ReplicationConfig,org.apache.hadoop.hdds.scm.pipeline.Pipeline$PipelineState,java.util.Collection,java.util.Collection) took 0 ms
scm_1    | 2022-04-14 18:44:44,889 [IPC Server handler 9 on default port 9863] INFO pipeline.PipelineManagerImpl: Cannot create new pipelines while pipeline creation is frozen.
scm_1    | 2022-04-14 18:44:44,889 [IPC Server handler 9 on default port 9863] WARN pipeline.WritableRatisContainerProvider: Pipeline creation failed for repConfig: RATIS/THREE. Retrying get pipelines call once.
scm_1    | java.io.IOException: Cannot create new pipelines while pipeline creation is frozen.
scm_1    | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.createPipeline(PipelineManagerImpl.java:165)
scm_1    | 	at org.apache.hadoop.hdds.scm.pipeline.WritableRatisContainerProvider.getContainer(WritableRatisContainerProvider.java:119)
scm_1    | 	at org.apache.hadoop.hdds.scm.pipeline.WritableContainerFactory.getContainer(WritableContainerFactory.java:52)
scm_1    | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:176)
scm_1    | 	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:194)
scm_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:180)
scm_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:130)
scm_1    | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:112)
scm_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13937)
scm_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
scm_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
scm_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
scm_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
scm_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
scm_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
{code}

Seems this time (almost) all DNs are down, according to SCM Web UI and Recon.

 !potential_bug2_scmwebui_1.png! 

 !potential_bug2_recon_1.png! 

 !potential_bug2_recon_2.png! 

btw I'm on latest master branch {{141ced1196d42330026148036e85f32317e011a4}} this time, only with some docker compose config changes to aid me in debugging the SCM issue.

Oh. It looks like the (potential) second bug in the comment above is due to SCM somehow failing to finalize the upgrade and it [got stuck with pipeline creation frozen|https://github.com/apache/ozone/blob/d6e0e4747f09ce5c04bcd2ac42cf9ce3e73198db/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/upgrade/SCMUpgradeFinalizer.java#L64-L67], how strange:

{code:java|title=SCMUpgradeFinalizer#preFinalizeUpgrade}
    // Pipeline creation will remain frozen until postFinalizeUpgrade()
    pipelineManager.freezePipelineCreation();

    waitForAllPipelinesToDestroy(pipelineManager);
{code}

And now it is steadily reproducible on my Linux box (Arch Linux. x86_64. Kernel 5.17.2-arch3-1. docker 1:20.10.14-1. containerd 1.6.2-1. docker-compose 1.29.2, build 5becea4c), even after I manually moved the data directory away ({{./upgrades/non-rolling-upgrade/1.1.0-1.2.0/data}}) and restart the test. Hmm

UPDATE: It turns out once I comment out these two trace log lines in ./ha/docker-compose.yaml , the problem goes away. There's definitely something wrong on the trace logging side. Potentially the trace logging path has some side effect that BREAKS the upgrade finalization path. (Or there's could be something wrong with my Arch Linux box, but very unlikely. Can be disproven if someone else runs my [debug branch|https://github.com/smengcl/hadoop-ozone/commits/HDDS-6546-debug2])

{code:yaml|title=trace logger wreaking havoc?}
#      OZONE_OPTS: -Dhadoop.root.logger=TRACE,console
#      OZONE_DAEMON_ROOT_LOGGER: TRACE,RFA
{code}

and btw CLI reports {{FINALIZATION_DONE}}

{code}
bash-4.2$ ozone admin scm finalizationstatus
FINALIZATION_DONE
bash-4.2$ ozone sh key put /new2-volume/new2-bucket/new2-key /opt/hadoop/NOTICE.txt
INTERNAL_ERROR Allocated 0 blocks. Requested 1 blocks
bash-4.2$ ozone admin scm finalizationstatus
FINALIZATION_DONE
bash-4.2$ ozone admin scm finalizeupgrade
Upgrade has already been finalized.
Exiting...
bash-4.2$ ozone sh key put /new2-volume/new2-bucket/new2-key /opt/hadoop/NOTICE.txt
INTERNAL_ERROR Allocated 0 blocks. Requested 1 blocks
{code}

My debug2 branch, which has the docker compose yaml and script mod to stop the cluster destruction on failure (it works locally, but not sure it works on GH action): https://github.com/smengcl/hadoop-ozone/commits/HDDS-6546-debug2

To run it

{code}
mvn clean install -DskipTests -e -Dmaven.javadoc.skip=true -DskipShade
cd hadoop-ozone/dist/target/ozone-*/compose/upgrade

while true; do ./test.sh; sleep 5; done
{code}

Then it should pause when the test fails.

Back to the CLOSED pipeline allocate block issue, so far it seems that the issue is a benign race condition that shouldn't cause serious consequences.

The CLOSED pipeline sometimes remains in the {{query2OpenPipelines}} map right after upgrade finalization, and is erroneously retrieved during {{allocateBlock}}, causing SCM's {{PipelineNotFoundException}} and client's {{INTERNAL_ERROR Allocated 0 blocks. Requested 1 blocks}}, but is seemingly cleaned up by {{ReplicationMonitor/ReplicationManager}} shortly after safemode exits (5 min):

{code:title=A second repro on my Linux x86_64 box. After another ~80 runs (took ~7 hours)}
scm_1    | 2022-04-15 11:44:02,526 [IPC Server handler 2 on default port 9860] INFO upgrade.UpgradeFinalizer: Open pipeline found after SCM finalization
scm_1    | 2022-04-15 11:44:02,526 [IPC Server handler 2 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization is done.
scm_1    | 2022-04-15 11:44:02,526 [IPC Server handler 2 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization is done.
scm_1    | 2022-04-15 11:44:06,868 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 562f32c1-7377-4e73-987e-61de0d68b16a, Nodes: 724eac09-3e86-4ab6-b7ee-
420ba505ea84{ip: 10.9.0.16, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState:
IN_SERVICE, persistedOpStateExpiryEpochSec: 0}f33c42e3-1206-4f40-a3a3-9b8b2e4fb6f4{ip: 10.9.0.17, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859],
networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c4ea4688-0390-4b98-8740-f9e3e25a30ce{ip: 10.9.0.15, host: ha_dn1_1.ha_net, ports: [REPLICATION=988
6, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/T
HREE, State:ALLOCATED, leaderId:c4ea4688-0390-4b98-8740-f9e3e25a30ce, CreationTimestamp2022-04-15T11:43:22.523Z[UTC]] moved to OPEN state
scm_1    | 2022-04-15 11:44:12,089 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 3 blocks to be deleted for 3 datanodes, task elapsed time: 1ms
scm_1    | 2022-04-15 11:44:37,677 [IPC Server handler 13 on default port 9863] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 1.
scm_1    | 2022-04-15 11:44:37,677 [IPC Server handler 13 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 1 to 1001.
scm_1    | 2022-04-15 11:44:37,682 [IPC Server handler 13 on default port 9863] ERROR block.BlockManagerImpl: Pipeline Machine count is zero.
scm_1    | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=d4e631f8-0594-49c2-8259-e9324f8c2637 not found
scm_1    |      at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:157)
scm_1    |      at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerImpl.getPipeline(PipelineStateManagerImpl.java:137)
scm_1    |      at jdk.internal.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
scm_1    |      at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm_1    |      at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm_1    |      at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeLocal(SCMHAInvocationHandler.java:83)
scm_1    |      at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:68)
scm_1    |      at com.sun.proxy.$Proxy16.getPipeline(Unknown Source)
scm_1    |      at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.getPipeline(PipelineManagerImpl.java:212)
scm_1    |      at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.newBlock(BlockManagerImpl.java:200)
scm_1    |      at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:180)
scm_1    |      at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:194)
scm_1    |      at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:180)
scm_1    |      at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:130)
scm_1    |      at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm_1    |      at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:112)
scm_1    |      at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13937)
scm_1    |      at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
scm_1    |      at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
scm_1    |      at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
scm_1    |      at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm_1    |      at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
scm_1    |      at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
scm_1    |      at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1    |      at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1    |      at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1    |      at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
scm_1    | 2022-04-15 11:47:10,652 [ReplicationMonitor] INFO container.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1    | 2022-04-15 11:52:10,655 [ReplicationMonitor] INFO container.ReplicationManager: Sending close container command for container #1 to datanode 724eac09-3e86-4ab6-b7ee-420ba505ea84{ip: 10.9.0.16, host: ha_d
n2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpoc
hSec: 0}.
scm_1    | 2022-04-15 11:52:10,655 [ReplicationMonitor] INFO container.ReplicationManager: Sending close container command for container #3 to datanode 724eac09-3e86-4ab6-b7ee-420ba505ea84{ip: 10.9.0.16, host: ha_d
n2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpoc
hSec: 0}.
scm_1    | 2022-04-15 11:52:10,655 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 4 milliseconds for processing 3 containers.
scm_1    | 2022-04-15 11:57:10,656 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 3 containers.
{code}

At this moment I cannot exclude the possibly of [the procedures in {{SCMUpgradeFinalizer}} class|https://github.com/apache/ozone/blob/d6e0e4747f09ce5c04bcd2ac42cf9ce3e73198db/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/upgrade/SCMUpgradeFinalizer.java#L89] causing this temporary glitch, one reason being this error is observed always after {{--- RUNNING WITH NEW VERSION 1.2.0 FINALIZED ---}}

On a side note, interestingly it seems it's much easier to repro this on Github Actions (triggered 1 in 3 ~ 1 in 5) than on my local Linux box (1 in 20 ~ 1 in 80). Maybe a slower machine / different distro / older Linux Kernel also helps in triggering this race condition (IDK).

Ozone 1.3.0 had been released and we currently have more than 600 open issues targeted for 1.3.0. I am moving the target field to 1.4.0.

What if anything needs to be discussed about the Target Version, Please reach out to me via Apache email or Slack.

[~smeng] any further updates on this?

[~umamaheswararao] I haven't seen this symptom recently. Might have been solved already by some other jiras.

[~nilotpalnandi] are you guys seeing this in any of your upgrade verifications you may be doing? Otherwise let's close this? Thanks

Resolving this now.
[~smeng] [~nilotpalnandi], feel free to reopen if you see this issue again.

