I'm filing this issue for tracking purposes and to raise visibility of the symptoms and workaround - I don't know if/how it can be "fixed" in Solr.

This popped up for me when dealing with a customer who noticed they were running out of disk on nodes hosting PULL replicas of large collection that were getting lots of document updates. The hung threads were holding {{SolrIndexSearcher}} references, which had open file handles to segments that were no longer in use by the "current" index but couldn't be overwritten by the filesystem because of the open file handles).

This problem could manifest in other ways for other people – the most likely (beyond the obvious failure or "slow" request metrics) being high memory usage as these threads and the object they hold references to taking up Heap space indefinitely.

 
----
 

While investigating open jetty issues that might pertain to this, I discovered that Ishan & Noble have apparently [been aware of this problem|https://github.com/fullstorydev/lucene-solr/pull/121] [for a while|https://github.com/fullstorydev/lucene-solr/pull/106], and already have [an open ticket with the Jetty project (#7192)|https://github.com/eclipse/jetty.project/issues/7192] regarding these symptoms.

(Ishan alluded to this problem in SOLR-15840, but that Jira is a general "Task" regarding questions about the performance of HTTP2 in Solr, that doesn't explicitly call out that this "Bug" exists and can hang threads - hence I've opened this new Jira.)

So far the only known [steps to reliably reproduce|https://github.com/eclipse/jetty.project/issues/7192#issuecomment-989727192] are fairly onerous (they crashed my machine while attempting to run them) which makes it hard to easily identify a root cause, but the [latest analysis from jetty dev|https://github.com/eclipse/jetty.project/issues/7192#issuecomment-991068558] {{sbordet}} is...
{quote}We were able to get a failure, so we instrumented the client to understand why it was waiting, and found that it was waiting for a request whose headers arrived fine (with a 200 status code) but the client was waiting for the response body (no errors).

We tried to track this request on the server logs, but we could not find any information.
Any attempt to increment the log level to DEBUG makes the failure non-reproducible, so we are a bit stuck trying to understand what's going on.

From our point of view, the client received response headers and it's waiting for the content, but it never arrives.
We cannot explain how the response headers arrived, but not the content, but we don't have information about the server – whether there was an error on the server or similar, because the logs don't show any error.

At this point I'm throwing the ball in your court: are you sure that you are sending the response content?

...

If we can track the tcpdump and see that the HTTP/2 frames reach the client, then it's a Jetty client problem;
otherwise the server did not send the HTTP/2 frames and this could be either for a Jetty server problem, or because the application did not send any data.
In this way we're trying to restrict the problem.
{quote}
The issue seems to have stalled out ~2 months ago while attempting to provide usable {{tcpdump}} trace of the affected requests.

[~ichattopadhyaya] / [~noblepaul] : Do you have any more info on this that you can share? If you can reliably reproduce this can you update the jetty ticket w/usable {{tcpdump}} logs?

 
----
 

One thing we may want to consider (in Solr) is replacing our usage of {{InputStreamResponseListener}} with a variant implementation that uses a "timeout" instead of an unlimited {{wait()}} (along the lines of a [spin-off jetty enhancement issue|https://github.com/eclipse/jetty.project/issues/7259] one of the jetty devs filed). We could probably (with some effort) tweak the impacted Solr APIs to propogate the (remaining) {{timeAllowed}} (if that option was specified) down to this class – and/or have an "extreme" default (ie: 30min) just to prevent threads from sticking around forever.

bq. One thing we may want to consider (in Solr) is replacing our usage of {{InputStreamResponseListener}} with a variant implementation...

 

I created SOLR-16129 to track this workaround/improvement.

FYI: 
* a new [Jetty Issue#8558|https://github.com/eclipse/jetty.project/issues/8558] has been opened with a more isolated test case that has helped the jetty devs understand the root cause of this issue.
* [Jetty Issue#7192|https://github.com/eclipse/jetty.project/issues/7192] has been closed

SOLR-15955 is trying to get to Jetty 10 - based on the commit in the fixed issue upstream - https://github.com/eclipse/jetty.project/commit/dab4fe60d305416dd1d4dbd8da855e0482b63de9 isn't in Jetty 10.0.12 yet.

Note, this is why some tests fail frequently, including {{TestDistributedStatsComponentCardinality}}

We have encountered similar problem:

https://www.mail-archive.com/users@solr.apache.org/msg04497.html

I have two questions that I hope can be answered:
 * Could this be the cause of people getting "Caused by: java.util.concurrent.RejectedExecutionException: Max requests queued per destination 3000 exceeded for HttpDestination[[http://localhost:8983|http://localhost:8983/]]@d368dfe,queue=3000,pool=MultiplexConnectionPool@3f16228[c=0/4/4,a=0,i=0,q=3000]"?
 ** That is a thread on the users list that I am looking at right now.  I found another thread on the mailing list where they solved the same problem by switching the solr client to use http1.
 * Would the upcoming upgrade to Jetty 10 fix it?

 

{quote}Could this be the cause of people getting "Caused by: java.util.concurrent.RejectedExecutionException: Max requests queued per destination 3000 exceeded for HttpDestination[http://localhost:8983@d368dfe,queue=3000,pool=MultiplexConnectionPool@3f16228[c=0/4/4,a=0,i=0,q=3000]"?
That is a thread on the users list that I am looking at right now.  I found another thread on the mailing list where they solved the same problem by switching the solr client to use http1.{quote}

No idea to be honest. I know I've seen that same max requests queued when you do load testing and open a connection per request since of sending requests over an existing connection. So the error message is not 100% clear what causes it.

{quote}Would the upcoming upgrade to Jetty 10 fix it?{quote}

See comment above. SOLR-15955 is trying to get to Jetty 10 - based on the commit in the fixed issue upstream - https://github.com/eclipse/jetty.project/commit/dab4fe60d305416dd1d4dbd8da855e0482b63de9 isn't in Jetty 10.0.12 yet.

There isn't a Jetty 10.x (or 9.x) release with the fix as of yet.

{quote}Could this be the cause of people getting "Caused by: java.util.concurrent.RejectedExecutionException: Max requests queued per destination 3000 exceeded for HttpDestination[[http://localhost:8983|http://localhost:8983/]]@d368dfe,queue=3000,pool=MultiplexConnectionPool@3f16228[c=0/4/4,a=0,i=0,q=3000]"?
{quote}
From our tests, such problems often occur in high stress tests.

In the scenario with many shards, the coordination node will encounter this situation. If a timeout exception occurs in a shard, all shard requests will be interrupted, which will lead to some resource leakage.

It is also related to the following issue  SOLR-16229

https://github.com/eclipse/jetty.project/releases/tag/jetty-10.0.13 just released has https://github.com/eclipse/jetty.project/commit/dab4fe60d305416dd1d4dbd8da855e0482b63de9 included.

Commit 7a96c5112b8544ddf0bc1c9ba5e80c4d1b454cd9 in solr's branch refs/heads/main from Kevin Risden
[ https://gitbox.apache.org/repos/asf?p=solr.git;h=7a96c5112b8 ]

SOLR-16099: Upgrade to Jetty 10.0.13 (#1230)



Commit 4c54fffb8aa6e3937f3b127f33b4b6f38beadb9b in solr's branch refs/heads/branch_9x from Kevin Risden
[ https://gitbox.apache.org/repos/asf?p=solr.git;h=4c54fffb8aa ]

SOLR-16099: Upgrade to Jetty 10.0.13 (#1230)



I'm going to mark this as resolved since the Jetty 10.0.13 upgrade has the fix for this. I commented on SOLR-16129 about the workaround that was never implemented.

Thanks for getting this done so fast [~krisden]! I was following the release, but not as closely haha

I'm really excited to see how many of the flaky tests are "fixed" by this!

Closing after the 9.2.0 release

