Proposed patch for the CAS-13849

Idea is to replace intrinsic lock (synchronized) everywhere in ActiveRepairService with ReentrantLock, and release it before start waiting on prepareLatch in prepareForRepair.

It's not really clear to me why we use a 1 hour timeout for the PrepareMessage callback in first place. The PrepareMessage should just propagate the repair session. If we really have to support such a long timeout period, we'd have to synchronize with any convict notification raised at some point while still waiting for the timeout. But simply lowering the timeout to something like 1 minute would make the gossip blocking behaviour much less likely and only block for at most that very period. 

Patch reducing 1 hour timeout for the PrepareMessage callback to 1 minute.


Stefan, thank you for your response.

I have tried your idea in our test setting - it fixes the problem for us. I have uploaded this as a patch ([^CAS-13849_2.patch]), could you please take a look?

I'd agree that the timeout probably doesn't need to be that long. In the meantime, you should probably re-evaluate whether this method actually needs to be synchronized. This method is unsynchronized in trunk, and I think it may be because it was a holdover from more synchronous repair code of the past. Just looking at it briefly in 3.0, the only thing that *might* need synchonization is the call to registerParentRepairSession, everything else if just method local message sending / receiving stuff.

Yes, ActiveRepairService.prepareForRepair is not synchronized in trunk, indeed.

I believe, however, it gives the way to a possible race condition.

In the receiving node org.apache.cassandra.repair.RepairMessageVerbHandler#doVerb method can be processing PREPARE_MESSAGE from coordinator node and run the ActiveRepairService.instance.registerParentRepairSession method. In the same time, receiving node org.apache.cassandra.tools.nodetool.Repair#execute method can also invoke the same ActiveRepairService.instance.registerParentRepairSession method. As a consequence, ActiveRepairService can be registered twice with the Gossiper.instance.register(this), for example.

Maybe it would be safer to have leave ActiveRepairService.prepareForRepair unsynchronized but make registerParentRepairSession synchronized.

What do you think?

I think that's reasonable, given the stuff that happens in this method, and the relative infrequency that it's called. It would also fix a visibility issue with the registeredForEndpointChanges variable. We should also check that a parent repair session doesn't already exist for the given key before adding a new one.

Please take a look at this patch: [^CAS-13849_3.patch]

Patch looks good. I've merged it up through trunk and started tests here:

|[3.0|https://github.com/bdeggleston/cassandra/tree/13849-3.0] | [utests|https://circleci.com/gh/bdeggleston/cassandra/152] | [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/408/]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/13849-3.11] | [utests|https://circleci.com/gh/bdeggleston/cassandra/153] | [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/409/]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/13849-trunk] | [utests|https://circleci.com/gh/bdeggleston/cassandra/154] | [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/410/] |

I'll commit once the tests are complete, assuming there aren't any problems.

I ran the trunk unit tests locally and everything passed. The rest of the test failures were either flaky tests, or unrelated tests that passed locally. Committed as {{49edd70740e2efae3681cb79a391369bfb7de02e}}.

Thanks [~slapukhov]!

