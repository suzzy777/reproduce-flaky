The failing query was (from impalad.c34abf949077.invalid-user.log.INFO.20210211-023758.1):
{noformat}
I0211 03:55:22.140455 102573 Frontend.java:1587] be46bb72819942fd:85934edd00000000] Analyzing query: select l_shipmode, o_orderpriority, count(*)
from tpch_nested_parquet.customer.c_orders o, o.o_lineitems l
where l_receiptdate < '1992-01-10'{noformat}
from test *test_parquet_stats.py::TestParquetStats::test_page_index*

It's hard to tell what went wrong without the data file. I'm planning to run load_nested and the above test in a loop, that'll hopefully reproduce this issue.

Looping load_nested and running the query didn't help. However, I was able to reproduce the error on a hacked version of Impala.

I added the following code toÂ HdfsParquetScanner::EvaluatePageIndex():
{noformat}
    skip_ranges.clear();
    const int RANGES = 5;
    int64_t num_vals = row_group.num_rows;
    int64_t range_length = num_vals / RANGES;
    for (int i = 0; i < RANGES; ++i) {
      int64_t range_start = i * range_length + range_length / 2;
      int64_t range_end = range_start + range_length / 2;
      skip_ranges.push_back({range_start, range_end});
    }{noformat}
The above code adds a couple of "skip ranges" that are unaligned with the Parquet pages. This reproduced the error and I was able to create a fix: [https://gerrit.cloudera.org/#/c/17071/]

Commit 89a5fc789cb7cdbab08e331c4c95d9cd4eb30b8d in impala's branch refs/heads/master from Zoltan Borok-Nagy
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=89a5fc7 ]

IMPALA-10501: Hit DCHECK in parquet-column-readers.cc: def_levels_.CacheRemaining() <= num_buffered_values_

We had a DCHECK in ScalarColumnReader::MaterializeValueBatch() that
checked that 'num_buffered_values_' is greater or equal to the
number of cached values in the Parquet definition level decoder.

In SkipTopLevelRows() we used decoder.ReadLevel() which loaded
the cache of the decoder with probably more values than the
actual value count. It is because literal runs are stored in groups
of 8, i.e. there might be padding zeros at the end.

Alternatively we can fill the cache of the decoder with
CacheNextBatch(num_vals). In this case we won't load more values
than the actual value count.

Testing
 * until this patch TestParquetStats::test_page_index was flaky
   because of this issue
 * I tested the solution on a hacked Impala that randomly generated
   skip ranges

Change-Id: Ic071473e7b315300fd5e163225d3e39735f09c4f
Reviewed-on: http://gerrit.cloudera.org:8080/17071
Reviewed-by: Zoltan Borok-Nagy <boroknagyz@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


