Simple patch which increases priority on GetLastFlushedSequenceId, RegionServerReport, RegionServerStartup, ReportRSFatalError, and ReportRegionStateTransition on MasterRpcServices.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707920/HBASE-13351.patch
  against master branch at commit 0a500e5d305b0c75a6a357a5ff7a9210a615a007.
  ATTACHMENT ID: 12707920

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.regionserver.TestQosFunction

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13465//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13465//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13465//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13465//console

This message is automatically generated.

Work around mockito's class being used instead of the actual server implementation's class.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707949/HBASE-13351-v1.patch
  against master branch at commit 0a500e5d305b0c75a6a357a5ff7a9210a615a007.
  ATTACHMENT ID: 12707949

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 8 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

     {color:red}-1 core zombie tests{color}.  There are 5 zombie test(s): 	at org.apache.hadoop.hbase.util.TestHBaseFsck.testHbckWithFewerReplica(TestHBaseFsck.java:706)
	at org.apache.hadoop.hbase.util.TestHBaseFsck.testRegionDeployedNotInHdfs(TestHBaseFsck.java:1888)
	at org.apache.hadoop.hbase.util.TestHBaseFsck.testValidLingeringSplitParent(TestHBaseFsck.java:1761)
	at org.apache.hadoop.hbase.util.TestHBaseFsck.testLingeringSplitParent(TestHBaseFsck.java:1713)
	at org.apache.hadoop.hbase.util.TestHBaseFsck.testHbckWithExcessReplica(TestHBaseFsck.java:779)
	at org.apache.hadoop.hbase.util.TestHBaseFsck.testQuarantineCorruptHFile(TestHBaseFsck.java:2155)
	at org.apache.hadoop.hbase.util.TestHBaseFsck.testFixHdfsHolesNotWorkingWithNoHdfsChecking(TestHBaseFsck.java:2085)
	at org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClient.testCloneSnapshotOfCloned(TestRestoreSnapshotFromClient.java:245)

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13469//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13469//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13469//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13469//console

This message is automatically generated.

Good thing! few notes:

 - QosPriority now needs to have javadoc IMO
 - at first I was a bit surprised why do we need that change in HMaster - since tests which don't need cluster generally don't start master either, but looks like that's a bit special case here
 - do you think these RPCs should go with admin QoS, or we should have something with lower QoS (say, 50?) If HRS couldn't report to master in time about something, that might be less urgent compared to admin ops? on the other hand, avoiding having to add one more thread pool for this class of methods would be either.

Thanks for the review.

bq. QosPriority now needs to have javadoc IMO

Easy enough.

bq. on the other hand, avoiding having to add one more thread pool for this class of methods would be either

I didn't think the attached patch actually changed the threadpools that were created (at least with SimpleRpcScheduler). Perhaps I missed something?

bq. do you think these RPCs should go with admin QoS, or we should have something with lower QoS (say, 50?)

I think that makes sense. Let me see if I can put up a new patch that does 50 like you suggest.

bq. I didn't think the attached patch actually changed the threadpools that were created (at least with SimpleRpcScheduler). 

That was I meant, yeah - if we run it with Admin QoS we reuse the according threadpool, otherwise we might need to add the new one for new level of priority, if we introduce one.

Ah, now I'm following you. Perhaps a pool of size 5? These calls should be quick, best I can tell.

Yeah, I think so. I'm also curious about others' opinions on whether its good idea to add one more threadpool here.

How about we commit this and come back later if we need another threadpool set on master (too many threads in hbase!)

WFM, I've been waffling about whether or not this will actually have a perceived difference (for the end-user/admin). I feel like if the master's handler threads are saturated, the fact that RS can still chat with the master won't really matter.

Ultimately, I think HBASE-13375 would have a much bigger positive impact than this, but these changes do have some net-positive effect.

Thanks [~elserj] I see this change as building block. Let me get [~mantonov] opinion. If he not averse, will commit. Thanks.

Yeah, let's commit it. 

That actually good point about saturation!

If we imagine we have 3 threadpools, (20 -regular, 5 - high, 5 - replication), for some some reason high QoS threads are saturated (as evidenced by retries), shall we be able to dynamically try to adjust request QoS to try to sneak in the other threadpool? :)

Hrm, given [~mantonov]'s [comment|https://issues.apache.org/jira/browse/HBASE-13375?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14484112#comment-14484112], maybe the above failures weren't unrelated.

Yesh, I think you're right. If someone else could reproduce it too, would make sense to wait till it's figured out, I'd say. Looking.

So, both this patch and the one for HBASE-13375 could be fixed by setting hbase.regionserver.metahandler.count to 10 (not 2, as it is not in beforeClass call), I tried both patches with this fix. As stop-gap, probably that would make sense to do. What do you guys think?

What's interesting, with this patch as well as HBASE-13375 for this particular test we route most of calls thru high-priority pool in SimpleRpcScheduler, but things work just fine without this patch. Suggests that high performance pool with same amount of handlers is less performand, than standard one.

The practical (in our case) small difference between the two is that callExecutor uses bounded priority array-based queue, and priority and replicator executors are using linked blocking queues, and also with default configs callExecutor would have 1 backing queue (when 2 handlers are in use), and priority executor is hardcoded to use 2. Which probably doesn't really matter.

Seems like in this case we're just happen to hit corner case in a sense that normally in prod cluster one expects user rpcs to be the prevailing type of calls, and in this test it's just not the case (btw..the same effect would apply when making admin-invoked calls to use high-priority thread pool, since inside minicluster..well, everyone is admin :)).

[~elserj], [~stack]  - how about we try this patch on hadoop qa with rpc metahandler count set in this test to something like 40, and see?

Thinking about corner case we're hitting here - setting metahandler count to 8 or 10 still causes some (but not all) TestHbaseFsck tests fail, and setting it to 20 causes rare occasional failures.

(on the other hand, out of curiosity, if I live metahandler count as 2 in the configuration, but change the QoS for these 5 methods in MasterRpcServices from admin to replication, everything works.. So..I don't think that's very realistic production usecase, but sometimes in corner case this contention may happen)

Interesting, I changed the metahandler count to 10 and it seems to be passing regularly for me. I need to look at your prev comment about the difference in queues used being a difference. Did you think that was the reason for the failure? Or were you just noting that as a difference?

I don't really think type of blocking queue matters in this case. It's just without this patch the requests get distributed between call* and priority* executors, and now most of them (in this particular test) go thru the latter one. Just saturation I think. Which OS are you using? On my mac 10 handlers from IDE don't pass always. I'll set it to something higher just to be on the safe side. I don't think it's going to be critical (as all threads get shut down when test completes).

bq. Just saturation I think

Ok, I figured that was the case, just wasn't sure if I was missing something more.

bq. Which OS are you using? On my mac 10 handlers from IDE don't pass always

I've running it on the CLI on my Mac as well. Just started a {{while true; do mvn... }} loop to see if I can make it fail that way :)

bq. I'll set it to something higher just to be on the safe side. I don't think it's going to be critical (as all threads get shut down when test completes).

Ok. I was poking at the history and noticed that they've been set low for some time. I didn't dig far enough to see if there was a reason other than trying to limit test-resources needed.

I see, thanks.

Btw.. did you try also running TestAdmin2 with same settings for metahandler count? Does it pass locally for you? It seems to have similar case (and was failing on my patch if I don't set metahandler count higher than now).

Hah, made a liar out of me. Failed with 10 handlers almost right away this time :)

TestAdmin2 fails w/o change and appears to pass regularly with more metahandler threads.

Same here, these 2 large tests seems to be the only ones affected.

Here's a v2 that ups the metahandler pool sizes to 30 for TestHBaseFsck and TestAdmin2. Let's see what jenkins thinks.

Thanks [~elserj], LGTM, non-binding +1 if jenkins passes. nit -

  // normal_QOS < qos_threshold < replicationQOS < replay_QOS < high_QOS ---- could mention admin qos here?

ack! good catch. I totally missed that.

Fixed consolidated QOS priorities comment (and normalized the capitalization and punctuation)

Thanks! Could you take a look at HBASE-13375's last patch, as it's changing the same code? 

Happy to. It was on my radar :)

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12724089/HBASE-13351-v2.patch
  against master branch at commit e2a90a71143f480621ccd935a0b9477d7ee4016f.
  ATTACHMENT ID: 12724089

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 16 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

     {color:red}-1 core zombie tests{color}.  There are 1 zombie test(s): 	at org.apache.hadoop.hbase.http.TestHttpServerLifecycle.testStartedServerIsAlive(TestHttpServerLifecycle.java:71)

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13633//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13633//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13633//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13633//console

This message is automatically generated.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12724099/HBASE-13351-v3.patch
  against master branch at commit e2a90a71143f480621ccd935a0b9477d7ee4016f.
  ATTACHMENT ID: 12724099

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 16 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

    {color:red}-1 site{color}.  The patch appears to cause mvn site goal to fail.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.security.access.TestZKPermissionsWatcher
                  org.apache.hadoop.hbase.TestAcidGuarantees

     {color:red}-1 core zombie tests{color}.  There are 9 zombie test(s): 	at org.apache.hadoop.hbase.client.TestFromClientSide3.testHTableExistsMethodMultipleRegionsSingleGet(TestFromClientSide3.java:338)
	at org.apache.hadoop.hbase.client.TestHTableMultiplexerFlushCache.testOnRegionChange(TestHTableMultiplexerFlushCache.java:114)

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13634//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13634//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13634//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13634//console

This message is automatically generated.

TestAcidGuarantees failed with Caused by: java.lang.RuntimeException: java.lang.OutOfMemoryError: unable to create new native thread. And when OOME shows up, a lot of other tests go flaky... I saw these a lot when trying to purge managed connection when there were leaks, but here..could've been caused by running more handler threads in 2 zombie tests? Don't really think so.

Want to retry again?


You fellows think we should up the default hbase.regionserver.metahandler.count given your findings here (and that this patch makes it so admin exclusively runs on this pool?)?

That's a very good question. I need to look more closely at the failing tests to better understand why the contention came about before I make up my mind, but my initial thought is to agree with you.

Yeah, now it's set to 10 (while some tests deliberately set it to lower value), and probably should be higher now. Let me try to bump it up to 20 in patch to HBASE-13375 and see how it goes.

Ah! I think I got to the bottom of why this deadlocks without sufficient priority-pool threads. {{MasterRpcServices#reportRegionStateTransition}} ultimately makes another {{Get}} to meta which automatically gets put at priority 200 (because it's a request against meta).

So, the region server fires off reportRegionStateTransition calls to the Master, these end up going back into the same thread pool which has no more threads to handle the requests. Boom, deadlock. The confusing part (or at least the part I don't understand) is why this is going back to the Master and not a RS. Maybe it's due to the Master acting as a RS? Maybe I just don't understand how this works completely :)

{noformat}
Daemon Thread [PriorityRpcServer.handler=1,queue=1,port=64100] (Suspended)	
	waiting for: AsyncCall  (id=891)	
	Object.wait(long) line: not available [native method]	
	AsyncCall(Object).wait(long, int) line: 461	
	AsyncCall(DefaultPromise<V>).await0(long, boolean) line: 355	
	AsyncCall(DefaultPromise<V>).await(long, TimeUnit) line: 266	
	AsyncCall(AbstractFuture<V>).get(long, TimeUnit) line: 42	
	AsyncRpcClient.call(PayloadCarryingRpcController, MethodDescriptor, Message, Message, User, InetSocketAddress) line: 226	
	AsyncRpcClient(AbstractRpcClient).callBlockingMethod(Descriptors$MethodDescriptor, PayloadCarryingRpcController, Message, Message, User, InetSocketAddress) line: 213	
	AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message, Message) line: 287	
	ClientProtos$ClientService$BlockingStub.get(RpcController, ClientProtos$GetRequest) line: 32391	
	HTable$3.call(int) line: 686	
	HTable$3.call(int) line: 1	
	RpcRetryingCallerImpl<T>.callWithRetries(RetryingCallable<T>, int) line: 117	
	HTable.get(Get) line: 694	
	MetaTableAccessor.getTableState(Connection, TableName) line: 1075	
	TableStateManager.readMetaState(TableName) line: 187	
	TableStateManager.getTableState(TableName) line: 171	
	TableStateManager.isTableState(TableName, TableState$State...) line: 130	
	AssignmentManager.onRegionOpen(RegionState, HRegionInfo, ServerName, RegionServerStatusProtos$RegionStateTransition) line: 2183	
	AssignmentManager.onRegionTransition(ServerName, RegionServerStatusProtos$RegionStateTransition) line: 2754	
	MasterRpcServices.reportRegionStateTransition(RpcController, RegionServerStatusProtos$ReportRegionStateTransitionRequest) line: 1264	
	RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message) line: 8623	
	RpcServer.call(BlockingService, MethodDescriptor, Message, CellScanner, long, MonitoredRPCHandler) line: 2095	
	CallRunner.run() line: 101	
	BalancedQueueRpcExecutor(RpcExecutor).consumerLoop(BlockingQueue<CallRunner>) line: 130	
	RpcExecutor$2.run() line: 107	
	Thread.run() line: 745	
  
Daemon Thread [PostOpenDeployTasks:d923ab785d95578230ec49fbb1f40e8e] (Suspended)	
	waiting for: AsyncCall  (id=808)	
	Object.wait(long) line: not available [native method]	
	AsyncCall(Object).wait(long, int) line: 461	
	AsyncCall(DefaultPromise<V>).await0(long, boolean) line: 355	
	AsyncCall(DefaultPromise<V>).await(long, TimeUnit) line: 266	
	AsyncCall(AbstractFuture<V>).get(long, TimeUnit) line: 42	
	AsyncRpcClient.call(PayloadCarryingRpcController, MethodDescriptor, Message, Message, User, InetSocketAddress) line: 226	
	AsyncRpcClient(AbstractRpcClient).callBlockingMethod(Descriptors$MethodDescriptor, PayloadCarryingRpcController, Message, Message, User, InetSocketAddress) line: 213	
	AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message, Message) line: 287	
	RegionServerStatusProtos$RegionServerStatusService$BlockingStub.reportRegionStateTransition(RpcController, RegionServerStatusProtos$ReportRegionStateTransitionRequest) line: 9030	
	MiniHBaseCluster$MiniHBaseClusterRegionServer(HRegionServer).reportRegionStateTransition(RegionServerStatusProtos$RegionStateTransition$TransitionCode, long, HRegionInfo...) line: 1949	
	MiniHBaseCluster$MiniHBaseClusterRegionServer(HRegionServer).postOpenDeployTasks(Region) line: 1884	
	OpenRegionHandler$PostOpenDeployTasksThread.run() line: 241	
{noformat}

Nice catch!

I believe meta is co-located with master (as per HBASE-10569), so I can see why it's going to master again. But, OTOH, call in this call should be going thru short-circuit connection and not RPC layer..Let me take a look.

bq. I believe meta is co-located with master (as per HBASE-10569)

Boom. This is the hunch I had, but I had no idea where to start looking to verify. Thanks!

bq. this call should be going thru short-circuit connection and not RPC layer..Let me take a look.

Absolutely. We shouldn't be making a separate RPC for this. MetaTableAccessor could hopefully shortcircuit out if we're running inside the Master (although I'm not directly sure how we would determine this).

SSC-s are set in HRegionServer#createClusterConnection.

But as I'm looking at it, MetaTableAccessor doesn't really make use of it?

As a note..Looks like we don't gather metrics as to what percentage of rpc calls went thru which thread pool. Would it be useful for prod environment to have one? Thoughts?

It should.

{panel:title=TableStateManager.java}
{code}
  @Nullable
  protected TableState readMetaState(TableName tableName) throws IOException {
    if (tableName.equals(TableName.META_TABLE_NAME))
      return new TableState(tableName, TableState.State.ENABLED);
    return MetaTableAccessor.getTableState(master.getConnection(), tableName);
  }
{code}
{panel}

When we read the state of a table from meta, we should be reusing the Master's cached connection (which has the opportunity to give us the short-circuit variant).

The question I have, is are we actually getting a ShortCircuit connection when we think we should be

{panel:title=ConnectionUtils.java}
{code}
      @Override
      public ClientService.BlockingInterface getClient(
          ServerName sn) throws IOException {
        return serverName.equals(sn) ? client : super.getClient(sn);
      }
{code}
{panel}

My current hunch is that we're failing that conditional for one reason or another. Will dig into this next.

Makes sense to me.

Short answer: {{ConnectionUtils#createShortCircuitHConnection}} doesn't actually do what it thinks its doing.

For callers which directly try to get the client service stub, things are fine. However, in our specific case where we're constructing an HTable and calling a Get on it, these methods are invoked on the ConnectionImplementation, not the SCC stub. In other words, the HTable never even sees the SCC and always does a normal RPC.

IMO, this is something to branch off into its own issue. The outstanding question at hand here is admin threadpool sizing concerns for real instances, of course this should likely also be done elsewhere :). I think the patch here is ready to go, but we might need some thought about how requests actually get routed since admin is already all meta requests + admin/server users (after HBASE-13375) + hbase server-to-server RPCs (this ticket).

Yeah, you're right, that's I've seen there too in ConnectionAdapter impl we're using there.

Regarding issue at hand - I _think_ for real cluster the percentage of all admin-QoS rpcs (as you enumeratad here) is still tiny % of all rpc requests, but for our miniclusters, so this patch looks good to me (if we could get the clean hadoop-qa run, perhaps bumping default metahandler count to 20? seems like it's prudent change to do regardless of the fix in SSC?)

Hrmph. Last patch had 30 which should be enough? Let me try to just kick off another test.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12725667/HBASE-13351-v3.patch
  against master branch at commit 14261bc9e515bbaf2fa40f5779cb83a3372f0e57.
  ATTACHMENT ID: 12725667

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 16 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.regionserver.TestRegionReplicaFailover

     {color:red}-1 core zombie tests{color}.  There are 5 zombie test(s): 

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13714//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13714//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13714//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13714//console

This message is automatically generated.

I meant - default hbase.regionserver.metahandler.count is 10 now (patch for HBASE-13375 modified default to 20), may be try to bump this default up to 20 (for all tests, not just this 1 or 2, since lot of them would be affected, and, more importantly, real cluster behavior would change a bit, too).

{quote}
-1 core tests. The patch failed these unit tests:
org.apache.hadoop.hbase.regionserver.TestRegionReplicaFailover
{quote}

This one is failing locally for me too. I'll dig into it.

v4 ups the metahandler thread counts to get TestRegionReplicaFailover passing.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12725963/HBASE-13351-v4.patch
  against master branch at commit e08ef99e3042767eaf2d11adae783674acfdddeb.
  ATTACHMENT ID: 12725963

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 20 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

     {color:red}-1 core zombie tests{color}.  There are 5 zombie test(s): 

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13724//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13724//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13724//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13724//console

This message is automatically generated.

test-patch.sh seems confused. Jenkins isn't reporting any test failures (best as I can tell). Should I just try to re-run again?

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12726057/HBASE-13351-v4.patch
  against master branch at commit 7fb6055ed7304b05a383a730b64e55e41772bcf8.
  ATTACHMENT ID: 12726057

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 20 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

     {color:red}-1 core zombie tests{color}.  There are 6 zombie test(s): 

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13729//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13729//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13729//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13729//console

This message is automatically generated.

TestRollingRestart seems to have failed (https://builds.apache.org/job/PreCommit-HBASE-Build/13729//testReport/TEST-org.apache.hadoop.hbase.master.TestRollingRestart/xml/_init_/), though it does look spurious to me..Want to retry?

I took a look at the stack dumps that have found a couple of tests that were just stuck in main. I think it's just maven or test-patch.sh not handling this goofball case. Will have a new patch in a little bit with some more test changes.

Just wondering if you were going to also change default metahandler count (DEFAULT_REGION_SERVER_HIGH_PRIORITY_HANDLER_COUNT prop) to 20 as it seems like something worth trying?

I think between the two of us, it should happen. I can include it in my next patch since I'm already working on it :)

I've included it in the patch for superusers QoS already :) but since that wasn't committed yet, this change doesn't help tests in this jira.

Ahh, forgot about that. FTR, it looks like 20 isn't enough for some of these still failing tests. Bumped up to 40 makes them pass. The short-circuit connection fix should help keep an upper bound there, so I'm not too worried about it ATM.

{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12726202/HBASE-13351-v5.patch
  against master branch at commit 0dfeba3d7854c66b80dfc4e0d465d63dce617bb9.
  ATTACHMENT ID: 12726202

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 36 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13734//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13734//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13734//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13734//console

This message is automatically generated.

+1 (non-binding), long-awaited green run.

I'm not exactly sure if the build took longer (2 hr 40m) as a direct result of that or due to some other reason, but in any case, tests were going to be affected somewhat by this change.

This patch has kind of stalled a bit on reviews..?

Gentle reminder here for reviewers. Have concerns been addressed? Thanks.

Skimmed. Looks good. The one nit I have though is whether the configs to do with metahandler count and the handler threads that are changed in the various tests should instead be put in the hbase-site.xml that we have for tests (e.g., hbase-server/src/test/resources/hbase-site.xml)

Looks good.
Any way we can write a FT test to assert that the RS->Master APIs are treated with higher priority. I see your UT for asserting the annotation.  Not blocking for commit though.
+1

bq. whether the configs to do with metahandler count and the handler threads that are changed in the various tests should instead be put in the hbase-site.xml that we have for tests (e.g., hbase-server/src/test/resources/hbase-site.xml)

Oh, how about that. I wasn't aware of that configuration file :). The drawback would be increasing the thread footprint of tests that don't need the larger thread pools. It would help from test cluster consistency so that may be worth it. Happy to do it if you think it's worth it.

bq. Any way we can write a FT test to assert that the RS->Master APIs are treated with higher priority. I see your UT for asserting the annotation. Not blocking for commit though.

Sure, I'd be happy to try to verify that. I'm not sure of the best way off the top of my head, maybe mock out the thread pools for each handler and spoof some calls and make sure they hit the right pools? I'll file a follow-on to make sure this doesn't get lost.

Thanks for the reviews, [~devaraj] and [~anoop.hbase]!

I am okay either way. +1 for commit.

The patch does set DEFAULT_REGION_SERVER_HIGH_PRIORITY_HANDLER_COUNT in HConstants to higher value, right?

I see some tentative +1's. Anyone willing to commit in the next couple hours?

Done [~ndimiduk]. Thanks for the patch Josh!

FAILURE: Integrated in HBase-1.1 #444 (See [https://builds.apache.org/job/HBase-1.1/444/])
HBASE-13351 Annotate internal MasterRpcServices methods with admin priority (Josh Elser) (ddas: rev 0165fe6ce47eb97b26cfe9b18a4564eb6c3063d8)
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterPriorityRpc.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicaFailover.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestQosFunction.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCloneSnapshotFromClient.java
* hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/QosPriority.java
HBASE-13351 Addendum to remove reference to MasterTests class (ddas: rev 2ac0855c901d5eb1ecf738a57bfc8d6f5fe9223d)
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterPriorityRpc.java


FAILURE: Integrated in HBase-TRUNK #6433 (See [https://builds.apache.org/job/HBase-TRUNK/6433/])
HBASE-13351. Annotate internal MasterRpcServices methods with admin priority (Josh Elser) (ddas: rev b27e9e70be888b50b2ceb21e16d201f6aba9ad42)
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCloneSnapshotFromClient.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
* hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterPriorityRpc.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicaFailover.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/QosPriority.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestQosFunction.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java


FAILURE: Integrated in HBase-1.2 #41 (See [https://builds.apache.org/job/HBase-1.2/41/])
HBASE-13351 Annotate internal MasterRpcServices methods with admin priority (Josh Elser) (ddas: rev 8aae3bfab67e2840a1d7c6c4a8d55ab9fce916d2)
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestCloneSnapshotFromClient.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicaFailover.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestQosFunction.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterPriorityRpc.java
* hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/QosPriority.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
HBASE-13351 Addendum to remove reference to MasterTests class (ddas: rev 699460aae006bcce8d1a3167d9eca2b21429f50b)
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterPriorityRpc.java


Sweet. Thanks [~devaraj] for the apply and everyone else who spent time looking over this and giving feedback!

Closing issues released in 1.1.0.

