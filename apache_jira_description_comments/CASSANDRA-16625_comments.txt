Good idea, it would replace my following scripts (which just runs the test in a loop till it fails)

{code}
$ cat ci-test-loop
#!/usr/bin/env bash

#set -o xtrace
set -o errexit
set -o pipefail
set -o nounset

bin="$(cd "$(dirname "$0")" > /dev/null; pwd)"

class_name="$1"
path="$(echo "$class_name" | tr '.' '/' ).java"

if [[ ! "$path" =~ distributed/upgrade ]]; then
  ant realclean && ant && ant generate-idea-files
fi

counter=1
echo ">>> Running attempt $counter"
while $bin/ci-test "$class_name" --reuse; do
  counter=$(( $counter + 1 ))
  echo ">>> Running attempt $counter"
done;
{code}

{code}
$ cat ci-test
#!/usr/bin/env bash

#set -o xtrace
set -o errexit
set -o pipefail
set -o nounset

usage() {
  if [[ $# -gt 0 ]]; then
    echo "$*" 1>&2
  fi
  cat <<EOF
Usage: $(basename $0) [class name] (options)*
Options:
--cdc           - runs with cdc enabled
--compression   - run with compression enabled
--reuse         - skip calling ant realclean
EOF
  exit 1
}

if [[ $# -eq 0 ]]; then
  usage "Missing required arguments"
fi

class_name="$1"
shift
path="$(echo "$class_name" | tr '.' '/' ).java"
prefix="$( find test | grep "$path" | awk -F/ '{print $2}' )"

task="testclasslist"
fresh_build=true
while [[ "$#" -gt 0 ]]; do
  case "$1" in
    --cdc)
      task="testclasslist-cdc"
      shift
      ;;
    --compression)
      task="testclasslist-compression"
      shift
      ;;
    --reuse)
      fresh_build=false
      shift
      ;;
    *)
      usage "Unknown argument $1"
      ;;
  esac
done

if [[ "$fresh_build" == true ]]; then
  if [[ ! "$path" =~ distributed/upgrade ]]; then
    ant realclean && ant && ant generate-idea-files
  fi
fi

# cleanup logs so w/e is around are for this run
rm -rf build/test/logs || true

test_timeout=$(grep "name=\"test.$prefix.timeout\"" build.xml | awk -F'"' '{print $4}' || true)
if [ -z "$test_timeout" ]; then
  test_timeout=$(grep 'name="test.timeout"' build.xml | awk -F'"' '{print $4}')
fi
ant "$task" -Dtest.timeout="$test_timeout" -Dtest.classlistfile=<(echo "$path") -Dtest.classlistprefix=$prefix
{code}


Is there a way to trigger a Circle CI build with provided params?  If so then this doesn't seem bad.

pytest-repeat comes in very handy and has been working for dtests for me:

{{pytest --count 30 ...}}

My 2cts.

EDIT: added pytest-repeat as part of CASSANDRA-16614

I have found sometimes the sh loop approach is a pain when the test happen to be a quick one. I need to run a test class N times fast rather than waiting 90% of the time for sh/ant to loop. I added a repeatable runner chasing a bug [here|https://github.com/apache/cassandra/pull/987/files#diff-3b232ce364185d49c362ae79ed940d43bb9acc2c90d8e996ae1f4bae2d611b35R1] that I find useful. I hope it serves inspiration for sthg more flexible and generic when we tackle this ticket.

Thanks for the feedback. I'm a few details away from having a CircleCI config patch for repeatedly running Python dtests, using pytest-repeat in each parallel test split.

For unit tests and in-JVM dtests I was thinking on adding a {{count}} argument to {{ant testsome}} doing something similar to what pytest-repeat does, so we can also use it locally. 

[Here|https://github.com/adelapena/cassandra/commit/682aa6082f55f0d508eeaf3d31021ce9363a3f84] is as simple patch adding jobs to run a specific Python dtest multiple times. The new jobs are:
 * j8_repeated-dtest-with-vnodes
 * j8_repeated-dtest-no-vnodes
 * 11_repeated-dtest-with-vnodes
 * j11_repeated-dtest-no-vnodes

All these jobs are optional and require user approval to start. The name of the test to run is defined by the env var {{REPEATED_DTEST_PYTHON_TEST_NAME}}. It can be anything of the form:
 * pending_range_test.py
 * pending_range_test.py::TestPendingRangeMovements
 * pending_range_test.py::TestPendingRangeMovements::test_pending_range

The number of iterations is set in the environment variable {{REPEATED_DTEST_PYTHON_COUNT}}, which defaults to 100. Sadly I haven't found a way to introduce these values in the CircleCI UI, so they should be set in the {{.circleci/config.yml}}, as we do when we want to run a specific dtest branch.

All the jobs use the parallel executor, evenly distributing the test iterations among the runners. For example, if we use the MIDRES config with a parallelism of 25 and we want to run a certain test 100 times, each runner will run the test 4 times.

[Here|https://app.circleci.com/pipelines/github/adelapena/cassandra/305/workflows/827ede69-9a87-4738-a326-4368e94daa0b/jobs/2737] we can see 100 runs of {{pending_range_test.py}}, which was recently fixed by CASSANDRA-16614. It has taken less than 3 minutes with HIGHRES.

Also, [here|https://app.circleci.com/pipelines/github/adelapena/cassandra/304/workflows/e848906f-86e5-4779-a982-b4887570c012/jobs/2729] we can see 100 runs of the class {{cqlsh_tests/test_cqlsh.py::TestCqlshSmoke}}, taking around six minutes with HIGHRES.

Not a formal review as my understanding is this is still work in progress.

In general it looks good to me, but we need to update the patch files, not directly the MIDRES, HIHRES ones. More info on that [here|https://github.com/apache/cassandra/blob/trunk/.circleci/readme.md].

Also, I think it would be nice to update the readme files and the docs with info about this new option when it is done.

Great idea, thanks [~adelapena]!

Same here, not a formal review but while you're at it you could make j11 unit tests not require user approval. I think that's just a bug on the ci file wdyt [~e.dimitrova]?

As per junit tests I would try to avoid the 'count' approach. That is good for slow tests but for quick tests it's a waste of time. I've had to loop sometimes 10K times a test programmatically and it would take a minute. If I had to do that at sh level or start/stop junit 10K times it would take ages imo. My 2cts. 

[~e.dimitrova] [~bereng] thanks for the feedback. Indeed this is still a work in progress.
{quote}In general it looks good to me, but we need to update the patch files, not directly the MIDRES, HIHRES ones. More info on that [here|https://github.com/apache/cassandra/blob/trunk/.circleci/readme.md].
{quote}
The MIDRES and HIGHRES have actually been generated with the patch files. Those patch files only modify the executors and, since I haven't done any changes on them, I think there is no need to modify the patch files. At least they seem to produce the desired changes in {{config-2_1.yml}}.
{quote}Also, I think it would be nice to update the readme files and the docs with info about this new option when it is done.
{quote}
I agree. Both [{{.circleci/readme.md}}|https://github.com/apache/cassandra/blob/trunk/.circleci/readme.md] and [the section in the documentation for CircleCI|https://cassandra.apache.org/doc/latest/development/testing.html?highlight=circleci#performance-testing] seem more focused on how to run with different resources than in describing the different workflows and jobs, so perhaps they need further work in that direction. Not sure whether that should be addressed here or in a separate ticket.
{quote}you could make j11 unit tests not require user approval. I think that's just a bug on the ci file
{quote}
Good idea, done [here|https://github.com/adelapena/cassandra/commit/61ba75d405d64d9c1c60f11be554de1b88ecd622] for the j11 workflow. By the way, I'm not sure why we have j11 jobs in the j8 workflow.
{quote}As per junit tests I would try to avoid the 'count' approach. That is good for slow tests but for quick tests it's a waste of time. I've had to loop sometimes 10K times a test programmatically and it would take a minute. If I had to do that at sh level or start/stop junit 10K times it would take ages imo. My 2cts.
{quote}
Passing a {{count}} argument to the different flavours of testsome/test-some is independent on whether we use an ant loop, a Junit runner or, in the future, Junit 5's {{@RepeatedTest}}. Junit-based solutions would just read that argument to determine the amount of iterations. I see the performance advantage of using Junit-based solutions to run the tests in the same JVM. However, it seems that we have several tests that currently don't work with that approach, for example {{ScrubTest}}, {{ViewTest}}, {{ReadRepairTest}} or {{SASIIndexTest}}. I guess we could have both approaches and use the one that is more adequate for the case at hand.

 

{quote}Good idea, done [here|https://github.com/adelapena/cassandra/commit/61ba75d405d64d9c1c60f11be554de1b88ecd622] for the j11 workflow. By the way, I'm not sure why we have j11 jobs in the j8 workflow.
{quote}
I don't think it is a bug to require J11 unit tests to be run. This should be a minimum for J11 support.

And we have J11 jobs on the J8 build as it exercises the case build on Java 8, run in Java 11. Please check table 1 [here|https://cassandra.apache.org/doc/latest/new/java11.html?highlight=support%20matrix].

As [~e.dimitrova] says under j8 section we have j11 tests to test that cross-jvm combination. What I was referring to is that under the j11 section the j11 unit tests should not require approval. I makes no sense to me that j11/cqlsh and j11/jvm_dtests don't require approval and always triggered automatically but the j11/junit does. Would you guys agree?

Raising it up here since it's a 1 loc change while we're at it and it's a pain imo for those that use circle. I can't think of any scenario you don't want to run only j11/junit for aq PR.

bq. several tests that currently don't work with that approach

Mmmm shouldn't be the case unless I am missing sthg. If you're not dirty-hack looping but using Junit to loop proper everything should be correctly isolated and equivalent to looping in sh or ant imo.

{quote} As Ekaterina Dimitrova says under j8 section we have j11 tests to test that cross-jvm combination. What I was referring to is that under the j11 section the j11 unit tests should not require approval. I makes no sense to me that j11/cqlsh and j11/jvm_dtests don't require approval and always triggered automatically but the j11/junit does. Would you guys agree?
{quote}
I agree, it makes sense to me to automatically start j11 unit tests for j11 builds, since we already do that with cqlsh and jvm_dtests. There is a comment in that part of the config saying that running j11 unit tests is currently not working ([here|https://github.com/adelapena/cassandra/commit/61ba75d405d64d9c1c60f11be554de1b88ecd622#diff-bc7cfdde47dcf913bec381c90c03b51e430f1e4b34ff2ac1c419d87436c85137L265]). Maybe they were originally set as optional just because they didn't work at the time of writing that part of the file? 

{quote}Mmmm shouldn't be the case unless I am missing sthg. If you're not dirty-hack looping but using Junit to loop proper everything should be correctly isolated and equivalent to looping in sh or ant imo.
{quote}
I have just added your [RepeatRunner|https://github.com/apache/cassandra/pull/987/files#diff-3b232ce364185d49c362ae79ed940d43bb9acc2c90d8e996ae1f4bae2d611b35R1] to {{ViewTest}} and run:
{code:java}
ant testsome -Dtest.name=org.apache.cassandra.cql3.ViewTest  -Dtest.methods=testCompoundPartitionKey
{code}
It fails in the second iteration because the driver's session has been closed (all runs share the same JVM). Did you manage to make one of these tests work with that Junit runner?

As for the bash looping approach, I think we can have a nice performance boost if we add a flag to optionally disable [the dependency from the {{build-test}} ant target|https://github.com/apache/cassandra/blob/trunk/build.xml#L1504] in each loop iteration, provided that we call {{build-test}} before starting the loop. That's never going to be as fast as looping inside the same JVM with JUnit, of course.

[~adelapena] I haven't tried with any of those tests. If some connection gets closed then that should be managed somehow imo. I would like to think junits being re-runnable. But if that is going to require extensive rewriting then it makes no sense on 4.0.

Also {{RepeatRunner}} is good for a quick hack but how do you chain runners? how do you specify the test to run without having to annotate files? etc. The junit native looping needs more work. If we can't think of a quick way around those then your sh/ant loop it is for the time being. We could though commit the {{RepeatRunner}} code. It will come handy sometimes, at least it has for me, wdyt?

[~bereng] Indeed JUnit looping needs more work, we should probably go for the bash loop approach for the moment. If at some point we have a working repeated JUnit runner we can easily change the CircleCI config to use it.

 I have added a loop-based CircleCI job for running repeated JUnit tests. That job can be used for regular unit tests,  long unit tests, jvm dtests, etc. The Ant target is set in the env var [{{REPEATED_UTEST_TARGET}}|https://github.com/apache/cassandra/blob/a63fc6b9dee772caa5014a543046a12a5d23eda5/.circleci/config-2_1.yml#L38-L44]. I still have to add some doc, but I think that the PR is ready for a first round of review.

As an example, I have used the repeated test jobs to reproduce some of the flaky tests that we have opened for 4.0:
||branch||config||run||
|CASSANDRA-16644|[diff|https://github.com/adelapena/cassandra/compare/16625-trunk..16625-trunk-run-16644]|[3270|https://app.circleci.com/pipelines/github/adelapena/cassandra/376/workflows/eb463f0a-ad93-488a-89a3-bb44fbc0c578/jobs/3270]|
|CASSANDRA-16627|[diff|https://github.com/adelapena/cassandra/compare/16625-trunk..16625-trunk-run-16627]|[3271|https://app.circleci.com/pipelines/github/adelapena/cassandra/375/workflows/1d48154f-b6eb-4d0b-bb88-fe40a8d40f27/jobs/3271]|
|CASSANDRA-16637|[diff|https://github.com/adelapena/cassandra/compare/16625-trunk..16625-trunk-run-16637]|[3255|https://app.circleci.com/pipelines/github/adelapena/cassandra/374/workflows/df3c4361-c6f3-4cbf-801f-dfedeef95652/jobs/3255]|
|CASSANDRA-16598|[diff|https://github.com/adelapena/cassandra/compare/16625-trunk..16625-trunk-run-16598]|[3260|https://app.circleci.com/pipelines/github/adelapena/cassandra/373/workflows/f1612e92-8a59-483b-bc79-d6f29a7f5766/jobs/3260]|

All the test failures reported in the tickets are reproduced. For some of the runs the CircleCI UI is not able to properly parse and highlight the failed test output, probably due to its large size. However, the failures can still be seen in the downloadable test output file.

Unfortunately there isn't an easy way to specify the repeated tests parameters in the CircleCI UI. It's however possible to [trigger a pipeline|https://support.circleci.com/hc/en-us/articles/360050351292-How-to-trigger-a-workflow-via-CircleCI-API-v2] with a POST request specifying parameters. The problem is that pipeline parameters should be in the final {{config.yml}} file, but {{circleci config process}} removes them from the configuration in {{config-2_1.yml}} (see [here|https://github.com/CircleCI-Public/circleci-cli/issues/359]), which is a CircleCI limitation if not a bug. We could generate patches to add the repeated tests parameters to the resource files that are generated from the config, so we can start repeated runs with a simple http request without editing the config files. However patching automatically generated files is probably too risky, given that they might change depending on the used circleCI CLI. wdyt?

bq. However patching automatically generated files is probably too risky, given that they might change depending on the used circleCI CLI. 

I am not sure I followed your thoughts correctly on this. Patching the final resource files? Similar to what we do with resources or something else? And what are the risks? 
If I understand you correctly, I guess we just need to ensure that patches will be always checked after generating new config files, similar to what we do with the current ones. Am I totally off here? 

What I mean is whoever prepares new config and the patches and adds those parameters will verify that nothing got broken in the final files. After that people just use the config files as usual. If at some point something does not work properly, we will think of another strategy. Does this make sense?

The patches in the current generation script are for the {{config-2_1.yml}} file, so first we patch and then we generate the final {{config.yml[.LOWRES|.MIDRES|.HIGHRES]}} file with the CircleCI CLI, using the command {{circleci config process}}. That's in line with the idea of never editing the {{config.yml}} manually.

However {{circleci config process}} gets rid of the pipeline parameters, so we would need to add another set of patches for the final {{config.yml[.LOWRES|.MIDRES|.HIGHRES]}} files. Since these files are generated automatically, I'm afraid that different versions of the CircleCI tool could produce different final resource files. That could make patching those files a more manual process, with an increased risk of wrongly applying the parameters changes to them. It's not that we can't do it, but just that we would make the process of editing the CircleCI config more difficult.

By the way, I have also observed that the CircleCI CLI tool also removes the license header when generating the config, so the current {{.circleci/generate.sh}} also requires some minor manual intervention to add the missed licenses. I have included a fix for this in the patch.

Thank you for the detailed explanation [~adelapena].
So my assumption that it will be issue for whoever updates/prepares new patches/final config files not for the end user is right. For the end user it will be still a matter of copy paste the file and just add what they want to do in their request. 
I am not sure how much this could be an issue to be honest and what could be the problems for that person who is updating the config. I see on average changes to those config files happen once per month. 
I guess it is a tradeoff:
- if we don't have additional patch and someone wants to use them, they should be adding manually the parameters to the file
- if we want to make it possible through POST, then we might make it prone to issues during development and updating the files.

I kind of feel that both options give great benefit but they are suboptimal but there are currently limitations in CircleCI that do not give us a better option. In both cases people will need a readme and either copy-paste parameters to the file or set parameters in the POST request. In that sense I guess too many patches becomes a bit of overkill.

bq. By the way, I have also observed that the CircleCI CLI tool also removes the license header when generating the config, so the current .circleci/generate.sh also requires some minor manual intervention to add the missed licenses. I have included a fix for this in the patch.

Great catch, thanks!

One thing that I am worried about and I am wondering whether there is a way to make our lives easier - under Artifacts we will have all the logs from all the runs which is super helpful but currently I don't see any identification of which logs are for which run... So in case of 3 fails in 1000 runs I am not sure how we can easily identify the logs we need. Any ideas? Probably we can try to add some identification to the logs for example. WDYT?

{quote}One thing that I am worried about and I am wondering whether there is a way to make our lives easier - under Artifacts we will have all the logs from all the runs which is super helpful but currently I don't see any identification of which logs are for which run... So in case of 3 fails in 1000 runs I am not sure how we can easily identify the logs we need. Any ideas? Probably we can try to add some identification to the logs for example. WDYT?
{quote}
Good idea, I have added a {{passes}}/{{fails}} prefix to the artifacts' paths, so it's easier to find the failing tests. The run number is also included in the path. I have also added the per-test standard output to the saved artifacts, so we don't have all the runner's output in a single gigantic file. Here are a couple of example runs, you can see that the artifacts are saved in directories such as {{logs/passes/}}, {{logs/fails/}}, etc.:
||ticket||config||run||
|CASSANDRA-16627|[diff|https://github.com/adelapena/cassandra/compare/16625-trunk..16625-trunk-run-16627]|[3359|https://app.circleci.com/pipelines/github/adelapena/cassandra/389/workflows/cd4d6b0a-1a66-4086-b173-b1e1303a11eb/jobs/3359]|
|CASSANDRA-16598|[diff|https://github.com/adelapena/cassandra/compare/16625-trunk..16625-trunk-run-16598]|[3346|https://app.circleci.com/pipelines/github/adelapena/cassandra/388/workflows/034b59a0-11ed-469f-bb24-d13922f8a942/jobs/3346]|

[~adelapena] I haven't started a review here yet. But please notice a std run has around [5K|https://app.circleci.com/pipelines/github/adelapena/cassandra/375/workflows/1d48154f-b6eb-4d0b-bb88-fe40a8d40f27/jobs/3241] tests. [This|https://app.circleci.com/pipelines/github/adelapena/cassandra/375/workflows/1d48154f-b6eb-4d0b-bb88-fe40a8d40f27/jobs/3271/tests] multiplex run of 10K does indeed run 10K times iiuc but reports 150K tests being run.

bq. 3 tests failed out of 150000

[~bereng] that run repeats the entire {{DirectoriesTest}} suite 10K times. Since the suite has 15 tests, the number of run tests is 150K. The env var [{{REPEATED_UTEST_METHODS}}|https://github.com/apache/cassandra/blob/0ff4e086dfada6b63f0c7b44551e6e53b902d5b2/.circleci/config-2_1.yml#L53] can be used to run only {{DirectoriesTest#testSecondaryIndexDirectories}}, but I left it blank in case something in the other tests was producing the failure.

My bad lol!

* The addition of  {{passes}}/{{fails}} prefix to the artifacts' paths plus the run number is a huge win IMHO. Thank you! It will save us time definitely.
 * Good catch to make the run of J11 unit tests a requirement.
 * Great catch fixing the _generate.sh_ script to add the license header when we update the files!

 +1, just added a few small formatting suggestions. 

Just one thing on my mind, when I hear REPEATED_UTEST_TARGET --> I don't think of the in-jvm tests but only about Unit tests so wondering about the naming, but I guess when we have the docs and the examples it should be clear.

As part of this ticket or a separate one, I think we should update at least this page, otherwise we risk people not to know about this multiplexer which could help us in many, many cases to save time - [https://cassandra.apache.org/doc/latest/development/testing.html?highlight=testing]

 

[~e.dimitrova] Thanks for the review, I've tried to address your suggestions and added some additional documentation.
{quote}Just one thing on my mind, when I hear REPEATED_UTEST_TARGET --> I don't think of the in-jvm tests but only about Unit tests so wondering about the naming, but I guess when we have the docs and the examples it should be clear.
{quote}
Yeah, I wasn't very sure about what names use for the jobs. Maybe we can use {{repeated-java-test}}/{{repeated-python-test}}, or {{repeated-junit-test}}/{{repeated-pytest-test}} instead? I used {{repeated-utest}}/{{repeated-dtest}} for brevity, which is convenient for the tiny boxes printed by the CircleCI UI, but I agree that it can be confusing. Perhaps we can also use {{junit-multiplexer}}/{{pytest-multiplexer}}, or {{java-multiplexer}}/{{python-multiplexer}}, although I'm not sure how the usage of "multiplexer" is known outside DS. wdyt?

Thanks [~adelapena].
I think we can keep the current names.
I left minor suggestion in the doc but overall all looks good to me! On my end this seems like work ready to commit. Thank you!

One thing on my mind, I think when this is committed we need to announce it on the mailing list to be sure people see it and they start using it as It is a big win.
I think it would be good also to encourage them to start using it both in case of failure and when they are adding new tests - pre-commit to be sure they didn't introduce new flaky test. WDYT?

Thanks, I have added PRs for 3.0, 3.11 and 4.0, in case we want to have the multiplexer in those branches too. I still have to further test them with a few configs but it seems that porting back the CircleCI config isn't especially problematic.

{quote}
One thing on my mind, I think when this is committed we need to announce it on the mailing list to be sure people see it and they start using it as It is a big win.
I think it would be good also to encourage them to start using it both in case of failure and when they are adding new tests - pre-commit to be sure they didn't introduce new flaky test. WDYT?
{quote}
Agree, we should announce this on the dev list. It would be great if repeated runs were assimilated into the process.

[~adelapena] I will try to review this soon but if you want to ahead and commit: is there a way to fail fast? IOW if I have a 10K run but I don't want to wait for completion but instead fail on the first failure can I do that?

[~bereng] good idea, I have just added a couple of options for stopping the iteration on the first failure. I think we can wait for your review, should I set you as second reviewer?

I have done a pass and I am +1 besides a couple minor questions. I would suggest adding QA for this ticket before merging to make sure we don't break anything. That is CI runs for low, mid and res. Wdyt?

{quote}I have done a pass and I am +1 besides a couple minor questions. I would suggest adding QA for this ticket before merging to make sure we don't break anything. That is CI runs for low, mid and res. Wdyt?
{quote}
[~bereng] here are the CI runs for the three resources configs, with and without tests:


 Runs with defaults and LOWRES:
 * [3.0 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/421/workflows/9a8bb777-0420-4caa-ae2b-4b5f298b3ef8]
 * [3.11 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/422/workflows/1f34cba7-17d4-47a9-b43f-c08683a9535b]
 * [4.0 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/424/workflows/9c6b37a3-c021-40da-99f8-e859efcd8869]
 * [4.0 j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/424/workflows/4e25a53b-35de-4665-a5bf-35ddf411efa6]
 * [trunk j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/423/workflows/febd92cf-89ff-4ae9-8abc-c530c9c42a6e]
 * [trunk j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/423/workflows/696e80be-0736-4890-a869-b81161437e28]

Runs with defaults and HIGHRES:
 * [3.0 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/427/workflows/6b9da82f-09ee-4b3d-a08f-1ab3097cb6c4]
 * [3.11 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/426/workflows/eb4d62e5-e719-45f4-afdd-d7e12478cb13]
 * [4.0 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/428/workflows/acc915a4-0d52-4140-99ca-f74e2384a6da]
 * [4.0 j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/428/workflows/bd408369-4a34-4b9e-b4d0-1118f84fcc38]
 * [trunk j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/425/workflows/ab0a87a8-bc1e-42cd-9d33-681720655ce1]
 * [trunk j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/425/workflows/b6b28b3f-4085-4875-8dfe-2aef8c8e98f3]

Runs with defaults and MIDRES:
 * [4.0 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/430/workflows/08338dc6-a337-467d-b0fd-2481c7731d4f]
 * [4.0 j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/430/workflows/2b25e012-da20-4306-85f9-6f09a94a560c]
 * [trunk j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/429/workflows/3946338e-4088-4384-b8ae-13e6850e712b]
 * [trunk j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/429/workflows/6ea7f868-92e5-4193-ba74-6e8a0ac9c859]

Runs with some tests and LOWRES:
 * [3.0 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/432/workflows/74351ae6-8281-4d23-9bba-b9805531418d]
 * [3.11 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/433/workflows/1fdd80ee-4179-4b5b-9d42-295ddc4d9a88]
 * [4.0 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/434/workflows/3e946bea-8288-40e3-b216-f4eee90a053d]
 * [4.0 j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/434/workflows/108a3ff2-4833-4778-97fb-89cf62e168ed]
 * [trunk j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/431/workflows/dafc73f5-e35f-455d-bf91-1eb31442f420]
 * [trunk j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/431/workflows/6858dab1-5fc4-4226-bf54-378a806fd738]

Runs with some tests and HIGHRES:
 * [3.0 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/437/workflows/9da6a939-4924-4e4c-9860-662f59144f3d]
 * [3.11 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/435/workflows/775f48bd-22d6-4b07-8d39-91f950eb0409]
 * [4.0 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/438/workflows/b5a641ab-d3ee-4e91-9808-eaf471978ea8]
 * [4.0 j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/438/workflows/52287cc8-c4ee-46e0-9687-dacdd10e9751]
 * [trunk j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/436/workflows/2b8e847f-2a2b-431a-87ed-68d78bd81a3c]
 * [trunk j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/436/workflows/1cff6266-f0bd-4e0e-9ad4-8b8c8b7d82f1]

Runs with some tests and MIDRES:
 * [4.0 j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/440/workflows/5a57b51c-ffdc-4d52-817e-2ef3e1b07b1f]
 * [4.0 j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/440/workflows/84b1f227-c53a-4c73-9c4c-9ad5877c32de]
 * [trunk j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/439/workflows/bbf4aa3d-8033-497d-9d74-e7a7ac30b391]
 * [trunk j11 |https://app.circleci.com/pipelines/github/adelapena/cassandra/439/workflows/f20d7165-2ad7-408c-b263-fc792d67d323]
  

 

[~bereng] [~e.dimitrova] I'd say the runs above look good, it's ok with you I think we are ready to commit.

Thanks [~adelapena], I would say, please, commit.

It looks great and serves its needs. If there are suggestions for further options or so to be added, we can always open a new ticket with new PRs. This one is already saving us time and it is a win for getting rid of the flaky tests. Thank you for all your work!

Great work +1

Thanks for the reviews, committed to 3.0 as [a01cbfdce5e6b8959b792b3f693f87c2fc8f6b92|https://github.com/apache/cassandra/commit/a01cbfdce5e6b8959b792b3f693f87c2fc8f6b92] and merged up to [3.11|https://github.com/apache/cassandra/commit/1e42c105ce31613a8c5e94e4c2d4c817e12a4dce], [4.0|https://github.com/apache/cassandra/commit/98b449f38aa5a5ae64d030c1f437847a3c938cee] and [trunk|https://github.com/apache/cassandra/commit/f637198484c74b71429b1bc884d3fe9a86a4a926].

bq. EDIT: I am also wondering about raising cql cmds timeouts...

I would do that in a separate ticket, since that doesn't seem a particular problem of the multiplexer but of dtests and CI envs in general. Nevertheless 30sec seems like a long timeout, perhaps we should investigate first if there's something in the tests or in the CI environments making us exceed such a long timeout?

Apologies [~adelapena] that comment belonged to a different ticket...

