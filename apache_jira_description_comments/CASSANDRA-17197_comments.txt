Here is the patch adding diagnostic events for guardrails:
||PR||CI||
|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1340/workflows/58cc1a02-7a5a-4d60-869f-c698b56d66df] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1340/workflows/2ea9c9a1-288f-4c64-9839-611e640da42b]|

It creates a new type of diagnostic event called [{{GuardrailEvent}}|https://github.com/adelapena/cassandra/blob/17197-trunk/src/java/org/apache/cassandra/db/guardrails/GuardrailEvent.java]. There are two types on guardrail event, which are {{WARNED}} and {{{}FAILED{}}}, matching the soft and hard activation of the guardrail. These events carry two properties, which are the name of the specific guardrail and the specific error message.

As for testing, there is a consumer/listener for the new type of diagnostic events attached to every {{{}GuardrailTester{}}}, so every time that we check the activation of a guardrail we also check that the proper events have also been emitted.

An important detail is that the messages emitted when a guardrail is triggered might contain user data. None of the current guardrails does this, but the proposed guardrails for collection items and size (CASSANDRA-17153) will include the primary key of the offending row, and it's likely that other incoming guardrails will do the same.

This user data shouldn't be included into diagnostic events, so it isn't sent to external systems monitoring diagnostic events. I have added [a commit|https://github.com/apache/cassandra/pull/1485/commits/97329b94fb3de770aaae64880cef2b9cff857a00] to the PR that allows to redact sensitive data in the messages that are included in guardrail diagnostic events. The approach for redacting the messages is based on a previous patch by [~Gerrrr].

I'd be fine doing this in a separate ticket, given that we don't yet have any guardrail publishing user data. However, since this is a security thing, I think I'd prefer to include this with diagnostic events, so we don't miss it when adding other guardrails.

Here is CI for the updated patch:
||PR||CI||
|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1345/workflows/1638df69-2729-4222-872f-4f3e081bff1b] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1345/workflows/ef04d0d2-04a4-4675-83db-7fb0ffe21ea4]|

CC [~smiklosovic] [~dcapwell] 

CASSANDRA-17430 has just added identifying names to guardrails, identically to what was proposed in this patch. That simplifies the patch a bit after rebasing on top of those changes: 
||PR||CI||
|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1360/workflows/7aeb9214-e7fd-40d0-b551-5740c577004e] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1360/workflows/657078e3-a6c2-4305-95c7-66c472fe3bc1]|

I am on this 100% now.

[~adelapena] could you please rebase this on top of the current trunk? I am getting conflicts when I am trying to rebase it on my own.

Great news! I think I rebased very recently and the PR doesn't show conflicts, but I have rebased it again just in case.
||PR||CI||
|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1368/workflows/88ab802f-ccea-4176-942b-914551df13bd] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1368/workflows/039fc777-3663-488a-b635-750ecccc0552]|

Ok so I dropped just a suggestion to add some more test lines but otherwise LGTM. I did check CI and also LGTM but the repeated runs have 2 failing workers. Test results were not uploaded and looking at some of the failed logs the tests I looked at passed. So it's probably some env issue causing the red. This would need to be confirmed or a new repeatable run be triggered. Otherwise +1 from me.

Thanks for the review. I have added your suggestion and I'm running CI again, let's see if we have better luck with the workers:
||PR||CI||
|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1375/workflows/5133718b-8143-4382-87df-ea2b1c3e7df4] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1375/workflows/a7bfde6d-4e03-428b-9886-3671213c0f1c]|

It seems some of the new tests are flaky, I'm trying to figure out what's going on.

It seems that the problem was that {{{}CQLTester{}}}'s asynchronous cleanup task was messing with the tests, as we have already found while fixing other flaky tests. That task produces an asynchronous flush of the tables created by the test. Since those tables contain large collections, their asynchronous flushing will emit diagnostic events that might be visible for the next tests.

I have modified the tests to immediately drop the tables at termination, so there isn't anything for the async cleanup task. I have also disabled autocompaction for the created tables, since that might be problematic too.

Here is CI doubling the number of iterations:
||PR||CI||
|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1384/workflows/93f9dee0-e2c8-4223-9b07-298d2f4a1ebe] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1384/workflows/d4fdc205-eed0-4e8a-babc-874602646fd6]|

Yes that makes sense. I checked the new CI and now I am convinced we are good to commit. Nice thing we have that repeteable feature :) +1

I think we're only waiting on Stefan to +1 now iiuc

+1

Thanks for the reviews.

Committed to {{trunk}} as [143a5e8b064e442970182cfb349b4f0826683e85|https://github.com/apache/cassandra/commit/143a5e8b064e442970182cfb349b4f0826683e85].

