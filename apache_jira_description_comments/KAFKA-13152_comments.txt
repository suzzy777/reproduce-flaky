hey [~guozhang], i would like to take this up.. Will go through and post some notes here.

[~guozhang], I was looking at this. One question i have is, what do you mean by introducing a global config? Is is at a StreamTask level or above? 

Also, in case the buffered records exceeds the config, what actions should we take? Should we for example, pause all consumers subscribed the the current StreamTask if the config is at a StreamTask level?

The current level is a per-partition config. The want a per-client config, ie, the provided value must be distributed over all threads/tasks/partitions. Also note, that because threads can be added/removed dynamically, and the tasks (and thus partition) assignment may change during a rebalance, that we need to dynamically adjust the current limit per partition on any such event.

We would pause the corresponding partition that exceeds it's "quota" (that is also what we already do right now), so nothing must be changed here.

[~sagarrao] [~mjsax] (also cc [~desai.p.rohan]) I had a slightly different idea (probably a more complex one, just to admit) when filing the ticket.

It is indeed a global config controlling the total number of bytes used for source partition record buffering, but it would not be distributed across all threads / tasks, instead we just monitor the aggregated total bytes across all tasks within the instance, when it has been reached, we can consider several options:

1) just pause all partitions; and then resume all partitions when it has dropped below the threshold. Not sure if it would result much "thrashing" on pausing / resuming, but since these actions are quite cheap anyways I'm not too worried about that.
2) pause some partitions, e.g. one heuristic is to pick the partition with most bytes; and then resume all paused partitions when it has dropped below the threshold.

Personally I'm leaning towards 1) for now for simplicity, and we can consider if this is sufficient after observing its behavior in production later.

For (1), I think we need to take time-synchronization into account. If a task has multiple partitions, and some are empty, we might delay processing base on `task.max.idle.ms` config – however, we should always allow to fetch for the partitions of empty buffers and never pause them; otherwise, we would sacrifice ordered processing and I think a tradeoff between semantics and "buffer size" would not be a good one? We could even end up in a "temporal deadlock": no task is processable as it has at least one empty buffer, but all partitions are paused because we exceeded the max-buffer-space – the deadlock is temporal, because we would go into "forced processing" after `task.max.idle.ms` passed though – or we need to change the behavior and go into "forced processed" right away for this case without waiting for `max.task.idle.ms` (but it might be desirable to ignore `task.max.idle.ms`).

Another question I have is about "balanced fetching": atm, we use the same buffer space for each partition and pause a single partition if its buffer space is exceeded. If we follow (1), could it happen that some partitions buffer much more data than others, and could this become a "fairness" problem? In the end, I agree that not have the exact same buffer space across all partitions can be beneficial: a high volume topic might be better off using more buffer space than a low volume topic. However, I am wonder if would still need some bounds to avoid that we go from the current extreme to give the exact same buffer space per partitions, to the opposite extreme for which some partitions might "starve" as their buffer space becomes too small?

 

Yeah that's a good point. I think we should not pause partitions that have no data yet. Maybe we can modify 1) above as to "pause all partitions that have some data already".

As for "fairness", I think this is either achieved or we've lost it at the consumer level, as we do round-robin fetching across all assigned partitions. And let's say if some partition A's message size is larger than partition B's message size, and assume their income record rate are the same, then partition B would have more records fetched than partition A on average.

[~mjsax], [~guozhang] I had a slightly different thought whereby we can provision it as a global setting and then distribute it, similar to how `cache.max.bytes.buffering` works atm. 

So, assuming we have T Threads and C is the max buffer size, each thread gets C/T bytes.

Once the individual StreamThreads have been assigned their respective shares of bytes, then we can look at the total bytes consumed across tasks for that thread and if it exceeds it's share, we can pause the partitions.

We can go 1 step further here and further assign each task it's share of bytes(by dividing equally) and finally bytes per partition(similar to the current per partition records config but considering bytes by further equal division) but that's just extra complexity so can be ignored.

I think, where assigning C/T bytes among threads might be beneficial as compared to the option suggested by Guozhang here: 

 *instead we just monitor the aggregated total bytes across all tasks within the instance, when it has been reached* 

is that, let's say there are more than 1 Stream Threads in an instance, and only one of them is exceeding the bounds individually, but because we are looking at the overall bytes count across all tasks, the other Threads would also end up paying the penalty and be paused. If the users provision the config properly, they should be able to pause only relevant tasks and not all. What do you think?

Regarding pausing the partitions, i think, it makes sense to pause only those partitions that have some data as you both had discussed for simplicity, Maybe, we can look at heuristics like if there's only one partition which is accounting for say X % of bytes or pick the one with most bytes and pause only those. That might make it more complicated, but lead towards pausing only relevant partitions which is what `buffered.records.per.partition` is able to achieve.

 

[~guozhang] – your proposal to only pause partitions with non-empty buffers SGTM; about fairness, I was not sure if we can/should rely on the consumer, but if you think it's the right way to go, I am ok with it. Just wanted to raise the question to ensure that we make a conscious decision.

[~sagarrao] – I think we should take the discussion into the KIP? It seem the scope is clear now, and we have two proposal: Dividing the given buffer size across thread (or maybe even task etc) or follow Guozhang's proposal. It seems your concern is similar to my concern about fairness. Guozhang pointed out the we should achieve fairness within a thread (due to consumer round robin fetching) but I guess your point is a good one, that it's unclear if we achieve fairness across threads? [~guozhang] WDYT about this?

In the end the question seems to be, if we can/should try to keep it simple vs. how complex we want to design the algorithm. Personally, I am afraid of pre-mature optimization and think keeping it simple might be the better way to get started.

It might be best if you start to work on a KIP, and explain pros/cons of both approaches, and put one into "rejected alternatives" sections and we can discuss on the mailing list?

I think it's a good idea to continue our discussion on the KIP. I'm not strongly suggest that we do one option over the other, and maybe I could be over-thinking trying to get to where the task -> thread mapping is no longer static, which would not happen yet.

Thanks [~mjsax]/[~guozhang]. I will start writing a KIP and send out an email soon.

The PR broke backward compatibility. Needed to revert the commit. Sorry about that.

Lets do a new PR based on the old commit that fixes the issue. Thx.

Is KIP-770 still in progress? If so do you want me to track it for 3.5.0?

Thanks [~mimaison], this is still quite relevant and we'd want to fix it forward. Let's move it to 3.5.0.

Ok, I've added it to the release plan. Thanks

Thanks [~guozhang] , [~mimaison] 

I don't see any changes or PRs in progress so I'm moving this to 3.6.0. If that's not correct, let me know.

hi [~mimaison] , the PR has already been raised [here|https://github.com/apache/kafka/pull/13283]. I am not sure if [~guozhang] has the bandwidth to review it though. Maybe wait for his comment and then decide if we want to move it to 3.6. or not?

Thanks for the quick reply! Somehow this PR was not attached to the ticket, I've added it now.

If you can get this merged quickly, I'm fine including it in 3.5.0.

Thanks Mickael.  I have a clarifying question, this KIP was already implemented partially in 3.4. So, in terms of freezes, is it being counted under KIP freeze or Feature/Code freeze? It's under Guozhang's radar but not sure when would he be able to review this. If it's holding up the release process, feel free to move it to 3.6 as well.

KIP Freeze denotes the deadline when a KIP must be voted in order to be considered in the next release. I typically don't see this deadline as a hard limit but rather as a way to limit bringing large changes at the last minute. KIP-770 was voted in 2021 so it's fine.

Feature freeze is when we stop merging PRs from KIPs or that introduce significant changes. The goal is to stabilize the code base before the release and have a couple of weeks to focus on fixing remaining bugs and stability issues (like flaky tests).

It's not really holding up the release process yet. I've currently marked KIP-770 for 3.6.0 but if Guozhang/Matthias think this would be a useful addition for 3.5 and review and merge the PR in the next few days, I'm ok including it in 3.5.0.





Thanks for the clarification [~mimaison] !

Moving it to 3.7.0 as we are near code freeze and it is not a blocker.

Given there hasn't been much movement in this PR since February 2023, are we targeting 3.7 as it's currently marked?

[~sagarrao] as far as I understand, you're blocked on PR reviews - is that correct?

 

[~guozhang] would you have interest to help getting this into 3.7? It keeps slipping... :(

At the moment I'm afraid I might become the bottleneck if I took any role as a primary reviewer for large PR / KIPs.. for this ticket, I think [~wcarlson@confluent.io] [~ableegoldman] and also [~lihaosky] has enough background to guide it through. I'm happy to act as a second pair of eyes but I concern not enough committed as a primary reviewer.

[~enether], that's correct.. the reviews are pending for this. I saw the PR and there are merge conflicts as well. I Will need to resolve those..

[~sagarrao] sounds good, I am fine either way. My bias as a release manager is to simply avoid ambiguity and make it clear what's hitting 3.7.0 or not. If we don't have medium-level confidence we will make 3.7, I would suggest making it clear earlier to avoid stress later on in the release process. Hope that makes sense!

 

Makes sense Stanislav! If we can have experts from Streams to review this, I would be more than happy to revive this. 

Hey [~sagarrao] – I'm happy to try and help you get this into 3.7. As a disclaimer I do have a few other KIPs I've already promised to help land in 3.7 so this would be 3rd on my list, but we still have a good amount of time and given the KIP itself is already accepted, I think we can make it.

Just give me a ping on the PR when it's ready for me to take a look. And just to refresh my memory, all the caching work – both config and metrics -- are already merged, so the only thing remaining is to add (and implement) the new input.buffer.max.bytes config. Does that sound right?

Thanks [~ableegoldman], there are merge conflicts which need resolution. I will ping you once those are resolved. It being 3rd in your pecking order of KIPs is fine with me. And yes, the config related changes have been merged and released. It's only the actual buffering part. 

Changing target fix version to 3.8 since this is not a blocker and we are cutting a 3.7 RC

 

