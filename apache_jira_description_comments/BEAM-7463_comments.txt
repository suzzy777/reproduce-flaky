I might know what causes the flakyness:

When the tests are run in parallel with nose they share class variables. The setup and tear down methods are called once for each test case in a class. Many big query test classes create a unique dataset name in the setup and delete this dataset in the tear down. Some of the tests share the same output table name

This can lead to the following executions:
 * when data is written with WRITE_EMPTY, it is possible that a second test already empties the table and right after this the data is checked in the first test. This would cause this checksum error
 * when the tear down method is called this deleted the dataset and it is possible that after deleting a dataset, the data is checked in another test. This would cause 
not found: Table apache-beam-testing:python_bq_file_loads_15595843114095.beam_load_2019_06_03_175817_9_5b3b6196cef9fd926078ec31862c8d0b_0 was not found in location US

 

I added a pr [https://github.com/apache/beam/pull/8751] to make sure that variables are not shared among different test executions.

Sumilar repro, but "but" field is empty:

[https://builds.apache.org/job/beam_PostCommit_Python3_Verify/1189/consoleFull]

[~pabloem] could you please take a look into BQ  suites? your expertise in BQ connector may help us track this down faster. 
[~Juta], Do you remember why we removed sorting in Bigquery matcher? See: https://github.com/apache/beam/pull/8621/files#diff-f1ec7e3a3e7e2e5082ddb7043954c108R134. Does BQ guarantee some order of returned result? if not, then that may very well cause the checksum errors, but there are also other errors with this test like table not found and  but "but" field is empty, perhaps we should investigate these separately.

Also, to echo some points from conversation on https://github.com/apache/beam/pull/8751 for visibility:  class attributes assigned in setup/teardown method are not shared between  execution of different tests cases defined in the same test class, so the race of table cleanup is not obvious to me.

taking a look...

I remember removing the sorting from the BigqueryFullResultMatcher because I think the order of the elements within a row should not be sorted. I updated the tests using the BigqueryFullResultMatcher to include the order in the query (and thus have a deterministic order for the elements within one row). The outer dimension is then still sorted.

I looked at the tests that are failing due to the checksum error and they seem to be using the BigqueryMatcher which also does not sort the elements within a row and when computing the checksum it does sort the outer dimension. However the checksum in sometimes incorrect even for test that include an order in the query (e.g. for this test: [https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/gcp/big_query_query_to_table_it_test.py#L196])

 

I've been running this over an over about 20 times, and I have not been able to reproduce an error with the matcher: apache_beam.io.gcp.bigquery_write_it_test:BigQueryWriteIntegrationTests.test_big_query_write_new_types

I am wondering if it could have to do with the machine where the tests run...

[~pabloem] Which runner did you use? Seeing this DirectRunner failure on apache-beam-jenkins-11:


{noformat}
14:21:16 > Task :sdks:python:directRunnerIT
14:21:16 test_big_query_legacy_sql (apache_beam.io.gcp.big_query_query_to_table_it_test.BigQueryQueryToTableIT) ... ok
14:21:16 test_big_query_new_types (apache_beam.io.gcp.big_query_query_to_table_it_test.BigQueryQueryToTableIT) ... FAIL
14:21:16 test_big_query_standard_sql (apache_beam.io.gcp.big_query_query_to_table_it_test.BigQueryQueryToTableIT) ... ok
14:21:16 test_big_query_standard_sql_kms_key_native (apache_beam.io.gcp.big_query_query_to_table_it_test.BigQueryQueryToTableIT) ... SKIP: This test doesn't work on DirectRunner.
14:21:19 test_streaming_data_only (apache_beam.io.gcp.pubsub_integration_test.PubSubIntegrationTest) ... ok
14:21:19 test_streaming_with_attributes (apache_beam.io.gcp.pubsub_integration_test.PubSubIntegrationTest) ... ok
14:21:20 test_wordcount_it (apache_beam.examples.wordcount_it_test.WordCountIT) ... ok
14:21:20 
14:21:20 ======================================================================
14:21:20 FAIL: test_big_query_new_types (apache_beam.io.gcp.big_query_query_to_table_it_test.BigQueryQueryToTableIT)
14:21:20 ----------------------------------------------------------------------
14:21:20 Traceback (most recent call last):
14:21:20   File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify_PR/src/sdks/python/apache_beam/io/gcp/big_query_query_to_table_it_test.py", line 211, in test_big_query_new_types
14:21:20     big_query_query_to_table_pipeline.run_bq_pipeline(options)
14:21:20   File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify_PR/src/sdks/python/apache_beam/io/gcp/big_query_query_to_table_pipeline.py", line 82, in run_bq_pipeline
14:21:20     result = p.run()
14:21:20   File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify_PR/src/sdks/python/apache_beam/testing/test_pipeline.py", line 107, in run
14:21:20     else test_runner_api))
14:21:20   File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify_PR/src/sdks/python/apache_beam/pipeline.py", line 406, in run
14:21:20     self._options).run(False)
14:21:20   File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify_PR/src/sdks/python/apache_beam/pipeline.py", line 419, in run
14:21:20     return self.runner.run_pipeline(self, self._options)
14:21:20   File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify_PR/src/sdks/python/apache_beam/runners/direct/test_direct_runner.py", line 51, in run_pipeline
14:21:20     hc_assert_that(self.result, pickler.loads(on_success_matcher))
14:21:20 AssertionError: 
14:21:20 Expected: (Test pipeline expected terminated in state: DONE and Expected checksum is 1631ca7060b89a01760c81874b988c46156e18b5)
14:21:20      but: 
14:21:20 
14:21:20 -------------------- >> begin captured logging << --------------------
14:21:20 root: DEBUG: Connecting using Google Application Default Credentials.
14:21:20 oauth2client.transport: INFO: Attempting refresh to obtain initial access_token
14:21:20 root: INFO: Running pipeline with DirectRunner.
14:21:20 root: DEBUG: Connecting using Google Application Default Credentials.
14:21:20 oauth2client.transport: INFO: Attempting refresh to obtain initial access_token
14:21:20 root: INFO: Using location u'US' from table <TableReference
14:21:20  datasetId: u'python_query_to_table_15626208616472'
14:21:20  projectId: u'apache-beam-testing'
14:21:20  tableId: u'python_new_types_table'> referenced by query SELECT bytes, date, time FROM [python_query_to_table_15626208616472.python_new_types_table]
14:21:20 root: WARNING: Dataset apache-beam-testing:temp_dataset_f7ed13498f964c5888d607bd200e0e62 does not exist so we will create it as temporary with location=US
14:21:20 root: DEBUG: Connecting using Google Application Default Credentials.
14:21:20 root: DEBUG: Creating or getting table <TableReference
14:21:20  datasetId: 'python_query_to_table_15626208616472'
14:21:20  projectId: 'apache-beam-testing'
14:21:20  tableId: 'output_table'> with schema {'fields': [{'type': u'BYTES', 'name': u'bytes', 'mode': 'NULLABLE'}, {'type': u'DATE', 'name': u'date', 'mode': 'NULLABLE'}, {'type': u'TIME', 'name': u'time', 'mode': 'NULLABLE'}]}.
14:21:20 oauth2client.transport: INFO: Attempting refresh to obtain initial access_token
14:21:20 root: DEBUG: Created the table with id output_table
14:21:20 root: INFO: Created table apache-beam-testing.python_query_to_table_15626208616472.output_table with schema <TableSchema
14:21:20  fields: [<TableFieldSchema
14:21:20  fields: []
14:21:20  mode: u'NULLABLE'
14:21:20  name: u'bytes'
14:21:20  type: u'BYTES'>, <TableFieldSchema
14:21:20  fields: []
14:21:20  mode: u'NULLABLE'
14:21:20  name: u'date'
14:21:20  type: u'DATE'>, <TableFieldSchema
14:21:20  fields: []
14:21:20  mode: u'NULLABLE'
14:21:20  name: u'time'
14:21:20  type: u'TIME'>]>. Result: <Table
14:21:20  creationTime: 1562620866768
14:21:20  etag: u'qVCmDaC3dDohfi+Eh7thdw=='
14:21:20  id: u'apache-beam-testing:python_query_to_table_15626208616472.output_table'
14:21:20  kind: u'bigquery#table'
14:21:20  lastModifiedTime: 1562620866835
14:21:20  location: u'US'
14:21:20  numBytes: 0
14:21:20  numLongTermBytes: 0
14:21:20  numRows: 0
14:21:20  schema: <TableSchema
14:21:20  fields: [<TableFieldSchema
14:21:20  fields: []
14:21:20  mode: u'NULLABLE'
14:21:20  name: u'bytes'
14:21:20  type: u'BYTES'>, <TableFieldSchema
14:21:20  fields: []
14:21:20  mode: u'NULLABLE'
14:21:20  name: u'date'
14:21:20  type: u'DATE'>, <TableFieldSchema
14:21:20  fields: []
14:21:20  mode: u'NULLABLE'
14:21:20  name: u'time'
14:21:20  type: u'TIME'>]>
14:21:20  selfLink: u'https://www.googleapis.com/bigquery/v2/projects/apache-beam-testing/datasets/python_query_to_table_15626208616472/tables/output_table'
14:21:20  tableReference: <TableReference
14:21:20  datasetId: u'python_query_to_table_15626208616472'
14:21:20  projectId: u'apache-beam-testing'
14:21:20  tableId: u'output_table'>
14:21:20  type: u'TABLE'>.
14:21:20 root: DEBUG: Attempting to flush to all destinations. Total buffered: 4
14:21:20 root: DEBUG: Flushing data to apache-beam-testing:python_query_to_table_15626208616472.output_table. Total 4 rows.
14:21:20 root: DEBUG: Passed: True. Errors are []
14:21:20 root: INFO: Start verify Bigquery data.
14:21:20 google.auth.transport._http_client: DEBUG: Making request: GET http://169.254.169.254
14:21:20 google.auth.transport._http_client: DEBUG: Making request: GET http://metadata.google.internal/computeMetadata/v1/project/project-id
14:21:20 urllib3.util.retry: DEBUG: Converted retries value: 3 -> Retry(total=3, connect=None, read=None, redirect=None, status=None)
14:21:20 google.auth.transport.requests: DEBUG: Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true
14:21:20 urllib3.connectionpool: DEBUG: Starting new HTTP connection (1): metadata.google.internal:80
14:21:20 urllib3.connectionpool: DEBUG: http://metadata.google.internal:80 "GET /computeMetadata/v1/instance/service-accounts/default/?recursive=true HTTP/1.1" 200 144
14:21:20 google.auth.transport.requests: DEBUG: Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/844138762903-compute@developer.gserviceaccount.com/token
14:21:20 urllib3.connectionpool: DEBUG: http://metadata.google.internal:80 "GET /computeMetadata/v1/instance/service-accounts/844138762903-compute@developer.gserviceaccount.com/token HTTP/1.1" 200 176
14:21:20 urllib3.connectionpool: DEBUG: Starting new HTTPS connection (1): www.googleapis.com:443
14:21:20 urllib3.connectionpool: DEBUG: https://www.googleapis.com:443 "POST /bigquery/v2/projects/apache-beam-testing/jobs HTTP/1.1" 200 None
14:21:20 urllib3.connectionpool: DEBUG: https://www.googleapis.com:443 "GET /bigquery/v2/projects/apache-beam-testing/queries/6882e326-23ab-4624-8d80-732b2bd6e108?location=US&maxResults=0 HTTP/1.1" 200 None
14:21:20 urllib3.connectionpool: DEBUG: https://www.googleapis.com:443 "GET /bigquery/v2/projects/apache-beam-testing/jobs/6882e326-23ab-4624-8d80-732b2bd6e108?location=US HTTP/1.1" 200 None
14:21:20 urllib3.connectionpool: DEBUG: https://www.googleapis.com:443 "GET /bigquery/v2/projects/apache-beam-testing/datasets/_7357fab0f784d2a7327ddbe81cdd1f4ca7e429cd/tables/anon90739891049f0700f371e74e94b02d37a805a270/data HTTP/1.1" 200 None
14:21:20 root: INFO: Read from given query (SELECT bytes, date, time FROM `python_query_to_table_15626208616472.output_table`;), total rows 0
14:21:20 root: INFO: Generate checksum: da39a3ee5e6b4b0d3255bfef95601890afd80709
14:21:20 root: INFO: Start verify Bigquery data.
14:21:20 google.auth.transport._http_client: DEBUG: Making request: GET http://169.254.169.254
14:21:20 google.auth.transport._http_client: DEBUG: Making request: GET http://metadata.google.internal/computeMetadata/v1/project/project-id
14:21:20 urllib3.util.retry: DEBUG: Converted retries value: 3 -> Retry(total=3, connect=None, read=None, redirect=None, status=None)
14:21:20 google.auth.transport.requests: DEBUG: Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true
14:21:20 urllib3.connectionpool: DEBUG: Starting new HTTP connection (1): metadata.google.internal:80
14:21:20 urllib3.connectionpool: DEBUG: http://metadata.google.internal:80 "GET /computeMetadata/v1/instance/service-accounts/default/?recursive=true HTTP/1.1" 200 144
14:21:20 google.auth.transport.requests: DEBUG: Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/844138762903-compute@developer.gserviceaccount.com/token
14:21:20 urllib3.connectionpool: DEBUG: http://metadata.google.internal:80 "GET /computeMetadata/v1/instance/service-accounts/844138762903-compute@developer.gserviceaccount.com/token HTTP/1.1" 200 176
14:21:20 urllib3.connectionpool: DEBUG: Starting new HTTPS connection (1): www.googleapis.com:443
14:21:20 urllib3.connectionpool: DEBUG: https://www.googleapis.com:443 "POST /bigquery/v2/projects/apache-beam-testing/jobs HTTP/1.1" 200 None
14:21:20 urllib3.connectionpool: DEBUG: https://www.googleapis.com:443 "GET /bigquery/v2/projects/apache-beam-testing/queries/d08af240-f21f-4d3e-a4d6-123520bd483a?location=US&maxResults=0 HTTP/1.1" 200 None
14:21:20 urllib3.connectionpool: DEBUG: https://www.googleapis.com:443 "GET /bigquery/v2/projects/apache-beam-testing/jobs/d08af240-f21f-4d3e-a4d6-123520bd483a?location=US HTTP/1.1" 200 None
14:21:20 urllib3.connectionpool: DEBUG: https://www.googleapis.com:443 "GET /bigquery/v2/projects/apache-beam-testing/datasets/_7357fab0f784d2a7327ddbe81cdd1f4ca7e429cd/tables/anon99c42be2_6544_4f73_a75e_cef6d47d7c8e/data HTTP/1.1" 200 None
14:21:20 root: INFO: Read from given query (SELECT bytes, date, time FROM `python_query_to_table_15626208616472.output_table`;), total rows 4
14:21:20 root: INFO: Generate checksum: 1631ca7060b89a01760c81874b988c46156e18b5
14:21:20 --------------------- >> end captured logging << ---------------------
14:21:20 
14:21:20 ----------------------------------------------------------------------
14:21:20 XML: /home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify_PR/src/sdks/python/nosetests.xml
14:21:20 ----------------------------------------------------------------------
14:21:20 Ran 9 tests in 24.097s
14:21:20 
14:21:20 FAILED (SKIP=1, failures=1)
14:21:20 
14:21:20 > Task :sdks:python:directRunnerIT FAILED
{noformat}
 

Renamed the issue to clarify that the failure is in DirectRunner. Also, the error may have to do with the fact that we run this test in parallel in 3 suites on Jenkins. We may be able to reproduce it by running a gradle command that starts all postcommit direct runner suites. If the feedback loop is too long we can try to remove all tests from the suite except for BigQueryQueryToTableIT and retry.

I think we should investigate separately the incorrect checksum error and empty assertion error, until we have a reason to believe that the rootcause is the same. 
Let's use https://issues.apache.org/jira/browse/BEAM-5874 to track failures due to an incomplete assertion a la
{noformat}
13:35:13 Expected: (Test pipeline expected terminated in state: DONE and Expected checksum is e1fbcb5ca479a5ca5f9ecf444d6998beee4d44c6)
13:35:13      but: "
{noformat}


For incorrect checksum error, if we cannot reproduce it, let's make the error message more informative, so that we can have more information when the issue reoccurs: perhaps print raw table data or don't clean up the bigquery table so that it can be inspected manually after test execution completes.
Note that we can configure a TTL on Bigquery tables [1], so keeping them around after test execution should not be a concern.
[1] https://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_expiration_time

gah - I was running on dataflow. 
What's the best command to run these tests on direct runner?
I am trying  `./scripts/run_integration_test.sh --runner TestDirectRunner --sdk_location dist/apache-beam-2.15.0.dev0.tar.gz --test_opts --tests=apache_beam.io.gcp.bigquery_write_it_test:BigQueryWriteIntegrationTests.test_big_query_write_new_types` - but it doesn't look like it's working properly.

We don't need to pass SDK location.

This is where the command that runs on Jenkins is evaluated, overall it looks similar to yours:

https://github.com/apache/beam/blob/5fb21e38d9d0e73db514e13a93c15578302c11fa/sdks/python/test-suites/direct/py35/build.gradle#L50


This tests is running in 4s, but they do seem to perform all of the actions that they're supposed to be performing. I've ran it a bunch of times >20, and I'm not seeing any problems.
It may really be about a specific machine? I'll investigate further.

FWIW I'm running with Py3.5

https://github.com/apache/beam/pull/8991 may have fixed this.
CC: [~udim]

Another instance: 

{noformat}
Expected: (Test pipeline expected terminated in state: DONE and Expected checksum is 24de460c4d344a4b77ccc4cc1acb7b7ffc11a214)
     but: Expected checksum is 24de460c4d344a4b77ccc4cc1acb7b7ffc11a214 Actual checksum is da39a3ee5e6b4b0d3255bfef95601890afd80709
{noformat}

https://builds.apache.org/job/beam_PostCommit_Python35/331/testReport/junit/apache_beam.io.gcp.big_query_query_to_table_it_test/BigQueryQueryToTableIT/test_big_query_new_types/

resolved by [~udim]

Looks like this is still failing, or failing again:

https://builds.apache.org/job/beam_PostCommit_Python37_PR/77/testReport/junit/apache_beam.io.gcp.big_query_query_to_table_it_test/BigQueryQueryToTableIT/test_big_query_new_types_native/

Expected: (Test pipeline expected terminated in state: DONE and Expected checksum is 24de460c4d344a4b77ccc4cc1acb7b7ffc11a214)
     but: Expected checksum is 24de460c4d344a4b77ccc4cc1acb7b7ffc11a214 Actual checksum is da39a3ee5e6b4b0d3255bfef95601890afd80709


This is still happening and is one of the top source of flakiness in postcommits.
Reopening in case any of the prior work provides useful context.

 Sample: https://ci-beam.apache.org/job/beam_PostCommit_Python37/2805/

{noformat}
Error Message
Expected: (Test pipeline expected terminated in state: DONE and Expected checksum is 24de460c4d344a4b77ccc4cc1acb7b7ffc11a214)
     but: Expected checksum is 24de460c4d344a4b77ccc4cc1acb7b7ffc11a214 Actual checksum is da39a3ee5e6b4b0d3255bfef95601890afd80709

-------------------- >> begin captured logging << --------------------
apache_beam.runners.direct.direct_runner: INFO: Running pipeline with DirectRunner.
apache_beam.io.gcp.bigquery_tools: INFO: Using location 'US' from table <TableReference
 datasetId: 'python_query_to_table_15998907212925'
 projectId: 'apache-beam-testing'
 tableId: 'python_new_types_table'> referenced by query SELECT bytes, date, time FROM [python_query_to_table_15998907212925.python_new_types_table]
apache_beam.io.gcp.bigquery_tools: WARNING: Dataset apache-beam-testing:temp_dataset_17c640c2cdb346ea8955d86ad3e60786 does not exist so we will create it as temporary with location=US
urllib3.connectionpool: DEBUG: Starting new HTTP connection (1): metadata:80
urllib3.connectionpool: DEBUG: http://metadata:80 "GET /computeMetadata/v1/instance/attributes/job_id HTTP/1.1" 404 1606
apache_beam.io.gcp.bigquery_tools: DEBUG: Created the table with id output_table
apache_beam.io.gcp.bigquery_tools: INFO: Created table apache-beam-testing.python_query_to_table_15998907212925.output_table with schema <TableSchema
 fields: [<TableFieldSchema
 fields: []
 mode: 'NULLABLE'
 name: 'bytes'
 type: 'BYTES'>, <TableFieldSchema
 fields: []
 mode: 'NULLABLE'
 name: 'date'
 type: 'DATE'>, <TableFieldSchema
 fields: []
 mode: 'NULLABLE'
 name: 'time'
 type: 'TIME'>]>. Result: <Table
 creationTime: 1599890727293
 etag: 'tQbsuXVPNUn0ygeb3OnTug=='
 id: 'apache-beam-testing:python_query_to_table_15998907212925.output_table'
 kind: 'bigquery#table'
 lastModifiedTime: 1599890727421
 location: 'US'
 numBytes: 0
 numLongTermBytes: 0
 numRows: 0
 schema: <TableSchema
 fields: [<TableFieldSchema
 fields: []
 mode: 'NULLABLE'
 name: 'bytes'
 type: 'BYTES'>, <TableFieldSchema
 fields: []
 mode: 'NULLABLE'
 name: 'date'
 type: 'DATE'>, <TableFieldSchema
 fields: []
 mode: 'NULLABLE'
 name: 'time'
 type: 'TIME'>]>
 selfLink: 'https://bigquery.googleapis.com/bigquery/v2/projects/apache-beam-testing/datasets/python_query_to_table_15998907212925/tables/output_table'
 tableReference: <TableReference
 datasetId: 'python_query_to_table_15998907212925'
 projectId: 'apache-beam-testing'
 tableId: 'output_table'>
 type: 'TABLE'>.
apache_beam.io.gcp.bigquery_tools: INFO: Writing 4 rows to apache-beam-testing:python_query_to_table_15998907212925.output_table table.
apache_beam.io.gcp.tests.bigquery_matcher: INFO: Attempting to perform query SELECT bytes, date, time FROM `python_query_to_table_15998907212925.output_table`; to BQ
google.auth._default: DEBUG: Checking None for explicit credentials as part of auth process...
google.auth._default: DEBUG: Checking Cloud SDK credentials as part of auth process...
google.auth._default: DEBUG: Cloud SDK credentials not found on disk; not using them
google.auth._default: DEBUG: Checking for App Engine runtime as part of auth process...
google.auth._default: DEBUG: No App Engine library was found so cannot authentication via App Engine Identity Credentials.
google.auth.transport._http_client: DEBUG: Making request: GET http://169.254.169.254
google.auth.transport._http_client: DEBUG: Making request: GET http://metadata.google.internal/computeMetadata/v1/project/project-id
urllib3.util.retry: DEBUG: Converted retries value: 3 -> Retry(total=3, connect=None, read=None, redirect=None, status=None)
google.auth.transport.requests: DEBUG: Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true
urllib3.connectionpool: DEBUG: Starting new HTTP connection (1): metadata.google.internal:80
urllib3.connectionpool: DEBUG: http://metadata.google.internal:80 "GET /computeMetadata/v1/instance/service-accounts/default/?recursive=true HTTP/1.1" 200 144
google.auth.transport.requests: DEBUG: Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/844138762903-compute@developer.gserviceaccount.com/token
urllib3.connectionpool: DEBUG: http://metadata.google.internal:80 "GET /computeMetadata/v1/instance/service-accounts/844138762903-compute@developer.gserviceaccount.com/token HTTP/1.1" 200 221
urllib3.connectionpool: DEBUG: Starting new HTTPS connection (1): bigquery.googleapis.com:443
urllib3.connectionpool: DEBUG: https://bigquery.googleapis.com:443 "POST /bigquery/v2/projects/apache-beam-testing/jobs HTTP/1.1" 200 None
urllib3.connectionpool: DEBUG: https://bigquery.googleapis.com:443 "GET /bigquery/v2/projects/apache-beam-testing/queries/ccb82856-c480-43ea-a4f9-1120813ebdaf?maxResults=0&location=US HTTP/1.1" 200 None
urllib3.connectionpool: DEBUG: https://bigquery.googleapis.com:443 "GET /bigquery/v2/projects/apache-beam-testing/datasets/_7357fab0f784d2a7327ddbe81cdd1f4ca7e429cd/tables/anona6eb3fbe74d882a4011ceed2377d0a2ca1258820/data HTTP/1.1" 200 None
apache_beam.io.gcp.tests.bigquery_matcher: INFO: Read from given query (SELECT bytes, date, time FROM `python_query_to_table_15998907212925.output_table`;), total rows 0
apache_beam.io.gcp.tests.bigquery_matcher: INFO: Generate checksum: da39a3ee5e6b4b0d3255bfef95601890afd80709
--------------------- >> end captured logging << ---------------------
Stacktrace
  File "/usr/lib/python3.7/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/usr/lib/python3.7/unittest/case.py", line 615, in run
    testMethod()
  File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python37/src/sdks/python/apache_beam/io/gcp/big_query_query_to_table_it_test.py", line 310, in test_big_query_new_types_native
    big_query_query_to_table_pipeline.run_bq_pipeline(options)
  File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python37/src/sdks/python/apache_beam/io/gcp/big_query_query_to_table_pipeline.py", line 113, in run_bq_pipeline
    result = p.run()
  File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python37/src/sdks/python/apache_beam/testing/test_pipeline.py", line 112, in run
    False if self.not_use_test_runner_api else test_runner_api))
  File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python37/src/sdks/python/apache_beam/pipeline.py", line 546, in run
    return self.runner.run_pipeline(self, self._options)
  File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python37/src/sdks/python/apache_beam/runners/direct/test_direct_runner.py", line 53, in run_pipeline
    hc_assert_that(self.result, pickler.loads(on_success_matcher))
  File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python37/src/build/gradleenv/1398941891/lib/python3.7/site-packages/hamcrest/core/assert_that.py", line 44, in assert_that
    _assert_match(actual=arg1, matcher=arg2, reason=arg3)
  File "/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python37/src/build/gradleenv/1398941891/lib/python3.7/site-packages/hamcrest/core/assert_that.py", line 60, in _assert_match
    raise AssertionError(description)

Expected: (Test pipeline expected terminated in state: DONE and Expected checksum is 24de460c4d344a4b77ccc4cc1acb7b7ffc11a214)
     but: Expected checksum is 24de460c4d344a4b77ccc4cc1acb7b7ffc11a214 Actual checksum is da39a3ee5e6b4b0d3255bfef95601890afd80709

{noformat}

2805 doesn't seem to have that error message

Sorry, must have copied from a different  suite, here's a sample: https://ci-beam.apache.org/job/beam_PostCommit_Python36/2919.

The verifier is reading back 0 rows. The table seems to be delete only after the verifier has had a chance to read. The logs also say "writing 4 rows", which implies that the write to output_table either doesn't happen or is happening asynchronously.

It seems that writing using direct runner in native mode (BigQuerySink) falls back to using streaming inserts. (BigQueryWriter)
The more maintained version of streaming inserts is in BigQueryWriteFn (via WriteToBigQuery).
Both use BigQueryWrapper.insert_rows() however as the implementation.

Note that the other tests in this class use BQ Load jobs to write. (95% sure)

{code}
Streaming inserts reside temporarily in the streaming buffer, which has different availability characteristics than managed storage.
{code}
https://cloud.google.com/bigquery/docs/error-messages#missingunavailable-data

Also: https://cloud.google.com/bigquery/streaming-data-into-bigquery#dataavailability

My conclusion is that this test is depending on streamed data to be immediately available, but that is not guaranteed.
Suggested solution is to modify the matcher to retry for a few seconds until the streaming buffer has been processed (like BigqueryFullResultStreamingMatcher).


Seems to be resolved

