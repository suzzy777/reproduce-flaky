Doesn't look test case specific, saw another instance of this here:
{code:java}
query_test.test_queries.TestQueries.test_sort[protocol: hs2-http | exec_option: {'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': '100', 'batch_size': 0, 'num_nodes': 0} | table_format: parquet/none] (from pytest)

Error Message

query_test/test_queries.py:96: in test_sort     self.run_test_case('QueryTest/sort', vector) common/impala_test_suite.py:567: in run_test_case     table_format_info, use_db, pytest.config.option.scale_factor) common/impala_test_suite.py:782: in change_database     impala_client.execute(query) common/impala_connection.py:331: in execute     handle = self.execute_async(sql_stmt, user) common/impala_connection.py:354: in execute_async     self.__cursor.execute_async(sql_stmt, configuration=self.__query_options) /data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:375: in execute_async     self._execute_async(op) /data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:394: in _execute_async     operation_fn() /data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:372: in op     run_async=True) /data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:1096: in execute     return self._operation('ExecuteStatement', req) /data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:1026: in _operation     resp = self._rpc(kind, request) /data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:994: in _rpc     err_if_rpc_not_ok(response) /data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:748: in err_if_rpc_not_ok     raise HiveServer2Error(resp.status.errorMessage) E   HiveServer2Error: Invalid session id: 6e4151bd334675ac:e82a4fc7ed2f94aa

Stacktrace

query_test/test_queries.py:96: in test_sort
    self.run_test_case('QueryTest/sort', vector)
common/impala_test_suite.py:567: in run_test_case
    table_format_info, use_db, pytest.config.option.scale_factor)
common/impala_test_suite.py:782: in change_database
    impala_client.execute(query)
common/impala_connection.py:331: in execute
    handle = self.execute_async(sql_stmt, user)
common/impala_connection.py:354: in execute_async
    self.__cursor.execute_async(sql_stmt, configuration=self.__query_options)
/data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:375: in execute_async
    self._execute_async(op)
/data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:394: in _execute_async
    operation_fn()
/data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:372: in op
    run_async=True)
/data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:1096: in execute
    return self._operation('ExecuteStatement', req)
/data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:1026: in _operation
    resp = self._rpc(kind, request)
/data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:994: in _rpc
    err_if_rpc_not_ok(response)
/data/jenkins/workspace/impala-asf-master-exhaustive-data-cache/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:748: in err_if_rpc_not_ok
    raise HiveServer2Error(resp.status.errorMessage)
E   HiveServer2Error: Invalid session id: 6e4151bd334675ac:e82a4fc7ed2f94aa{code}

Saw this for a third time, this time on test_sort. Wondering if this could be related to any of the transparent query retry work. Going to try to repo locally.

No luck reproducing locally, but saw this again on the following test:
{code:java}
query_test.test_queries.TestQueries.test_top_n[protocol: hs2-http | exec_option: {'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': '100', 'batch_size': 0, 'num_nodes': 0} | table_format: parquet/none]  {code}

Again, seems specific to the hs2-http protocol.

Did some investigation, and I don't think this is actually a bug in the code, just the way the tests are run.

It looks like the hs2-http connections are just timing out and getting cleaned up by the session maintenance threads:

{code}
logs/ee_tests/impalad.impala-ec2-centos74-m5-4xlarge-ondemand-13fb.vpc.cloudera.com.jenkins.log.INFO.20200522-193649.23127:I0523 02:20:07.126468 13972 impala-server.cc:1079] 28419d6d1127487e:0e10689800000000] Registered query query_id=28419d6d1127487e:0e10689800000000 session_id=f94437c282d6c1c3:204ad55e51c6d6a9
logs/ee_tests/impalad.impala-ec2-centos74-m5-4xlarge-ondemand-13fb.vpc.cloudera.com.jenkins.log.INFO.20200522-193649.23127:I0523 03:20:37.240694 24158 impala-server.cc:2229] Closing session: f94437c282d6c1c3:204ad55e51c6d6a9, user: jenkins, because it no longer  has any open connections. The last connection was closed at: 2020-05-23 02:20:37.158
logs/ee_tests/impalad.impala-ec2-centos74-m5-4xlarge-ondemand-13fb.vpc.cloudera.com.jenkins.log.INFO.20200522-193649.23127:I0523 03:48:52.916052  5569 impala-server.cc:1403] GetSessionState(): Invalid session id: f94437c282d6c1c3:204ad55e51c6d6a9
logs/ee_tests/impalad.impala-ec2-centos74-m5-4xlarge-ondemand-13fb.vpc.cloudera.com.jenkins.log.INFO.20200522-193649.23127:I0523 03:48:53.082515  5611 impala-server.cc:1334] Closing session: f94437c282d6c1c3:204ad55e51c6d6a9
logs/ee_tests/impalad.impala-ec2-centos74-m5-4xlarge-ondemand-13fb.vpc.cloudera.com.jenkins.log.INFO.20200522-193649.23127:I0523 03:48:53.082520  5611 impala-server.cc:1351] CloseSessionInternal(): Invalid session id: f94437c282d6c1c3:204ad55e51c6d6a9
{code}

Looking through the code, the session gets invalidated if it is not used for more than 15 minutes. It looks like in some exhaustive tests this can happen. The hs2-http connection is opened when the class is created (see impala_test_suite.py), a few tests run that use the hs2-http, then some other tests run that don't use hs2-http. 15 minutes elapse and the hs2-http connection is closed, and then future attempts to use the hs2-http connection fail. It looks like this only happens in exhaustive tests because we typically only add the client protocol as a test dimension in exhaustive builds.

A simple fix might be to just bump the session timeout for tests.

hmm we actually already bump up the session timeout to 3600 in {{bin/start-impala-cluster.py}} and I checked the log file it has:

{code}
logs/ee_tests/impalad.impala-ec2-centos74-m5-4xlarge-ondemand-13fb.vpc.cloudera.com.jenkins.log.INFO.20200522-193649.23127:--disconnected_session_timeout=3600
{code}

So maybe there is a problem somewhere with the session timeout logic.

I ran into some issues like this when I tried to set the idle session timeout for our tests a while back - a lot of the tests do just leave the connections sitting there for a very long time, then suddenly decide to use them. The disconnected session timeout isn't an issue for the TCP clients, since those just leave the connection open indefinitely.

It does look like it was sitting there for an hour, based on the timestamps:
logs/ee_tests/impalad.impala-ec2-centos74-m5-4xlarge-ondemand-13fb.vpc.cloudera.com.jenkins.log.INFO.20200522-193649.23127:I0523 03:20:37.240694 24158 impala-server.cc:2229] Closing session: f94437c282d6c1c3:204ad55e51c6d6a9, user: jenkins, because it no longer  has any open connections. The last connection was closed at: 2020-05-23 02:20:37.158

Maybe we do just need to increase that timeout further?

{quote} I ran into some issues like this when I tried to set the idle session timeout for our tests a while back - a lot of the tests do just leave the connections sitting there for a very long time, then suddenly decide to use them. The disconnected session timeout isn't an issue for the TCP clients, since those just leave the connection open indefinitely.
{quote}
Good to know we have seen this issue before. Makes me more confident that my analysis above makes sense :).
{quote} It does look like it was sitting there for an hour, based on the timestamps
{quote}
Whoops, you are right. I didn't notice the 2 --> 3 change.
{quote} Maybe we do just need to increase that timeout further?
{quote}
Agreed, will do.

At the time I tried to modify the tests so that they could gracefully reconnect when timed out, but that proved to be somewhat tricky...

Saw this again, this time in an exhaustive-release run, in {code}
query_test/test_queries.py:111: in test_subquery     self.run_test_case('QueryTest/subquery', vector)
{code}

Commit a7866a94578be6289bbac31686de4d9032ad9261 in impala's branch refs/heads/master from Sahil Takiar
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=a7866a9 ]

IMPALA-9757: Bump disconnected_session_timeout in start-impala-cluster.py

Bump the disconnected_session_timeout to 6 hours in
./bin/start-impala-cluster.py.

This reduces test flakiness when running tests against the mini-cluster
using the hs2-http protocol. The issue is that a lot of the E2E tests
open a hs2-http connection on test startup, but might not use the
connection for a long time. The connection gets cleaned up and then
tests start to fail with "HiveServer2Error: Invalid session id"
exceptions.

The commonly happens in exhaustive tests where we add test dimensions on
the protocol used to execute E2E tests. This causes the test to switch
between the beeswax, hs2, and hs2-http protocols. If a test spends over
an hour using the beeswax protocol, the hs2-http will get closed.

Change-Id: I061a6f96311d406daee14454f71699c8727292d1
Reviewed-on: http://gerrit.cloudera.org:8080/16014
Reviewed-by: Tim Armstrong <tarmstrong@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


For some reason exhaustive-release builds can still run into this issue:
{code}
query_test/test_queries.py:107: in test_inline_view_limit
    self.run_test_case('QueryTest/inline-view-limit', vector)
common/impala_test_suite.py:567: in run_test_case
    table_format_info, use_db, pytest.config.option.scale_factor)
common/impala_test_suite.py:782: in change_database
    impala_client.execute(query)
common/impala_connection.py:331: in execute
    handle = self.execute_async(sql_stmt, user)
common/impala_connection.py:354: in execute_async
    self.__cursor.execute_async(sql_stmt, configuration=self.__query_options)
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:375: in execute_async
    self._execute_async(op)
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:394: in _execute_async
    operation_fn()
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:372: in op
    run_async=True)
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:1096: in execute
    return self._operation('ExecuteStatement', req)
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:1026: in _operation
    resp = self._rpc(kind, request)
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:994: in _rpc
    err_if_rpc_not_ok(response)
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:748: in err_if_rpc_not_ok
    raise HiveServer2Error(resp.status.errorMessage)
E   HiveServer2Error: Invalid session id: e147fb9ca4610a44:c28af1b0beb882b9
{code}

[~laszlog] are you only seeing this in cdpd-master builds? This fix isn't in cdpd-master yet, although I'm working on changing that.

That's right, I'm sorry; closing it.

False alarm; I saw the last occurrence on a frozen branch. Moving the ticket back to "resolved".

