I haven't been able to reproduce this exact error, but it looks like it could be a timeout, and timeouts I can reproduce with this test on a machine with plenty of resources.  I went back to when this test was committed, and this behavior has been there since inception:

{noformat}
[junit-timeout] Testcase: org.apache.cassandra.distributed.test.repair.ForceRepairTest:forceWithDifference:     Caused an ERROR
[junit-timeout] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
[junit-timeout] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
[junit-timeout]         at java.util.Vector.forEach(Vector.java:1277)
[junit-timeout]         at java.util.Vector.forEach(Vector.java:1277)
[junit-timeout] 
[junit-timeout] 
[junit-timeout] Test org.apache.cassandra.distributed.test.repair.ForceRepairTest FAILED (timeout)
{noformat}

I added some logging around the [args loop|https://github.com/apache/cassandra/blob/trunk/test/distributed/org/apache/cassandra/distributed/test/repair/ForceRepairTest.java#L83], which shows that sometimes there's a delay in the [nodetool failure|https://github.com/apache/cassandra/blob/trunk/test/distributed/org/apache/cassandra/distributed/test/repair/ForceRepairTest.java#L95]:

{noformat}
INFO  [main] <main> 2022-04-26 10:41:32,673 beginning --full
INFO  [main] <main> 2022-04-26 10:41:32,686 failure finished
INFO  [main] <main> 2022-04-26 10:41:32,723 success finished
INFO  [main] <main> 2022-04-26 10:41:32,724 finished --full
INFO  [main] <main> 2022-04-26 10:41:32,724 beginning --preview
INFO  [main] <main> 2022-04-26 10:46:32,734 failure finished
INFO  [main] <main> 2022-04-26 10:46:32,794 success finished
INFO  [main] <main> 2022-04-26 10:46:32,796 finished --preview
INFO  [main] <main> 2022-04-26 10:46:32,796 beginning --validate
INFO  [main] <main> 2022-04-26 10:51:32,803 failure finished
INFO  [main] <main> 2022-04-26 10:51:32,840 success finished
INFO  [main] <main> 2022-04-26 10:51:32,848 finished --validate
{noformat}
 
 which is interestingly always 5 minutes.  Here is node1's relevant log for the preview at 10:41:32,724:

{noformat}
DEBUG [node1_Repair#6:1] node1 2022-04-26 10:41:32,726 [preview repair #5906ca00-c577-11ec-b392-2548bbd24495] session task executor shut down gracefully
INFO  [node1_Repair-Task:1] node1 2022-04-26 10:41:32,732 Starting repair command #7 (590b0fc0-c577-11ec-b392-2548bbd24495), repairing keyspace distributed_test_keyspace with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], previewKind: UNREPAIRED, # of ranges: 3, pull repair: false, force repair: false, optimise streams: false, ignore unreplicated keyspaces: false, repairPaxos: false, paxosOnly: false)
ERROR [node1_Repair-Task:1] node1 2022-04-26 10:41:32,733 Repair 590b0fc0-c577-11ec-b392-2548bbd24495 failed:
java.lang.RuntimeException: Endpoint not alive: /127.0.0.2:7012
        at org.apache.cassandra.service.ActiveRepairService.failRepair(ActiveRepairService.java:726)
        at org.apache.cassandra.service.ActiveRepairService.prepareForRepair(ActiveRepairService.java:652)
        at org.apache.cassandra.repair.RepairRunnable.prepare(RepairRunnable.java:401)
        at org.apache.cassandra.repair.RepairRunnable.runMayThrow(RepairRunnable.java:280)
        at org.apache.cassandra.repair.RepairRunnable.run(RepairRunnable.java:249)
        at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:81)
        at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:47)
        at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:57)
        at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:81)
        at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:47)
        at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:57)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
INFO  [node1_Repair-Task:1] node1 2022-04-26 10:41:32,733 [preview repair #590b0fc0-c577-11ec-b392-2548bbd24495]Repair command #7 finished with error
{noformat}

which indicates it should have returned the failure at 10:41:32,733, but this doesn't happen for 5 minutes, and the only thing in node1's log is:

{noformat}
INFO  [node1_Messaging-EventLoop-3-1] node1 2022-04-26 10:42:01,657 /127.0.0.1:7012->/127.0.0.2:7012-URGENT_MESSAGES-[no-channel] failed to connect
io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /127.0.0.2:7012
Caused by: java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
        at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)
        at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
{noformat}

until the next test begins. This feels like perhaps something in the dtest framework to me but I'm not sure.  WDYT [~dcapwell] ?

[~dcapwell]Â I noticed you are also the original author of the test, maybe you can bring some light here?

5m feels weird, but likely client... see org.apache.cassandra.tools.NodeProbe#JMX_NOTIFICATION_POLL_INTERVAL_SECONDS

Server:
{code}
if (!prepareLatch.await(getRpcTimeout(MILLISECONDS), MILLISECONDS) || timeouts.get() > 0)
  failRepair(parentRepairSession, "Did not get replies from all endpoints.");
{code}

our timeout should be based off RPC timeout, or "request_timeout" which defaults too 10s.  Here is the list of files which touch this timeout value

{code}
 $ grep -r request_timeout test/distributed/ | awk -F: '{print $1}' | sort -u
test/distributed//org/apache/cassandra/distributed/test/CASAddTest.java
test/distributed//org/apache/cassandra/distributed/test/CASContentionTest.java
test/distributed//org/apache/cassandra/distributed/test/CASMultiDCTest.java
test/distributed//org/apache/cassandra/distributed/test/CASTest.java
test/distributed//org/apache/cassandra/distributed/test/CasCriticalSectionTest.java
test/distributed//org/apache/cassandra/distributed/test/LargeColumnTest.java
test/distributed//org/apache/cassandra/distributed/test/LegacyCASTest.java
test/distributed//org/apache/cassandra/distributed/test/MessageFiltersTest.java
test/distributed//org/apache/cassandra/distributed/test/PaxosRepairTest.java
test/distributed//org/apache/cassandra/distributed/test/PaxosRepairTest2.java
test/distributed//org/apache/cassandra/distributed/test/ReadRepairEmptyRangeTombstonesTest.java
test/distributed//org/apache/cassandra/distributed/test/ReadRepairQueryTester.java
test/distributed//org/apache/cassandra/distributed/test/ReadRepairTest.java
test/distributed//org/apache/cassandra/distributed/test/ring/ReadsDuringBootstrapTest.java
test/distributed//org/apache/cassandra/distributed/upgrade/MixedModeAvailabilityTestBase.java
test/distributed//org/apache/cassandra/distributed/upgrade/MixedModeConsistencyTestBase.java
test/distributed//org/apache/cassandra/distributed/upgrade/MixedModeMessageForwardTest.java
{code}

ForceRepairTest isn't in that list, and it exists TestBaseImpl which also isn't in the list; and the jvm-dtest code doesn't either...


bq. which indicates it should have returned the failure at 10:41:32,733, but this doesn't happen for 5 minutes

I agree, it should fail around that time, did the client side assert match or was it 5m delayed?  I am mostly asking if the cluster took 5m to shutdown or did it take 5m for the client to notice?  org.apache.cassandra.tools.NodeProbe#JMX_NOTIFICATION_POLL_INTERVAL_SECONDS defaults to 5m which means a JMX message was dropped (jmx is lossy), so client noticing repair failed 5m later makes sense as that matches our poll timeout logic; lowering -Dcassandra.nodetool.jmx_notification_poll_interval_seconds would speed up those checks

{code}
stderr:
error: Repair job has failed with the error message: Repair command #2 failed with error Did not get replies from all endpoints.. Check the logs on the repair participants for further details
-- StackTrace --
java.lang.RuntimeException: Repair job has failed with the error message: Repair command #2 failed with error Did not get replies from all endpoints.. Check the logs on the repair participants for further details
	at org.apache.cassandra.tools.RepairRunner.progress(RepairRunner.java:137)
	at org.apache.cassandra.utils.progress.jmx.JMXNotificationProgressListener.handleNotification(JMXNotificationProgressListener.java:77)
	at javax.management.NotificationBroadcasterSupport.handleNotification(NotificationBroadcasterSupport.java:275)
	at javax.management.NotificationBroadcasterSupport$SendNotifJob.run(NotificationBroadcasterSupport.java:352)
	at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:124)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
{code}

If we lower the client poll logic the test should fail faster, but it should still fail.  It looks like a connection issue broke prepare when it wasn't expected, and atm we do not have retry logic in repair (known issue), so failing would be expected behavior atm.

I think you're right about the poll interval, but I don't think any JMX messages should be lost here.  That said, I can no longer reproduce that failure, but I can now reproduce the original, which may be related.  The crux of that issue is that sometimes we can win the race and beat the failure detector to marking node2 down when we force the repair, which then correctly fails.  I have a branch that ensures node2 is marked down before proceeding:

||Branch||CI||
|[4.1|https://github.com/driftx/cassandra/tree/CASSANDRA-17655-4.1]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/510/workflows/89d3d809-3f27-4085-9064-661ba1af16e2], [J11|https://app.circleci.com/pipelines/github/driftx/cassandra/510/workflows/fbec3999-1ca2-4ec3-b691-caf7a834fffd], [+500|https://app.circleci.com/pipelines/github/driftx/cassandra/510/workflows/fbec3999-1ca2-4ec3-b691-caf7a834fffd/jobs/5917]|
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-17655-trunk]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/512/workflows/eb4f1761-5777-4136-a21b-86fa38671aec], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/512/workflows/a72d1336-ad51-46d2-937c-0f7874e533bd], [+500|https://app.circleci.com/pipelines/github/driftx/cassandra/512/workflows/a72d1336-ad51-46d2-937c-0f7874e533bd/jobs/5909]|

And an unpatched [500 run|https://app.circleci.com/pipelines/github/driftx/cassandra/511/workflows/9c5061c9-46ac-4fb6-82ed-d6116140a97f/jobs/5927] for comparison.

LGTM +1

Committed, thanks.

