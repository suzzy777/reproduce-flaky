I've had this looping on multiple platforms for the better part of a week and have not been able to reproduce. In sleuthing I have not come up with anything more to go on. Logs are gone from the linked run in Circle, and I haven't figured out how to get them from Jenkins. 

Jenkins shows the test at 13% flaky on trunk, with the last failure on Dec 15. Curiously, the runtimes reported for failures are always ~0.5 seconds. Not sure if that is possible, or if it's a problem with interpreting test results.

I'm going to set this aside for now. Maybe somebody else can think of a way to reproduce.

0% flakiness and 100% stability [where the last fail|https://ci-cassandra.apache.org/job/Cassandra-trunk/232/testReport/dtest-novnode.replace_address_test/TestReplaceAddress/test_insert_data_during_replace_same_address/]ures are from a month ago. I say we close it until it resurfaces, rather than keeping it open, as we have nothing to on with.

If the network were broken, I think what we see here would be the result (or on any other dtest, for that matter.)  If we can't reproduce after dutiful effort, I that's a sufficient enough possibility to solve for now and reopen if we see it again.

bq. 0% flakiness and 100% stability where the last failures are from a month ago.

Here's the link(s) based off the latest build (rather than a build number that will expire to 404 soon)
-  [trunk dtest-novnode|https://ci-cassandra.apache.org/job/Cassandra-trunk/lastCompletedBuild/testReport/dtest-novnode.replace_address_test/TestReplaceAddress/test_insert_data_during_replace_same_address/]
-  [trunk dtest|https://ci-cassandra.apache.org/job/Cassandra-trunk/lastCompletedBuild/testReport/dtest.replace_address_test/TestReplaceAddress/test_insert_data_during_replace_same_address/]
- [trunk dtest-offheap|https://ci-cassandra.apache.org/job/Cassandra-trunk/lastCompletedBuild/testReport/dtest-offheap.replace_address_test/TestReplaceAddress/test_insert_data_during_replace_same_address/]

h4. Searching nightlies.a.o for past failures…

We can use the nightlies archives for a complete search:
{code}
# webdav mount `https://nightlies.apache.org/cassandra/` to `/Volumes/cassandra`
cd /Volumes/cassandra

# define what we are looking for
export DTEST_CLASS="replace_address_test.TestReplaceAddress"
export DTEST_NAME="test_insert_data_during_replace_same_address"

# find builds where it failed
find Cassandra-trunk-dtest*/label=cassandra,split=* -name nosetests.xml -exec grep -q ${DTEST_CLASS} {} \; -exec grep -l "name=\"${DTEST_NAME}\" time=\"[0-9.]*\"><error message" {} \;

# alongside the nosetest.xml files you will find the `test_stdout.txt.xz` and `ccm_logs.tar.xz` logs
{code}
Alternatively, to control check, find builds where it succeeded
{code}
# alternatively, to control check, find builds where it succeeded
find Cassandra-trunk-dtest*/label=cassandra,split=* -name nosetests.xml -exec grep -q ${DTEST_CLASS} {} \; -exec grep -l "name=\"${DTEST_NAME}\" time=\"[0-9.]*\"></testcase>" {} \;
{code}

h4. Past failures and logs
The above recipe lists 54 failures (from currently what is archived in nighties.a.o).
 Links to the folders containing the logs are listed in the attachment  [^16238-archived-failures.txt] 
Within just {{Cassandra-trunk-dtest}} there's 17 failures from 193 archived runs. (9% flakiness)


I am reopening as this error impacts any multi-node jvm-dtest and not localized to a single test

Examples:
* https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/659/tests/

hostReplaceAbruptShutdown – org.apache.cassandra.distributed.test.hostreplacement.HostReplacementAbruptDownedInstanceTest

{code}
java.lang.IllegalStateException: Unable to contact any seeds!
	at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:1749)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:1054)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:1015)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:799)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:729)
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$10(Instance.java:541)
{code}

* CASSANDRA-15239

Branch [here|https://github.com/driftx/cassandra/tree/CASSANDRA-16238].

[!https://ci-cassandra.apache.org/job/Cassandra-devbranch/714/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/714/pipeline]

{noformat}
[junit-timeout] INFO  [node4_GossipStage:1] node4 2021-04-23 16:23:05,381 Gossiper.java:1296 - Node /127.0.0.1:7012 is now part of the cluster
[junit-timeout] DEBUG [node4_GossipStage:1] node4 2021-04-23 16:23:05,381 StorageService.java:2730 - Node /127.0.0.1:7012 state NORMAL, token [-3074457345618258603]
[junit-timeout] INFO  [node4_GossipTasks:1] node4 2021-04-23 16:23:05,393 Gossiper.java:997 - FatClient /127.0.0.1:7012 has been silent for 0ms, removing from gossip
[junit-timeout] DEBUG [node4_GossipStage:1] node4 2021-04-23 16:23:05,398 StorageService.java:2677 - New node /127.0.0.1:7012 at token -3074457345618258603
{noformat}

We can see here that .1 is detected for the first time via gossip, and as it is going through StorageService but before it is added to TokenMetatadata, the gossiper's status check has begun running.  Since the quarantine delay is overridden to zero, without a presence in TMD the node is not a member yet and thus deemed a fat client, and removed.

{noformat}
[junit-timeout] INFO  [node4_GossipStage:1] node4 2021-04-23 16:23:05,407 TokenMetadata.java:505 - Updating topology for /127.0.0.1:7012
[junit-timeout] INFO  [node4_GossipStage:1] node4 2021-04-23 16:23:05,414 TokenMetadata.java:505 - Updating topology for /127.0.0.1:7012
[junit-timeout] DEBUG [node4_GossipStage:1] node4 2021-04-23 16:23:05,422 StorageService.java:2730 - Node /127.0.0.1:7012 state NORMAL, token [-3074457345618258603]
[junit-timeout] INFO  [node4_GossipStage:1] node4 2021-04-23 16:23:05,422 StorageService.java:2733 - Node /127.0.0.1:7012 state jump to NORMAL
[junit-timeout] DEBUG [node4_GossipStage:1] node4 2021-04-23 16:23:05,447 Gossiper.java:1243 - removing expire time for endpoint : /127.0.0.1:7012
[junit-timeout] INFO  [node4_GossipStage:1] node4 2021-04-23 16:23:05,447 Gossiper.java:1244 - InetAddress /127.0.0.1:7012 is now UP
[junit-timeout] INFO  [node4_GossipStage:1] node4 2021-04-23 16:23:05,447 Gossiper.java:579 - removed /127.0.0.1:7012 from seeds, updated seeds list = []
[junit-timeout] WARN  16:23:05 Seeds list is now empty!
[junit-timeout] WARN  [node4_GossipStage:1] node4 2021-04-23 16:23:05,447 Gossiper.java:581 - Seeds list is now empty!
[junit-timeout] DEBUG [node4_GossipStage:1] node4 2021-04-23 16:23:05,452 Gossiper.java:590 - removing endpoint /127.0.0.1:7012
[junit-timeout] DEBUG [node4_GossipStage:1] node4 2021-04-23 16:23:05,452 Gossiper.java:561 - evicting /127.0.0.1:7012 from gossip
[junit-timeout] DEBUG [node4_GossipTasks:1] node4 2021-04-23 16:23:06,453 Gossiper.java:1025 - 0 elapsed, /127.0.0.1:7012 gossip quarantine over
{noformat}

Crucially, as part of this removal the node is also removed from the seeds list, since it is listed there. The warning about the empty seed list is added from my branch.

{noformat}
[junit-timeout] INFO  [node4_GossipStage:1] node4 2021-04-23 16:23:07,176 Gossiper.java:1296 - Node /127.0.0.1:7012 is now part of the cluster
[junit-timeout] DEBUG [node4_GossipStage:1] node4 2021-04-23 16:23:07,177 StorageService.java:2730 - Node /127.0.0.1:7012 state NORMAL, token [-3074457345618258603]
[junit-timeout] DEBUG [node4_GossipStage:1] node4 2021-04-23 16:23:07,189 StorageService.java:2677 - New node /127.0.0.1:7012 at token -3074457345618258603
[junit-timeout] INFO  [node4_GossipStage:1] node4 2021-04-23 16:23:07,198 TokenMetadata.java:505 - Updating topology for /127.0.0.1:7012
[junit-timeout] INFO  [node4_GossipStage:1] node4 2021-04-23 16:23:07,201 TokenMetadata.java:505 - Updating topology for /127.0.0.1:7012
[junit-timeout] DEBUG [node4_GossipStage:1] node4 2021-04-23 16:23:07,208 StorageService.java:2730 - Node /127.0.0.1:7012 state NORMAL, token [-3074457345618258603]
[junit-timeout] INFO  [node4_GossipStage:1] node4 2021-04-23 16:23:07,208 StorageService.java:2733 - Node /127.0.0.1:7012 state jump to NORMAL
[junit-timeout] DEBUG [node4_GossipStage:1] node4 2021-04-23 16:23:07,220 Gossiper.java:1243 - removing expire time for endpoint : /127.0.0.1:7012
[junit-timeout] INFO  [node4_GossipStage:1] node4 2021-04-23 16:23:07,220 Gossiper.java:1244 - InetAddress /127.0.0.1:7012 is now UP
[junit-timeout] DEBUG [node4_BatchlogTasks:1] node4 2021-04-23 16:23:07,383 BatchlogManager.java:246 - Updating batchlog replay throttle to 1024 KB/s, 256 KB/s per endpoint
[junit-timeout] DEBUG [node4_isolatedExecutor:1] node4 2021-04-23 16:23:12,457 Gossiper.java:2142 - Gossip looks settled.
{noformat}

Another round of gossip mostly papers over this problem, except:

{noformat}
[junit-timeout] Unable to contact any seeds: []
[junit-timeout] java.lang.IllegalStateException: Unable to contact any seeds: []
[junit-timeout]         at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:1752)
[junit-timeout]         at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:1054)
[junit-timeout]         at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:1015)
[junit-timeout]         at org.apache.cassandra.service.StorageService.initServer(StorageService.java:799)
[junit-timeout]         at org.apache.cassandra.service.StorageService.initServer(StorageService.java:729)
[junit-timeout]         at org.apache.cassandra.distributed.impl.Instance.lambda$startup$10(Instance.java:541)
[junit-timeout]         at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[junit-timeout]         at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[junit-timeout]         at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[junit-timeout]         at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[junit-timeout]         at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[junit-timeout]         at java.base/java.lang.Thread.run(Thread.java:834)
{noformat}

There's (correctly) no logic to add a removed node back as a seed.  This branch also prints the seeds list when this happens to disambiguate between problems contacting seeds, and problems like this where the problem is not having any seeds left to contact.

Ultimately, quarantine delay can't be set this low.  I tried low non-zero values, but it needs to be set to at least one gossiper interval to avoid not accidentally reaping nodes as fat clients prematurely, and since ring delay was already being overridden and quarantine delay derived from that, I just removed the quarantine delay override.

The host replacement tests set it low, but what about the other classes which hit this (such as the original test https://app.circleci.com/pipelines/github/dcapwell/cassandra/745/workflows/1c7e589e-b5af-4a56-b40a-43da424602c7/jobs/4231)?  We have seen this with nodeDownDuringMove(org.apache.cassandra.distributed.test.GossipTest) as well.

Review

* https://github.com/apache/cassandra/compare/trunk...driftx:CASSANDRA-16238#diff-99267a2170b04fd7dd24d6c6bf2ba1fc26d6dc896cd74f8c5bd56c476e2540e4R580 - nit: you can call isEmpty rather than size

For the host replacement tests, I set that field low to help find issues, so if this case happens more frequently because of that I am cool removing it; but we do see this outside of these classes as well.

bq. .1 is detected for the first time via gossip, and as it is going through StorageService but before it is added to TokenMetatadata, the gossiper's status check has begun running

If I understand you, the call to StorageService.onChange (which calls handleStateNormal) happens-after gossip status check, so removes the state? GossipDigestAck2 should be handled in the gossip stage and eventually call applyNewStates to apply the state and trigger notifications, but doStatusCheck is called in the GossipTasks thread pool, which checks isGossipOnlyMember which returns true in this case (as state isn't fully settled yet), at which point we schedule a task in the gossip stage to remove (but at this point the isGossipOnlyMember(endpoint) == false).  

If I understand you correctly, this feels like a race condition where we read data not fully committed, which feels like a bug (which is why it was set to 0 in the first place).  Do I understand you [~brandon.williams]?

bq. but we do see this outside of these classes as well.

I haven't seen those yet.

bq. If I understand you, the call to StorageService.onChange (which calls handleStateNormal) happens-after gossip status check, so removes the state?

It's hard to know the exact order but it's clear from the first few log lines that the status check and onChange are happening concurrently.

bq. If I understand you correctly, this feels like a race condition where we read data not fully committed, which feels like a bug (which is why it was set to 0 in the first place). Do I understand you Brandon Williams?

That may be the case as well, but yes you understand my theory thus far.




If this is a race reading un-committed data, then the patch below might work around it (double check locking, but with queues)

Patch:

{code}
$ git diff
diff --git a/src/java/org/apache/cassandra/gms/Gossiper.java b/src/java/org/apache/cassandra/gms/Gossiper.java
index 699f235bd3..e82b107bac 100644
--- a/src/java/org/apache/cassandra/gms/Gossiper.java
+++ b/src/java/org/apache/cassandra/gms/Gossiper.java
@@ -928,6 +928,11 @@ public class Gossiper implements IFailureDetectionEventListener, GossiperMBean
                 {
                     logger.info("FatClient {} has been silent for {}ms, removing from gossip", endpoint, fatClientTimeout);
                     runInGossipStageBlocking(() -> {
+                        if (!isGossipOnlyMember(endpoint))
+                        {
+                            // updating gossip and token metadata are not atomic, but rely on the single threaded gossip stage
+                            // since status checks are done outside the gossip stage, need to confirm the state of the endpoint
+                            // to make sure that the previous read data was correct
+                            logger.info("Race condition marking {} as a FatClient; ignoring", endpoint);
+                            return;
+                        }
                         removeEndpoint(endpoint); // will put it in justRemovedEndpoints to respect quarantine delay
                         evictFromMembership(endpoint); // can get rid of the state immediately
                     });
{code}

{noformat}
[junit-timeout] WARN  [node4_GossipStage:1] node4 2021-04-28 16:02:24,532 Gossiper.java:1002 - Race condition marking /127.0.0.2:7012 as a FatClient; ignoring
{noformat}

That will work too.

[Branch|https://github.com/driftx/cassandra/tree/CASSANDRA-16238] [!https://ci-cassandra.apache.org/job/Cassandra-devbranch/716/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/716/pipeline]



+1

Committed.

