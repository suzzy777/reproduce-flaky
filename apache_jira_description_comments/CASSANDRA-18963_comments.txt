Trying to identify affected versions. Running the multiplexer here:

4.1 - https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=18963-4.1

5.0 - https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=18963-5.0

The test was less flaky in 4.1, and the error was different:
{code:java}
failed on teardown with "Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [SSTableBatchOpen:1] 2023-11-01 00:49:29,910 JVMStabilityInspector.java:68 - Exception in thread Thread[SSTableBatchOpen:1,5,SSTableBatchOpen]\njava.lang.AssertionError: Stats component is missing for sstable /tmp/dtest-14cqh6up/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nb-4-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:470)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:381)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader$2.run(SSTableReader.java:551)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)', [node1] 'ERROR [SSTableBatchOpen:3] 2023-11-01 00:49:29,910 JVMStabilityInspector.java:68 - Exception in thread Thread[SSTableBatchOpen:3,5,SSTableBatchOpen]\njava.lang.AssertionError: Stats component is missing for sstable /tmp/dtest-14cqh6up/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nb-5-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:470)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:381)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader$2.run(SSTableReader.java:551)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)']" Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [SSTableBatchOpen:1] 2023-11-01 00:49:29,910 JVMStabilityInspector.java:68 - Exception in thread Thread[SSTableBatchOpen:1,5,SSTableBatchOpen]\njava.lang.AssertionError: Stats component is missing for sstable /tmp/dtest-14cqh6up/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nb-4-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:470)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:381)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader$2.run(SSTableReader.java:551)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)', [node1] 'ERROR [SSTableBatchOpen:3] 2023-11-01 00:49:29,910 JVMStabilityInspector.java:68 - Exception in thread Thread[SSTableBatchOpen:3,5,SSTableBatchOpen]\njava.lang.AssertionError: Stats component is missing for sstable /tmp/dtest-14cqh6up/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nb-5-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:470)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:381)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader$2.run(SSTableReader.java:551)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)']{code}
This ticket shows a regression in 5.0. I will open a separate ticket for the 4.1 issue discovered. 

[~blerer], [~e.dimitrova], [~jlewandowski] it looks like CASSANDRA-18747 is responsible for both this ticket and CASSANDRA-18991.  On 4.1 the [initial commit|https://app.circleci.com/pipelines/github/driftx/cassandra/1355/workflows/066deee5-e1a4-434c-91e5-fe8602084e86/jobs/60281] fails while the one [before passes|https://app.circleci.com/pipelines/github/driftx/cassandra/1357/workflows/97a73b15-8b23-4617-969b-9077284e664d/jobs/60483] (CASSANDRA-18991), and then the [merge to 5.0 fails|https://app.circleci.com/pipelines/github/driftx/cassandra/1354/workflows/5dc4d5b4-4f1a-46c8-8208-78715b409068/jobs/60184] where it previously [did not|https://app.circleci.com/pipelines/github/driftx/cassandra/1356/workflows/fe99cc03-588a-469c-8ffb-ae62bf73d672/jobs/60386].

Thanks [~brandon.williams] ! I will close the other ticket then and try to look into this today. I do not have an immediate response to what happened, unfortunately.

When I looked there wasn't anything obvious to me either, but I was actually doublechecking the bisect with the circle links, so I'm pretty certain it's correct.

{quote}but I was actually doublechecking the bisect with the circle links, so I'm pretty certain it's correct.
{quote}
I might not like what Circle shows us, but I trust it :D 

I can confirm that the original patch that was suggested in  CASSANDRA-18747 does not trigger the problem - [https://github.com/ekaterinadimitrova2/cassandra/commit/1dece14f4ea88024663af45fb3cd1a2eab01279d]

(not that I expected to, but I wanted to eliminate things) - https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2561/workflows/115b12a8-38ae-45b4-8111-c233f06ba68e

So, something went wrong with the additional fixes that were applied in the improved version.

I still do not know what yet, though. 

I was looking into this side on the side, adding some additional logging, but I did not see anything obvious.

I will be off until Thursday, and I can try to look into it again when I am back. I am wondering whether we did not uncover another bug with the changes. Maybe it is worth it to talk also to [~jlewandowski] or [~blerer] 

One odd thing here is the table is always prepared_statements that I've seen, which the test itself isn't directly touching (though stress uses prepared mode by default.)  I changed the test to use unprepared mode and it hasn't failed.

It did eventually fail, we also have a problem with compaction_history:

{quote}
  node1: ERROR [SSTableBatchOpen:3] 2023-11-14 11:04:07,460 DefaultFSErrorHandler.java:129 - Exiting forcefully due to file system exception on startup, disk failure policy "stop"
  org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /tmp/dtest-orisqlb8/test/node1/data0/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/nb-4-big-Statistics.db
        at org.apache.cassandra.io.sstable.format.StatsComponent.load(StatsComponent.java:67)
        at org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:77)
        at org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:58)
        at org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:92)
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:397)
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:353)
        at org.apache.cassandra.io.sstable.format.SSTableReader.lambda$openAll$4(SSTableReader.java:414)
        at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
        at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
        at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829)
  Caused by: java.io.EOFException: null
        at org.apache.cassandra.io.util.RebufferingInputStream.readByte(RebufferingInputStream.java:180)
        at org.apache.cassandra.io.util.RebufferingInputStream.readPrimitiveSlowly(RebufferingInputStream.java:142)
        at org.apache.cassandra.io.util.RebufferingInputStream.readInt(RebufferingInputStream.java:222)
        at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:160)
        at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:135)
        at org.apache.cassandra.io.sstable.format.StatsComponent.load(StatsComponent.java:63)
        ... 13 common frames omitted   
  node1: ERROR [SSTableBatchOpen:4] 2023-11-14 11:04:07,460 DefaultFSErrorHandler.java:129 - Exiting forcefully due to file system exception on startup, disk failure policy "stop"
  org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /tmp/dtest-orisqlb8/test/node1/data0/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/nb-5-big-Statistics.db
        at org.apache.cassandra.io.sstable.format.StatsComponent.load(StatsComponent.java:67)
        at org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:77)
        at org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:58)
        at org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:92)
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:397)
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:353)
        at org.apache.cassandra.io.sstable.format.SSTableReader.lambda$openAll$4(SSTableReader.java:414)
        at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
        at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
        at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829)
  Caused by: java.io.EOFException: null
        at org.apache.cassandra.io.util.RebufferingInputStream.readByte(RebufferingInputStream.java:180)
        at org.apache.cassandra.io.util.RebufferingInputStream.readPrimitiveSlowly(RebufferingInputStream.java:142)
        at org.apache.cassandra.io.util.RebufferingInputStream.readInt(RebufferingInputStream.java:222)
        at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:160)
        at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:135)
        at org.apache.cassandra.io.sstable.format.StatsComponent.load(StatsComponent.java:63)
        ... 13 common frames omitted
{quote}

Both of these are non-distributed system tables.

I'll try to fix it


It looks like that the problem is caused by the fact we try to open an sstable which misses STATS component. We can see that from "moving..." log entries - when the node starts, it moves sstables of the system keyspace from non-first to the first data location - thanks to that fact I noticed that there is no STATS file moved for the problematic table. 

Another observation is that, the table in question was being compacted when the compactions were interrupted and the process was stopped. I'd expect that the incomplete sstable is marked as such in the transaction log file and then it is skipped during scan when the node restarts. I'd like to try that scenario in the isolated test - see the state of the transaction log before the stats component is written, and then, if such transaction log prevents such sstable from loading.


I think I can see how it can happen...

When the sstable writer prepares for committing the transaction, it does the following stuff in {{doPrepare}} method:

{code:java}
        protected void doPrepare()
        {
            transactionals.get().forEach(Transactional::prepareToCommit);
            new StatsComponent(finalizeMetadata()).save(descriptor);
...
{code}

When entering this method, there is only data and index components in the directory. After the first line, the rest of the components are created - but stats component. There is also a transaction log file which seems to say that the sstable is in-progress of writing. In the problematic scenario, the execution is interrupted between the first and the second line of doPrepare, which means that we do not have stats component but we do have transaction log. 

The problem is, those files are moved to a different directory on startup (as mentioned in the previous comment) and the transaction log identifies sstables by full absolute path, which is not updated on move. That's way the directory lister does not recognize this sstable as incomplete and tries to load it, which leads to the problem.

It does not look new to me. It seems like the problem is there probably as long as we use {{migrateSystemDataIfNeeded}} which was probably introduced in CASSANDRA-14793, thus the problem is there for a while.  I don't think it is a blocker for any release and the workaround is simple - just delete the affected sstables - those data are duplicated in the uncompacted sstables or in the commit log because since the transaction is not finished, those should not be wiped as well. 

For the solution I think we should use directory lister (or at least the same filter) when moving sstables around so that we do not move incomplete ones. /cc [~blerer]




bq. the workaround is simple - just delete the affected sstables

After the node has shut itself down due to corruption, though. If the bisect is correct, this seems like a new problem.

No, the node is restarted during the test, second start fails

More details:

Why {{prepared_statements}}? According to the ticket mentioned above, the system tables should go to the first data location unless a specific data location for those sstables was specified. However, it turns out that it does not apply to some system tables, in particular those mentioned in {{SystemKeyspace.TABLES_SPLIT_ACROSS_MULTIPLE_DISKS}}, namely: batches, paxos, compaction_history, *prepared_statements* and repairs. {{prepared_statements}} is probably the only one extensively populated during a normal run thus have something to compact. This explains why there were sstables of that table spread across multiple data dirs, and why they were being compacted.

Now, why those sstables are moved on startup although they should be excluded from the procedure introduced by CASSANDRA-14793? There seem to be an oversight/mistake in filtering tables which should be moved:

{code:java}
                        Path[] tableDirectories = keyspaceChildren.filter(Files::isDirectory)
                                                                  .filter(p -> !SystemKeyspace.TABLES_SPLIT_ACROSS_MULTIPLE_DISKS
                                                                                              .contains(p.getFileName()
                                                                                                         .toString()))
                                                                  .toArray(Path[]::new);
{code}

The directory name of the table obviously includes the table id so no item from the set matches it. Therefore sstables of all the tables are moved.

So we should fix two things in this ticket:
- fix the filtering (4.0 and on)
- fix the way how the files are moved - we should skip sstables which are a part of an incomplete transaction - this will be removed automatically anyway


For fixing the sstables move, I've created https://issues.apache.org/jira/browse/CASSANDRA-19108, here we can focus on filtering.


https://app.circleci.com/pipelines/github/jacek-lewandowski/cassandra/1128/workflows/dec615da-5659-4f7c-b19d-73104f2fb8f0/jobs/51874

^ this is the only questionable test - it failed only on trunk and only once (well, we had 2000 run of this test in total). Also the failure is different to the one raised in this ticket. In the failed test, it fails to run sstableutil, it just returns exit code 1. It is hard to judge which of trunk failures are legit and which are not today. 

After discussing with [~blerer] we decided to merge it.



It seemed like a random failure to me and I did not see any errors in the logs. 

To confirm things with our current trunk I ran the repeated run on clean trunk - all green, [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2568/workflows/796c0947-fc33-4915-bd30-5bdd2a5548ca/jobs/47452]

Thank you for your work all!

