This does not cover DataNode, since its front-end is netty-based. The HttpServer2/jetty based server is internal. Unlike HttpServer2, the netty-based DatanodeHttpServer still uses SSLFactory. We have internally modified SSLFactory to enable automatic reloading of cert.  This will also make secure mapreduce shuffle server to reload cert.  I can add it to this patch if people are interested. We have used it for several years in production.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 22s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 20m  5s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 18m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 18s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 17s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 55s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 14m 55s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m  1s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m 24s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 37s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 93m 46s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.ha.TestZKFailoverController |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.1 Server=19.03.1 Image:yetus/hadoop:bdbca0e |
| JIRA Issue | HADOOP-16524 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12978221/HADOOP-16524.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 8a3d71b0c9b6 4.4.0-157-generic #185-Ubuntu SMP Tue Jul 23 09:17:01 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 2ae7f44 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_212 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/16497/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/16497/testReport/ |
| Max. process+thread count | 1422 (vs. ulimit of 10000) |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/16497/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



Should it retry immediately upon a loading failure? Say the credential file is written partially and the hadoop daemon loads it at the same time, like HDFS-14567.

If {{reload()}} throws an exception, {{lastLoaded}} is not modified, triggering immediate retry.

Bulk update: moved all 3.3.0 non-blocker issues, please move back if it is a blocker.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m 12s{color} | {color:red} HADOOP-16524 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HADOOP-16524 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12978221/HADOOP-16524.patch |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/16837/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



Hi all,

First time contributor here. It appears this is still an open issue and I hope you don't mind me taking a stab at it. Could anyone take a look at the following PR - [https://github.com/apache/hadoop/pull/2470] - and let me know what you think? The implementation also covers reloading the keystore in DataNode and it introduces a new configuration parameter that governs the polling interval. I've tried to follow the existing ReloadingX509TrustManager closely, but to avoid code duplication and to reduce the number of extra threads spun, I've consolidated the polling in a standard java.util.Timer. Otherwise, I've tried to follow the contributor coding guidelines, hope I did a good job there. Please let me know if I've missed something with respect to the process etc.

Best, Boris

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime ||  Logfile || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} |  | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m 25s{color} |  | {color:red} HADOOP-16524 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HADOOP-16524 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12978221/HADOOP-16524.patch |
| Console output | https://ci-hadoop.apache.org/job/PreCommit-HADOOP-Build/119/console |
| versions | git=2.17.1 |
| Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |


This message was automatically generated.



Attached PR LGTM. Would appreciate someone else taking a looksee if they have a chance. Was looking to merge in a few days. Thanks.

Merged to trunk. Put up #2609 backport to branch-3.3.

Merged to trunk and branch-3.3. Thanks for the patch [~borislav.iordanov] (I added you as contributor and assigned you this issue).

[~stack] Can you please take a look at the failures in qbt-report https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/381/#showFailuresLink ?

There is at least one unit test is broken by this jira.


{code:bash}
Test Result (11 failures / +4)
org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsPseudo
org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsKerberos
org.apache.hadoop.hdfs.web.TestURLConnectionFactory.testSSLFactoryCleanup
org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeRespectsBindHostKeys.testHttpsBindHostKey
org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks
org.apache.hadoop.yarn.client.api.impl.TestTimelineClient.testTimelineClientCleanup
org.apache.hadoop.yarn.server.resourcemanager.metrics.TestSystemMetricsPublisher.testPublishApplicationMetrics
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerAutoQueueCreation.testDynamicAutoQueueCreationWithTags
org.apache.hadoop.yarn.server.router.clientrm.TestFederationClientInterceptor.testGetApplicationAttemptReport
org.apache.hadoop.tools.dynamometer.TestDynamometerInfra.org.apache.hadoop.tools.dynamometer.TestDynamometerInfra
org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken
{code}



Sorry about that. Thanks for the ping. I applied because 'all' tests passed but looks like we need the yarn tests to run too.

Reopening to revert change until fix failing yarn test.

Reverted from trunk and branch-3.3.

I looked at the failures and most pass locally. The two that didn't pass and were clearly affected by the PR are org.apache.hadoop.yarn.client.api.impl.TestTimelineClient.testTimelineClientCleanup and org.apache.hadoop.hdfs.web.TestURLConnectionFactory.testSSLFactoryCleanup. I created a new PR with fixes for those:

[https://github.com/apache/hadoop/pull/2693]

 

However, some of the above tests are still failing on the YARN side, plus a new HBase related failure :

org.apache.hadoop.yarn.server.timelineservice.storage.TestTimelineWriterHBaseDown

org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken
org.apache.hadoop.yarn.client.api.impl.TestTimelineClient.testTimelineClientCleanup

 

I could not reproduce any of those failures locally. Tried running from IDE or the Hadoop  docker based environment. I also didn't see how they could be affected by the keystore reloading changes. So eventually I just made a cosmetic change to trigger another test run on the same PR and see what happens. Now all those tests passed, but there is a new failure:

hadoop.yarn.applications.distributedshell.TestDSTimelineV10

Seems like those are some flaky tests. Any advice how to proceed? Is it an option for the tests to be auto retried using this option [https://maven.apache.org/surefire/maven-surefire-plugin/examples/rerun-failing-tests.html] for example?

 

Pushed new PR that fixes yarn issue to branch-3.3 and trunk (took a few attempts for me to get the commit message format right). Ran the PR a few times and got different flakies each time through: none seemed related. Please shout if we broke anything.

Resolving again. Thanks for the feature [~borislav.iordanov] contrib.

[~stack]/[~borislav.iordanov]

There is a test failing :

[https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java8-linux-x86_64/475/testReport/junit/org.apache.hadoop.hdfs.qjournal.server/TestJournalNodeRespectsBindHostKeys/testHttpsBindHostKey/]

 

It is throwing exception from the newly added code:

 
{code:java}
java.lang.NullPointerException
	at sun.nio.fs.UnixPath.normalizeAndCheck(UnixPath.java:77)
	at sun.nio.fs.UnixPath.<init>(UnixPath.java:71)
	at sun.nio.fs.UnixFileSystem.getPath(UnixFileSystem.java:281)
	at java.nio.file.Paths.get(Paths.java:84)
	at org.apache.hadoop.http.HttpServer2$Builder.makeConfigurationChangeMonitor(HttpServer2.java:609)
	at org.apache.hadoop.http.HttpServer2$Builder.createHttpsChannelConnector(HttpServer2.java:592)
	at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:518)
{code}
I suspect it could be because of this change, Can you folks check once.

 

ACK [~ayushtkn]  Thanks for ping.  Looking...

[~ayushtkn] I've been trying to reproduce this locally, but no luck so far. Neither trunk nor the revision on which the above built run reproduce that exception. From the logs, I can see the following warning, which I don't have when running locally:

 

2021-04-12 07:49:33,154 [Time-limited test] WARN hdfs.DFSUtil (DFSUtil.java:loadSslConfiguration(1625)) - SSL config ssl.server.keystore.location is missing. If dfs.https.server.keystore.resource is specified, make sure it is a relative path

 

So I need to find what in the built environment affects that particular configuration parameter. Any advice how I can replicate the environment? Or re-run the built, potentially with some extra traces?

Hey [~borislav.iordanov]

In the hadoop directory you will find a file named: {{start-build-env.sh}}, just execute that and it will give you docker environment where the tests run and  then execute: 
{{mvn  -Dtest=TestJournalNodeRespectsBindHostKeys#testHttpsBindHostKey -Ptest-patch -Pparallel-tests -Pshelltest -Pnative -Drequire.fuse -Drequire.openssl -Drequirsnappy -Drequire.valgrind -Drequire.zstd -Drequire.test.libhadoop -Pyarn-ui clean test}}

That is what I just did and it got me the exception:


{noformat}
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeRespectsBindHostKeys
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 16.92 s <<< FAILURE! - in org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeRespectsBindHostKeys
[ERROR] testHttpsBindHostKey(org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeRespectsBindHostKeys)  Time elapsed: 15.785 s  <<< ERROR!
java.lang.NullPointerException
	at sun.nio.fs.UnixPath.normalizeAndCheck(UnixPath.java:77)
	at sun.nio.fs.UnixPath.<init>(UnixPath.java:71)
	at sun.nio.fs.UnixFileSystem.getPath(UnixFileSystem.java:281)
	at java.nio.file.Paths.get(Paths.java:84)
	at org.apache.hadoop.http.HttpServer2$Builder.makeConfigurationChangeMonitor(HttpServer2.java:609)
	at org.apache.hadoop.http.HttpServer2$Builder.createHttpsChannelConnector(HttpServer2.java:592)
	at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:518)
	at org.apache.hadoop.hdfs.qjournal.server.JournalNodeHttpServer.start(JournalNodeHttpServer.java:81)
	at org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:238)
	at org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.<init>(MiniJournalCluster.java:120)
	at org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.<init>(MiniJournalCluster.java:47)
	at org.apache.hadoop.hdfs.qjournal.MiniJournalCluster$Builder.build(MiniJournalCluster.java:79)
	at org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeRespectsBindHostKeys.testHttpsBindHostKey(TestJournalNodeRespectsBindHostKeys.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   TestJournalNodeRespectsBindHostKeys.testHttpsBindHostKey:180 ? NullPointer
[INFO] 
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0
[INFO] 
[ERROR] There are test failures.

{noformat}


hi [~ayushtkn] , [~stack] thanks for the instruction. I was able to reproduce the failure with your instructions. The issue is that it fails to load a configuration file (ssl-server.xml) and as a consequence the location of the SSL keystore is null. After playing with various options, I came to the conclusion that the failure happens only when the yarn-ui profile is enabled, but I couldn't figure out why that affects it. The file ssl-server.xml is expected to be under test-classes directly, but what we find there instead is a file named ssl-server-fork-1.xml, probably has to do with Maven fork options and parallel tests ([https://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html),] but I'm familiar with that to troubleshoot. 

 

However - the problem appears fixed on the latest commit on trunk and I'm only reproduce on the commit hash that I found from your link above. On the latest trunk, we have test-classes/ssl-server.xml so the file is found. So I'm guessing there were configuration changes that fixed it. 

 

What I could do is create a PR with a more helpful error that says "No keystore configured for HTTPS" or some such?

bq. What I could do is create a PR with a more helpful error that says "No keystore configured for HTTPS" or some such?

Make a sub-issue here [~borislav.iordanov] ?

This is failing since long:

[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/496/testReport/junit/org.apache.hadoop.hdfs.qjournal.server/TestJournalNodeRespectsBindHostKeys/testHttpsBindHostKey/]

If there isn't a quick fix. We can revert this and rework. Guess more than 15 days.

[~stack] I plan to revert by tomorrow 

Thanks for following up [~ayushtkn]. Lets see if Boris has any come back, else I'll revert.

[~ayushtkn] Apologies for procrastinating here, I hadn't realized the urgency. I will do the fix today or tomorrow or tomorrow at the latest.

Thanks [~borislav.iordanov]. Is that ok by you [~ayushtkn]? Otherwise I'll revert. Thanks.

Absolutely Ok, Thanx [~stack] and [~borislav.iordanov] for chasing this.

[~borislav.iordanov] Let me know if you face any issue with the fix, I will try to help. 

In case there is no solution found, Can try this:

{noformat}
   @Test (timeout=300000)
   public void testHttpsBindHostKey() throws Exception {
+    conf.set(DFS_SERVER_HTTPS_KEYSTORE_RESOURCE_KEY,
+        KeyStoreTestUtil.getServerSSLConfigFileName());
     LOG.info("Testing behavior without " + DFS_JOURNALNODE_HTTPS_BIND_HOST_KEY);
{noformat}

Adding this conf, makes things work for me. [~borislav.iordanov] can you try once and if it works for you as well, can you raise an Addendum PR/patch to fix the test.

[~ayushtkn] I submitted a PR: [https://github.com/apache/hadoop/pull/2979]  with a fix. Fixing the configuration will work I'm sure, as I did something similar while troubleshooting, but the PR I made is better, let's see if it break any new tests and if not I'm hoping we could use that instead. If it breaks tests, I'll make the config fix as you suggest as the quickest/safest way to get pass this.

[~ayushtkn] [~stack], the only failure that I see is this one [https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2979/1/testReport/junit/org.apache.hadoop.yarn.server.resourcemanager.reservation/TestCapacityOverTimePolicy/testAllocation_Duration_90_000_000__height_0_25__numSubmission_1__periodic_86400000__/] , which doesn't seem related. Full test report: [https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2979/1/testReport/]

 

There were some check style issues flagged and I addressed them and amended the PR. 

I merged the sub-task PR to trunk and branch-3.3. Where else should it go do you know [~ayushtkn]? Thanks.

Thanx [~stack], the sub-task PR is needed on trunk and branch-3.3 only, following the original commit. 

Thanx [~borislav.iordanov] for the fixing this!!!

{quote}We have internally modified SSLFactory to enable automatic reloading of cert.  This will also make secure mapreduce shuffle server to reload cert.  I can add it to this patch if people are interested. We have used it for several years in production.
{quote}
[~kihwal] Do you know if this patch is already part of OSS. If not, will be great if you could share this. I can create one more Jira for the same.

