Added 2 member variable (optional in protobuf format) in FsServerDefaults:
1. namenodeSupportsProviderUri: to differentiate between empty keyProviderUri and namenode not supporting encrpytion zones and supporting backward compatibility.
2. keyProviderUri: 

New namenode returns namenodeSupportsProviderUri as true.
If namenodeSupportsProviderUri is true, then the client will always trust whatever namenode returned in keyProviderUri.
If namenodeSupportsProviderUri is false (that means the namenode is not upgraded), then the client will use it own conf as before thereby supporting backwards compatibility.

{code:title=DFSClient.java|borderStyle=solid}
  public boolean isHDFSEncryptionEnabled() {
    try {
      return DFSUtilClient.isHDFSEncryptionEnabled(getKeyProviderUri());
    } catch(IOException ioe) {
      return false;
    }
  }
{code}

{code:title=DFSClient.java|borderStyle=solid}
  private String getKeyProviderUri() throws IOException {
    FsServerDefaults serverDefaults = getServerDefaults();
    return serverDefaults.getNamenodeSupportsProviderUri() ?
        serverDefaults.getKeyProviderUri() : conf.getTrimmed(
        CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH, "");
  }
{code}

{{DFSClient#getServerDefaults}} throws IOExcpetion (StandbyException) 
If {{DFSUtilClient#isHDFSEncryptionEnabled}} throws IOException then I am returning false. 
Not sure what should I return.
Comments are welcome.

Please review.



Thanks for posting the patch Rushabh! I like this idea, it'll simplify client configuration.

I took a quick look, few questions:

* Could you update the documentation and core-default.xml about this new feature?
* Do we actually need the PB boolean? PB can differentiate between not specified, empty string, and set. IIUC it's not specified, then it's an old NN, and we can fallback.

[~andrew.wang]: Thanks for quick review.
bq. Could you update the documentation and core-default.xml about this new feature?
Will update in next revision.

bq. Do we actually need the PB boolean? PB can differentiate between not specified, empty string, and set
The default value for string format in protobuf  is "" (empty string).
If the client receives an empty string, it either means its talking to an old namenode or the namenode has provider path config not set.
Maybe if the namenode doesn't have security path set, then I can return null value and on the client side, I have to differentiate between null and empty string. But need to test if protobuf convert null string to empty string in response or not ?


bq. PB can differentiate between not specified, empty string, and set.
Please ignore my previous comment. I understood what you actually meant.
I will update the patch by monday.

Canceling the patch to address review comments.
Will post a new patch shortly.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 20s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 13s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 50s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 53s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 55s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 10s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 27s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 19s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 28s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green} 14m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 14m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 21s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m 22s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 33s{color} | {color:red} hadoop-hdfs-project_hadoop-hdfs-client generated 1 new + 1 unchanged - 0 fixed = 2 total (was 1) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  9m 27s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 20s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 66m 14s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 39s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}175m 56s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.security.TestKDiag |
|   | hadoop.hdfs.TestDFSUtil |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-14104 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12854608/HADOOP-14104-trunk.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  cc  |
| uname | Linux 6936cba05be6 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / d2b3ba9 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| javadoc | https://builds.apache.org/job/PreCommit-HADOOP-Build/11714/artifact/patchprocess/diff-javadoc-javadoc-hadoop-hdfs-project_hadoop-hdfs-client.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11714/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11714/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11714/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11714/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Attached a new patch which removed PB boolean.
Updated documentation and core-default.xml

Test Failures from the previous patch:
 hadoop.security.TestKDiag -- Ran fine on my node.
 hadoop.hdfs.TestDFSUtil  -- Was actually a bug in my patch. Fixed in the latest revision.
Please review.

Thanks for working on this Rushabh, patch looks pretty close, just some nits:

* FtpConfigKeys and LocalConfigKeys, would still be nice to introduce a new variable for documentation purposes
* I dislike non-trivial ternary statements, mind rewriting DFSClient#getKeyProviderUri a bit? We can also dedupe the call to {{getKeyProviderUri}} for clarity.
* Still could use a doc update in TransparentEncryption.md about how this is fetched from server-side defaults.

Could you comment on potential additional overheads from invoking the NNto query this config value? I don't see {{getServerDefaults}} used much right now, so it looks like this will add another NN RPC to many client operations, for both unencrypted and encrypted clusters. This is concerning.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 22s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 49s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 53s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m  0s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 44s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 23s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green} 13m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 13m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 21s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  2s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m 30s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 40s{color} | {color:red} hadoop-hdfs-project_hadoop-hdfs-client generated 1 new + 1 unchanged - 0 fixed = 2 total (was 1) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m 39s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 10s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 92m 51s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 45s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}210m 16s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.fs.sftp.TestSFTPFileSystem |
|   | hadoop.hdfs.server.namenode.TestNameNodeMetadataConsistency |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-14104 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12855020/HADOOP-14104-trunk-v1.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  cc  |
| uname | Linux cdb23fbd7c99 3.13.0-107-generic #154-Ubuntu SMP Tue Dec 20 09:57:27 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 5f5b031 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| javadoc | https://builds.apache.org/job/PreCommit-HADOOP-Build/11726/artifact/patchprocess/diff-javadoc-javadoc-hadoop-hdfs-project_hadoop-hdfs-client.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11726/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11726/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11726/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11726/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



bq.     FtpConfigKeys and LocalConfigKeys, would still be nice to introduce a new variable for documentation purposes
Will update in the next revision.
bq.    I dislike non-trivial ternary statements, mind rewriting DFSClient#getKeyProviderUri a bit? 
Will update in next revision.
bq.We can also dedupe the call to getKeyProviderUri for clarity.
Can you please elaborate more ?

bq.     Still could use a doc update in TransparentEncryption.md about how this is fetched from server-side defaults.
Will update in next revision.

{quote} 
Could you comment on potential additional overheads from invoking the NNto query this config value? I don't see getServerDefaults used much right now, so it looks like this will add another NN RPC to many client operations, for both unencrypted and encrypted clusters. This is concerning.
{quote}
The results of {{namenode#getServerDefaults}} on DFSClient are cached for 1 hour.
On the namenode side also, this rpc call doesn't need any lock. So I don't think this should add any significant load on Namenode.
For DFSInputStream, we already do call severDefaults to get {{DFSClient#newDataEncryptionKey}}

Hi [~shahrs87] thanks much for your patch. Looks good overall but I still have a few questions.

{quote}
According to current implementation of kms provider in client conf, there can only be one kms.
{quote}
This is not a precise statement though, considering that multiple kms instances can be added for load balancing purposes. Would you mind to update the release note once this patch gets committed?

Preferably  also state (in the doc, maybe?) that this only works if both client and namenodes are on a supported version, otherwise the client's local kms config is used.

Can we also add a test to ensure clients can access files in an encrypted remote cluster using the token obtained from the remote NameNode?

HI [~shahrs87],

Thanks much for working on this issue!

I had a few nits and question about the patch here:

1. Since this is public API, we should introduce a new constructor with the addiitional parameter keyProviderUri to be backward compatible, instead of just modifying the existing one.
{code}
@InterfaceAudience.Public
@InterfaceStability.Evolving
public class FsServerDefaults implements Writable {
......
  public FsServerDefaults(long blockSize, int bytesPerChecksum,
      int writePacketSize, short replication, int fileBufferSize,
      boolean encryptDataTransfer, long trashInterval,
      DataChecksum.Type checksumType,
      String keyProviderUri) {
{code}

2. Suggest to add a KEY_PROVIDER_URI_DEFAULT to replce the "" here (in both FtpConfigKeys.java and LocalConfigKeys.java): 
{code}
  protected static FsServerDefaults getServerDefaults() throws IOException {
    return new FsServerDefaults(
        BLOCK_SIZE_DEFAULT,
        ......
        CHECKSUM_TYPE_DEFAULT,
        "");
{code}

3. the following method swallows IOException and return false. Suggest to remove the {{@throws IOException}} and add a comment in the catch block about why it can be iegnored and false should be returned here. And a
dd a space after the word {{catrch}}.
{code}
  /**
   * Probe for encryption enabled on this filesystem.
   * See {@link DFSUtilClient#isHDFSEncryptionEnabled(Configuration)}
   * @return true if encryption is enabled
   * @throws IOException
   */
  public boolean isHDFSEncryptionEnabled() {
    try {
      return DFSUtilClient.isHDFSEncryptionEnabled(getKeyProviderUri());
    } catch(IOException ioe) {
      return false;
    }
  }
{code}

4. Line 84 of KeyProviderCache.java (not introduced by your changei)
{code}
LOG.error("Could not create KeyProvider for DFSClient !!", e.getCause());
{code}
suggest to replace e.getCause() with e, so we can see the full stack.

5. Currently {{getServerDefaults()}} contact NN every hour, to find if there is any update of keyprovider. If keyprovider changed within the hour,
client code may get into exception, wonder if we have mechanism to handle the exception and update the keyprovider and try again?
 
Thanks.


Looked closer, I think #5 of my prior comment is indeed a potential problem because I don't see the kind of handling I was looking for in the current implementation.



Thanks for reviewing Wei-chiu and Yongjun, let me reply to a few comments of Rushabh:

bq. This is not a precise statement though, considering that multiple kms instances can be added for load balancing purposes. Would you mind to update the release note once this patch gets committed?

By this, I think you mean we should update the JIRA summary and related to say "for the KeyProvider URI"?

bq. Can we also add a test to ensure clients can access files in an encrypted remote cluster using the token obtained from the remote NameNode?

What constitutes "remote" here? Clients fetch delegation tokens based on the specified filesystem(s), so I don't think it matters where the client or cluster are located.

bq. 5. Currently getServerDefaults() contact NN every hour, to find if there is any update of keyprovider. If keyprovider changed within the hour, client code may get into exception, wonder if we have mechanism to handle the exception and update the keyprovider and try again?

What kind of error handling do you recommend here? I do think at least some a log message would be nice.

Some new questions of my own:

* I like that getServerDefaults is lock-free, but I'm still worried about the overhead. MR tasks are short lived and thus don't benefit from the caching. This also affects all clients, on both encrypted and unencrypted clusters. I think getServerDefault is also currently only called when SASL is enabled. Have you done any performance testing of this RPC?
* Another option is to put this behind a config key which defaults off, and file a new JIRA to track defaulting it on in a later release.

Thanks [~andrew.wang].

I gave some more thoughts, I think a better solution instead of the period polling is, just like how client handle namenode HA, we can do the same for KeyProvider. Say, if we specify keyproviderservice in config file to associate with a list of KeyProviders, if one keyProvider is down, the client can try to access the next one in the list (client failover). This is essentially KeyProvider HA.  But this would be a larger scope solution. Does this make sense to you?






Hi Yongjun, what you describe is how the existing KMS HA already works.

One thing I might recommend is that we don't query getServerDefaults after we get the KP initially. This way we don't need to worry about the value changing later (or an exception being thrown).

bq. ... how client handle namenode HA ...  we specify keyproviderservice in config file ...

The need for configs is the problem, so more configs is not the answer (aside: the NN HA token handling is an example of exactly what not to do).

bq. One thing I might recommend is that we don't query getServerDefaults after we get the KP initially. 

Enabling EZ on a cluster must not require a restart of all daemon and proxy services that communicate with said cluster.  It can't be cached forever.

––

I reviewed Rushabh's approach with him this morning.  The main goal should be a config-free token acquisition and selection.  How do we get there?

The first challenge is how does a client intelligently request a kms token, when needed, and from the right kms?   The NN is the authoritative and dynamic source for the correct kms, ala this patch..  Token acquisition should use the kp uri provided by the NN, and I'm not too worried about caching when a typical cluster has a few dozen app submits/sec (equaling token requests) vs 10s of thousand of NN ops/sec.  This is only a small part of the problem.

The second challenge is how does a client select the correct kms for a given NN?  The client could again ask the NN but you stumble into the morass of caching.  However as soon as the NN reports a different key provider than when a job launched, clients won't be able to find a token for the new kms - even when the old one is still legit.  Now jobs fail that should/could have completed.  It's very messy.  The simpler answer is a client should always use the key provider for a given NN as it existed when the token was acquired (ie. job submit).

So how do we implement a config-free mapping of NN to key provider?  When the hdfs and kms tokens are acquired we need a way to later associate them as a pair.  I think the cleanest/most-compatible way is leveraging the Credentials instead of the config.  We could inject a mapping of filesystem uri to kms uri via the secrets map.  So now when the client needs to talk to the kms it can check the map, else fallback to getServerDefaults.


Discussed with [~weichiu] and [~jzhuge], we looked at the code together and saw KMS provider already support multiple key provider servers in its configuration, for example:

{code}
 <property>
    <name>hadoop.security.key.provider.path</name>
    <value>kms://https@kms01.example.com;kms02.example.com:16000/kms</value>
  </property>
{code}

and
{code}
     * If multiple hosts are provider, the Factory will create a
     * {@link LoadBalancingKMSClientProvider} that round-robins requests
     * across the provided list of hosts.
{code}
This is a form of KeyProvider HA handling (also handles load balancing).  

[~andrew.wang], 
{quote}
I like that getServerDefaults is lock-free, but I'm still worried about the overhead. MR tasks are short lived and thus don't benefit from the caching. This also affects all clients, on both encrypted and unencrypted clusters. I think getServerDefault is also currently only called when SASL is enabled. Have you done any performance testing of this RPC?
{quote}
getServerDefaults mechanism has been there and the patch here just included an additional field. Calling it once an hour should not be a problem to me, at least not a regression. It's just that if things changed within the hour,  the errors may not be handled correctly (for example, all old key provider hosts are replaced with new ones), but it's less of a concern assuming that's rare. I don't see that sasl is checked when getServerDefaults is called.

Thanks.











Hm, my jira window was not refreshed so I missed updates from you guys when I did my last one. Thanks for further discussions.


Hi Daryn, thanks for commenting! Looks like you have a more ambitious implementation in mind, since your usecases include dynamic configuration changes without client restarts (something not possible with the current config-based approach).

Generally speaking, I think it's pretty rare to change the KMS URI. I think the two situations are:

* Enabling HDFS encryption for the first time. This currently requires a client restart.
* Enabling KMS HA. As long as the old KMS is part of the HA group, then clients with the old value will still work.

Since the KMS is just a proxy, you can swap out the backing KeyProvider implementation without changing the URI.

I'm not familiar with the Credentials APIs, but I like the sound of your proposal. It lets most clients avoid calling getServerDefaults, which was my main concern about the current patch.

Since we're very interested in a NN-specified KMS URI but less interested in dynamic refresh, so if it's reasonable to do refresh as a follow-on JIRA, that'd be optimal from our perspective.

Thanks [~andrew.wang], good comments.

Hi [~daryn],

I like the sound of your proposal too
{quote}
 I think the cleanest/most-compatible way is leveraging the Credentials instead of the config. We could inject a mapping of filesystem uri to kms uri via the secrets map. So now when the client needs to talk to the kms it can check the map, else fallback to getServerDefaults.
{quote}

Did you mean to use the following UserProvider method
{code}
  @Override
  public synchronized CredentialEntry createCredentialEntry(String name, char[] credential) 
      throws IOException {
    Text nameT = new Text(name);
    if (credentials.getSecretKey(nameT) != null) {
      throw new IOException("Credential " + name + 
          " already exists in " + this);
    }
    credentials.addSecretKey(new Text(name), 
        new String(credential).getBytes("UTF-8"));
    return new CredentialEntry(name, credential);
  }
{code}
to add <fs-uri, keyProvider> mapping to the credential map? This mapping info for a remote cluster need to come from either the remote cluster conf, or the NN of the remote cluster,  what's your thinking here?

Would you please elaborate this approach? Is there any in-compatibility here?
 
Thanks.


Goal is naturally no incompatibility but expanded functionality.  To that end, the general principal when acquiring tokens:
# If getServerDefaults doesn't return a keyprovider field: it's a previous version to the client, don't know if kms is applicable, so fallback to config just as today.
# If getServerDefaults does return a keyprovider: it's authoritative, add a kvp to the credentials mapping the fs uri to the kms uri.

When the dfsclient needs a key provider, check credentials:
# If fs/kms provider mapping is present, use the value as authoritative since a token in the credentials was obtained from that provider.
# If no mapping, check getServerDefaults, fallback to config.

Thanks [~daryn]!

Some follow-up questions:

My understanding is that the credential map exists at the server side (NN of a cluster), thus NN should populate the fs/kms provider mapping, is that understanding correct? If so, where and when does NN populate the mapping?

How the client gets the credential map?

Does this work only for kerberized cluster, but not non-kerberized cluster? If that's the case, how we solve the problem of non-kerberized cluster with encryption zone? 

Thanks much.


Thanks [~daryn] [~andrew.wang] [~yzhangal] [~weichiu] for the discussion and reviews.
I will put up a patch in couple of days with test cases implementing Daryn's idea from [this comment|https://issues.apache.org/jira/browse/HADOOP-14104?focusedCommentId=15892979&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15892979].
We can then discuss further ahead.

Credentials are simply a container in the ugi for tokens and/or secrets.  There is no notion of client, server, etc.  Credentials are the mechanism by which tokens are propagated throughout a job.

I may be understanding the EZ w/o security question (which seems an entirely contrived use case), but regardless: if tokens are available, so are the secrets since they are both packaged in the credentials object.  If this use case works today then it will continue to work with the mappings based approach.

Thank you guys very much [~daryn] and [~rushabh.shah]! 

Much appreciated that you will provide the revised patch Rushabh!




Hi [~daryn],

Took a further look, my understanding is:

Credentials is an object stored in UGI, and it is passed around to various components such as mappers and reducers to get a job done. The Credentials object contains two maps:
{code}
  private  Map<Text, byte[]> secretKeysMap = new HashMap<Text, byte[]>();
  private  Map<Text, Token<? extends TokenIdentifier>> tokenMap =
      new HashMap<Text, Token<? extends TokenIdentifier>>();
{code}

When initializing the Credentials object for the client, the token map is populated by asking NN for the tokens with FSNamesystem#getDelegationToken(Text renewer)

The secretKeysMap is populated by UserProvider.

To add fs/keyProvider entries to secretKeysMap, we call getServerDefaults once to get back the keyProvider info, and update secretKeysMap with entries <fs, keyProvider>.

Is that understanding correct?

Thanks.

--Yongjun


This patch addresses most of the previous comments.
What changed in this patch compared to previous ?
1. During job submission, added a new secret in the credential's secret map.
DFS-KMS-<namenodeUri> --> keyproviderUri 
This mapping captures the namenode's keyProviderUri at the time of job submission.
Every task that is scheduled to run will contact this key provider uri for decrypting EDEK's.
2. The key provider uri will be searched in the following order.
- from the credentials secrets map.
- Query the namenode via server defaults.
- Local conf.

Previous concerns:
1. From [~andrew.wang]
bq. I like that getServerDefaults is lock-free, but I'm still worried about the overhead. 
The namenode#getServerDefaults will be queried only once at the time of job submission.

2. From [~yzhangal]
{quote}
Currently getServerDefaults() contact NN every hour, to find if there is any update of keyprovider. If keyprovider changed within the hour,
client code may get into exception, wonder if we have mechanism to handle the exception and update the keyprovider and try again?
{quote}
This was a very good question which I didn't think while writing previous patch. Thanks !
We glue the namenode uri to key provider uri at the time of job submission and persist in ugi's credentials object.
The task will find it in credentials object and no longer need to contact namenode.
If there is an update(hardware update or put in maintenance mode ) for key provider, we plan to keep in decommission mode for 7 days.
So all the tokens which were given out while it was still active will be valid for 7 days and then new key provider will issue tokens for newly submitted jobs.

I tried to incorporate all the previous comments in the current patch (v2) but let me know if I missed any.

I need one suggestion.

{code:title=DFSClient.java|borderStyle=solid}

  public boolean isHDFSEncryptionEnabled() {
    try {
      return DFSUtilClient.isHDFSEncryptionEnabled(getKeyProviderUri());
    } catch (IOException ioe) {
      // This means the ClientProtocol#getServerDefautls threw StandbyException
      return false;
    }
  }
{code}
{{getKeyProviderUri}} is calling NamenodeRpcServer#getServerDefaults and it can throw an Standby Exception in which case, I am returning false.
I don't know what is the right thing to do.
 {{DFSClient.isHDFSEncryptionEnabled()}} is being called by {{DistributedFileSystem.getTrashRoot(Path path)}} which doesn't throw any IOException so I need to take some decision if an Exception is encountered.
Your help is much appreciated.
Please review.

Thanks much for your quick turnaround, [~rushabh.shah], I will review it tonight. 


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 26s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 58s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m  8s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 20s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 16s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 41s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 18s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green} 12m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 12m 32s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m 24s{color} | {color:orange} root: The patch generated 41 new + 575 unchanged - 2 fixed = 616 total (was 577) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 21s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 2 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 29s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m 23s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 13s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 66m 36s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 54s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}177m 23s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.ipc.TestRPC |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure100 |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-14104 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12856391/HADOOP-14104-trunk-v2.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  cc  |
| uname | Linux f938878faaed 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 5e74196 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11767/artifact/patchprocess/diff-checkstyle-root.txt |
| whitespace | https://builds.apache.org/job/PreCommit-HADOOP-Build/11767/artifact/patchprocess/whitespace-eol.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11767/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11767/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11767/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11767/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



HI [~rushabh.shah], 

Thanks again for your updated patch. Below are my comments:

1. Currently we do 
{code}
      final CryptoCodec codec = getCryptoCodec(conf, feInfo);
      KeyVersion decrypted = decryptEncryptedDataEncryptionKey(feInfo);
{code}
in {{createWrappedOutputStream(}}, where conf is the configuration of local cluster. There is a possibility that the local configuration is different than remote cluster's. So it's possible to fail here.

2. suggest to introduce
{code}
public satic final String HADOOP_SECURITY_KEY_PROVIDER_PATH_DEFAULT = "";
{code}
in parallel to HADOOP_SECURITY_KEY_PROVIDER_PATH in CommonConfigurationKeysPublic.java (@awang would you please confirm if it's ok to do so since this class is public), and use this constant at multiple places that current uses "".

3. Notice that "dfs.encryption.key.provider.uri" is deprecated and replaced with hadoop.security.key.provider.path (see HDFS-10489). So suggest to replace variable name keyProviderUri with keyProviderPath

4. Suggest to add two methods of package scope in DFSClient
{code}
  void addKmsKeyProviderPath(...)
  String getKmsKeyProviderPath(...)
{code}
and call them from needed places.

5.The uri used in DistributedFileSystem and DFSClient may be different, see DistributedFileSystem#initialize below
{code}
  public void initialize(URI uri, Configuration conf) throws IOException {
    ...
    this.dfs = new DFSClient(uri, conf, statistics);
    this.uri = URI.create(uri.getScheme()+"://"+uri.getAuthority());
    this.workingDir = getHomeDirectory();
{code}
So the update and retrieve of fs/keyProvider from the credentials may be mismatching, because uri.toString() is used as the key in the credentials map. We may just use {{(uri.getScheme()+"://"+uri.getAuthority()}} as the key and encapsulate this in the add/get methods I proposed in 4. 

6. Seems we need a similar change in WebHdfsFileSystem when calling addDelegationTokens

7. About your question w.r.t. {{public boolean isHDFSEncryptionEnabled()}} throwing StandbyException. There is a solution, that is, we need to incorporate remote's cluster's nameservices configurations in the client (distcp for example) configuration, and let the client handle the NN failover and retry. We need to document this.

Hi [~daryn] and [~andrew.wang], would you please help doing a review too? and see if my above comments make sense to you?

Thanks.


HI [~rushabh.shah],

We need a solution soon, so I hope you don't mind I just uploaded a patch to address most of my comments.

I also included some changes to avoid overhead for cluster that doesn't have encryption enabled, by inserting an empty entry into the secret map.

Would you please take a look?

Hi [~daryn] and [~andrew.wang], would you please help taking a look? thanks a lot.


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 50s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 48s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  7s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m  5s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 31s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 24s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 10m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green} 10m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 10m 14s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m 21s{color} | {color:orange} root: The patch generated 43 new + 735 unchanged - 2 fixed = 778 total (was 737) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 32s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m 30s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 13s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 64m 13s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 53s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}169m 18s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.security.TestKDiag |
|   | hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA |
|   | hadoop.hdfs.security.TestDelegationToken |
|   | hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer |
|   | hadoop.hdfs.security.TestDelegationTokenForProxyUser |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-14104 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12856909/HADOOP-14104-trunk-v3.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  cc  |
| uname | Linux 434822a71527 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 33a38a5 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11785/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11785/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11785/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11785/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11785/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



{quote}
      final CryptoCodec codec = getCryptoCodec(conf, feInfo);
 in createWrappedOutputStream(, where conf is the configuration of local cluster. There is a possibility that the local configuration is different than remote cluster's. So it's possible to fail here.
{quote}
We are not reading the key provider path from conf within {{getCryptoCodec}} method. So I don't think my change (form v2 patch) will fail anything.

bq. public satic final String HADOOP_SECURITY_KEY_PROVIDER_PATH_DEFAULT = "";
Before the v2 of patch, there was no default value for {{HADOOP_SECURITY_KEY_PROVIDER_PATH_DEFAULT}} and I kept it the same way.
I think this change is out of the scope of this jira. Suggest you to create a new jira for this change. Let's not mix multiple things in one jira.

bq. 3. Notice that "dfs.encryption.key.provider.uri" is deprecated and replaced with hadoop.security.key.provider.path (see HDFS-10489). So suggest to replace variable name keyProviderUri with keyProviderPath
I didn't even know this config used to be called as {{dfs.encryption.key.provider.uri}}. 
But this {{hadoop.security.key.provider.path}} on the client side is just an URI and not path by any way. We convert this to a path via {{KMSClientProvider(URI uri, Configuration conf)}} where we extract the path via {{KMSClientProvider#extractKMSPath(URI uri)}}. Thats why I named it {{keyProviderUri}}.
But if you feel that strongly about the variable name, I can change it to provider path in my next revision.

{quote}
4. Suggest to add two methods of package scope in DFSClient
 void addKmsKeyProviderPath(...)
  String getKmsKeyProviderPath(...)
{quote}
I think we use add method if that data structure is going to contain more than one entry. I don't think the provider uri on the client side is going to contain more than one entry.
I already have {{getKeyProviderUri}}  in v2 of patch.

bq. 5.The uri used in DistributedFileSystem and DFSClient may be different, see DistributedFileSystem#initialize below
This is very good observation. I think its safe to flip these 2 statements in DistributedFilesystem class.
Will do it in next revision.
{noformat}
  this.dfs = new DFSClient(uri, conf, statistics);
    this.uri = URI.create(uri.getScheme()+"://"+uri.getAuthority());
{noformat}

bq. 6. Seems we need a similar change in WebHdfsFileSystem when calling addDelegationTokens
I don't think key provider is used by WebHDFSFileSystem. Maybe I'm missing something.
Can you please elaborate your comment ?

{quote}
7. About your question w.r.t. public boolean isHDFSEncryptionEnabled() throwing StandbyException. There is a solution, that is, we need to incorporate remote's cluster's nameservices configurations in the client (distcp for example) configuration, and let the client handle the NN failover and retry. We need to document this.
{quote}
I didn't understand this comment also.  Please elaborate.

bq. We need a solution soon, so I hope you don't mind I just uploaded a patch to address most of my comments.
I don't see any sense of urgency from the community since providing key provider path via conf was the standard way since the feature was introduced since hadoop 2.6 (Aug 2014).
Having said that I was waiting for comments from [~daryn] and [~andrew.wang] before putting up a new patch.
I know [~daryn] was out of office for last couple of days and [~andrew.wang] was involved in 2.8.0 release work so I thought to wait for few days before pinging them.
[~yzhangal]: Since I am the assignee of the jira and I am updating patches regularly, _please allow_ me to upload the patches henceforth. I would like to get valuable review comments from you.
I hope you don't mind that.

[~andrew.wang] [~daryn]: Just FYI, please review  HADOOP-14104-trunk-v2.patch patch.

Thanks for the update [~rushabh.shah],  sorry about that, but please be assured that I did not mean to intrude, my sincere apology if you felt so. 

I should have given some background, I was looking into HDFS-9868 earlier because we need a solution very soon to let distcp to be able to see the keyProvider of the remote cluster, then we found that HADOOP-14104 may be a better solution. As far as I know, the existing "providing key provider path via conf " implementation doesn't support external cluster, we could do an implementation to extend the conf support for external cluster for keyprovider, as an alternative solution for HADOOP-14104. 

Will comment on your other points soon.



Here's my review on v2, along with responses to some of Yongjun's questions. Thanks for working on this Rushabh!

bq. in createWrappedOutputStream(, where conf is the configuration of local cluster. There is a possibility that the local configuration is different than remote cluster's. So it's possible to fail here.

The conf is only used to configure the CryptoCodec, which does not need a KMS URI. I think this is okay, all the CC parameters are included in the {{feInfo}}.

bq. @awang would you please confirm if it's ok to do so since this class is public), and use this constant at multiple places that current uses ""

Yea it's fine. I would like to improve this in this patch if possible, since it removes redundancies.

bq. 3. Notice that "dfs.encryption.key.provider.uri" is deprecated and replaced with hadoop.security.key.provider.path (see HDFS-10489). So suggest to replace variable name keyProviderUri with keyProviderPath

I think we can ignore this for now like Rushabh said, can handle it in another JIRA if necessary.

bq. Seems we need a similar change in WebHdfsFileSystem when calling addDelegationTokens

The DN does the encryption/decryption in WebHDFS, so the client doesn't need to do any KMS communication.

It does bring up a question regarding the DN DFSClient though. It looks like WebHdfsHandler creates a new DFSClient each time, which means we won't benefit from getServerDefaults caching.

Is the fix to make config preferred over getServerDefaults?

bq. <StandbyException>

Does this exception actually make it to clients? The HA RPC proxy normally catches the StandbyException and fails over to the other NN. We can write a unit test for this to verify if we're unsure.

My additional review comments:

* typo nameonde
* ServerDefaults#getKeyProviderUri needs javadoc explaining how to interpret null and empty (IIUC null means not set, empty means means set but not enabled)
* In docs, "DFSClients" is an internal name. Please rename to say "HDFS clients" or similar. Same for "dfs clients" in core-default.xml
* There are a lot of whitespace errors, please take a look at what's flagged by checkstyle. Recommend using IDE autoformatting in the future.
* An actual mini-cluster test that mimics an MR job submitter and task's call pattern would also be good.

TestEncryptionZones:
* Would like to see the test reversed so it covers the fallback behavior, i.e.
    * set client config with kp1, check that it returns kp1
    * mock getServerDefaults() with kp2, check it returns kp2
    * set Credentials with kp3, check it returns kp3
* typo originalNameodeUri
* {{String lookUpKey = DFSClient.DFS_KMS_PREFIX + originalNameodeUri.toString();}} should this be a {{getKey}} helper method in DFSClient rather than having the test code also construct the key?

HI [~rushabh.shah],

Thanks again for your work. Below are my comments in reply to yours. Just saw [~andrew.wang]'s, thanks Andrew!

1.
{quote}
final FileEncryptionInfo feInfo = dfsos.getFileEncryptionInfo();
final CryptoCodec codec = getCryptoCodec(conf, feInfo);
in createWrappedOutputStream, where conf is the configuration of local cluster. There is a possibility that the local configuration is different than remote cluster's. So it's possible to fail here.
{quote}
Sorry I was a bit unclear earlier, this comment is not about the change of this jira, but about the existing implementation. My concern is about Codec rather than keyProvider here. We get feInfo from the target file, getting Codec based on conf and feInfo. The conf here is the configuration of the *local* cluster. When the target is the remote cluster, could we fail to find Codec info or wrong codec info because we don't have remote cluster's configuration? That's what wanted to say. So I hope [~andrew.wang] who was involved in the original development of encryption feature could comment. (Andrew, saw you above comment, but would you please look at my comment here again and see if it makes sense?

2. It looks having a {{HADOOP_SECURITY_KEY_PROVIDER_PATH_DEFAULT}} constant would help making the code more consistent, even for the patch here, but having a separate jira works for me too. 

3. Keeping the name keyProviderUri instead of keyProviderPath is actually fine. I did some study before creating patch rev3, the format appears to be Uri. I wish HDFS-10489 made it uri instead of path. So I did not change uri to path in rev3.

4. About adding two methods to add/get from credentials, it's a way of encapsulating how it's handled in one place, and share the way how the key in the map is generated (e.g. uri.getScheme()+"://"+uri.getAuthority()). You can see my example in rev3. These methodd are also called in Test code. 

5. 
{quote}
I don't think key provider is used by WebHDFSFileSystem. Maybe I'm missing something.
Can you please elaborate your comment ?
{quote}
I was just guessing, but I'm not so sure, but hope [~daryn] can comment.

6. 
{quote}
7. About your question w.r.t. public boolean isHDFSEncryptionEnabled() throwing StandbyException. There is a solution, that is, we need to incorporate remote's cluster's nameservices configurations in the client (distcp for example) configuration, and let the client handle the NN failover and retry. We need to document this.
{quote}
For HA cluster, we can access NN via nameservice (for example, hadoop distcp hdfs://nameservice1:/xyz hdfs://nameservice2:/abc) , so the StandbyException can be detected and different NN will be tried automatically. See https://issues.apache.org/jira/browse/HDFS-6376. We actually see a non-intrusive way to do that without using dfs.internal.nameservices", that is, we copy the local cluster's conf to a new dir, and append the nameservice portion of the remote cluster's conf to the hdfs-site.xml in the new dir. Then pass the new dir to distcp.
 


 


Hi Yongjun,

bq. When the target is the remote cluster, could we fail to find Codec info or wrong codec info because we don't have remote cluster's configuration?

The FileEncryptionInfo has a cipher type field. If the client doesn't support the cipher, then it can't read/write the file, and will throw an exception. If it does support that cipher, it uses its local config to determine the correct codec implementation to use (i.e. java or native).

From a safety point of view, we're okay to use the local config. If the client is too old and doesn't understand a new cipher type, it'll abort. Supporting a new cipher necessarily requires upgrading the client (and potentially also installing native libraries), so I think this behavior is okay.



Thanks [~andrew.wang].

{quote}
The FileEncryptionInfo has a cipher type field. If the client doesn't support the cipher, then it can't read/write the file, and will throw an exception
{quote}
Does the exception get thrown when we do {{final CryptoCodec codec = getCryptoCodec(conf, feInfo);}} or later when we write file? I prefer the former.

BTW [~rushabh.shah], 

In rev2,  {{DistributedFileSyste#addDelegationTokens}} put the keyProvider entry to scretMap only when {{dfs.isHDFSEncryptionEnabled()}} is true.  {{dfs.isHDFSEncryptionEnabled()}} calls {{getServerDefaults}} to find out whether encryption is enabled when there is no keyProvider entry in the secretMap. This means, If encryption is not enabled, then we don't put any entry into the secretMap, and each task of mapreduce job will always call getServerDefaults, which we try to avoid.

I suggest to add an empty entry to the secretMap after we call getServerDefaults for the first time and find encryption is disabled. Then later if we find an entry in the map, and if it's empty, we know encryption is not enabled, if it's non-empty, it's enabled. This is what I tried to do in rev3 to avoid the extra getServerDefaults call for the case when encryption is disabled. For your reference.
 
Thanks.
 





HI [~rushabh.shah], 

To avoid confusions to other reviewers, I'm taking out rev3. Thanks.



bq. This means, If encryption is not enabled, then we don't put any entry into the secretMap, and each task of mapreduce job will always call getServerDefaults, which we try to avoid.
I don't follow this.
{{DFSClient#isHDFSEncryptionEnabled}} is being only called by {{DistributedFileSystem#addDelegationTokens}} (job submission) and {{DistributedFileSystem.getTrashRoot}} (FSShell commands).
{{DFSClient#getKeyProvider}} is being called by {{DistributedFileSystem#addDelegationTokens}} (again job submission) and {{DFSClient#decryptEncryptedDataEncryptionKey}}.
When calling via {{DFSClient#decryptEncryptedDataEncryptionKey}}, we already know that this directory or parent directory is already encrypted.
So I don't understand why each mapreduce task which is accessing {{non-encrypted directory}} will call getKeyProvider and in turn call serverDefaults ?
Am I missing something ?





Hi [~rushabh.shah],

Thanks for pointing it out. Because {{DFSClient#isHDFSEncryptionEnabled}} is a public method of {{DFSClient}}, I thought anyone could call it including any mapreduce task.

Not an ask for this jira, - we can do it later - If this method is only intended to be called by {{DistributedFileSystem#addDelegationTokens}} and {{DistributedFileSystem.getTrashRoot}}, maybe we can add a javadoc to indicate that.  BTW, One could create FsShell object and call shell from any java code too, we can worry about that later.

Thanks.
 






Sorry for the delay, I've been OOO on and off the past week.  Reviewing this morning.

Still scrutinizing, initial comments:
 
Regarding need for constants, in general, would be preferable for “null” to mean no provider instead of both empty string and null meaning no provider – in which case a constant is not necessary.

Regarding concerns over getServerDefaults being called via webhdfs: if you are using EZ with webhdfs, one little getServerDefaults call is the least of your worries...  You should terrified that apparently one compromised DN renders encryption moot.  At any rate, we'll see what we can do to reduce the number of calls from webhdfs.

+DFSClient+
# {{get/setKeyProviderUri}} methods should use URI instances, not strings, as name implies.  The caller should have already done the string to uri conversions.
# {{getServerDefaults}} never returns null, so don’t null check. Just assign keyProviderUri and check for null.

+KMSUtil+
# If {{createKeyProviderFromUri(Configuration, String)}} fails to create a client, it throws with a hardcoded {{CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH}} which may not be the key that was used.  Although see comments in _KeyProviderCache_ that may negate need for these changes at all.

+KeyProviderCache+
# The new {{get(Configuration,String)}} should take a URI, not String, and just look it up.  No need to perform all the String -> URI logic for every lookup since DFSClient should call with a computed URI.
# Consider just calling {{KeyProviderFactory.get(uri, conf)}} directly instead of roundabout trip through DFSUtilClient/KMSUtil.  Then you probably don’t need to change those classes since their methods are all about getting the key provider from the config.  We know what the URI is.
# Minor, in {{get}}, leave the unwrapping of the ExecutionException in place.

+DFSClient+
# In {{isHDFSEncryptionEnabled}}, it’s completely broken to swallow exceptions and return false…  It’s not just a StandbyException, per the comment, that may bubble out.  It could be the retry proxy giving up, connect refused, etc.  Change this method to allow IOE because DFSClient is a private interface.  Then the filesystem’s getTrashRoot can do the wrong thing by swallowing the exception.

+DistributedFileSystem+
# I’d rather see {{addDelegationTokens}} not call the problematic {{isHDFSEncryptionEnabled}} method, but instead call {{dfs.getKeyProvider()}}.  If not null, do the block of code to get a kms token.

I'll double check that token selection works correctly.

Thanks [~daryn] a lot for the review and comments.

Hi [~shahrs87],

Wonder if you have chance to look at Daryn's comments?

Many thanks!


bq. Wonder if you have chance to look at Daryn's comments?
I was out last week and before that busy working on internal stuff.
I should be able to upload a new patch today.
bq. An actual mini-cluster test that mimics an MR job submitter and task's call pattern would also be good.
I was able to address all the review comments except the above one.
Only way I can think to mimic job submitter is by creating MiniYARNCluster. I don't have any experience writing yarn tests. I may take additional time if you want me to write miniyarn cluster test.
[~yzhangal], [~andrew.wang]: Any pointers would be appreciated.



Thanks much [~shahrs87]!

Hi [~andrew.wang], about your comment Rushabh was asking, would you please elaborate? seems a more comprehensive test, do you agree that we could address it in later jira? Thanks.




I think a full YARN job is a bit much; basically I wanted a unit test that did an encrypted read/write using the KP URI from the credentials.

This might be partially addressed by an existing HDFS encryption test, but I'd like to see the same verification as in the newly added unit tests that the value from the credentials overrides the value in the config.

Thanks [~daryn] [~andrew.wang] [~yzhangal] for your valuable reviews.
I tried to address most of the review comments.
For the following comment:
bq. basically I wanted a unit test that did an encrypted read/write using the KP URI from the credentials.
I added another unit test {{TestEncryptionZones#testEncryptedReadWriteUsingDiffKeyProvider}}
This test tried to read/write a file from/to encrypted zone.
The test adds key provider uri to credentials object and then unsets the local conf.
While reading the file, it gets the key provider uri from the credentials object.
The way we try to resolve the provider uri is first credentials map, then from namenode and then from conf.
So among this chain, the testcase can get the key provider uri from namenode but I verified by adding a log line that it got from credentials map.

Note: I haven't resolved the checkstyle warnings from the previous patch since the precommit build logs were removed from jenkins server.
Once the precommit build runs, I will fix all the checkstyle warnings and then upload another patch.
Request for reviews.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 21s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 47s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 56s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 54s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m  5s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 51s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 13s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green} 13m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 13m 11s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 42s{color} | {color:orange} root: The patch generated 16 new + 475 unchanged - 2 fixed = 491 total (was 477) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  7m 15s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 13s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 61m 32s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 44s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}165m 25s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
|   | hadoop.hdfs.web.TestWebHdfsTimeouts |
|   | hadoop.hdfs.tools.TestDFSZKFailoverController |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-14104 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12861492/HADOOP-14104-trunk-v3.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  cc  |
| uname | Linux f9179210540c 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 318bfb0 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/12002/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/12002/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/12002/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12002/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Removing the patch to address checkstyle comments.

Addressed some of the checkstyle comments.
Since the config key name "CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH" is so big, it can't fit in a line within 80 characters and remaining are not added by this patch.
Test failures:
1. TestWebHdfsTimeouts - Fails consistently with and without the patch.
Tracked via HDFS-11043
2. TestDataNodeVolumeFailureReporting - Passes locally on my box. Ran multiple times. Seems like a flaky test.
3. TestDFSZKFailoverController - Passes locally on my box. Proven to be a flaky test. We tried to fix it via HDFS-9333 but still failing. Left a comment on HDFS-9333.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 33s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m 26s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 50s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m  8s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green} 13m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 13m  5s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  3s{color} | {color:orange} root: The patch generated 9 new + 475 unchanged - 2 fixed = 484 total (was 477) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 47s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 17s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  7m 57s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m  7s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 71m  4s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 49s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}179m 11s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.security.TestKDiag |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-14104 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12861654/HADOOP-14104-trunk-v4.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  cc  |
| uname | Linux 49d97b6d6637 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 845529b |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/12008/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/12008/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/12008/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/12008/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12008/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Hi Rushabh, thanks for revving. Yongjun and I reviewed this together, posted here are our combined review comments. Looks really good overall!

Nits:
* Change DFS_KMS_PREFIX to private
* Rename getKmsSecretKey to getKeyProviderMapKey (included in item below), since "SecretKey" sounds like an encryption key, a javadoc would also help


Bigger things:

In DistributedFileSystem, this changes the uri passed to DFSClient:
{code}
   this.dfs = new DFSClient(uri, conf, statistics);
   this.uri = URI.create(uri.getScheme()+"://"+uri.getAuthority());
{code}
to
{code}
   this.uri = URI.create(uri.getScheme()+"://"+uri.getAuthority());
   this.dfs = new DFSClient(uri, conf, statistics);
{code}

To be safe, I'd suggest that we don't change the order of the above code, and instead change the method in DFSClient.java to just grab the scheme and authority:

{code}
 public Text getKmsSecretKey() {
    return new Text(DFS_KMS_PREFIX + namenodeUri.toString());
  }
{code}

to
{code}
  public Text getKeyProviderMapKey() {
    return new Text(DFS_KMS_PREFIX + nnUri.getScheme()
        + "://" + nnUri.getAuthority());
  }
{code}

Thanks [~andrew.wang] and [~yzhangal] for reviewing and for your valubale feedback.
Will upload a new patch by EOB tomorrow addressing your review comments.

Hopefully the last patch for trunk. :)
[~andrew.wang] [~yzhangal]: I really appreciate your feedback.
Thanks for being extremely patient with me.
Attaching a new patch addressing your comments and fixing some checkstyle issues.
Please review.
If this patch is good for trunk, I will then attach branch-2 and branch-2.8 patch immediately.

Test failures from previous precommit build. Both tests ran fine on my machine.
1. hadoop.security.TestKDiag: Known to be flaky. Tracked via HADOOP-14030
2. hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting:
This also is known to be flaky. Tried to fix via HDFS-11316, HDFS-11398. But still flaky.



| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 23s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 13s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 47s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 16s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 54s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m  8s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 28s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green} 14m 28s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 14m 28s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m 11s{color} | {color:orange} root: The patch generated 6 new + 475 unchanged - 2 fixed = 481 total (was 477) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 55s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  2s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 28s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 16s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  7m 51s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m  8s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 69m 18s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 37s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}176m 50s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.security.TestKDiag |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-14104 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12861904/HADOOP-14104-trunk-v5.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  cc  |
| uname | Linux b68e09d7a21e 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 6eba792 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/12021/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/12021/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/12021/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12021/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



+1 LGTM, Yongjun also pinged me to say it looked good to him. I'll commit this shortly.

Committed to trunk, leaving this open for the branch-2 and branch-2.8 patches. Thanks again Rushabh!

SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11521 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11521/])
HADOOP-14104. Client should always ask namenode for kms provider path. (wang: rev 18432130a7f580f206adf023507678c534487f2e)
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/KMSUtil.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/TransparentEncryption.md
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/proto/hdfs.proto
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/local/LocalConfigKeys.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestKeyProviderCache.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocolPB/TestPBHelper.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsServerDefaults.java
* (edit) hadoop-common-project/hadoop-common/src/main/resources/core-default.xml
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/KeyProviderCache.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java


Thanks [~andrew.wang] for committing. Nice work [~shahrs87]!


Attaching branch-2 and branch-2.8 patch.
No code change compared to trunk.

[~andrew.wang] Thanks for reviewing and committing to trunk.
[~yzhangal] [~daryn]: Thanks for reviews. :)

Hi [~shahrs87], I think you attached a wrong branch-2 patch which has 0.8 K only? :)

| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 14m 30s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 14s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  9m  7s{color} | {color:green} branch-2.8 passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m  3s{color} | {color:green} branch-2.8 passed with JDK v1.8.0_121 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  7m  0s{color} | {color:green} branch-2.8 passed with JDK v1.7.0_121 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 12s{color} | {color:green} branch-2.8 passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 30s{color} | {color:green} branch-2.8 passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 46s{color} | {color:green} branch-2.8 passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 15s{color} | {color:green} branch-2.8 passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m  4s{color} | {color:green} branch-2.8 passed with JDK v1.8.0_121 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 59s{color} | {color:green} branch-2.8 passed with JDK v1.7.0_121 {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  7m  4s{color} | {color:green} the patch passed with JDK v1.8.0_121 {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green}  7m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  7m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  7m  8s{color} | {color:green} the patch passed with JDK v1.7.0_121 {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green}  7m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  7m  8s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 14s{color} | {color:orange} root: The patch generated 9 new + 502 unchanged - 7 fixed = 511 total (was 509) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  0s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 14s{color} | {color:green} the patch passed with JDK v1.8.0_121 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m 11s{color} | {color:green} the patch passed with JDK v1.7.0_121 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 27s{color} | {color:green} hadoop-common in the patch passed with JDK v1.7.0_121. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m  6s{color} | {color:green} hadoop-hdfs-client in the patch passed with JDK v1.7.0_121. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 47m 19s{color} | {color:green} hadoop-hdfs in the patch passed with JDK v1.7.0_121. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 31s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}226m 25s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:5af2af1 |
| JIRA Issue | HADOOP-14104 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12862096/HADOOP-14104-branch-2.8.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  cc  |
| uname | Linux 83ede633861d 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | branch-2.8 / 9c1cf63 |
| Default Java | 1.7.0_121 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_121 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/12030/artifact/patchprocess/diff-checkstyle-root.txt |
| JDK v1.7.0_121  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/12030/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12030/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



bq. Hi Rushabh S Shah, I think you attached a wrong branch-2 patch which has 0.8 K only? 
Thanks [~junping_du] !
I attached branch-2 version of another jira. :)
Attached the right branch-2 patch now.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 10s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 37s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  5m 38s{color} | {color:green} branch-2 passed with JDK v1.8.0_121 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m 38s{color} | {color:green} branch-2 passed with JDK v1.7.0_121 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 33s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 25s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 46s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 21s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m  3s{color} | {color:green} branch-2 passed with JDK v1.8.0_121 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m  1s{color} | {color:green} branch-2 passed with JDK v1.7.0_121 {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  5m 32s{color} | {color:green} the patch passed with JDK v1.8.0_121 {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green}  5m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  5m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m 40s{color} | {color:green} the patch passed with JDK v1.7.0_121 {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green}  6m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  6m 40s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 34s{color} | {color:orange} root: The patch generated 6 new + 457 unchanged - 2 fixed = 463 total (was 459) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  0s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 15s{color} | {color:green} the patch passed with JDK v1.8.0_121 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m 11s{color} | {color:green} the patch passed with JDK v1.7.0_121 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 18s{color} | {color:green} hadoop-common in the patch passed with JDK v1.7.0_121. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 19s{color} | {color:green} hadoop-hdfs-client in the patch passed with JDK v1.7.0_121. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 49m 57s{color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_121. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 31s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}216m 39s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_121 Failed junit tests | hadoop.tracing.TestTracing |
|   | hadoop.hdfs.server.datanode.TestFsDatasetCache |
| JDK v1.7.0_121 Failed junit tests | hadoop.hdfs.server.blockmanagement.TestReplicationPolicyWithUpgradeDomain |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:b59b8b7 |
| JIRA Issue | HADOOP-14104 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12862314/HADOOP-14104-branch-2.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  cc  |
| uname | Linux 1825e1ad1689 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | branch-2 / 9016614 |
| Default Java | 1.7.0_121 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_121 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/12042/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/12042/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_121.txt |
| JDK v1.7.0_121  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/12042/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12042/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Committed to branch-2 and branch-2.8 as well, thanks again for working on this Rushabh, as well as Yongjun and Daryn for reviews.

Thanks all ! :)

SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11591 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11591/])
HADOOP-14104. Client should always ask namenode for kms provider path. (wang: rev 18432130a7f580f206adf023507678c534487f2e)
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/KMSUtil.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocolPB/TestPBHelper.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsServerDefaults.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/local/LocalConfigKeys.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/TransparentEncryption.md
* (edit) hadoop-common-project/hadoop-common/src/main/resources/core-default.xml
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/proto/hdfs.proto
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestKeyProviderCache.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/KeyProviderCache.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java


HI [~rushabh.shah], [~andrew.wang], [~daryn],

Just noticed that he change of adding "throws" in the public API of DFSClient breaks hive, which accesses DFSClient, thought it should not because DFSClient is private. 

public boolean isHDFSEncryptionEnabled() throws IOException{

{code}
 * Hadoop DFS users should obtain an instance of 
 * DistributedFileSystem, which uses DFSClient to handle
 * filesystem tasks.
 *
 ********************************************************/
@InterfaceAudience.Private
public class DFSClient implements java.io.Closeable, RemotePeerFactory,
    DataEncryptionKeyFactory {
{code}

Since the impact is wide (many different versions of Hive), we can remove the throws from this API as a temporary solution. And simply return false when it throws, do you guys agree?

We should also let hive to fix their code not to access private interface of Hadoop.

Thanks.



[~yzhangal] thanks for reporting !
I will discuss with [~daryn] and will update this jira soon.

bq. Since the impact is wide (many different versions of Hiv
[~yzhangal] which version of hive is calling ?
I tried to search on grepcode but couldn't find any hit.

Thanks a lot [~shahrs87].

I created HADOOP-14333. This is a temp solution, and we are requesting hive to make changes to use public API, we can add public APIs if they are not there. I hope we can get this in asap today.


Hive code
{code}
private boolean isEncryptionEnabled(DFSClient client, Configuration conf) {
  try {
    DFSClient.class.getMethod("isHDFSEncryptionEnabled");
  } catch (NoSuchMethodException e) {
    // the method is available since Hadoop-2.7.1
    // if we run with an older Hadoop, check this ourselves
    return !conf.getTrimmed(DFSConfigKeys.DFS_ENCRYPTION_KEY_PROVIDER_URI, "").isEmpty();
  }
  return client.isHDFSEncryptionEnabled();
}
{code}


2.8.1 became a security release. Moving fix-version to 2.8.2 after the fact.

The patch cause incompatible API change here as omit previous constructor of FsServerDefaults which is a public API. Just filed HADOOP-14814 and put up a fix there. Please help to review and commit/fix this issue.

Thanks Rushabh and all for the contribution here.

Just a note:
We have had a downstream application broken, due to the 'cache the nameservice to provider mapping into UGI credentials' logic:
- application operates with 2 clusters, which have the same NN nameservice.
- application loads to configuration objects with corresponding cluster, and creates 2 separate dfsclients.
- All these are done using the same UGI.

This worked for them before. 

Upon going into a version with HADOOP-14104, accessing one of the clusters would fail with 'key not found'. This is due to there is only 1 mapping from nameservice -> kms in the UGI credentials. So {{[DFSClient#getKeyProviderUri|https://github.com/apache/hadoop/blob/branch-3.0.0-beta1/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java#L2991]}} always find the same KMS provider uri, for both clusters.

Having identical nameservices for multiple clusters is arguably a mis-configuration (and is how we moved over the issue at the time - luckily one of the cluster could be changed without too much trouble). But ideally this should work regardless. I don't have a great idea on how to fix this, but figured I'd at least share the problem statement.

bq. We have had a downstream application broken, due to the 'cache the nameservice to provider mapping into UGI credentials' logic:
The key for the cache is {{"dfs-kms-hdfs://" + namenodeUri.getAuthority()}}. I am confused about the usage of word {{nameservice}}.
The key differentiator is the {{namenodeUri.getAuthority()}}. Do you mean both clusters has the same {{fs.defaultFS}} config ?

Right, 2 separate clusters, running with the same {{fs.defaultFS}}, e.g. {{hdfs://nameservice1}}. That creative application is the only one that does cross cluster work.

[~xiaochen], can we use a unique id (e.g., guid) instead of the the nameservice id that can be arbitrarily configured to be the same for the key of the map? 

[~xyao]: I _don't think_ we need code change. The cluster is wrongly configured.

bq. That creative application is the only one that does cross cluster work.
I don't know why do you need the same {{fs.defaultFS}} to make cross cluster work.
We have more than dozen clusters and all of them can talk to each other with different nameservice id.

bq. I don't know why do you need the same fs.defaultFS to make cross cluster work.
As I said this is arguably a misconfiguration, and for the time being changing a cluster's nameservice worked. We also suggested that application to manage the nameservices locally so it can work even with this jira and the same name service.

In practice, it's possible to have 2 separate existing clusters initially, and later on have the need to talk to each other. So I think it's worth sharing here for this potential problem - which happened in reality on production clusters after an upgrade (albeit a creative application). Good admin instruction is to plan for this ahead of time, and change one of the name service in a maintenance window. Otherwise, they would meet this failure.

At the root of the issue, the assumption that the same UGI will be used by downstream applications to contact namespaces with different name service may not always be true. 

bq. can we use a unique id (e.g., guid)
I thought along similar lines too. But not sure what's the best definition for the 'key' in the credential map. Maybe an id for the dfsclient together with the nameservice would solve the problem, but also with the trade off that different dfsclients accessing really the same nameservice would result in multiple entries in the UGI's credentials.

It's not clear to me how much performance gain this caching in the UGI gives us. DFSClients already caches {{serverDefaults}} for 1 hour (The '2.' in {{getKeyProviderUri}}). Is that sufficient?

bq. Having identical nameservices for multiple clusters is arguably a mis-configuration
No arguably, it is a misconfiguration.

Instead of adding more complexity like guids to an already terrible idea – a conf-based nameservice which is ironically what allows this problem to exist – in an attempt to disambiguate the shared name,  I have a simpler solution: _uniquely name your clusters_.  There's nothing to fix.

As trivia: RPC has the same "issue", although it's not as evident due to persistent connections unlike the kms & http.  If the RPC connection goes down (idle closes, connection issue, retriable exception, etc), it's going to reconnect with a token, possibly the wrong token because it was for the other NN.





I agree if there were a non-config-based nameservice, this would be solved for all. Following that thought, perhaps in the current setting if the mapping were <ip:kpUri> instead of <nameserver:kpUri> it would also work. :)

Anyways, I guess my goal of sharing the problem statement is done here. Thanks all for the discussions, and bonus points for Daryn's trivia.

Great work !
Sharing an issue we have which is likely an unintended side-effect of this change..
We do a hdfs distcp of ./reserved/raw from source cluster to destination cluster. The source cluster don't have access to destination KMS and it doesn't need that access since it is a copy of raw bytes with xatrributes.
However after uptaking this change, during copy, source cluster finds that there's a kms configured at the destination cluster from destination cluster namenode and tries to contact the remote KMS to get delegation token, generating encrypted key etc. In our case contact to KMS fails and distcp fails.

Hi [~aajayaraj],

As the above discussion goes, the way to have this working again after taking this change, is to configure the 2 clusters to have different HDFS name service. Can do this by either changing one of the cluster's nameservice altogether, or hand-craft a specific configuration for the distcp job and pass that in with --config

[~daryn] or [~shahrs87] or [~xiaochen] I would be curious if there was a reason this was removed we used distcp and it no longer works as it wants to get to our Remote Clusters KMS Server which is isolated even if not attempting to distcp an encrypted zone so by removing "if (dfs.isHDFSEncryptionEnabled())" we now have situation where 1) folks are able to move TDE/Encrypted data between a local and remote clusters which is undesirable. And now we have to figure out how to open our KMSServer in our remote Cluster for moving data that does not reside in an Encrypted Zone.

Code change

 public Token<?>[] addDelegationTokens(
      final String renewer, Credentials credentials) throws IOException {
    Token<?>[] tokens = super.addDelegationTokens(renewer, credentials);
*    if (dfs.isHDFSEncryptionEnabled()) {
*      KeyProviderDelegationTokenExtension keyProviderDelegationTokenExtension =
          KeyProviderDelegationTokenExtension.
              createKeyProviderDelegationTokenExtension(dfs.getKeyProvider());
      Token<?>[] kpTokens = keyProviderDelegationTokenExtension.
          addDelegationTokens(renewer, credentials);
      if (tokens != null && kpTokens != null) {
        Token<?>[] all = new Token<?>[tokens.length + kpTokens.length];
        System.arraycopy(tokens, 0, all, 0, tokens.length);
        System.arraycopy(kpTokens, 0, all, tokens.length, kpTokens.length);
        tokens = all;
      } else {
        tokens = (tokens != null) ? tokens : kpTokens;
      }
    }
    return tokens;
  }

vs


  @Override
  public Token<?>[] addDelegationTokens(
      final String renewer, Credentials credentials) throws IOException {
    Token<?>[] tokens = super.addDelegationTokens(renewer, credentials);
    URI keyProviderUri = dfs.getKeyProviderUri();
    if (keyProviderUri != null) {
      KeyProviderDelegationTokenExtension keyProviderDelegationTokenExtension =
          KeyProviderDelegationTokenExtension.
              createKeyProviderDelegationTokenExtension(dfs.getKeyProvider());
      Token<?>[] kpTokens = keyProviderDelegationTokenExtension.
          addDelegationTokens(renewer, credentials);
      credentials.addSecretKey(dfs.getKeyProviderMapKey(),
          DFSUtil.string2Bytes(keyProviderUri.toString()));
      if (tokens != null && kpTokens != null) {
        Token<?>[] all = new Token<?>[tokens.length + kpTokens.length];
        System.arraycopy(tokens, 0, all, 0, tokens.length);
        System.arraycopy(kpTokens, 0, all, tokens.length, kpTokens.length);
        tokens = all;
      } else {
        tokens = (tokens != null) ? tokens : kpTokens;
      }
    }
    return tokens;
  }

Stack trace showing exception:
$ hadoop distcp -Ddfs.namenode.kerberos.principal.pattern=* -Dmapreduce.job.hdfs-servers.token-renewal.exclude=tech hdfs:///processed/public/opendata/samples/distcp_test/distcp_file.txt hdfs://tech/processed/public/opendata/samples/distcp_test/distcp_file2.txt
19/05/29 14:06:09 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, overwrite=false, append=false, useDiff=false, fromSnapshot=null, toSnapshot=null, skipCRC=false, blocking=true, numListstatusThreads=0, maxMaps=20, mapBandwidth=100, sslConfigurationFile='null', copyStrategy='uniformsize', preserveStatus=[], preserveRawXattrs=false, atomicWorkPath=null, logPath=null, sourceFileListing=null, sourcePaths=[hdfs:/processed/public/opendata/samples/distcp_test/distcp_file.txt], targetPath=hdfs://tech/processed/public/opendata/samples/distcp_test/distcp_file2.txt, targetPathExists=true, filtersFile='null', verboseLog=false}
19/05/29 14:06:09 INFO client.AHSProxy: Connecting to Application History server at ha21d53mn.unit.hdp.example.com/10.70.49.2:10200
19/05/29 14:06:10 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 5093920 for gss2002 on ha-hdfs:unit
19/05/29 14:06:10 INFO security.TokenCache: Got dt for hdfs://unit; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:unit, Ident: (HDFS_DELEGATION_TOKEN token 5093920 for gss2002)
19/05/29 14:06:10 INFO security.TokenCache: Got dt for hdfs://unit; Kind: kms-dt, Service: ha21d53en.unit.hdp.example.com:9292, Ident: (owner=gss2002, renewer=yarn, realUser=, issueDate=1559153170120, maxDate=1559757970120, sequenceNumber=237, masterKeyId=2)
19/05/29 14:06:10 INFO tools.SimpleCopyListing: Paths (files+dirs) cnt = 1; dirCnt = 0
19/05/29 14:06:10 INFO tools.SimpleCopyListing: Build file listing completed.
19/05/29 14:06:10 INFO tools.DistCp: Number of paths in the copy list: 1
19/05/29 14:06:10 INFO tools.DistCp: Number of paths in the copy list: 1
19/05/29 14:06:10 INFO client.AHSProxy: Connecting to Application History server at ha21d53mn.unit.hdp.example.com/10.70.49.2:10200
19/05/29 14:06:10 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 556079 for gss2002 on ha-hdfs:tech
19/05/29 14:06:10 ERROR tools.DistCp: Exception encountered 
java.io.IOException: java.net.NoRouteToHostException: No route to host (Host unreachable)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:1029)
	at org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension.addDelegationTokens(KeyProviderDelegationTokenExtension.java:110)
	at org.apache.hadoop.hdfs.DistributedFileSystem.addDelegationTokens(DistributedFileSystem.java:2407)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:140)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
	at org.apache.hadoop.tools.mapred.CopyOutputFormat.checkOutputSpecs(CopyOutputFormat.java:124)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:266)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
	at org.apache.hadoop.tools.DistCp.createAndSubmitJob(DistCp.java:193)
	at org.apache.hadoop.tools.DistCp.execute(DistCp.java:155)
	at org.apache.hadoop.tools.DistCp.run(DistCp.java:128)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.tools.DistCp.main(DistCp.java:462)
Caused by: java.net.NoRouteToHostException: No route to host (Host unreachable)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558)
	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242)
	at sun.net.www.http.HttpClient.New(HttpClient.java:339)
	at sun.net.www.http.HttpClient.New(HttpClient.java:357)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:188)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:133)
	at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:216)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.doDelegationTokenOperation(DelegationTokenAuthenticator.java:299)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.getDelegationToken(DelegationTokenAuthenticator.java:171)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.getDelegationToken(DelegationTokenAuthenticatedURL.java:373)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider$4.run(KMSClientProvider.java:1016)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider$4.run(KMSClientProvider.java:1011)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:1011)
	... 19 more


