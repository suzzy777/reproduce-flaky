[~bbejeck]
Could you please post the stack trace? I ran the test case multiple times repeatedly in my IDE. I'm not getting any error.

Hi [~ckamal],

I don't have it anymore, but I've only seen this failure once in the build, and I haven't been able to reproduce it locally either.Â 

I'll close the ticket for now, and if it occurs again, I'll reopen it.

I've seen it again with the following error:

{code}
Stacktrace
org.apache.kafka.streams.errors.StreamsException: Could not lock global state directory. This could happen if multiple KafkaStreams instances are running on the same host using the same state directory.
	at org.apache.kafka.streams.processor.internals.GlobalStreamThread.initialize(GlobalStreamThread.java:356)
	at org.apache.kafka.streams.processor.internals.GlobalStreamThread.run(GlobalStreamThread.java:274)
Caused by: org.apache.kafka.streams.errors.LockException: Failed to lock the global state directory: /tmp/kafka-9Bd3Q/appId/global
	at org.apache.kafka.streams.processor.internals.GlobalStateManagerImpl.initialize(GlobalStateManagerImpl.java:116)
	at org.apache.kafka.streams.processor.internals.GlobalStateUpdateTask.initialize(GlobalStateUpdateTask.java:62)
	at org.apache.kafka.streams.processor.internals.GlobalStreamThread$StateConsumer.initialize(GlobalStreamThread.java:233)
	at org.apache.kafka.streams.processor.internals.GlobalStreamThread.initialize(GlobalStreamThread.java:349)
	... 1 more
Standard Output
[2019-09-20 23:18:34,814] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase:117)
[2019-09-20 23:18:34,814] INFO minSessionTimeout set to 1600 (org.apache.zookeeper.server.ZooKeeperServer:938)
[2019-09-20 23:18:34,814] INFO maxSessionTimeout set to 16000 (org.apache.zookeeper.server.ZooKeeperServer:947)
[2019-09-20 23:18:34,814] INFO Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-8348092513924318612/version-2 snapdir /tmp/kafka-8312065212834966169/version-2 (org.apache.zookeeper.server.ZooKeeperServer:166)
[2019-09-20 23:18:34,815] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 48 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory:673)
[2019-09-20 23:18:34,815] INFO binding to port /127.0.0.1:0 (org.apache.zookeeper.server.NIOServerCnxnFactory:686)
[2019-09-20 23:18:34,816] INFO Snapshotting: 0x0 to /tmp/kafka-8312065212834966169/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog:372)
[2019-09-20 23:18:34,817] INFO Snapshotting: 0x0 to /tmp/kafka-8312065212834966169/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog:372)
[2019-09-20 23:18:34,819] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit553134282039358692/junit6683862867816141078
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:46499
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:347)
[2019-09-20 23:18:34,820] INFO starting (kafka.server.KafkaServer:66)
[2019-09-20 23:18:34,821] INFO Connecting to zookeeper on 127.0.0.1:46499 (kafka.server.KafkaServer:66)
[2019-09-20 23:18:34,821] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:46499. (kafka.zookeeper.ZooKeeperClient:66)
[2019-09-20 23:18:34,822] INFO Initiating client connection, connectString=127.0.0.1:46499 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@2f45f421 (org.apache.zookeeper.ZooKeeper:868)
[2019-09-20 23:18:34,822] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket:237)
[2019-09-20 23:18:34,823] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn:1653)
[2019-09-20 23:18:34,823] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2019-09-20 23:18:34,824] INFO Opening socket connection to server localhost/127.0.0.1:46499. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn:1112)
[2019-09-20 23:18:34,824] INFO Socket connection established, initiating session, client: /127.0.0.1:35806, server: localhost/127.0.0.1:46499 (org.apache.zookeeper.ClientCnxn:959)
[2019-09-20 23:18:34,826] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog:216)
[2019-09-20 23:18:34,827] INFO Session establishment complete on server localhost/127.0.0.1:46499, sessionid = 0x105f951d0960000, negotiated timeout = 10000 (org.apache.zookeeper.ClientCnxn:1394)
[2019-09-20 23:18:34,827] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2019-09-20 23:18:34,854] INFO Cluster ID = 8PghfDCFTWSX7hjpVMlF6g (kafka.server.KafkaServer:66)
[2019-09-20 23:18:34,855] WARN No meta.properties file under dir /tmp/junit553134282039358692/junit6683862867816141078/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2019-09-20 23:18:34,856] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit553134282039358692/junit6683862867816141078
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:46499
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:347)
[2019-09-20 23:18:34,859] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit553134282039358692/junit6683862867816141078
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:46499
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:347)
[2019-09-20 23:18:34,861] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:18:34,862] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:18:34,862] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:18:34,864] INFO Loading logs. (kafka.log.LogManager:66)
[2019-09-20 23:18:34,864] INFO Logs loading complete in 0 ms. (kafka.log.LogManager:66)
[2019-09-20 23:18:34,865] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2019-09-20 23:18:34,865] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2019-09-20 23:18:34,867] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2019-09-20 23:18:34,869] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2019-09-20 23:18:34,892] INFO Awaiting socket connections on localhost:41278. (kafka.network.Acceptor:66)
[2019-09-20 23:18:34,895] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : Endpoint(listener='PLAINTEXT', securityProtocol=PLAINTEXT, host='localhost', port=0) (kafka.network.SocketServer:66)
[2019-09-20 23:18:34,896] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer:66)
[2019-09-20 23:18:34,896] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:18:34,897] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:18:34,897] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:18:34,898] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:18:34,901] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2019-09-20 23:18:34,902] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2019-09-20 23:18:34,903] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1569021514902,1569021514902,1,0,0,73739098708639744,190,0,24
 (kafka.zk.KafkaZkClient:66)
[2019-09-20 23:18:34,903] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(Endpoint(listener='PLAINTEXT', securityProtocol=PLAINTEXT, host='localhost', port=41278)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient:66)
[2019-09-20 23:18:34,945] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2019-09-20 23:18:34,945] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:18:34,948] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:18:34,948] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:18:34,949] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2019-09-20 23:18:34,949] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2019-09-20 23:18:34,950] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2019-09-20 23:18:34,950] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager:66)
[2019-09-20 23:18:34,951] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,951] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,951] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager:66)
[2019-09-20 23:18:34,951] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,952] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,953] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,953] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2019-09-20 23:18:34,953] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2019-09-20 23:18:34,954] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2019-09-20 23:18:34,955] INFO [Controller id=0] Initialized broker epochs cache: Map(0 -> 24) (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,959] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2019-09-20 23:18:34,960] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2019-09-20 23:18:34,960] INFO [Controller id=0] DEPRECATED: Partitions being reassigned through ZooKeeper: Map() (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,961] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,961] INFO [Controller id=0] Currently shutting brokers in the cluster: Set() (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,961] INFO [Controller id=0] Current list of topics in the cluster: Set() (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,961] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,961] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,961] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,961] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,961] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: Set() (kafka.controller.TopicDeletionManager:66)
[2019-09-20 23:18:34,962] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,962] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer:66)
[2019-09-20 23:18:34,962] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2019-09-20 23:18:34,962] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:18:34,963] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:18:34,962] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:41278 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2019-09-20 23:18:34,962] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2019-09-20 23:18:34,963] INFO Kafka startTimeMs: 1569021514813 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:18:34,963] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2019-09-20 23:18:34,963] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2019-09-20 23:18:34,963] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2019-09-20 23:18:34,963] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2019-09-20 23:18:34,963] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,965] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,965] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,965] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,965] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,965] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggerd by ZkTriggered (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,966] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2019-09-20 23:18:34,972] INFO StreamsConfig values: 
	application.id = appId
	application.server = 
	bootstrap.servers = [localhost:41278]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 10485760
	client.id = clientId
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	max.task.idle.ms = 0
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 2
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-1131390747131717459
	topology.optimization = none
	upgrade.from = null
	windowstore.changelog.additional.retention.ms = 86400000
 (org.apache.kafka.streams.StreamsConfig:347)
[2019-09-20 23:18:34,973] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:41278]
	client.dns.lookup = default
	client.id = clientId-admin
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2019-09-20 23:18:34,975] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:18:34,975] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:18:34,975] INFO Kafka startTimeMs: 1569021514975 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:18:34,975] INFO stream-thread [clientId-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread:585)
[2019-09-20 23:18:34,976] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:41278]
	check.crcs = true
	client.dns.lookup = default
	client.id = clientId-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2019-09-20 23:18:34,978] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:18:34,978] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:18:34,979] INFO Kafka startTimeMs: 1569021514978 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:18:34,979] INFO stream-thread [clientId-StreamThread-1] Creating shared producer client (org.apache.kafka.streams.processor.internals.StreamThread:595)
[2019-09-20 23:18:34,979] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:41278]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = clientId-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2019-09-20 23:18:34,982] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:18:34,982] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:18:34,982] INFO Kafka startTimeMs: 1569021514982 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:18:34,983] INFO stream-thread [clientId-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread:639)
[2019-09-20 23:18:34,983] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41278]
	check.crcs = true
	client.dns.lookup = default
	client.id = clientId-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2019-09-20 23:18:34,985] WARN The configuration 'admin.retries' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:355)
[2019-09-20 23:18:34,986] WARN The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:355)
[2019-09-20 23:18:34,986] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:18:34,986] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:18:34,986] INFO Kafka startTimeMs: 1569021514986 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:18:34,987] INFO stream-thread [clientId-StreamThread-2] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread:585)
[2019-09-20 23:18:34,987] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:41278]
	check.crcs = true
	client.dns.lookup = default
	client.id = clientId-StreamThread-2-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2019-09-20 23:18:34,989] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:18:34,989
...[truncated 1990148 chars]...
eamsConfig values: 
	application.id = appId
	application.server = 
	bootstrap.servers = [localhost:41278]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 10485760
	client.id = clientId
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	max.task.idle.ms = 0
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 2
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-8478351407381807200
	topology.optimization = none
	upgrade.from = null
	windowstore.changelog.additional.retention.ms = 86400000
 (org.apache.kafka.streams.StreamsConfig:347)
[2019-09-20 23:20:46,067] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:41278]
	client.dns.lookup = default
	client.id = clientId-admin
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:347)
[2019-09-20 23:20:46,069] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:20:46,069] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:20:46,069] INFO Kafka startTimeMs: 1569021646069 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:20:46,070] INFO stream-thread [clientId-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread:585)
[2019-09-20 23:20:46,070] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:41278]
	check.crcs = true
	client.dns.lookup = default
	client.id = clientId-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2019-09-20 23:20:46,072] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:20:46,072] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:20:46,073] INFO Kafka startTimeMs: 1569021646072 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:20:46,073] INFO stream-thread [clientId-StreamThread-1] Creating shared producer client (org.apache.kafka.streams.processor.internals.StreamThread:595)
[2019-09-20 23:20:46,073] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:41278]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = clientId-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2019-09-20 23:20:46,075] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:20:46,076] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:20:46,076] INFO Kafka startTimeMs: 1569021646075 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:20:46,076] INFO stream-thread [clientId-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread:639)
[2019-09-20 23:20:46,076] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41278]
	check.crcs = true
	client.dns.lookup = default
	client.id = clientId-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2019-09-20 23:20:46,079] WARN The configuration 'admin.retries' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:355)
[2019-09-20 23:20:46,079] WARN The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:355)
[2019-09-20 23:20:46,080] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:20:46,080] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:20:46,080] INFO Kafka startTimeMs: 1569021646080 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:20:46,081] INFO stream-thread [clientId-StreamThread-2] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread:585)
[2019-09-20 23:20:46,081] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:41278]
	check.crcs = true
	client.dns.lookup = default
	client.id = clientId-StreamThread-2-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2019-09-20 23:20:46,083] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:20:46,083] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:20:46,083] INFO Kafka startTimeMs: 1569021646083 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:20:46,084] INFO stream-thread [clientId-StreamThread-2] Creating shared producer client (org.apache.kafka.streams.processor.internals.StreamThread:595)
[2019-09-20 23:20:46,084] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:41278]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = clientId-StreamThread-2-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:347)
[2019-09-20 23:20:46,086] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:20:46,087] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:20:46,087] INFO Kafka startTimeMs: 1569021646086 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:20:46,087] INFO stream-thread [clientId-StreamThread-2] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread:639)
[2019-09-20 23:20:46,087] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41278]
	check.crcs = true
	client.dns.lookup = default
	client.id = clientId-StreamThread-2-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = [org.apache.kafka.test.MockMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:347)
[2019-09-20 23:20:46,089] WARN The configuration 'admin.retries' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:355)
[2019-09-20 23:20:46,090] WARN The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:355)
[2019-09-20 23:20:46,090] INFO Kafka version: 2.4.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-09-20 23:20:46,090] INFO Kafka commitId: 5a6d916604c77d9a (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-09-20 23:20:46,090] INFO Kafka startTimeMs: 1569021646090 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-09-20 23:20:46,091] INFO stream-client [clientId] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams:264)
[2019-09-20 23:20:46,091] INFO stream-thread [clientId-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread:759)
[2019-09-20 23:20:46,091] INFO stream-thread [clientId-StreamThread-2] Starting (org.apache.kafka.streams.processor.internals.StreamThread:759)
[2019-09-20 23:20:46,092] INFO stream-client [clientId] State transition from REBALANCING to PENDING_SHUTDOWN (org.apache.kafka.streams.KafkaStreams:264)
[2019-09-20 23:20:46,092] INFO stream-thread [clientId-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread:218)
[2019-09-20 23:20:46,092] INFO stream-thread [clientId-StreamThread-2] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread:218)
[2019-09-20 23:20:46,093] INFO [Consumer clientId=clientId-StreamThread-1-consumer, groupId=appId] Subscribed to pattern: '' (org.apache.kafka.clients.consumer.KafkaConsumer:1016)
[2019-09-20 23:20:46,094] INFO [Consumer clientId=clientId-StreamThread-2-consumer, groupId=appId] Subscribed to pattern: '' (org.apache.kafka.clients.consumer.KafkaConsumer:1016)
[2019-09-20 23:20:46,093] INFO stream-thread [clientId-StreamThread-1] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread:1192)
[2019-09-20 23:20:46,094] INFO stream-thread [clientId-StreamThread-1] State transition from STARTING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread:218)
[2019-09-20 23:20:46,094] INFO stream-thread [clientId-StreamThread-2] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread:1192)
[2019-09-20 23:20:46,094] INFO stream-thread [clientId-StreamThread-2] State transition from STARTING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread:218)
[2019-09-20 23:20:46,098] INFO [Consumer clientId=clientId-StreamThread-1-consumer, groupId=appId] Cluster ID: 8PghfDCFTWSX7hjpVMlF6g (org.apache.kafka.clients.Metadata:261)
[2019-09-20 23:20:46,098] INFO [Consumer clientId=clientId-StreamThread-2-consumer, groupId=appId] Cluster ID: 8PghfDCFTWSX7hjpVMlF6g (org.apache.kafka.clients.Metadata:261)
[2019-09-20 23:20:46,098] INFO [Consumer clientId=clientId-StreamThread-1-consumer, groupId=appId] Discovered group coordinator localhost:41278 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:710)
[2019-09-20 23:20:46,098] INFO [Consumer clientId=clientId-StreamThread-2-consumer, groupId=appId] Discovered group coordinator localhost:41278 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:710)
[2019-09-20 23:20:46,099] INFO [Consumer clientId=clientId-StreamThread-1-consumer, groupId=appId] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:486)
[2019-09-20 23:20:46,099] INFO [Consumer clientId=clientId-StreamThread-2-consumer, groupId=appId] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:486)
[2019-09-20 23:20:46,101] INFO [Consumer clientId=clientId-StreamThread-1-consumer, groupId=appId] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:486)
[2019-09-20 23:20:46,101] INFO [Consumer clientId=clientId-StreamThread-2-consumer, groupId=appId] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:486)
[2019-09-20 23:20:46,102] INFO [GroupCoordinator 0]: Preparing to rebalance group appId in state PreparingRebalance with old generation 14 (__consumer_offsets-4) (reason: Adding new member clientId-StreamThread-1-consumer-29c6ca8b-af1d-481d-9ac1-96d5ed59c73b with group instanceid None) (kafka.coordinator.group.GroupCoordinator:66)
[2019-09-20 23:20:46,176] INFO [Producer clientId=clientId-StreamThread-1-producer] Cluster ID: 8PghfDCFTWSX7hjpVMlF6g (org.apache.kafka.clients.Metadata:261)
[2019-09-20 23:20:46,187] INFO [Producer clientId=clientId-StreamThread-2-producer] Cluster ID: 8PghfDCFTWSX7hjpVMlF6g (org.apache.kafka.clients.Metadata:261)
[2019-09-20 23:20:46,194] INFO stream-thread [clientId-StreamThread-2] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread:1206)
[2019-09-20 23:20:46,194] INFO stream-thread [clientId-StreamThread-1] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread:1206)
[2019-09-20 23:20:46,194] INFO [Consumer clientId=clientId-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1061)
[2019-09-20 23:20:46,195] INFO [Consumer clientId=clientId-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1061)
[2019-09-20 23:20:46,195] INFO [Producer clientId=clientId-StreamThread-2-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1174)
[2019-09-20 23:20:46,195] INFO [Producer clientId=clientId-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1174)
[2019-09-20 23:20:46,555] WARN [AdminClient clientId=adminclient-629] Connection to node 0 (localhost/127.0.0.1:46535) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:47,657] WARN [AdminClient clientId=adminclient-629] Connection to node 0 (localhost/127.0.0.1:46535) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:48,860] WARN [AdminClient clientId=adminclient-629] Connection to node 0 (localhost/127.0.0.1:46535) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:49,862] WARN [AdminClient clientId=adminclient-629] Connection to node 0 (localhost/127.0.0.1:46535) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:50,865] WARN [AdminClient clientId=adminclient-629] Connection to node 0 (localhost/127.0.0.1:46535) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:51,867] WARN [AdminClient clientId=adminclient-629] Connection to node 0 (localhost/127.0.0.1:46535) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:52,870] WARN [AdminClient clientId=adminclient-629] Connection to node 0 (localhost/127.0.0.1:46535) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:54,073] WARN [AdminClient clientId=adminclient-629] Connection to node 0 (localhost/127.0.0.1:46535) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:55,276] WARN [AdminClient clientId=adminclient-629] Connection to node 0 (localhost/127.0.0.1:46535) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:55,659] INFO [GroupCoordinator 0]: Member clientId-StreamThread-1-consumer-24f7eb70-df91-4beb-bbea-15bf98ac140c in group appId has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator:66)
[2019-09-20 23:20:55,659] INFO [GroupCoordinator 0]: Member clientId-StreamThread-2-consumer-36b7270b-62a6-4557-8e0d-1b17ebf59b5f in group appId has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator:66)
[2019-09-20 23:20:55,661] INFO [GroupCoordinator 0]: Stabilized group appId generation 15 (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2019-09-20 23:20:55,662] INFO stream-thread [clientId-StreamThread-2-consumer] Assigned tasks to clients as 
a6519244-700e-4716-aa55-045025e6fae9=[activeTasks: ([]) standbyTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) prevAssignedTasks: ([]) capacity: 2]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:530)
[2019-09-20 23:20:55,663] INFO [GroupCoordinator 0]: Assignment received from leader for group appId for generation 15 (kafka.coordinator.group.GroupCoordinator:66)
[2019-09-20 23:20:55,665] INFO [Consumer clientId=clientId-StreamThread-2-consumer, groupId=appId] Successfully joined group with generation 15 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:450)
[2019-09-20 23:20:55,665] INFO [Consumer clientId=clientId-StreamThread-1-consumer, groupId=appId] Successfully joined group with generation 15 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:450)
[2019-09-20 23:20:55,669] INFO stream-thread [clientId-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread:218)
[2019-09-20 23:20:55,670] INFO stream-thread [clientId-StreamThread-1] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread:1226)
[2019-09-20 23:20:55,670] INFO stream-thread [clientId-StreamThread-2] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread:218)
[2019-09-20 23:20:55,670] INFO stream-thread [clientId-StreamThread-2] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread:1226)
[2019-09-20 23:20:55,672] INFO stream-client [clientId] State transition from PENDING_SHUTDOWN to NOT_RUNNING (org.apache.kafka.streams.KafkaStreams:264)
[2019-09-20 23:20:55,672] INFO stream-client [clientId] Streams client stopped completely (org.apache.kafka.streams.KafkaStreams:899)
[2019-09-20 23:20:55,673] INFO stream-client [clientId] Already in the pending shutdown state, wait to complete shutdown (org.apache.kafka.streams.KafkaStreams:851)
[2019-09-20 23:20:55,673] INFO stream-client [clientId] Streams client stopped completely (org.apache.kafka.streams.KafkaStreams:899)
[2019-09-20 23:20:55,673] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2019-09-20 23:20:55,674] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2019-09-20 23:20:55,679] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2019-09-20 23:20:55,681] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2019-09-20 23:20:55,683] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2019-09-20 23:20:55,683] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2019-09-20 23:20:55,683] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2019-09-20 23:20:55,684] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2019-09-20 23:20:55,688] WARN [Producer clientId=clientId-StreamThread-1-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:55,688] WARN [AdminClient clientId=clientId-admin] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:55,689] WARN [Producer clientId=clientId-StreamThread-2-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:55,689] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2019-09-20 23:20:55,690] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2019-09-20 23:20:55,694] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2019-09-20 23:20:55,696] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2019-09-20 23:20:55,696] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,739] WARN [Producer clientId=clientId-StreamThread-1-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:55,740] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,740] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,741] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2019-09-20 23:20:55,741] INFO [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0 (kafka.coordinator.transaction.ProducerIdManager:66)
[2019-09-20 23:20:55,741] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2019-09-20 23:20:55,742] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2019-09-20 23:20:55,743] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2019-09-20 23:20:55,743] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2019-09-20 23:20:55,743] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2019-09-20 23:20:55,744] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2019-09-20 23:20:55,744] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,789] WARN [AdminClient clientId=clientId-admin] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:55,790] WARN [Producer clientId=clientId-StreamThread-2-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:55,839] WARN [Producer clientId=clientId-StreamThread-1-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:55,859] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,859] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,859] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,890] WARN [AdminClient clientId=clientId-admin] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:55,890] WARN [Producer clientId=clientId-StreamThread-2-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:55,897] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,897] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,897] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2019-09-20 23:20:55,897] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2019-09-20 23:20:55,898] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2019-09-20 23:20:55,899] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2019-09-20 23:20:55,899] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2019-09-20 23:20:55,899] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2019-09-20 23:20:55,899] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2019-09-20 23:20:55,900] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2019-09-20 23:20:55,900] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2019-09-20 23:20:55,900] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,969] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,969] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,969] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,972] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,972] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:55,972] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:56,090] WARN [Producer clientId=clientId-StreamThread-1-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:56,091] WARN [Producer clientId=clientId-StreamThread-2-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:56,171] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:56,171] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:56,171] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:56,172] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:56,172] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-09-20 23:20:56,178] WARN [AdminClient clientId=adminclient-629] Connection to node 0 (localhost/127.0.0.1:46535) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:56,191] WARN [AdminClient clientId=clientId-admin] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:56,195] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2019-09-20 23:20:56,195] INFO Shutting down. (kafka.log.LogManager:66)
[2019-09-20 23:20:56,196] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2019-09-20 23:20:56,196] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2019-09-20 23:20:56,196] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2019-09-20 23:20:56,196] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2019-09-20 23:20:56,219] INFO [ProducerStateManager partition=__consumer_offsets-4] Writing producer snapshot at offset 20 (kafka.log.ProducerStateManager:66)
[2019-09-20 23:20:56,221] INFO [ProducerStateManager partition=inMemoryStatefulTopologyShouldNotCreateStateDirectory-global-0] Writing producer snapshot at offset 1 (kafka.log.ProducerStateManager:66)
[2019-09-20 23:20:56,223] INFO [ProducerStateManager partition=inMemoryStatefulTopologyShouldNotCreateStateDirectory-input-0] Writing producer snapshot at offset 1 (kafka.log.ProducerStateManager:66)
[2019-09-20 23:20:56,225] INFO [ProducerStateManager partition=input-0] Writing producer snapshot at offset 1 (kafka.log.ProducerStateManager:66)
[2019-09-20 23:20:56,226] INFO [ProducerStateManager partition=__consumer_offsets-3] Writing producer snapshot at offset 4 (kafka.log.ProducerStateManager:66)
[2019-09-20 23:20:56,231] INFO [ProducerStateManager partition=statelessTopologyShouldNotCreateStateDirectory-input-0] Writing producer snapshot at offset 1 (kafka.log.ProducerStateManager:66)
[2019-09-20 23:20:56,305] INFO Shutdown complete. (kafka.log.LogManager:66)
[2019-09-20 23:20:56,306] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2019-09-20 23:20:56,306] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2019-09-20 23:20:56,306] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2019-09-20 23:20:56,307] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2019-09-20 23:20:56,308] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2019-09-20 23:20:56,308] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2019-09-20 23:20:56,309] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2019-09-20 23:20:56,309] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2019-09-20 23:20:56,310] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2019-09-20 23:20:56,310] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2019-09-20 23:20:56,414] INFO EventThread shut down for session: 0x105f951d0960000 (org.apache.zookeeper.ClientCnxn:524)
[2019-09-20 23:20:56,414] INFO Session: 0x105f951d0960000 closed (org.apache.zookeeper.ZooKeeper:1422)
[2019-09-20 23:20:56,416] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2019-09-20 23:20:56,416] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:20:56,442] WARN [Producer clientId=clientId-StreamThread-1-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:56,493] WARN [Producer clientId=clientId-StreamThread-2-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:56,592] WARN [AdminClient clientId=clientId-admin] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:56,876] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:20:56,876] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:20:56,876] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:20:57,144] WARN [Producer clientId=clientId-StreamThread-1-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:57,381] WARN [AdminClient clientId=adminclient-629] Connection to node 0 (localhost/127.0.0.1:46535) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:57,394] WARN [AdminClient clientId=clientId-admin] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:57,446] WARN [Producer clientId=clientId-StreamThread-2-producer] Connection to node 0 (localhost/127.0.0.1:41278) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:748)
[2019-09-20 23:20:57,876] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:20:57,876] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:20:57,877] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:20:57,877] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:20:57,877] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-09-20 23:20:57,878] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2019-09-20 23:20:57,911] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer:66)
[2019-09-20 23:20:57,915] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[2019-09-20 23:20:57,920] INFO ConnnectionExpirerThread interrupted (org.apache.zookeeper.server.NIOServerCnxnFactory:583)
[2019-09-20 23:20:57,922] INFO accept thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:219)
[2019-09-20 23:20:57,922] INFO selector thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:420)
[2019-09-20 23:20:57,922] INFO selector thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:420)
[2019-09-20 23:20:57,923] INFO selector thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:420)
[2019-09-20 23:20:57,926] INFO shutting down (org.apache.zookeeper.server.ZooKeeperServer:558)
[2019-09-20 23:20:57,926] INFO Shutting down (org.apache.zookeeper.server.SessionTrackerImpl:237)
[2019-09-20 23:20:57,926] INFO Shutting down (org.apache.zookeeper.server.PrepRequestProcessor:1007)
[2019-09-20 23:20:57,927] INFO Shutting down (org.apache.zookeeper.server.SyncRequestProcessor:191)
[2019-09-20 23:20:57,927] INFO PrepRequestProcessor exited loop! (org.apache.zookeeper.server.PrepRequestProcessor:155)
[2019-09-20 23:20:57,927] INFO SyncRequestProcessor exited! (org.apache.zookeeper.server.SyncRequestProcessor:169)
[2019-09-20 23:20:57,927] INFO shutdown of request processor complete (org.apache.zookeeper.server.FinalRequestProcessor:514)
{code}

Run locally 100 times, cannot see it again.

guozhangwang commented on pull request #7382: KAFKA-8319: Make KafkaStreamsTest a non-integration test class
URL: https://github.com/apache/kafka/pull/7382
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


Should have been fixed via https://github.com/apache/kafka/pull/7382

cadonna commented on pull request #8352: KAFKA-8319: Make KafkaStreamsTest a non-integration test class
URL: https://github.com/apache/kafka/pull/8352
 
 
   Previous KafkaStreamsTest takes 2min20s on my local laptop, because lots of its integration test which is producing / consuming records, and checking state directory file system takes lots of time. On the other hand, these tests should be well simplified with mocks.
   
   This test reduces the test from a clumsy integration test class into a unit tests with mocks of its internal modules. And some other test functions should not be in KafkaStreamsTest actually and have been moved to other modular test classes. Now it takes 2s.
   
   Also it helps removing the potential flakiness of the following (some of them are claimed resolved only because we have not seen them recently, but after looking at the test code I can verify they are still flaky):
   
   * KAFKA-5818 (the original JIRA ticket indeed exposed a real issue that has been fixed, but the test itself remains flaky)
   * KAFKA-6215
   * KAFKA-7921
   * KAFKA-7990
   * KAFKA-8319
   * KAFKA-8427
   
   Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Bruno Cadonna <bruno@confluent.io>
   
   This commit was cherry-picked from trunk and adapted.
   
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


guozhangwang commented on pull request #8352: KAFKA-8319: Make KafkaStreamsTest a non-integration test class
URL: https://github.com/apache/kafka/pull/8352
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


