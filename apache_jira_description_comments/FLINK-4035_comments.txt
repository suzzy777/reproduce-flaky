We'll probably need a new package for Kafka 0.10 consumers/producers, right [~rmetzger]. Similar to how we have it now for 0.8 and 0.9.

I haven't looked into the Kafka 0.10 changes in detail.

If the API didn't change between 9 and 10, users can probably manually force the Kafka 0.9 connector to use the 0.10 code (by putting the kafka 0.10 client into their pom).

AFAIK there haven't been changes to the producer API in 0.10, just protocol changes.  I'll try dropping the 0.10 jars into my job to give it a try and report back.

GitHub user radekg opened a pull request:

    https://github.com/apache/flink/pull/2231

    [FLINK-4035] Bump Kafka producer in Kafka sink to Kafka 0.10.0.0

    Hi everyone,
    
    At The Weather Company we bumped into a problem while trying to use Flink with Kafka 0.10.x. This PR introduces the support for `FlinkKafkaConsumer010` and `FlinkKafkaProducer010`. Unit test coverage is provided and `mvn clean verify` passes.
    
    The output is below:
    
    ```
    [INFO] ------------------------------------------------------------------------
    [INFO] Reactor Summary:
    [INFO]
    [INFO] force-shading ...................................... SUCCESS [  1.311 s]
    [INFO] flink .............................................. SUCCESS [  2.939 s]
    [INFO] flink-annotations .................................. SUCCESS [  1.476 s]
    [INFO] flink-shaded-hadoop ................................ SUCCESS [  0.152 s]
    [INFO] flink-shaded-hadoop2 ............................... SUCCESS [  7.065 s]
    [INFO] flink-shaded-include-yarn-tests .................... SUCCESS [  8.543 s]
    [INFO] flink-shaded-curator ............................... SUCCESS [  0.112 s]
    [INFO] flink-shaded-curator-recipes ....................... SUCCESS [  1.080 s]
    [INFO] flink-shaded-curator-test .......................... SUCCESS [  0.210 s]
    [INFO] flink-test-utils-parent ............................ SUCCESS [  0.126 s]
    [INFO] flink-test-utils-junit ............................. SUCCESS [  2.019 s]
    [INFO] flink-core ......................................... SUCCESS [ 34.501 s]
    [INFO] flink-java ......................................... SUCCESS [ 26.266 s]
    [INFO] flink-runtime ...................................... SUCCESS [04:57 min]
    [INFO] flink-optimizer .................................... SUCCESS [  7.914 s]
    [INFO] flink-clients ...................................... SUCCESS [  6.537 s]
    [INFO] flink-streaming-java ............................... SUCCESS [ 37.732 s]
    [INFO] flink-test-utils ................................... SUCCESS [  6.166 s]
    [INFO] flink-scala ........................................ SUCCESS [ 24.626 s]
    [INFO] flink-runtime-web .................................. SUCCESS [ 12.831 s]
    [INFO] flink-examples ..................................... SUCCESS [  1.123 s]
    [INFO] flink-examples-batch ............................... SUCCESS [ 11.919 s]
    [INFO] flink-contrib ...................................... SUCCESS [  0.096 s]
    [INFO] flink-statebackend-rocksdb ......................... SUCCESS [  7.770 s]
    [INFO] flink-tests ........................................ SUCCESS [06:22 min]
    [INFO] flink-streaming-scala .............................. SUCCESS [ 26.831 s]
    [INFO] flink-streaming-connectors ......................... SUCCESS [  0.100 s]
    [INFO] flink-connector-flume .............................. SUCCESS [  2.425 s]
    [INFO] flink-libraries .................................... SUCCESS [  0.084 s]
    [INFO] flink-table ........................................ SUCCESS [02:02 min]
    [INFO] flink-connector-kafka-base ......................... SUCCESS [  4.604 s]
    [INFO] flink-connector-kafka-0.8 .......................... SUCCESS [02:01 min]
    [INFO] flink-connector-kafka-0.9 .......................... SUCCESS [02:38 min]
    [INFO] flink-connector-kafka-0.10 ......................... SUCCESS [02:04 min]
    [INFO] flink-connector-elasticsearch ...................... SUCCESS [ 19.310 s]
    [INFO] flink-connector-elasticsearch2 ..................... SUCCESS [ 17.086 s]
    [INFO] flink-connector-rabbitmq ........................... SUCCESS [  2.885 s]
    [INFO] flink-connector-twitter ............................ SUCCESS [  2.649 s]
    [INFO] flink-connector-nifi ............................... SUCCESS [  1.339 s]
    [INFO] flink-connector-cassandra .......................... SUCCESS [01:21 min]
    [INFO] flink-connector-redis .............................. SUCCESS [  5.738 s]
    [INFO] flink-connector-filesystem ......................... SUCCESS [ 24.871 s]
    [INFO] flink-batch-connectors ............................. SUCCESS [  0.103 s]
    [INFO] flink-avro ......................................... SUCCESS [  9.788 s]
    [INFO] flink-jdbc ......................................... SUCCESS [  4.839 s]
    [INFO] flink-hadoop-compatibility ......................... SUCCESS [ 10.026 s]
    [INFO] flink-hbase ........................................ SUCCESS [  2.938 s]
    [INFO] flink-hcatalog ..................................... SUCCESS [  5.383 s]
    [INFO] flink-examples-streaming ........................... SUCCESS [ 24.339 s]
    [INFO] flink-gelly ........................................ SUCCESS [ 40.416 s]
    [INFO] flink-gelly-scala .................................. SUCCESS [ 29.050 s]
    [INFO] flink-gelly-examples ............................... SUCCESS [ 21.418 s]
    [INFO] flink-python ....................................... SUCCESS [ 58.016 s]
    [INFO] flink-ml ........................................... SUCCESS [ 57.270 s]
    [INFO] flink-cep .......................................... SUCCESS [  6.984 s]
    [INFO] flink-cep-scala .................................... SUCCESS [  8.521 s]
    [INFO] flink-scala-shell .................................. SUCCESS [03:28 min]
    [INFO] flink-quickstart ................................... SUCCESS [  1.248 s]
    [INFO] flink-quickstart-java .............................. SUCCESS [  0.610 s]
    [INFO] flink-quickstart-scala ............................. SUCCESS [  0.237 s]
    [INFO] flink-storm ........................................ SUCCESS [ 14.975 s]
    [INFO] flink-storm-examples ............................... SUCCESS [ 37.513 s]
    [INFO] flink-streaming-contrib ............................ SUCCESS [  8.452 s]
    [INFO] flink-tweet-inputformat ............................ SUCCESS [  3.075 s]
    [INFO] flink-operator-stats ............................... SUCCESS [  6.521 s]
    [INFO] flink-connector-wikiedits .......................... SUCCESS [ 18.022 s]
    [INFO] flink-yarn ......................................... SUCCESS [  7.539 s]
    [INFO] flink-dist ......................................... SUCCESS [ 11.453 s]
    [INFO] flink-metrics ...................................... SUCCESS [  0.101 s]
    [INFO] flink-metrics-dropwizard ........................... SUCCESS [  2.699 s]
    [INFO] flink-metrics-ganglia .............................. SUCCESS [  1.320 s]
    [INFO] flink-metrics-graphite ............................. SUCCESS [  1.188 s]
    [INFO] flink-metrics-statsd ............................... SUCCESS [  2.271 s]
    [INFO] flink-fs-tests ..................................... SUCCESS [ 27.916 s]
    [INFO] flink-java8 ........................................ SUCCESS [ 12.209 s]
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD SUCCESS
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time: 37:24 min
    [INFO] Finished at: 2016-07-12T15:18:39-04:00
    [INFO] Final Memory: 234M/1833M
    [INFO] ------------------------------------------------------------------------
    ```
    
    The only thing not provided right in this moment in time, is the documentation updates. Not sure how to take on that one, some guidance would be appreciated.
    
    What would be the best way to proceed with the contribution?

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/TheWeatherCompany/flink kafka-0.10

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/2231.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2231
    
----

----


Hi everyone,

I'm the author of the above PR. Happy to hear what else would be necessary to make this contribution go into the main repo.

The tests are passing.

Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Hi @radekg , thank you for opening a PR for this!
    From a first look it seems that there isn't much changes to the code of `flink-connector-kafka-0.9` and this PR. Also, from the original discussion / comments in the JIRA, the Kafka API doesn't seem to have changed between 0.9 and 0.10, so it might be possible to let the Kafka 0.9 connector use the 0.10 client by putting the Kafka 0.10 dependency into the user pom.
    
    May I ask whether you have tried this approach out already? Also,
    > At The Weather Company we bumped into a problem while trying to use Flink with Kafka 0.10.x.
    What was the problem? If you can describe, it'll be helpful for deciding how we can proceed with this :) There's another contributor who was trying this out, I'll also try to ask for his feedback on this in the JIRA.


Hi [~elevy], do you have any feedback on the approach of forcing the 0.9 connector to use 0.10 jars? It'll be helpful to see how we should proceed with the PR :)

Github user radekg commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Sure, the problems are the following:
    
    - https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-aba21cf86694f3f2cd85e2e5e9b04972R305 in 0.9, `consumer.assign` (https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-aba21cf86694f3f2cd85e2e5e9b04972R180) takes a `List`, in 0.10 it takes `Collection`
    - for unit tests: https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-ab65f3156ed8820677f3420152b78908R130, if we use 0.9 kafka version with 0.10 client, the concrete client tests fail as they catch wrong exception type in: https://github.com/TheWeatherCompany/flink/blob/06936d7c5acc0897348019161c9ced4596a0a4dd/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java#L185
    
    Silly stuff. Everything else works just fine. Fell free to reuse this stuff.
    
    FYI: I'd be confused it I was to use a class indicating 0.9 when working with 0.10, that's the reason I assembled separate module. 0.9 is done and there's no future work required, it makes sense to have 0.10. Just my opinion.


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Thank you for the description, @radekg .
    
    I think the problems you mentioned should be solvable by working on the 0.9 connector to be just a bit more general, then users can simply manually use the 0.10 jars. However, you also have a point on the possible confusion. IMHO, I think it is redundant to have two connector modules with almost the same code, and it doesn't also seem feasible for code maintainability to keep adding modules for new Kafka versions even if they don't have changes in the API.
    
    I think we'll need to loop in @rmetzger and @aljoscha to decide on how we can proceed with this. The solutions I currently see is to work on the 0.9 connector on the above problems so it can be compatible with the 0.10 API, and either rename the module to be `flink-connecto-kafka-0.10` (doesn't seem good because it'll be breaking user's pom's), or add information to the documentation on how to work with Kafka 0.10. Either way, in the long-run, we'll probably still need to sort out a way to better manage the connector codes in situations of new external system versions like this.


Sorry. I am on a boat in the middle of Indonesia for a few weeks without
much access.

On Thursday, July 14, 2016, Tzu-Li (Gordon) Tai (JIRA) <jira@apache.org>



Github user aljoscha commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    I agree with @tzulitai that it would be nice if you could have minimum code duplication in the long-run but it might not be possible with the current design of the consumers.
    
    What about the new timestamps that were introduced in Kafka 0.10? This is also something that wouldn't work with the 0.9 consumer and could only be implemented for the 0.10-specific consumer, correct?


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    @aljoscha Ah right, for the new timestamps we will definitely need a 0.10-specific consumer. I think it makes sense to include a new module then.
    
    This current PR does not add the new 0.10 timestamps ( @radekg correct me if I'm wrong here ), but I think we can add this as a separate follow up JIRA / PR afterwards, because it'll probably require changing some code in the `flink-connector-kafka-base` and the user-facing deserialization schemas that will need more discussion.
    
    I'll find time this week to give this PR a test.


Github user radekg commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    @tzulitai yes, this pr does not deal with 0.10 specific timestamps. It makes a simple consumer application work.


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Hi @radekg , 
    There was a recent change to the connector code to how the Kafka metrics are reported, so right now the PR has conflicts and can't be built. Would you like to rebase this PR on the current master branch so we can start testing it?


Github user radekg commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Merged with `upstream/master` and I'm getting this when running `mvn clean verify`:
    
    ```
    [INFO] -------------------------------------------------------------
    [ERROR] COMPILATION ERROR :
    [INFO] -------------------------------------------------------------
    [ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[30,69] cannot find symbol
      symbol:   class DefaultKafkaMetricAccumulator
      location: package org.apache.flink.streaming.connectors.kafka.internals.metrics
    [ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[105,17] constructor AbstractFetcher in class org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher<T,KPH> cannot be applied to given types;
      required: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext<T>,java.util.List<org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks<T>>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks<T>>,org.apache.flink.streaming.api.operators.StreamingRuntimeContext,boolean
      found: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext<T>,java.util.List<org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks<T>>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks<T>>,org.apache.flink.streaming.api.operators.StreamingRuntimeContext
      reason: actual and formal argument lists differ in length
    [ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[192,49] cannot find symbol
      symbol:   class DefaultKafkaMetricAccumulator
      location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher<T>
    [ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[193,65] cannot find symbol
      symbol:   variable DefaultKafkaMetricAccumulator
      location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher<T>
    [INFO] 4 errors
    [INFO] -------------------------------------------------------------
    [INFO] ------------------------------------------------------------------------
    [INFO] Reactor Summary:
    [INFO]
    [INFO] force-shading ...................................... SUCCESS [  1.210 s]
    [INFO] flink .............................................. SUCCESS [  4.416 s]
    [INFO] flink-annotations .................................. SUCCESS [  1.551 s]
    [INFO] flink-shaded-hadoop ................................ SUCCESS [  0.162 s]
    [INFO] flink-shaded-hadoop2 ............................... SUCCESS [  6.451 s]
    [INFO] flink-shaded-include-yarn-tests .................... SUCCESS [  7.929 s]
    [INFO] flink-shaded-curator ............................... SUCCESS [  0.110 s]
    [INFO] flink-shaded-curator-recipes ....................... SUCCESS [  0.986 s]
    [INFO] flink-shaded-curator-test .......................... SUCCESS [  0.200 s]
    [INFO] flink-test-utils-parent ............................ SUCCESS [  0.111 s]
    [INFO] flink-test-utils-junit ............................. SUCCESS [  2.417 s]
    [INFO] flink-core ......................................... SUCCESS [ 37.825 s]
    [INFO] flink-java ......................................... SUCCESS [ 23.620 s]
    [INFO] flink-runtime ...................................... SUCCESS [06:25 min]
    [INFO] flink-optimizer .................................... SUCCESS [ 12.698 s]
    [INFO] flink-clients ...................................... SUCCESS [  9.795 s]
    [INFO] flink-streaming-java ............................... SUCCESS [ 43.709 s]
    [INFO] flink-test-utils ................................... SUCCESS [  9.363 s]
    [INFO] flink-scala ........................................ SUCCESS [ 37.639 s]
    [INFO] flink-runtime-web .................................. SUCCESS [ 19.749 s]
    [INFO] flink-examples ..................................... SUCCESS [  1.006 s]
    [INFO] flink-examples-batch ............................... SUCCESS [ 14.276 s]
    [INFO] flink-contrib ...................................... SUCCESS [  0.104 s]
    [INFO] flink-statebackend-rocksdb ......................... SUCCESS [ 10.938 s]
    [INFO] flink-tests ........................................ SUCCESS [07:34 min]
    [INFO] flink-streaming-scala .............................. SUCCESS [ 33.365 s]
    [INFO] flink-streaming-connectors ......................... SUCCESS [  0.106 s]
    [INFO] flink-connector-flume .............................. SUCCESS [  5.626 s]
    [INFO] flink-libraries .................................... SUCCESS [  0.100 s]
    [INFO] flink-table ........................................ SUCCESS [02:31 min]
    [INFO] flink-connector-kafka-base ......................... SUCCESS [ 10.033 s]
    [INFO] flink-connector-kafka-0.8 .......................... SUCCESS [02:06 min]
    [INFO] flink-connector-kafka-0.9 .......................... SUCCESS [02:12 min]
    [INFO] flink-connector-kafka-0.10 ......................... FAILURE [  0.197 s]
    [INFO] flink-connector-elasticsearch ...................... SKIPPED
    [INFO] flink-connector-elasticsearch2 ..................... SKIPPED
    [INFO] flink-connector-rabbitmq ........................... SKIPPED
    [INFO] flink-connector-twitter ............................ SKIPPED
    [INFO] flink-connector-nifi ............................... SKIPPED
    [INFO] flink-connector-cassandra .......................... SKIPPED
    [INFO] flink-connector-redis .............................. SKIPPED
    [INFO] flink-connector-filesystem ......................... SKIPPED
    [INFO] flink-batch-connectors ............................. SKIPPED
    [INFO] flink-avro ......................................... SKIPPED
    [INFO] flink-jdbc ......................................... SKIPPED
    [INFO] flink-hadoop-compatibility ......................... SKIPPED
    [INFO] flink-hbase ........................................ SKIPPED
    [INFO] flink-hcatalog ..................................... SKIPPED
    [INFO] flink-examples-streaming ........................... SKIPPED
    [INFO] flink-gelly ........................................ SKIPPED
    [INFO] flink-gelly-scala .................................. SKIPPED
    [INFO] flink-gelly-examples ............................... SKIPPED
    [INFO] flink-python ....................................... SKIPPED
    [INFO] flink-ml ........................................... SKIPPED
    [INFO] flink-cep .......................................... SKIPPED
    [INFO] flink-cep-scala .................................... SKIPPED
    [INFO] flink-scala-shell .................................. SKIPPED
    [INFO] flink-quickstart ................................... SKIPPED
    [INFO] flink-quickstart-java .............................. SKIPPED
    [INFO] flink-quickstart-scala ............................. SKIPPED
    [INFO] flink-storm ........................................ SKIPPED
    [INFO] flink-storm-examples ............................... SKIPPED
    [INFO] flink-streaming-contrib ............................ SKIPPED
    [INFO] flink-tweet-inputformat ............................ SKIPPED
    [INFO] flink-operator-stats ............................... SKIPPED
    [INFO] flink-connector-wikiedits .......................... SKIPPED
    [INFO] flink-yarn ......................................... SKIPPED
    [INFO] flink-dist ......................................... SKIPPED
    [INFO] flink-metrics ...................................... SKIPPED
    [INFO] flink-metrics-dropwizard ........................... SKIPPED
    [INFO] flink-metrics-ganglia .............................. SKIPPED
    [INFO] flink-metrics-graphite ............................. SKIPPED
    [INFO] flink-metrics-statsd ............................... SKIPPED
    [INFO] flink-fs-tests ..................................... SKIPPED
    [INFO] flink-java8 ........................................ SKIPPED
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD FAILURE
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time: 25:46 min
    [INFO] Finished at: 2016-07-22T21:52:55+02:00
    [INFO] Final Memory: 159M/1763M
    [INFO] ------------------------------------------------------------------------
    [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project flink-connector-kafka-0.10_2.10: Compilation failure: Compilation failure:
    [ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[30,69] cannot find symbol
    [ERROR] symbol:   class DefaultKafkaMetricAccumulator
    [ERROR] location: package org.apache.flink.streaming.connectors.kafka.internals.metrics
    [ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[105,17] constructor AbstractFetcher in class org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher<T,KPH> cannot be applied to given types;
    [ERROR] required: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext<T>,java.util.List<org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks<T>>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks<T>>,org.apache.flink.streaming.api.operators.StreamingRuntimeContext,boolean
    [ERROR] found: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext<T>,java.util.List<org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks<T>>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks<T>>,org.apache.flink.streaming.api.operators.StreamingRuntimeContext
    [ERROR] reason: actual and formal argument lists differ in length
    [ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[192,49] cannot find symbol
    [ERROR] symbol:   class DefaultKafkaMetricAccumulator
    [ERROR] location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher<T>
    [ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[193,65] cannot find symbol
    [ERROR] symbol:   variable DefaultKafkaMetricAccumulator
    [ERROR] location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher<T>
    [ERROR] -> [Help 1]
    [ERROR]
    [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
    [ERROR] Re-run Maven using the -X switch to enable full debug logging.
    [ERROR]
    [ERROR] For more information about the errors and possible solutions, please read the following articles:
    [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
    [ERROR]
    [ERROR] After correcting the problems, you can resume the build with the command
    [ERROR]   mvn <goals> -rf :flink-connector-kafka-0.10_2.10
    ```
    
    Any advice?


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    The errors are due to some of the changes to `AbstractFetcher` in https://github.com/apache/flink/commit/41f58182289226850b23c61a32f01223485d4775. Some of the Kafka 0.9 connector code that has changed accordingly, so you'll probably need to reflect those changes in the Kafka 0.10 code too.


Github user radekg commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Thanks, running `verify` again.


Github user radekg commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Travis is going to run.


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    @radekg Thank you for the quick fix. I hope to find time over the weekend to test + review this, if not than early next week :)


Github user radekg commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Tests are failing for random setups on travis. Seems to be something scala related.


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Sorry for the delay on the review. Reviewing + testing now ...
    
    @radekg Yup the failure is unrelated to the changes here. Some of the tests are currently a bit flaky.


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2231#discussion_r73646737
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaTestEnvironmentImpl.java ---
    @@ -0,0 +1,331 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import kafka.admin.AdminUtils;
    +import kafka.common.KafkaException;
    +import kafka.network.SocketServer;
    +import kafka.server.KafkaConfig;
    +import kafka.server.KafkaServer;
    +import kafka.utils.SystemTime$;
    +import kafka.utils.ZkUtils;
    +import org.I0Itec.zkclient.ZkClient;
    +import org.apache.commons.io.FileUtils;
    +import org.apache.curator.test.TestingServer;
    +import org.apache.flink.streaming.connectors.kafka.testutils.ZooKeeperStringSerializer;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;
    +import org.apache.flink.util.NetUtils;
    +import org.apache.kafka.common.protocol.SecurityProtocol;
    +import org.apache.kafka.common.requests.MetadataResponse;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import java.io.File;
    +import java.net.BindException;
    +import java.util.ArrayList;
    +import java.util.List;
    +import java.util.Properties;
    +import java.util.UUID;
    +
    +import static org.apache.flink.util.NetUtils.hostAndPortToUrlString;
    +import static org.junit.Assert.assertTrue;
    +import static org.junit.Assert.fail;
    +
    +/**
    + * An implementation of the KafkaServerProvider for Kafka 0.10
    + */
    +public class KafkaTestEnvironmentImpl extends KafkaTestEnvironment {
    +
    +	protected static final Logger LOG = LoggerFactory.getLogger(KafkaTestEnvironmentImpl.class);
    +	private File tmpZkDir;
    +	private File tmpKafkaParent;
    +	private List<File> tmpKafkaDirs;
    +	private List<KafkaServer> brokers;
    +	private TestingServer zookeeper;
    +	private String zookeeperConnectionString;
    +	private String brokerConnectionString = "";
    +	private Properties standardProps;
    +	private Properties additionalServerProperties;
    +
    +	public String getBrokerConnectionString() {
    +		return brokerConnectionString;
    +	}
    +
    +	@Override
    +	public Properties getStandardProperties() {
    +		return standardProps;
    +	}
    +
    +	@Override
    +	public String getVersion() {
    +		return "0.10";
    +	}
    +
    +	@Override
    +	public List<KafkaServer> getBrokers() {
    +		return brokers;
    +	}
    +
    +	@Override
    +	public <T> FlinkKafkaConsumerBase<T> getConsumer(List<String> topics, KeyedDeserializationSchema<T> readSchema, Properties props) {
    +		return new FlinkKafkaConsumer010<>(topics, readSchema, props);
    +	}
    +
    +	@Override
    +	public <T> FlinkKafkaProducerBase<T> getProducer(String topic, KeyedSerializationSchema<T> serSchema, Properties props, KafkaPartitioner<T> partitioner) {
    +		FlinkKafkaProducer010<T> prod = new FlinkKafkaProducer010<>(topic, serSchema, props, partitioner);
    +		prod.setFlushOnCheckpoint(true);
    +		return prod;
    +	}
    +
    +	@Override
    +	public void restartBroker(int leaderId) throws Exception {
    +		brokers.set(leaderId, getKafkaServer(leaderId, tmpKafkaDirs.get(leaderId)));
    +	}
    +
    +	@Override
    +	public int getLeaderToShutDown(String topic) throws Exception {
    +		ZkUtils zkUtils = getZkUtils();
    +		try {
    +			MetadataResponse.PartitionMetadata firstPart = null;
    +			do {
    +				if (firstPart != null) {
    +					LOG.info("Unable to find leader. error code {}", firstPart.error().code());
    +					// not the first try. Sleep a bit
    +					Thread.sleep(150);
    +				}
    +
    +				List<MetadataResponse.PartitionMetadata> partitionMetadata = AdminUtils.fetchTopicMetadataFromZk(topic, zkUtils).partitionMetadata();
    +				firstPart = partitionMetadata.get(0);
    +			}
    +			while (firstPart.error().code() != 0);
    +
    +			return firstPart.leader().id();
    +		} finally {
    +			zkUtils.close();
    +		}
    +	}
    +
    +	@Override
    +	public int getBrokerId(KafkaServer server) {
    +		return server.config().brokerId();
    +	}
    +
    +	@Override
    +	public void prepare(int numKafkaServers, Properties additionalServerProperties) {
    +		this.additionalServerProperties = additionalServerProperties;
    +		File tempDir = new File(System.getProperty("java.io.tmpdir"));
    +
    +		tmpZkDir = new File(tempDir, "kafkaITcase-zk-dir-" + (UUID.randomUUID().toString()));
    +		assertTrue("cannot create zookeeper temp dir", tmpZkDir.mkdirs());
    +
    +		tmpKafkaParent = new File(tempDir, "kafkaITcase-kafka-dir*" + (UUID.randomUUID().toString()));
    +		assertTrue("cannot create kafka temp dir", tmpKafkaParent.mkdirs());
    +
    +		tmpKafkaDirs = new ArrayList<>(numKafkaServers);
    +		for (int i = 0; i < numKafkaServers; i++) {
    +			File tmpDir = new File(tmpKafkaParent, "server-" + i);
    +			assertTrue("cannot create kafka temp dir", tmpDir.mkdir());
    +			tmpKafkaDirs.add(tmpDir);
    +		}
    +
    +		zookeeper = null;
    +		brokers = null;
    +
    +		try {
    +			LOG.info("Starting Zookeeper");
    +			zookeeper = new TestingServer(-1, tmpZkDir);
    +			zookeeperConnectionString = zookeeper.getConnectString();
    +
    +			LOG.info("Starting KafkaServer");
    +			brokers = new ArrayList<>(numKafkaServers);
    +
    +			for (int i = 0; i < numKafkaServers; i++) {
    +				brokers.add(getKafkaServer(i, tmpKafkaDirs.get(i)));
    +
    +				SocketServer socketServer = brokers.get(i).socketServer();
    +				brokerConnectionString += hostAndPortToUrlString(KafkaTestEnvironment.KAFKA_HOST, brokers.get(i).socketServer().boundPort(SecurityProtocol.PLAINTEXT)) + ",";
    +			}
    +
    +			LOG.info("ZK and KafkaServer started.");
    +		}
    +		catch (Throwable t) {
    +			t.printStackTrace();
    +			fail("Test setup failed: " + t.getMessage());
    +		}
    +
    +		standardProps = new Properties();
    +		standardProps.setProperty("zookeeper.connect", zookeeperConnectionString);
    +		standardProps.setProperty("bootstrap.servers", brokerConnectionString);
    +		standardProps.setProperty("group.id", "flink-tests");
    +		standardProps.setProperty("auto.commit.enable", "false");
    +		standardProps.setProperty("zookeeper.session.timeout.ms", "30000"); // 6 seconds is default. Seems to be too small for travis.
    +		standardProps.setProperty("zookeeper.connection.timeout.ms", "30000");
    +		standardProps.setProperty("auto.offset.reset", "earliest"); // read from the beginning. (earliest is kafka 0.10 value)
    +		standardProps.setProperty("fetch.message.max.bytes", "256"); // make a lot of fetches (MESSAGES MUST BE SMALLER!)
    +	}
    +
    +	@Override
    +	public void shutdown() {
    +		for (KafkaServer broker : brokers) {
    +			if (broker != null) {
    +				broker.shutdown();
    +			}
    +		}
    +		brokers.clear();
    +
    +		if (zookeeper != null) {
    +			try {
    +				zookeeper.stop();
    +			}
    +			catch (Exception e) {
    +				LOG.warn("ZK.stop() failed", e);
    +			}
    +			zookeeper = null;
    +		}
    +
    +		// clean up the temp spaces
    +
    +		if (tmpKafkaParent != null && tmpKafkaParent.exists()) {
    +			try {
    +				FileUtils.deleteDirectory(tmpKafkaParent);
    +			}
    +			catch (Exception e) {
    +				// ignore
    +			}
    +		}
    +		if (tmpZkDir != null && tmpZkDir.exists()) {
    +			try {
    +				FileUtils.deleteDirectory(tmpZkDir);
    +			}
    +			catch (Exception e) {
    +				// ignore
    +			}
    +		}
    +	}
    +
    +	public ZkUtils getZkUtils() {
    +		ZkClient creator = new ZkClient(zookeeperConnectionString, Integer.valueOf(standardProps.getProperty("zookeeper.session.timeout.ms")),
    +				Integer.valueOf(standardProps.getProperty("zookeeper.connection.timeout.ms")), new ZooKeeperStringSerializer());
    +		return ZkUtils.apply(creator, false);
    +	}
    +
    +	@Override
    +	public void createTestTopic(String topic, int numberOfPartitions, int replicationFactor, Properties topicConfig) {
    +		// create topic with one client
    +		LOG.info("Creating topic {}", topic);
    +
    +		ZkUtils zkUtils = getZkUtils();
    +		try {
    +			AdminUtils.createTopic(zkUtils, topic, numberOfPartitions, replicationFactor, topicConfig, new kafka.admin.RackAwareMode.Enforced$());
    --- End diff --
    
    The proper usage of `RackAwareMode` here seems to be `kafka.admin.RackAwareMode.Enforced$.MODULE$` (this is how tests in Kafka use this). IntelliJ complains that `new kafka.admin.RackAwareMode.Enforced$()` has private access, I'm not sure why the build is passing on this though ...


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2231#discussion_r73648473
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java ---
    @@ -134,6 +134,10 @@
     	@Rule
     	public RetryRule retryRule = new RetryRule();
     
    +	public String getExpectedKafkaVersion() {
    +		return "0.9";
    +	}
    --- End diff --
    
    We should make this an abstract method in the abstract test base.


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2231#discussion_r73652372
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java ---
    @@ -186,7 +190,7 @@ public void runFailOnNoBrokerTest() throws Exception {
     			stream.print();
     			see.execute("No broker test");
     		} catch(RuntimeException re) {
    -			if(kafkaServer.getVersion().equals("0.9")) {
    +			if(kafkaServer.getVersion().equals(getExpectedKafkaVersion())) {
    --- End diff --
    
    I think the original intent of this assert here was that 0.8 connector throws different exception messages than 0.9.
    So adding the `getExpectedKafkaVersion()` is a bit confusing with respect to the test intent.
    Perhaps we should remove the new `getExpectedKafkaVersion()` and simply change this to `kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")`?


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2231#discussion_r73656446
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java ---
    @@ -0,0 +1,259 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;
    +import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;
    +import org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher;
    +import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;
    +import org.apache.flink.streaming.util.serialization.DeserializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper;
    +import org.apache.flink.util.SerializedValue;
    +
    +import org.apache.kafka.clients.consumer.ConsumerConfig;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;
    +import org.apache.kafka.common.PartitionInfo;
    +import org.apache.kafka.common.serialization.ByteArrayDeserializer;
    +
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.List;
    +import java.util.Properties;
    +
    +import static org.apache.flink.util.Preconditions.checkNotNull;
    +
    +/**
    + * The Flink Kafka Consumer is a streaming data source that pulls a parallel data stream from
    + * Apache Kafka 0.10.x. The consumer can run in multiple parallel instances, each of which will pull
    + * data from one or more Kafka partitions. 
    + * 
    + * <p>The Flink Kafka Consumer participates in checkpointing and guarantees that no data is lost
    + * during a failure, and that the computation processes elements "exactly once". 
    + * (Note: These guarantees naturally assume that Kafka itself does not loose any data.)</p>
    + *
    + * <p>Please note that Flink snapshots the offsets internally as part of its distributed checkpoints. The offsets
    + * committed to Kafka / ZooKeeper are only to bring the outside view of progress in sync with Flink's view
    + * of the progress. That way, monitoring and other jobs can get a view of how far the Flink Kafka consumer
    + * has consumed a topic.</p>
    + *
    + * <p>Please refer to Kafka's documentation for the available configuration properties:
    + * http://kafka.apache.org/documentation.html#newconsumerconfigs</p>
    + *
    + * <p><b>NOTE:</b> The implementation currently accesses partition metadata when the consumer
    + * is constructed. That means that the client that submits the program needs to be able to
    + * reach the Kafka brokers or ZooKeeper.</p>
    + */
    +public class FlinkKafkaConsumer010<T> extends FlinkKafkaConsumerBase<T> {
    +
    +	private static final long serialVersionUID = 2324564345203409112L;
    +
    +	private static final Logger LOG = LoggerFactory.getLogger(FlinkKafkaConsumer010.class);
    +
    +	/**  Configuration key to change the polling timeout **/
    +	public static final String KEY_POLL_TIMEOUT = "flink.poll-timeout";
    +
    +	/** Boolean configuration key to disable metrics tracking **/
    +	public static final String KEY_DISABLE_METRICS = "flink.disable-metrics";
    --- End diff --
    
    This is redundant. It's already declared in the `FlinkKafkaConsumerBase`.


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Finished first review of the code.
    
    Let me summarize parts of the Kafka 0.10 API that requires us to have a separate module:
    
     1. `ConsumerRecord` in 0.10 has a new `ConsumerRecord#timestamp()` method to retrieve Kafka server-side timestamps. If we want to attach this timestamp to the records as the default event time in the future, we'd definitely need a separate module (0.10 timestamp feature not included in this PR).
     2. `PartitionMetaData` (used in `KafkaTestEnvironmentImpl`s) has a breaking change to the APIs for retrieving the info, so we can't simply bump the version either.
    
    Other than the above, the rest of the code is the same between (or changes are irrelevant to Kafka API changes) the 0.9 connector.


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    @radekg One thing that's missing is update to the docs. You can find the Kafka connector documentation at `flink/docs/apis/streaming/connectors/kafka.md`. We'lll probably only need to update the Maven dependency table to include the 0.10 connector.


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2231#discussion_r73662520
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/pom.xml ---
    @@ -0,0 +1,179 @@
    +<?xml version="1.0" encoding="UTF-8"?>
    +<!--
    +Licensed to the Apache Software Foundation (ASF) under one
    +or more contributor license agreements.  See the NOTICE file
    +distributed with this work for additional information
    +regarding copyright ownership.  The ASF licenses this file
    +to you under the Apache License, Version 2.0 (the
    +"License"); you may not use this file except in compliance
    +with the License.  You may obtain a copy of the License at
    +
    +  http://www.apache.org/licenses/LICENSE-2.0
    +
    +Unless required by applicable law or agreed to in writing,
    +software distributed under the License is distributed on an
    +"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    +KIND, either express or implied.  See the License for the
    +specific language governing permissions and limitations
    +under the License.
    +-->
    +<project xmlns="http://maven.apache.org/POM/4.0.0"
    +		 xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    +		 xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
    +
    +	<modelVersion>4.0.0</modelVersion>
    +
    +	<parent>
    +		<groupId>org.apache.flink</groupId>
    +		<artifactId>flink-streaming-connectors</artifactId>
    +		<version>1.1-SNAPSHOT</version>
    --- End diff --
    
    We've recently just bumped version to 1.2-SNAPSHOT.


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Sorry for joining this discussion late. I've been on vacation.
    I also stumbled across the code duplicates. I'll check out the code from this pull request and see if there's a good way of re-using most of the 0.9 connector code.


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    @radekg, are you okay with me using your pull request as a base for adding Kafka 0.10 to Flink?
    I've started changing your code from the PR so that we don't need to copy so much code: https://github.com/rmetzger/flink/commits/kafka-0.10


Github user radekg commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    @rmetzger it's absolutely fine to reuse the code. If I can help in any way, please let me know.


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    Okay, cool. Thank you. I'll probably open a pull request with your and my changes. I'll let you know so that you can help reviewing it.



FWIW I generated a flink-connector-kafka-0.9_2.11-1.1.1.jar that uses kaka-clients 0.10.0.1 (it required hacking around some issues in one of the tests which I largely ignore).  I've tested it in a Flink 1.1.1 cluster against a 0.10.0.1 Kafka cluster without any issues.  Making use of the 0.10.0.1 clients dropped the CPU usage on the Kafka brokers from 100% to 2%, as previously the broker had to transcode the messages from the 0.10 format to the 0.9 format, whereas with the 0.10 client it can make use of zero copy from disk to the socket.

It is really too bad that the Kafka clients are not backwards compatible with older brokers.  If they were, that would obviate the need to support multiple Kafka client version concurrently in Flink and similar system.  We'd just have to keep up with the latest version of the client.

GitHub user rmetzger opened a pull request:

    https://github.com/apache/flink/pull/2369

    [FLINK-4035] Add a streaming connector for Apache Kafka 0.10.x

    This pull request subsumes https://github.com/apache/flink/pull/2231.
    
    Compared to #2231, I've based the connector on the existing 0.9 code (by extending it, reducing the amount of copied code), added a test case for the timestamp functionality and updated the documentation.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rmetzger/flink flink4035_rebased

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/2369.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2369
    
----
commit e3b2ede004b3c4ab6e37df1d6a268a52c1565316
Author: radekg <radek@gruchalski.com>
Date:   2016-07-12T17:19:01Z

    [FLINK-4035] Add support for Kafka 0.10.x.

commit 9d358a9ad5bcdd1ea644bc5b902240423586faf3
Author: Robert Metzger <rmetzger@apache.org>
Date:   2016-08-09T14:38:21Z

    [FLINK-4035] Refactor the Kafka 0.10 connector to be based upon the 0.9 connector
    
    Add a test case for Kafka's new timestamp functionality and update the documentation.

----


Thank you for trying this out.
I've opened a pull request yesterday that extends the kafka 0.9 connector, but uses the 0.10.0.0 Kafka dependency (I'll update to Kafka 0.10.0.1 before merging).

The missing backwards compatibility in Kafka is indeed an issue for systems like ours.

Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    @radekg I've opened a PR based on your code: https://github.com/apache/flink/issues/2369 feel free to review it.


Github user StephanEwen commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    Just looked over this briefly.
    What struck me first is that this again uses the dirty trick of adding a dependency to "Flink Kafka 0.9" and then transitively excluding "Kafka 0.9". Is there a nicer way to solve this?


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r75569347
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcher.java ---
    @@ -207,15 +207,14 @@ public void restoreOffsets(HashMap<KafkaTopicPartition, Long> snapshotState) {
     	// ------------------------------------------------------------------------
     
     	/**
    -	 * 
     	 * <p>Implementation Note: This method is kept brief to be JIT inlining friendly.
     	 * That makes the fast path efficient, the extended paths are called as separate methods.
    -	 * 
     	 * @param record The record to emit
     	 * @param partitionState The state of the Kafka partition from which the record was fetched
    -	 * @param offset The offset from which the record was fetched
    +	 * @param offset The offset of the record
    +	 * @param kafkaRecord The original Kafka record
     	 */
    -	protected final void emitRecord(T record, KafkaTopicPartitionState<KPH> partitionState, long offset) {
    +	protected <R> void emitRecord(T record, KafkaTopicPartitionState<KPH> partitionState, long offset, R kafkaRecord) throws Exception {
    --- End diff --
    
    Is there a reason we need to have an extra `kafkaRecord`? It doesn't seem to be used in the function.


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r75569873
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java ---
    @@ -0,0 +1,91 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka.internal;
    +
    +import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;
    +import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;
    +import org.apache.flink.util.SerializedValue;
    +
    +import org.apache.kafka.clients.consumer.ConsumerRecord;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;
    +import org.apache.kafka.common.TopicPartition;
    +
    +import java.util.List;
    +import java.util.Properties;
    +
    +/**
    + * A fetcher that fetches data from Kafka brokers via the Kafka 0.10 consumer API.
    + * 
    + * @param <T> The type of elements produced by the fetcher.
    + */
    +public class Kafka010Fetcher<T> extends Kafka09Fetcher<T> {
    +
    +	public Kafka010Fetcher(
    +			SourceContext<T> sourceContext,
    +			List<KafkaTopicPartition> assignedPartitions,
    +			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,
    +			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,
    +			StreamingRuntimeContext runtimeContext,
    +			KeyedDeserializationSchema<T> deserializer,
    +			Properties kafkaProperties,
    +			long pollTimeout,
    +			boolean useMetrics) throws Exception
    +	{
    +		super(sourceContext, assignedPartitions, watermarksPeriodic, watermarksPunctuated, runtimeContext, deserializer, kafkaProperties, pollTimeout, useMetrics);
    +	}
    +
    +	@Override
    +	protected void assignPartitionsToConsumer(KafkaConsumer<byte[], byte[]> consumer, List<TopicPartition> topicPartitions) {
    +		consumer.assign(topicPartitions);
    +	}
    +
    +	/**
    +	 * Emit record Kafka-timestamp aware.
    +	 */
    +	@Override
    +	protected <R> void emitRecord(T record, KafkaTopicPartitionState<TopicPartition> partitionState, long offset, R kafkaRecord) throws Exception {
    --- End diff --
    
    Just realized here the reason for the new `kafkaRecord`.
    
    However, would it be better to add an `recordTimestamp` argument instead of passing the original Kafka record?
    If we don't have such a timestamp (for 0.8, 0.9), a `null` can be accepted. In `AbstractFetcher#emitRecord()`, we check if the value is `null` or not and correspondingly call `collect()` or `collectWithTimestamp`. The same goes for passing the `Long.MIN_VALUE` / timestamp to `emitRecordWithTimestampAndPeriodicWatermark` and `emitRecordWithTimestampAndPunctuatedWatermark `.
    
    IMHO, I think this way the new code will be more meaningful and less confusing for the base `AbstractFetcher`, and also we won't need to override `emitRecord` in 0.10.


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r75571320
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -0,0 +1,198 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;
    +import org.apache.flink.streaming.api.datastream.DataStream;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
    +import org.apache.flink.streaming.api.operators.StreamSink;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;
    +import org.apache.kafka.clients.producer.ProducerRecord;
    +
    +import java.util.Properties;
    +
    +
    +/**
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x
    + *
    + * Implementation note: This Producer wraps a Flink Kafka 0.9 Producer, overriding only
    + * the "processElement" / "invoke" method.
    + */
    +public class FlinkKafkaProducer010<T> extends StreamSink<T> {
    --- End diff --
    
    Overall, I think the solution here seems a bit too "hacky" to me. Here's a few disadvantages I see: 1) the code in `processElement()` of this class is still quite a bit duplicate of the `invoke()` in `FlinkKafaProducerBase`. 2) with `writeToKafka()` we need to have a different usage style for the Kafka producer in 0.10, compared to previous versions.
    
    If I'm correct, I guess its due to the fact that we don't have access to the embedded record timestamp in the provided `next` in the usual `invoke`?
    
    I'm wondering whether or not a cleaner solution is to introduce a `serializeTimestamp(T element)` method in the `KeyedSerializationSchema`, and we can simply use that to make `FlinkKafkaProducerBase#invoke()` more general by replacing instantiation of `ProducerRecord`s with an abstract version-specific method. Then, `FlinkKafkaProducer010` can simply extend `FlinkKafkaProducer09`.
    
    We'll need a migration plan though if we're going to change the serialization schema interface. If we're going for it, might as well migrate the `KeyedDeserializationSchema` to include the timestamp too (give `null` for 0.8, 0.9).
    Otherwise, I think the solution here is ok for a short term solution. What do you think?


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r75571478
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -0,0 +1,198 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;
    +import org.apache.flink.streaming.api.datastream.DataStream;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
    +import org.apache.flink.streaming.api.operators.StreamSink;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;
    +import org.apache.kafka.clients.producer.ProducerRecord;
    +
    +import java.util.Properties;
    +
    +
    +/**
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x
    + *
    + * Implementation note: This Producer wraps a Flink Kafka 0.9 Producer, overriding only
    + * the "processElement" / "invoke" method.
    + */
    +public class FlinkKafkaProducer010<T> extends StreamSink<T> {
    --- End diff --
    
    But on the other hand, the downside for the approach above would be that users always have to explicitly extract the timestamp again (even if the record already has an embedded timestamp) to emit them to Kafka.


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r75572212
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -0,0 +1,198 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;
    +import org.apache.flink.streaming.api.datastream.DataStream;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
    +import org.apache.flink.streaming.api.operators.StreamSink;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;
    +import org.apache.kafka.clients.producer.ProducerRecord;
    +
    +import java.util.Properties;
    +
    +
    +/**
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x
    + *
    + * Implementation note: This Producer wraps a Flink Kafka 0.9 Producer, overriding only
    + * the "processElement" / "invoke" method.
    + */
    +public class FlinkKafkaProducer010<T> extends StreamSink<T> {
    +
    +	/**
    +	 * Flag controlling whether we are writing the Flink record's timestamp into Kafka.
    +	 */
    +	private boolean writeTimestampToKafka = false;
    +
    +	// ---------------------- "Constructors" for the producer ------------------ //
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    +										String topicId,
    +										KeyedSerializationSchema<T> serializationSchema,
    +										Properties producerConfig) {
    +		return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner<T>());
    +	}
    +
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined (keyless) serialization schema.
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    +										String topicId,
    +										SerializationSchema<T> serializationSchema,
    +										Properties producerConfig) {
    +		return writeToKafka(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FixedPartitioner<T>());
    +	}
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to
    +	 * the topic.
    +	 *  @param inStream The stream to write to Kafka
    +	 *  @param topicId The name of the target topic
    +	 *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages
    +	 *  @param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument.
    +	 *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafka(DataStream<T> inStream,
    +																String topicId,
    +																KeyedSerializationSchema<T> serializationSchema,
    +																Properties producerConfig,
    +																KafkaPartitioner<T> customPartitioner) {
    +		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class);
    +		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner);
    +		SingleOutputStreamOperator<Object> transformation = inStream.transform("FlinKafkaProducer 0.10.x", objectTypeInfo, kafkaProducer);
    +		return new FlinkKafkaProducer010Configuration<>(transformation, kafkaProducer);
    +	}
    +
    +	/**
    +	 * Configuration object returned by the writeToKafka() call.
    +	 */
    +	public static class FlinkKafkaProducer010Configuration<T> extends DataStreamSink<T> {
    --- End diff --
    
    I wonder if we can move the config setter methods into `FlinkKafkaProducer010`, and let `writeToKafka` return the created 0.10 producer instead of instantiating a new `FlinkKafkaProducer010Configuration`? My reasoning is that I find it a bit strange to set these config values on a separate config object, but not "applying" it anywhere. I think users would normally expect they have to "apply" a completed config object somewhere to take effect. So, I have something like this in mind:
    
    In constructor of `FlinkKafkaProducer010`
    ```
    private FlinkKafaProducer010(...) {
      super(new FlinkKafkaProducer09<>(...));
      this.wrapped09Producer = (FlinkKafkaProducer09<T>) userFunction; // wrapped09Producer as FlinkKafkafProducer010's class field
    }
    ```
    Then FlinkKafkaProducer010 can directly have the setter methods:
    ```
    public void setFlushOnCheckpoint(boolean flush) {
        this.wrapped09producer.setFlushOnCheckpoint(flush);
    }
    
    public void setLogFailuresOnly(boolean logFailuresOnly) {
    	this.wrapped09producer.setLogFailuresOnly(logFailuresOnly);
    }
    
    public void setWriteTimestampToKafka(boolean writeTimestampToKafka) {
    	this.writeTimestampToKafka = writeTimestampToKafka;
    }
    ```
    
    Then users can use the producer like this:
    ```
    FlinkKafkaProducer010 producer = FlinkKafkaProducer010.writeToKafka(...)
    producer.setLogFailuresOnly(...)
    producer.setFlushOnCheckpoint(...)
    producer.setWriteTimestampToKafka(...)
    ```
    
    Although still a bit different from the usage patterns of previous versions, it looks a bit more similar compared to what the PR has now. I'm not sure if I may be missing something that lead to choosing to have a separate `FlinkKafkaProducer010Configuration`. Is there any particular reason?


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r75572263
  
    --- Diff: docs/apis/streaming/connectors/kafka.md ---
    @@ -291,3 +301,35 @@ higher value.
     There is currently no transactional producer for Kafka, so Flink can not guarantee exactly-once delivery
     into a Kafka topic.
     
    +### Using Kafka timestamps and Flink event time in Kafka 0.10
    +
    +Since Apache Kafka 0.10., Kafka's messages can carry [timestamps](https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message), indicating
    +the time the event has occurred (see ["event time" in Apache Flink](../event_time.html)) or the time when the message
    +has been written to the Kafka broker.
    +
    +The `FlinkKafkaConsumer010` will emit records with the timestamp attached, if the time characteristic in Flink is 
    +set to `TimeCharacteristic.EventTime` (`StreamExecutionEnvironment.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)`).
    +
    +The Kafka consumer does not emit watermarks. To emit watermarks, the same mechanisms as described above in 
    +"Kafka Consumers and Timestamp Extraction/Watermark Emission"  using the `assignTimestampsAndWatermarks` method are applicable.
    +
    +There is no need to define a timestamp extractor when using the timestamps from Kafka. The `previousElementTimestamp` argument of 
    +the `extractTimestamp()` method contains the timestamp carried by the Kafka message.
    +
    +A timestamp extractor for a Kafka consumer would look like this:
    +{% highlight java %}
    +public long extractTimestamp(Long element, long previousElementTimestamp) {
    +    return previousElementTimestamp;
    +}
    +{% endhighlight %}
    +
    +
    +
    +The `FlinkKafkaProducer010` only emits the record timestamp, if `setWriteTimestampToKafka(true)` is set.
    +
    +{% highlight java %}
    +FlinkKafkaProducer010.FlinkKafkaProducer010Configuration config = FlinkKafkaProducer010.writeToKafka(streamWithTimestamps, topic, new SimpleStringSchema(), standardProps);
    +config.setWriteTimestampToKafka(true);
    +{% endhighlight %}
    --- End diff --
    
    I find the usage pattern of this a bit unfamiliar. I've explained this in inline comments of the `FlinkKafkaProducer010` class.


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    Left a few comments on some high-level design choices for a first review. Mostly on `FlinkKafkaProducer010`, I wonder if there are other better possibilities over there?


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    @StephanEwen: The explicit exclude is actually not needed, because the kafka version defined in the connector has precedence over transitive kafka versions.
    ```
    [INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ flink-connector-kafka-0.10_2.10 ---
    [INFO] org.apache.flink:flink-connector-kafka-0.10_2.10:jar:1.2-SNAPSHOT
    [INFO] +- org.apache.flink:flink-connector-kafka-0.9_2.10:jar:1.2-SNAPSHOT:compile
    [INFO] |  \- org.apache.flink:flink-connector-kafka-base_2.10:jar:1.2-SNAPSHOT:compile
    [INFO] +- org.apache.kafka:kafka-clients:jar:0.10.0.1:compile
    [INFO] |  +- net.jpountz.lz4:lz4:jar:1.3.0:compile
    [INFO] |  \- org.xerial.snappy:snappy-java:jar:1.1.2.6:compile
    ```



Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r75902757
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -0,0 +1,198 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;
    +import org.apache.flink.streaming.api.datastream.DataStream;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
    +import org.apache.flink.streaming.api.operators.StreamSink;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;
    +import org.apache.kafka.clients.producer.ProducerRecord;
    +
    +import java.util.Properties;
    +
    +
    +/**
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x
    + *
    + * Implementation note: This Producer wraps a Flink Kafka 0.9 Producer, overriding only
    + * the "processElement" / "invoke" method.
    + */
    +public class FlinkKafkaProducer010<T> extends StreamSink<T> {
    +
    +	/**
    +	 * Flag controlling whether we are writing the Flink record's timestamp into Kafka.
    +	 */
    +	private boolean writeTimestampToKafka = false;
    +
    +	// ---------------------- "Constructors" for the producer ------------------ //
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    +										String topicId,
    +										KeyedSerializationSchema<T> serializationSchema,
    +										Properties producerConfig) {
    +		return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner<T>());
    +	}
    +
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined (keyless) serialization schema.
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    +										String topicId,
    +										SerializationSchema<T> serializationSchema,
    +										Properties producerConfig) {
    +		return writeToKafka(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FixedPartitioner<T>());
    +	}
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to
    +	 * the topic.
    +	 *  @param inStream The stream to write to Kafka
    +	 *  @param topicId The name of the target topic
    +	 *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages
    +	 *  @param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument.
    +	 *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafka(DataStream<T> inStream,
    +																String topicId,
    +																KeyedSerializationSchema<T> serializationSchema,
    +																Properties producerConfig,
    +																KafkaPartitioner<T> customPartitioner) {
    +		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class);
    +		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner);
    +		SingleOutputStreamOperator<Object> transformation = inStream.transform("FlinKafkaProducer 0.10.x", objectTypeInfo, kafkaProducer);
    +		return new FlinkKafkaProducer010Configuration<>(transformation, kafkaProducer);
    +	}
    +
    +	/**
    +	 * Configuration object returned by the writeToKafka() call.
    +	 */
    +	public static class FlinkKafkaProducer010Configuration<T> extends DataStreamSink<T> {
    --- End diff --
    
    I'll update it.


Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r75902922
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -0,0 +1,198 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;
    +import org.apache.flink.streaming.api.datastream.DataStream;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
    +import org.apache.flink.streaming.api.operators.StreamSink;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;
    +import org.apache.kafka.clients.producer.ProducerRecord;
    +
    +import java.util.Properties;
    +
    +
    +/**
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x
    + *
    + * Implementation note: This Producer wraps a Flink Kafka 0.9 Producer, overriding only
    + * the "processElement" / "invoke" method.
    + */
    +public class FlinkKafkaProducer010<T> extends StreamSink<T> {
    --- End diff --
    
    Exactly. The problem is that there is no way of accessing the system's event time record.
    
    Maybe I can try and build something like a hybrid producer that works with both invocation methods.


Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r75903103
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java ---
    @@ -0,0 +1,91 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka.internal;
    +
    +import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;
    +import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;
    +import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;
    +import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;
    +import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;
    +import org.apache.flink.util.SerializedValue;
    +
    +import org.apache.kafka.clients.consumer.ConsumerRecord;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;
    +import org.apache.kafka.common.TopicPartition;
    +
    +import java.util.List;
    +import java.util.Properties;
    +
    +/**
    + * A fetcher that fetches data from Kafka brokers via the Kafka 0.10 consumer API.
    + * 
    + * @param <T> The type of elements produced by the fetcher.
    + */
    +public class Kafka010Fetcher<T> extends Kafka09Fetcher<T> {
    +
    +	public Kafka010Fetcher(
    +			SourceContext<T> sourceContext,
    +			List<KafkaTopicPartition> assignedPartitions,
    +			SerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,
    +			SerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,
    +			StreamingRuntimeContext runtimeContext,
    +			KeyedDeserializationSchema<T> deserializer,
    +			Properties kafkaProperties,
    +			long pollTimeout,
    +			boolean useMetrics) throws Exception
    +	{
    +		super(sourceContext, assignedPartitions, watermarksPeriodic, watermarksPunctuated, runtimeContext, deserializer, kafkaProperties, pollTimeout, useMetrics);
    +	}
    +
    +	@Override
    +	protected void assignPartitionsToConsumer(KafkaConsumer<byte[], byte[]> consumer, List<TopicPartition> topicPartitions) {
    +		consumer.assign(topicPartitions);
    +	}
    +
    +	/**
    +	 * Emit record Kafka-timestamp aware.
    +	 */
    +	@Override
    +	protected <R> void emitRecord(T record, KafkaTopicPartitionState<TopicPartition> partitionState, long offset, R kafkaRecord) throws Exception {
    --- End diff --
    
    I agree that this is confusing at the `AbstractFetcher` level. I'll look into that as well.


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    Thank you for the review @tzulitai. I'll try to find some time soon to look into your comments in detail.


Github user eliaslevy commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    This may be the wrong place to bring this up, but as you are discussing changes to the Kafka connector API, I think it is worth bring it up.  
    
    As I've pointed out elsewhere, the current connector API makes it difficult to make use of Kafka native serializer or deserializer (`org.apache.kafka.common.[Serializer, Deserializer]`), which can be configured via the Kafka client and producer configs.  
    
    The connector code assumes that `ConsummerRecord`s and `ProducerRecord`s are both parametrized as `<byte[], byte[]>`, with the Flink serdes performing the conversion to/from `byte[]`.  This makes it difficult to make use of Confluent's `KafkaAvroSerializer` and `KafkaAvroDecoder`, which make use of their [schema registry](http://docs.confluent.io/3.0.0/schema-registry/docs/serializer-formatter.html#serializer).
    
    If you are going to change the connector API, it would be good to tackle this issue at the same time to avoid future changes.  The connector should allow the type parametrization of the Kafka consumer and producer, and should make use of a pass through Flink serde by default.


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    @eliaslevy, I assume you are referring to https://issues.apache.org/jira/browse/FLINK-4050. 
    Its good that you are mentioning the issue again, so I can move it a bit up on my TODO list.
    
    I would personally prefer to first add the Kafka 0.10 module in this pull request and then resolve FLINK-4050 independently. I know that this might lead to a little bit of duplicate work on the Kafka 0.10 code, but on the other hand its easier to discuss one issue at a time :)


Github user eliaslevy commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    @rmetzger that's the one. NP.  I realize breaking it up makes things easier.  I just thought I'd mention it.


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    @tzulitai I've addressed your comments.
    The Producer is now "hybrid": you can use it with both invocation methods.
    The AbstractFetcher now accepts a long timestamp instead of a record.


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r76264614
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -0,0 +1,399 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import org.apache.flink.api.common.functions.IterationRuntimeContext;
    +import org.apache.flink.api.common.functions.RichFunction;
    +import org.apache.flink.api.common.functions.RuntimeContext;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;
    +import org.apache.flink.configuration.Configuration;
    +import org.apache.flink.streaming.api.datastream.DataStream;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
    +import org.apache.flink.streaming.api.functions.sink.SinkFunction;
    +import org.apache.flink.streaming.api.operators.StreamSink;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;
    +import org.apache.kafka.clients.producer.ProducerRecord;
    +
    +import java.util.Properties;
    +
    +import static org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.getPropertiesFromBrokerList;
    +
    +
    +/**
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x
    + *
    + * Implementation note: This producer is a hybrid between a regular regular sink function (a)
    + * and a custom operator (b).
    + *
    + * For (a), the class implements the SinkFunction and RichFunction interfaces.
    + * For (b), it extends the StreamTask class.
    + *
    + * Details about approach (a):
    + *
    + *  Pre Kafka 0.10 producers only follow approach (a), allowing users to use the producer using the
    + *  DataStream.addSink() method.
    + *  Since the APIs exposed in that variant do not allow accessing the the timestamp attached to the record
    + *  the Kafka 0.10 producer has a section invocation option, approach (b).
    --- End diff --
    
    'section' --> second?


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r76268268
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -0,0 +1,399 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import org.apache.flink.api.common.functions.IterationRuntimeContext;
    +import org.apache.flink.api.common.functions.RichFunction;
    +import org.apache.flink.api.common.functions.RuntimeContext;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;
    +import org.apache.flink.configuration.Configuration;
    +import org.apache.flink.streaming.api.datastream.DataStream;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
    +import org.apache.flink.streaming.api.functions.sink.SinkFunction;
    +import org.apache.flink.streaming.api.operators.StreamSink;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;
    +import org.apache.kafka.clients.producer.ProducerRecord;
    +
    +import java.util.Properties;
    +
    +import static org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.getPropertiesFromBrokerList;
    +
    +
    +/**
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x
    + *
    + * Implementation note: This producer is a hybrid between a regular regular sink function (a)
    + * and a custom operator (b).
    + *
    + * For (a), the class implements the SinkFunction and RichFunction interfaces.
    + * For (b), it extends the StreamTask class.
    + *
    + * Details about approach (a):
    + *
    + *  Pre Kafka 0.10 producers only follow approach (a), allowing users to use the producer using the
    + *  DataStream.addSink() method.
    + *  Since the APIs exposed in that variant do not allow accessing the the timestamp attached to the record
    + *  the Kafka 0.10 producer has a section invocation option, approach (b).
    + *
    + * Details about approach (b):
    + *  Kafka 0.10 supports writing the timestamp attached to a record to Kafka. When adding the
    + *  FlinkKafkaProducer010 using the FlinkKafkaProducer010.writeToKafka() method, the Kafka producer
    + *  can access the internal record timestamp of the record and write it to Kafka.
    + *
    + * All methods and constructors in this class are marked with the approach they are needed for.
    + */
    +public class FlinkKafkaProducer010<T> extends StreamSink<T> implements SinkFunction<T>, RichFunction {
    +
    +	/**
    +	 * Flag controlling whether we are writing the Flink record's timestamp into Kafka.
    +	 */
    +	private boolean writeTimestampToKafka = false;
    +
    +	// ---------------------- "Constructors" for timestamp writing ------------------
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    +										String topicId,
    +										KeyedSerializationSchema<T> serializationSchema,
    +										Properties producerConfig) {
    +		return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner<T>());
    +	}
    +
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined (keyless) serialization schema.
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    --- End diff --
    
    Now with hybrid invocation methods, would it be reasonable to name this as `writeToKafkaWithTimestamps` so that it's more meaningful and differentiable from the usual invocation?


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r76268687
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -0,0 +1,399 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import org.apache.flink.api.common.functions.IterationRuntimeContext;
    +import org.apache.flink.api.common.functions.RichFunction;
    +import org.apache.flink.api.common.functions.RuntimeContext;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;
    +import org.apache.flink.configuration.Configuration;
    +import org.apache.flink.streaming.api.datastream.DataStream;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
    +import org.apache.flink.streaming.api.functions.sink.SinkFunction;
    +import org.apache.flink.streaming.api.operators.StreamSink;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;
    +import org.apache.kafka.clients.producer.ProducerRecord;
    +
    +import java.util.Properties;
    +
    +import static org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.getPropertiesFromBrokerList;
    +
    +
    +/**
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x
    + *
    + * Implementation note: This producer is a hybrid between a regular regular sink function (a)
    + * and a custom operator (b).
    + *
    + * For (a), the class implements the SinkFunction and RichFunction interfaces.
    + * For (b), it extends the StreamTask class.
    + *
    + * Details about approach (a):
    + *
    + *  Pre Kafka 0.10 producers only follow approach (a), allowing users to use the producer using the
    + *  DataStream.addSink() method.
    + *  Since the APIs exposed in that variant do not allow accessing the the timestamp attached to the record
    + *  the Kafka 0.10 producer has a section invocation option, approach (b).
    + *
    + * Details about approach (b):
    + *  Kafka 0.10 supports writing the timestamp attached to a record to Kafka. When adding the
    + *  FlinkKafkaProducer010 using the FlinkKafkaProducer010.writeToKafka() method, the Kafka producer
    + *  can access the internal record timestamp of the record and write it to Kafka.
    + *
    + * All methods and constructors in this class are marked with the approach they are needed for.
    + */
    +public class FlinkKafkaProducer010<T> extends StreamSink<T> implements SinkFunction<T>, RichFunction {
    +
    +	/**
    +	 * Flag controlling whether we are writing the Flink record's timestamp into Kafka.
    +	 */
    +	private boolean writeTimestampToKafka = false;
    +
    +	// ---------------------- "Constructors" for timestamp writing ------------------
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    +										String topicId,
    +										KeyedSerializationSchema<T> serializationSchema,
    +										Properties producerConfig) {
    +		return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner<T>());
    +	}
    +
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined (keyless) serialization schema.
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    +										String topicId,
    +										SerializationSchema<T> serializationSchema,
    +										Properties producerConfig) {
    +		return writeToKafka(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FixedPartitioner<T>());
    +	}
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)
    +	 *
    +	 *  @param inStream The stream to write to Kafka
    +	 *  @param topicId The name of the target topic
    +	 *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages
    +	 *  @param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument.
    +	 *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafka(DataStream<T> inStream,
    +																String topicId,
    +																KeyedSerializationSchema<T> serializationSchema,
    +																Properties producerConfig,
    +																KafkaPartitioner<T> customPartitioner) {
    +
    +		GenericTypeInfo<Object> objectTypeInfo = new GenericTypeInfo<>(Object.class);
    +		FlinkKafkaProducer010<T> kafkaProducer = new FlinkKafkaProducer010<>(topicId, serializationSchema, producerConfig, customPartitioner);
    +		SingleOutputStreamOperator<Object> transformation = inStream.transform("FlinKafkaProducer 0.10.x", objectTypeInfo, kafkaProducer);
    +		return new FlinkKafkaProducer010Configuration<>(transformation, kafkaProducer);
    +	}
    +
    +	// ---------------------- Regular constructors w/o timestamp support  ------------------
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * @param brokerList
    +	 *			Comma separated addresses of the brokers
    +	 * @param topicId
    +	 * 			ID of the Kafka topic.
    +	 * @param serializationSchema
    +	 * 			User defined (keyless) serialization schema.
    +	 */
    +	public FlinkKafkaProducer010(String brokerList, String topicId, SerializationSchema<T> serializationSchema) {
    +		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), getPropertiesFromBrokerList(brokerList), new FixedPartitioner<T>());
    +	}
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * @param topicId
    +	 * 			ID of the Kafka topic.
    +	 * @param serializationSchema
    +	 * 			User defined (keyless) serialization schema.
    +	 * @param producerConfig
    +	 * 			Properties with the producer configuration.
    +	 */
    +	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig) {
    +		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FixedPartitioner<T>());
    +	}
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * @param topicId The topic to write data to
    +	 * @param serializationSchema A (keyless) serializable serialization schema for turning user objects into a kafka-consumable byte[]
    +	 * @param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument.
    +	 * @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions (when passing null, we'll use Kafka's partitioner)
    +	 */
    +	public FlinkKafkaProducer010(String topicId, SerializationSchema<T> serializationSchema, Properties producerConfig, KafkaPartitioner<T> customPartitioner) {
    +		this(topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, customPartitioner);
    +
    --- End diff --
    
    nit: unnecessary empty line


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r76274826
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java ---
    @@ -60,12 +60,17 @@ protected void assignPartitionsToConsumer(KafkaConsumer<byte[], byte[]> consumer
     		consumer.assign(topicPartitions);
     	}
     
    +	@Override
    +	protected void emitRecord(T record, KafkaTopicPartitionState<TopicPartition> partition, long offset, ConsumerRecord consumerRecord) throws Exception {
    +		// pass timestamp
    +		super.emitRecord(record, partition, offset, consumerRecord.timestamp());
    +	}
    +
     	/**
     	 * Emit record Kafka-timestamp aware.
     	 */
     	@Override
    -	protected <R> void emitRecord(T record, KafkaTopicPartitionState<TopicPartition> partitionState, long offset, R kafkaRecord) throws Exception {
    -		long timestamp = ((ConsumerRecord) kafkaRecord).timestamp();
    +	protected void emitRecord(T record, KafkaTopicPartitionState<TopicPartition> partitionState, long offset, long timestamp) throws Exception {
     		if (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) {
    --- End diff --
    
    Is it possible to let `AbstractFetcher#emitRecord` determine whether to call `collectWithTimestamp` or `collect` depending on the provided timestamp (== `Long.MIN_VALUE`)? Then, we won't need to override the base `emitRecord` here, correct?


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r76275805
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java ---
    @@ -376,7 +376,7 @@ else if (partitionsRemoved) {
     								continue partitionsLoop;
     							}
     							
    -							owner.emitRecord(value, currentPartition, offset, msg);
    +							owner.emitRecord(value, currentPartition, offset, Long.MAX_VALUE);
    --- End diff --
    
    I think this is supposed to give a `Long.MIN_VALUE` instead of MAX? 


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r76275879
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java ---
    @@ -262,6 +262,10 @@ public void run() {
     		}
     	}
     
    +	// Kafka09Fetcher ignores the timestamp.
    +	protected void emitRecord(T record, KafkaTopicPartitionState<TopicPartition> partition, long offset, ConsumerRecord consumerRecord) throws Exception {
    +		emitRecord(record, partition, offset, Long.MAX_VALUE);
    --- End diff --
    
    Same here: I think this is supposed to give a `Long.MIN_VALUE` instead of MAX?


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r76277062
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTimestampsTest.java ---
    @@ -135,45 +135,45 @@ public void testPeriodicWatermarks() throws Exception {
     		// elements generate a watermark if the timestamp is a multiple of three
     
     		// elements for partition 1
    -		fetcher.emitRecord(1L, part1, 1L, new ConsumerRecord<byte[], byte[]>(testTopic, 7, new byte[]{0}, 1L));
    -		fetcher.emitRecord(2L, part1, 2L, new ConsumerRecord<byte[], byte[]>(testTopic, 7, new byte[]{0}, 2L));
    -		fetcher.emitRecord(3L, part1, 3L, new ConsumerRecord<byte[], byte[]>(testTopic, 7, new byte[]{0}, 3L));
    +		fetcher.emitRecord(1L, part1, 1L, Long.MAX_VALUE);
    +		fetcher.emitRecord(2L, part1, 2L, Long.MAX_VALUE);
    +		fetcher.emitRecord(3L, part1, 3L, Long.MAX_VALUE);
     		assertEquals(3L, sourceContext.getLatestElement().getValue().longValue());
     		assertEquals(3L, sourceContext.getLatestElement().getTimestamp());
     
     		// elements for partition 2
    -		fetcher.emitRecord(12L, part2, 1L, new ConsumerRecord<byte[], byte[]>(testTopic, 13, new byte[]{0}, 1L));
    +		fetcher.emitRecord(12L, part2, 1L, Long.MAX_VALUE);
     		assertEquals(12L, sourceContext.getLatestElement().getValue().longValue());
     		assertEquals(12L, sourceContext.getLatestElement().getTimestamp());
     
     		// elements for partition 3
    -		fetcher.emitRecord(101L, part3, 1L, new ConsumerRecord<byte[], byte[]>(testTopic, 21, new byte[]{0}, 1L));
    -		fetcher.emitRecord(102L, part3, 2L, new ConsumerRecord<byte[], byte[]>(testTopic, 21, new byte[]{0}, 2L));
    +		fetcher.emitRecord(101L, part3, 1L, Long.MAX_VALUE);
    +		fetcher.emitRecord(102L, part3, 2L, Long.MAX_VALUE);
     		assertEquals(102L, sourceContext.getLatestElement().getValue().longValue());
     		assertEquals(102L, sourceContext.getLatestElement().getTimestamp());
     
     		// now, we should have a watermark (this blocks until the periodic thread emitted the watermark)
     		assertEquals(3L, sourceContext.getLatestWatermark().getTimestamp());
     
     		// advance partition 3
    -		fetcher.emitRecord(1003L, part3, 3L, new ConsumerRecord<byte[], byte[]>(testTopic, 21, new byte[]{0}, 3L));
    -		fetcher.emitRecord(1004L, part3, 4L, new ConsumerRecord<byte[], byte[]>(testTopic, 21, new byte[]{0}, 4L));
    -		fetcher.emitRecord(1005L, part3, 5L, new ConsumerRecord<byte[], byte[]>(testTopic, 21, new byte[]{0}, 5L));
    +		fetcher.emitRecord(1003L, part3, 3L, Long.MAX_VALUE);
    +		fetcher.emitRecord(1004L, part3, 4L, Long.MAX_VALUE);
    +		fetcher.emitRecord(1005L, part3, 5L, Long.MAX_VALUE);
     		assertEquals(1005L, sourceContext.getLatestElement().getValue().longValue());
     		assertEquals(1005L, sourceContext.getLatestElement().getTimestamp());
     
     		// advance partition 1 beyond partition 2 - this bumps the watermark
    -		fetcher.emitRecord(30L, part1, 4L, new ConsumerRecord<byte[], byte[]>(testTopic, 7, new byte[]{0}, 4L));
    +		fetcher.emitRecord(30L, part1, 4L, Long.MAX_VALUE);
     		assertEquals(30L, sourceContext.getLatestElement().getValue().longValue());
     		assertEquals(30L, sourceContext.getLatestElement().getTimestamp());
     		
     		// this blocks until the periodic thread emitted the watermark
     		assertEquals(12L, sourceContext.getLatestWatermark().getTimestamp());
     
     		// advance partition 2 again - this bumps the watermark
    -		fetcher.emitRecord(13L, part2, 2L, new ConsumerRecord<byte[], byte[]>(testTopic, 13, new byte[]{0}, 2L));
    -		fetcher.emitRecord(14L, part2, 3L, new ConsumerRecord<byte[], byte[]>(testTopic, 13, new byte[]{0}, 3L));
    -		fetcher.emitRecord(15L, part2, 3L, new ConsumerRecord<byte[], byte[]>(testTopic, 13, new byte[]{0}, 3L));
    +		fetcher.emitRecord(13L, part2, 2L, Long.MAX_VALUE);
    +		fetcher.emitRecord(14L, part2, 3L, Long.MAX_VALUE);
    +		fetcher.emitRecord(15L, part2, 3L, Long.MAX_VALUE);
    --- End diff --
    
    Same here: I think these are supposed to give a `Long.MIN_VALUE` instead of MAX?


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    Thanks Robert for addressing my comments :)
    
    Overall, I like the new hybrid producer approach. However, I'm still curious whether or not it is possible / reasonable to drop the `FlinkKafkaProducer010Configuration` return type of invocation (b), and let both invocation methods return `FlinkKafkaProducer010` instead. So,
    
    ```
    FlinkKafkaProducer010 kafka = new FlinkKafkaProducer010(...)
    // or FlinkKafkaProducer010 kafka = FlinkKafkaProducer010.writeToKafkaWithTimestamps(...) for timestamp support
    
    // setter config methods directly done on the FlinkKafkaProducer010 instance regardless of (a) or (b)
    kafka.setLogFailuresOnly(true)
    kafka.setFlushOnCheckpoint(true)
    kafka.setWriteTimestampToKafka(true) // would not have effect if original invocation method (a) was used
    ```
    
    But we'll need to be bit hacky in `invokeInternal(element, elementTimestamp)`, something like only letting the given `timestamp` to `ProducerRecord` be non-null if `writeTimestampToKafka && elementTimestamp != Long.MIN_VALUE`.
    
    What do you think?


Github user nemccarthy commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    +1 for this pr


Github user cjstehno commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    Any thoughts on when this might make it into a release? We are having issues running Flink with Kafka 0.10 and would like to have an idea of whether we can/should wait for this or pull it and try building our own. Thanks.


The PR for kerberos integration for Kafka 0.9 has been merged recently. I don't know how does it work but maybe the work to make it compatible with this component should be done in this PR so we don't have a partial support for Kerberos on Kafka. 

Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    Looks like we need to rebase this PR on the recently merged Kerberos support.


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    @cjstehno I would expect this to be in the 1.2.0 major release, which would probably be ~2 months from now according to Flink's past release cycle. The Flink community usually doesn't release major new features like this between minor bugfix releases.


Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r80491475
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -0,0 +1,399 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import org.apache.flink.api.common.functions.IterationRuntimeContext;
    +import org.apache.flink.api.common.functions.RichFunction;
    +import org.apache.flink.api.common.functions.RuntimeContext;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;
    +import org.apache.flink.configuration.Configuration;
    +import org.apache.flink.streaming.api.datastream.DataStream;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
    +import org.apache.flink.streaming.api.functions.sink.SinkFunction;
    +import org.apache.flink.streaming.api.operators.StreamSink;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;
    +import org.apache.kafka.clients.producer.ProducerRecord;
    +
    +import java.util.Properties;
    +
    +import static org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.getPropertiesFromBrokerList;
    +
    +
    +/**
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x
    + *
    + * Implementation note: This producer is a hybrid between a regular regular sink function (a)
    + * and a custom operator (b).
    + *
    + * For (a), the class implements the SinkFunction and RichFunction interfaces.
    + * For (b), it extends the StreamTask class.
    + *
    + * Details about approach (a):
    + *
    + *  Pre Kafka 0.10 producers only follow approach (a), allowing users to use the producer using the
    + *  DataStream.addSink() method.
    + *  Since the APIs exposed in that variant do not allow accessing the the timestamp attached to the record
    + *  the Kafka 0.10 producer has a section invocation option, approach (b).
    + *
    + * Details about approach (b):
    + *  Kafka 0.10 supports writing the timestamp attached to a record to Kafka. When adding the
    + *  FlinkKafkaProducer010 using the FlinkKafkaProducer010.writeToKafka() method, the Kafka producer
    + *  can access the internal record timestamp of the record and write it to Kafka.
    + *
    + * All methods and constructors in this class are marked with the approach they are needed for.
    + */
    +public class FlinkKafkaProducer010<T> extends StreamSink<T> implements SinkFunction<T>, RichFunction {
    +
    +	/**
    +	 * Flag controlling whether we are writing the Flink record's timestamp into Kafka.
    +	 */
    +	private boolean writeTimestampToKafka = false;
    +
    +	// ---------------------- "Constructors" for timestamp writing ------------------
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    +										String topicId,
    +										KeyedSerializationSchema<T> serializationSchema,
    +										Properties producerConfig) {
    +		return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner<T>());
    +	}
    +
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined (keyless) serialization schema.
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    --- End diff --
    
    Its a good idea, I'll rename the methods


Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r80496915
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java ---
    @@ -60,12 +60,17 @@ protected void assignPartitionsToConsumer(KafkaConsumer<byte[], byte[]> consumer
     		consumer.assign(topicPartitions);
     	}
     
    +	@Override
    +	protected void emitRecord(T record, KafkaTopicPartitionState<TopicPartition> partition, long offset, ConsumerRecord consumerRecord) throws Exception {
    +		// pass timestamp
    +		super.emitRecord(record, partition, offset, consumerRecord.timestamp());
    +	}
    +
     	/**
     	 * Emit record Kafka-timestamp aware.
     	 */
     	@Override
    -	protected <R> void emitRecord(T record, KafkaTopicPartitionState<TopicPartition> partitionState, long offset, R kafkaRecord) throws Exception {
    -		long timestamp = ((ConsumerRecord) kafkaRecord).timestamp();
    +	protected void emitRecord(T record, KafkaTopicPartitionState<TopicPartition> partitionState, long offset, long timestamp) throws Exception {
     		if (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) {
    --- End diff --
    
    Yes, I'll do that.


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    I'm currently working on rebasing the PR and addressing the comments.


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    @tzulitai I addressed all your comments except the one relating `FlinkKafkaProducer010Configuration`: I had a quick offline discussion with @StephanEwen about the issue and he suggested to add the timestamp to the regular sink interface.
    But I would like to make that change separate from this one, and merge the Kafka 0.10. support as-is. This will make it easier for people to try it out now and provide us with feedback. Also, I think some other Kafka related pull requests are blocked on this one.
    
    @tzulitai could you do a final pass over the changes. If you agree, I'd like to merge it afterwards.


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    @rmetzger Thanks for addressing the comments! Did a final pass, and the changes look good to me.
    I agree with merging the connector as is. Adding the timestamp to the regular sink interface seems like a good long term solution.
    
    +1 to merge once travis turns green ;)


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r80628475
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -0,0 +1,398 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.streaming.connectors.kafka;
    +
    +import org.apache.flink.api.common.functions.IterationRuntimeContext;
    +import org.apache.flink.api.common.functions.RichFunction;
    +import org.apache.flink.api.common.functions.RuntimeContext;
    +import org.apache.flink.api.java.typeutils.GenericTypeInfo;
    +import org.apache.flink.configuration.Configuration;
    +import org.apache.flink.streaming.api.datastream.DataStream;
    +import org.apache.flink.streaming.api.datastream.DataStreamSink;
    +import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
    +import org.apache.flink.streaming.api.functions.sink.SinkFunction;
    +import org.apache.flink.streaming.api.operators.StreamSink;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner;
    +import org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner;
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;
    +import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;
    +import org.apache.flink.streaming.util.serialization.SerializationSchema;
    +import org.apache.kafka.clients.producer.ProducerRecord;
    +
    +import java.util.Properties;
    +
    +import static org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.getPropertiesFromBrokerList;
    +
    +
    +/**
    + * Flink Sink to produce data into a Kafka topic. This producer is compatible with Kafka 0.10.x
    + *
    + * Implementation note: This producer is a hybrid between a regular regular sink function (a)
    + * and a custom operator (b).
    + *
    + * For (a), the class implements the SinkFunction and RichFunction interfaces.
    + * For (b), it extends the StreamTask class.
    + *
    + * Details about approach (a):
    + *
    + *  Pre Kafka 0.10 producers only follow approach (a), allowing users to use the producer using the
    + *  DataStream.addSink() method.
    + *  Since the APIs exposed in that variant do not allow accessing the the timestamp attached to the record
    + *  the Kafka 0.10 producer has a second invocation option, approach (b).
    + *
    + * Details about approach (b):
    + *  Kafka 0.10 supports writing the timestamp attached to a record to Kafka. When adding the
    + *  FlinkKafkaProducer010 using the FlinkKafkaProducer010.writeToKafkaWithTimestamps() method, the Kafka producer
    + *  can access the internal record timestamp of the record and write it to Kafka.
    + *
    + * All methods and constructors in this class are marked with the approach they are needed for.
    + */
    +public class FlinkKafkaProducer010<T> extends StreamSink<T> implements SinkFunction<T>, RichFunction {
    +
    +	/**
    +	 * Flag controlling whether we are writing the Flink record's timestamp into Kafka.
    +	 */
    +	private boolean writeTimestampToKafka = false;
    +
    +	// ---------------------- "Constructors" for timestamp writing ------------------
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined serialization schema supporting key/value messages
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafkaWithTimestamps(DataStream<T> inStream,
    +																					String topicId,
    +																					KeyedSerializationSchema<T> serializationSchema,
    +																					Properties producerConfig) {
    +		return writeToKafkaWithTimestamps(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner<T>());
    +	}
    +
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. the sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)
    +	 *
    +	 * @param inStream The stream to write to Kafka
    +	 * @param topicId ID of the Kafka topic.
    +	 * @param serializationSchema User defined (keyless) serialization schema.
    +	 * @param producerConfig Properties with the producer configuration.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafkaWithTimestamps(DataStream<T> inStream,
    +																					String topicId,
    +																					SerializationSchema<T> serializationSchema,
    +																					Properties producerConfig) {
    +		return writeToKafkaWithTimestamps(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FixedPartitioner<T>());
    +	}
    +
    +	/**
    +	 * Creates a FlinkKafkaProducer for a given topic. The sink produces a DataStream to
    +	 * the topic.
    +	 *
    +	 * This constructor allows writing timestamps to Kafka, it follow approach (b) (see above)
    +	 *
    +	 *  @param inStream The stream to write to Kafka
    +	 *  @param topicId The name of the target topic
    +	 *  @param serializationSchema A serializable serialization schema for turning user objects into a kafka-consumable byte[] supporting key/value messages
    +	 *  @param producerConfig Configuration properties for the KafkaProducer. 'bootstrap.servers.' is the only required argument.
    +	 *  @param customPartitioner A serializable partitioner for assigning messages to Kafka partitions.
    +	 */
    +	public static <T> FlinkKafkaProducer010Configuration<T> writeToKafkaWithTimestamps(DataStream<T> inStream,
    +																					   String topicId,
    +																					   KeyedSerializationSchema<T> serializationSchema,
    +																					   Properties producerConfig,
    +																					   KafkaPartitioner<T> customPartitioner) {
    --- End diff --
    
    Checkstyle failed for these few lines here, have leading spaces :P


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r80629075
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -83,11 +83,11 @@
     	 * @param serializationSchema User defined serialization schema supporting key/value messages
     	 * @param producerConfig Properties with the producer configuration.
     	 */
    -	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    -										String topicId,
    -										KeyedSerializationSchema<T> serializationSchema,
    -										Properties producerConfig) {
    -		return writeToKafka(inStream, topicId, serializationSchema, producerConfig, new FixedPartitioner<T>());
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafkaWithTimestamps(DataStream<T> inStream,
    --- End diff --
    
    Add the generic type parameter `T` to `FlinkKafkaProducer010Configuration` here too?


Github user tzulitai commented on a diff in the pull request:

    https://github.com/apache/flink/pull/2369#discussion_r80629176
  
    --- Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java ---
    @@ -102,11 +102,11 @@
     	 * @param serializationSchema User defined (keyless) serialization schema.
     	 * @param producerConfig Properties with the producer configuration.
     	 */
    -	public static <T> FlinkKafkaProducer010Configuration writeToKafka(DataStream<T> inStream,
    -										String topicId,
    -										SerializationSchema<T> serializationSchema,
    -										Properties producerConfig) {
    -		return writeToKafka(inStream, topicId, new KeyedSerializationSchemaWrapper<>(serializationSchema), producerConfig, new FixedPartitioner<T>());
    +	public static <T> FlinkKafkaProducer010Configuration writeToKafkaWithTimestamps(DataStream<T> inStream,
    --- End diff --
    
    Add the generic type parameter `T` to `FlinkKafkaProducer010Configuration` here too?


Github user tzulitai commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    Just found some minor issues that can be fixed when merging.


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    Thank you for the review. I'll address your comments, rebase again, test it, and if it turns green merge it ;)


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2369
  
    Tests are running https://travis-ci.org/rmetzger/flink/builds/166441047


Kafka 0.10 support added in http://git-wip-us.apache.org/repos/asf/flink/commit/63859c64

Github user asfgit closed the pull request at:

    https://github.com/apache/flink/pull/2369


Github user asfgit closed the pull request at:

    https://github.com/apache/flink/pull/2231


Github user rmetzger commented on the issue:

    https://github.com/apache/flink/pull/2231
  
    @radekg I've added Kafka 0.10 support to Flink, that's why I closed this pull request. My change preserved your commit from this pull request. Thank you for the contribution!


