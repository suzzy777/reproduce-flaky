One idea that was relayed (with attribution to [~mck]] and [~blambov]):

{quote}remove the bootstrapping from all dtests that don't need it. e.g. to bootstrap parameterized ccm clusters and save them as templates, and then have each dtest to copy a ccm template and start it up. that deduces sequential bootstrap time with parallel startup time, on basically every dtest.{quote}

That is a really great idea, I don't think we have all that many combinations of options for bootstrapping in the dtests currently.

{noformat}
grep -ri cluster\.populate\( --include \*.py |wc -l
410
grep -ri cluster\.populate\(1 --include \*.py |wc -l
171
{noformat}

410 cluster starts & 171 single node clusters = 40% single node starts. Say 50% are convertible to unit or jvm-dtest = 20% less node starts. Say node start time is 50% of test time = 10% less time. Dtest run time is about 50m (dtest, offheap & novnode) -> 50m * 10% * 3 = potential 15m cut finger in the air numbers.



I wonder if we could start some docker nodes in a [checkpointed|https://docs.docker.com/engine/reference/commandline/checkpoint/] state or snapshoted vm machines for each node. Then it would be a matter of perf testing snapshot load vs pure node startup mmmm

The ccm used in each dtest run is from `origin/master`. So pre-bootstrapping the parameterised ccm clusters (to create the template/cache), would have to be done per overall dtest run. Caching templates for past ccm SHAs doesn't hold a lot of value.

It would be _nice_ to introduce the improvement inside dtest+ccm codebases, so it's consistently used by all (rather than docker/vm snapshots). 



Is it possible to implement some sort of "reset" operation in CCM so that it drops all non-system keyspaces so that the clusters that don't explicitly test cluster membership operations can just be reused as has been said?  We could disable snapshotting on them as well so they wouldn't build up state over time too.

In other words, it sounds like if we made the time for starting single node clusters essentially instant, that's 171 * single node startup time that we've reduced for the overall dtests.

A suggested plan moving forward was both:
 - look at ten slowest dtests, and see whether they can be -broken up, or- rewritten to in-jvm tests, and
 - providing caching of pre-bootstrapped ccm clusters (based on sha, size, config).


The ten slowest dtests can be seen in
  !Screenshot 2020-09-19 at 12.32.21.png|width=300px! 

They are…
- dtest.materialized_views_test (64m)
- dtest.replica_side_filtering_test (54m)
- dtest.repair_tests.repair_test (48m)
- dtest.auth_test (38m)
- dtest.disk_balance_test (34m)
- dtest.cqlsh_tests.test_cqlsh_copy (31m)
- dtest.paging_test (27m)
- dtest.repair_tests.incremental_repair_test (21m)
- dtest.compaction_test (19m)

{quote}They are…
- dtest.materialized_views_test (64m)
- dtest.replica_side_filtering_test (54m)
- dtest.repair_tests.repair_test (48m)
- dtest.auth_test (38m)
- dtest.disk_balance_test (34m)
- dtest.cqlsh_tests.test_cqlsh_copy (31m)
- dtest.paging_test (27m)
- dtest.repair_tests.incremental_repair_test (21m)
- dtest.compaction_test (19m){quote}

This breakdown wasn't of any value. DTest CI splits is already happening at the method level. 
Looking at ci-cassandra.a.o (which splits only on round-robin, rather than timings-based), the splits cover a duration range from ~5 to ~20 minutes. 

Looking at those upper range splits I can see that there's a few dtest test methods taking more than two minutes, but I can't see how addressing them is going to have any reasonable ROI for overall dtests runtime.

- dtest.repair_tests.repair_test.TestRepair.test_dc_repair (6m38s)
- dtest.repair_tests.repair_test.TestRepair.test_simple_sequential_repair (4m05s)
- dtest.materialized_views_test.TestMaterializedViews.test_interrupt_build_process (3m52s)
- dtest.bootstrap_test.TestBootstrap.test_cleanup (2m44s)
- dtest.replace_address_test.TestReplaceAddress.test_multi_dc_replace_with_rf1 (2m14s)

I was hoping that the timings would be visible in the {{cassandra-test-report.txt}}, but since the parallelisation of dtests only timings of splits are now listed. The timings will become accessible post CASSANDRA-16128 (with the full test xml [kept|https://github.com/apache/cassandra-builds/compare/master...thelastpickle:mck/jenkins--print-git-shas-and-website-build#diff-a76ac7fa36779199fe7e4fe917ac41bcR252] on nightlies.a.o).

If splits are happening at method level then node reusage between classes/methods makes no sense. Hence the pre-bootstrap route is the strongest candidate imo...

I have started on a PoC for the caching of bootstrapped ccm, to help identify whether (and how much) time can be saved.

I second what Berenguer said here. Also, I think the reusage of classes/methods is a bit more tricky. The way I was thinking of it and experimented over the weekend a bit was - reusable cluster but if cleaning is not done properly this is more error-prone solution. Also, it will be of benefit only for Circle at the moment and Circle (circle splits are on time&class level as per the config file now if I read this correctly ---split-by=timings --timings-type=classname) is not the main project CI.

Also, caching of bootstrapped ccm should be of use for both Circle and Jenkins. If there is a need probably we can do additional splits/grouping of test classes to help Circle more which shouldn't be hard(right?).

A thing we can explore in this case too is the caching layer in circle whether we can use it now, maybe? Not sure also credit-wise whether this will be of benefit but we can explore it. 

bq. I have started on a PoC for the caching of bootstrapped ccm, to help identify whether (and how much) time can be saved.

This [approach|https://github.com/thelastpickle/cassandra-dtest/commit/7e6a7b186e#diff-4cbdcd9896ce5009acd37ed830ce8959R279-R334] did not bear much fruit.

To cache bootstrapped ccm clusters it meant that those ccm clusters had to be bootstrapped, then stopped, then the directory copied, and then started up again for the test in question. This added ~2 seconds per node. When a bootstrapped ccm cluster was cached, it still had to have its directory copied (back into place of the current dtest ccm directory), and only saved up to ~1 second. These numbers are pre- CASSANDRA-13701. While there might have been some improvement post 13701, it wouldn't have been as much as hoped as each dtest split in jenkins is only running ~20 dtest methods and the re-use of bootstrapped ccm clusters would have only been a fraction of that. 

This approach could be better utilised if dtests could be run in a "bootstrap and cache all ccm clusters" mode, and the resulting cache zipped and re-used between runs (e.g. stored and downloaded from nightlies.a.o).


{quote}To cache bootstrapped ccm clusters it meant that those ccm clusters had to be bootstrapped, then stopped, then the directory copied, and then started up again for the test in question.
{quote}
Is it necessary to stop the cluster to cache it and then start it again? Can't we just flush and copy the directory without stopping the cluster? This might shave some restart time.

One optimization we can do is to *NOT* enforce +_wait_for_binary_proto_+ before starting the next node on [~adejanovski]'s [patch|https://github.com/riptano/ccm/commit/e6e4abcff375debde8195104c5cffd1cecb8d6cf#diff-52bf69a0e35f795af17fff7a2447bb26R479], but only _+wait_other_notice+_, since this will guarantee the next node will take the previous into account during token allocation without waiting it to fully bootstrap (which takes at least 30s due to *_cassandra.ring_delay_ms=30000_*). This should return some level of parallelism to the cluster bootstrap process.

In case the above suggestion does not bring acceptable results, another point to consider is that when enabling _*allocate_tokens_for_local_replication_factor: 3*_ by default on CASSANDRA-13701, we are now exercising the new token allocation algorithm on all dtests, even the ones that are not specifically testing this.

While it's probably a good idea to run all dtests with the new token allocation algorithm to catch unintended edge cases the downside is that this increases dtest execution time. Couldn't we keep use random allocation by default on dtests to be able to bootstrap in parallel, and have a new suite of dtests executed nightly and before release (similar to _novnode_) with the new token allocation enabled?

bq.  Is it necessary to stop the cluster to cache it and then start it again? Can't we just flush and copy the directory without stopping the cluster? This might shave some restart time.

It might shave some time. But I think the benefits, based on what re-use there'll be within splits, will be touch-n-go without a centralised cache approach. The copy of a file tree isn't atomic either, so I would presume we'd be opening for edge-case bugs here. No one wants more flakey dtests :(

Note, in that approach the subsequent ccm start was all nodes in parallel (which is much faster ofc).

bq. Couldn't we keep use random allocation by default on dtests to be able to bootstrap in parallel, and have a new suite of dtests executed nightly and before release (similar to novnode) with the new token allocation enabled?

A number of folk have raised this.  I believe the general preference is that we want the dtests to test the default config settings, especially to help identify flaky dtests during 4.0-beta.

[~pauloricardomg], your CI build for this comment https://issues.apache.org/jira/browse/CASSANDRA-13701?focusedCommentId=17210491&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17210491 is at https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-dtest/79/

[~pauloricardomg], the dtest branch is old (from before {{`--keep-failed-test-dir`}} was added). Could you rebase please.

Resubmitted job after rebasing: https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-dtest/80/

Taking advantage of CASSANDRA-16205 this patch pre-generates tokens using the allocation strategy (for RF=3).
https://github.com/riptano/ccm/compare/cassandra-test...thelastpickle:mck/16205 


Those dtests that want the token allocation strategy generation to happen when the node bootstraps can disable it using
{code}
cluster.set_environment_variable('CASSANDRA_TOKEN_PREGENERATION_DISABLED', 'True')
{code}

CI run [here|https://ci-cassandra.apache.org/job/Cassandra-devbranch/204/], and some example failure logs [here|https://nightlies.apache.org/cassandra/Cassandra-devbranch-dtest/label=cassandra,split=42/203/].

A handful of dtests that need to fixed/adjusted.

Did you notice any speed improvements? All runs seem to be more or less the same when I look. Also should we better put this ticket back to In Progress?

bq. Also should we better put this ticket back to In Progress?

Sure, just flagging it as there's now a solution on the table, ready for review (despite ongoing dtest work). 
As long as there's no slowdown with 13701, i.e. the improvement counters what 13701 slows it down, then I think we're good.

Down to 4 failures: https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/208/#showFailuresLink
I still need to test the novnode dtests, and also to identify which dtests need normal bootstrap…

Here's a clean run: https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-dtest/249/


This builds on the following patches:
- cassandra [mck/trunk_13701|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_13701]
- cassandra [thelastpickle:paulo/trunk_16205|https://github.com/apache/cassandra/compare/trunk...thelastpickle:paulo/trunk_16205]
- ccm [mck/16205|https://github.com/riptano/ccm/compare/trunk...thelastpickle:mck/16205]
- cassandra-dtest [mck/16079|https://github.com/apache/cassandra-dtest/compare/trunk...thelastpickle:mck/16079]

16205 looks to be in final review, once that's merged, then this ticket is open for review.

Now that CASSANDRA-16205 is merged…

For just the work in this ticket, here's a run: https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/247/pipeline
This builds on the following patches:
- ccm [mck/16205|https://github.com/riptano/ccm/compare/cassandra-test...thelastpickle:mck/16205]
- cassandra-dtest [mck/16079|https://github.com/apache/cassandra-dtest/compare/trunk...thelastpickle:mck/16079]

And, for work in this ticket in combination with CASSANDRA-13701, here's a run: https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/248/pipeline
This builds on the following patches:
- cassandra [mck/trunk_13701|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_13701]
- ccm [mck/16205|https://github.com/riptano/ccm/compare/cassandra-test...thelastpickle:mck/16205]
- cassandra-dtest [mck/16079|https://github.com/apache/cassandra-dtest/compare/trunk...thelastpickle:mck/16079]

Both these runs have the same performance, so any degradation is no longer attributable to the 13701 patch. 

It's not yet clear what the performance degradation or improvement from this ticket's patches are. Looking at https://ci-cassandra.apache.org/job/Cassandra-trunk-dtest/buildTimeTrend it suggests that the ccm and dtest patches for this ticket could be improving dtest performance by ~20% (without looking at jenkins agents involved or system saturation at the time of each run).

(update: fixed CI links)

bq. Did you notice any speed improvements? All runs seem to be more or less the same when I look.

The objective of this ticket is, from the description: "…optimize dtests in an attempt to gain back some of that runtime."

If runs looks to have more or less the same performance, given they are including 13701, then that is a win.

Here's is another run, again showing performance consistent (or possibly better) than trunk: https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-dtest/262/

So… so long as we have made improvements that mostly offset the cost introduced by 13701 then this ticket I believe should be good to go. That is, this ticket is a blocker to 13701 getting merged into 4.0, hence this ticket is a blocker to 4.0. I don't have a problem with identified improvements to further improve dtest performance being spun out to further tickets (that don't block 4.0).

What are your thoughts [~Bereng], [~aholmber], [~paulo]?

For me the only remaining question here is if there are other dtests that should go through the runtime token allocation strategy rather than using the pre-generated tokens, that is using the cluster env variable: {{'CASSANDRA_TOKEN_PREGENERATION_DISABLED'}}.

Thanks for the ping. I'm looking at it now.

(also paging [~paulo] with his active handle)

 Everything looks pretty good to me. Thanks Mick for exploring this through multiple iterations!

I put a handful of comments on the ccm changes. Nothing too crazy -- most are "take 'em or leave 'em"
Tentative +1 from me on both ccm and dtest changes.

Have dtests been run on any of the previous supported C* branches? I tried kicking a few off and the dtests all show "blocked" in CircleCI. I've not tried to run Circle on earlier branches before, so I wasn't sure what to expect.

bq. Have dtests been run on any of the previous supported C* branches? I tried kicking a few off and the dtests all show "blocked" in CircleCI. I've not tried to run Circle on earlier branches before, so I wasn't sure what to expect.

Good point. Here are all the ci-cassandra runs (with your ccm review comments addressed)… 

For ci-cassandra
 - [2.2|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-dtest/269/]
 - [3.0|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/270/]
 - [3.11|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/271/]
 - [trunk|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/272/]

If someone could kick-off 2.2, 3.0, 3.11 and trunk, circleci runs that would be great! (and share the links here).


bq. For ci-cassandra

cqlsh tests look like they might be hurting in earlier branches. I haven't looked into it.

bq. If someone could kick-off 2.2, 3.0, 3.11 and trunk, circleci runs 

Here's a batch on circleci:
[2.2|https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-16079-2.2]
[3.0|https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-16079-3.0]
[3.11|https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-16079-3.11]
[trunk|https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-16079-trunk]

-edit: these were a bust due to ccm install conflict. Will try to see what needs to change.-
fixed

If that's not too late I'll take a look at this tomorrow.

bq. If that's not too late I'll take a look at this tomorrow

I don't think it's too late. Input welcome.

I got Circle to run them for all four branches. It was having trouble installing the patched ccm over the existing package. This change was just needed to run in our current patched situation, but will we want to update the docker image when ccm is committed?

The links in previous comment are still valid, and will point to the last build.

The 2.2 cqlsh tests are failing in Circle the same was as in ci-cassandra:

{noformat}assandraDaemon.java:787 - Exception encountered during startup
org.apache.cassandra.exceptions.ConfigurationException: Invalid yaml. Please remove properties [enable_scripted_user_defined_functions] from your cassandra.yaml{noformat}

Seems unrelated to this change. Maybe we just don't run these against 2.2...

New comment on the dtest changes following CI.
https://github.com/apache/cassandra-dtest/commit/774f1d65328aed1991fcc7f0be24c9511bd5d1fb#r44928505

Attribute is unavailable in earlier versions of C*.

bq. New comment on the dtest changes following CI.

Looking into that. A number of oversights from me there, thanks for spotting them.

New round with [ccm:f782b17|https://github.com/thelastpickle/ccm/commit/f782b178e85bf9d37d9754400420f80a41933c9d] and [cassandra-dtest:391709d|https://github.com/apache/cassandra-dtest/commit/391709d4a9bbfbc89672c18758ca802df4b3fa17]
 - [2.2|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-dtest/276/]
 - [3.0|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/275/]
 - [3.11|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/273/]
 - [3.11 novnode|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest-novnode/38/]
 - [trunk|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/274/]
 - [trunk novnode|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest-novnode/39/]



I forgot to mention, I launched the retest on circle (same links as my previous list).
2.2 has the same unrelated issue. 3.0+ I think looks good.

Last build in my [trunk branch|https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-16079-trunk] is rebased on 13701. Compared with the previous build, timings are on-par or better in most cases.
+1

WDYT about setting {{CASSANDRA_TOKEN_PREGENERATION_DISABLED}} on the constructor of {{TestBootstrap}} instead, and unset where not needed? This way new tests added there would use the property unless explicitly disabled.

Also, can you maybe include [this commit |https://github.com/pauloricardomg/ccm/commit/ab3f6c68f01ae335ed38576fe24b48f28e9c2483] (which requires [this|https://github.com/pauloricardomg/ccm/commit/1b33d565f2734d7b2abfe46ed3a6b1fe40780316]) on the ccm patch? It may shave a few seconds from the tests using the token allocation algorithm, by setting the ring delay to zero on the first started node and reducing the ring delay from the default of 30s to 5s which should reduce the wait time between sequential node starts.

Other than these minor nits, LGTM, great job!

bq. WDYT about setting CASSANDRA_TOKEN_PREGENERATION_DISABLED on the constructor of TestBootstrap instead, and unset where not needed? This way new tests added there would use the property unless explicitly disabled.

I like the idea. But i haven't been able to implement it so far. The dtest classes can't have constructors, and when using @pytest.fixtures the {{self.cluster}} was not available yet. Most dtests seem to approach this with a {{_prepare_cluster}} function, but that approach doesn't add anything here.

bq. Also, can you maybe include this commit  (which requires this) on the ccm patch?

I included the latter sha [ab3f6c6|https://github.com/pauloricardomg/ccm/commit/ab3f6c68f01ae335ed38576fe24b48f28e9c2483], which seems to supersede the former [1b33d56|https://github.com/pauloricardomg/ccm/commit/1b33d565f2734d7b2abfe46ed3a6b1fe40780316] with ring_delay_ms according to the function's argument.

CI run without 13701 [here|https://ci-cassandra.apache.org/job/Cassandra-devbranch/257/]
and with [here|https://ci-cassandra.apache.org/job/Cassandra-devbranch/258/].

_investigating/validating dtest failures in those last CI runs…_

bq. I like the idea. But i haven't been able to implement it so far. 

Don't bother changing unless it's straightforward.

bq. investigating/validating dtest failures in those last CI runs…

It could be the too low ring delay. I've [increased it|https://github.com/pauloricardomg/ccm/commit/1346c2ebdaaf1655366eac3b9f47839563ed987c] to 10s here and resubmitted CI:
* [with 13701|https://ci-cassandra.apache.org/job/Cassandra-devbranch/262/]
* [without|https://ci-cassandra.apache.org/job/Cassandra-devbranch/260/]

[~paulo], I've confirmed with [this|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/288/] and [this|https://ci-cassandra.apache.org/job/Cassandra-devbranch/263/] CI build that it is related to the ring_delay addition. If it is not quick+easy to fix, and because this ticket is blocking our last {{4.0-alpha}} ticket 13701, can be push it out to a separate follow-up ticket?

bq. If it is not quick+easy to fix, and because this ticket is blocking our last 4.0-alpha ticket 13701, can be push it out to a separate follow-up ticket?

Sounds good, I'll look into it separately. Feel free to move forward with this commit, +1.

Final patches (squashed and rebased):
 - ccm [mck/16205|https://github.com/riptano/ccm/compare/cassandra-test...thelastpickle:mck/16205]
 - cassandra-dtest [mck/16079|https://github.com/apache/cassandra-dtest/compare/trunk...thelastpickle:mck/16079]

