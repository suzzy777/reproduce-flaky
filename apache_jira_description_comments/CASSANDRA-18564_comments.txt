The very similar dtest {{MixedModeAvailabilityV30AllOneTest.testAvailabilityCoordinatorUpgraded}} has failed in the same Jenkins run with a mostly identical message, so I guess both failures have the same cause.

This is the Jenkins run: [https://ci-cassandra.apache.org/job/Cassandra-trunk/1585/testReport/org.apache.cassandra.distributed.upgrade/MixedModeAvailabilityV30AllOneTest/testAvailabilityCoordinatorUpgraded__jdk11/]

Saw this today in a 5.0-based branch: [https://app.circleci.com/pipelines/github/adelapena/cassandra/3105/workflows/3cf378e2-999f-4d1b-afc4-d517ce0f77e0/jobs/67928/tests#failed-test-0]

CASSANDRA-18506 has merged {{MixedModeAvailabilityV30AllOneTest}} and {{MixedModeAvailabilityV3XAllOneTest}} without fixing the bug, so now the failing test is {{{}MixedModeAvailabilityV30AllOneTest{}}}. I'm renaming this accordingly.

The failure can easily be reproduced with:
{code:java}
.circleci/generate.sh -p \
  -e REPEATED_JVM_UPGRADE_DTESTS_COUNT=100 \
  -e REPEATED_JVM_UPGRADE_DTESTS="org.apache.cassandra.distributed.upgrade.MixedModeAvailabilityV30AllOneTest"
{code}
With these results: [https://app.circleci.com/pipelines/github/adelapena/cassandra/3124/workflows/2da49e8e-0846-4397-afce-c3cb4b291975/jobs/70185/tests]

It seems that the failing upgrade paths are those starting on 4.0x, 4.x and 5.0.x. Upgrades from 3.0.x and 3.x don't seem to fail.

The test seems to pass in 4.1: [https://app.circleci.com/pipelines/github/adelapena/cassandra/3141/workflows/5e7f85e7-ce66-4e8a-af8d-23f7de9d2205]

It only fails in 5.0 and trunk. This test has always been quite long and prone to timeouts. The new v50 and v51 branches have almost doubled the number of upgrade paths, and this probably is what is producing the timeouts.

[Increasing {{read_request_timeout}} and {{write_request_timeout}} to 30 seconds|https://github.com/apache/cassandra/commit/d7775660bfe94ab2353faab95a4a7cd0e2cf79ee] seems to be enough to make the test survive 500 runs: https://app.circleci.com/pipelines/github/adelapena/cassandra/3142/workflows/0c680c43-c988-4fa5-8e05-7ab81aea8ae1

However, increasing the timeouts to one minute seems to risk a CircleCI timeout: https://app.circleci.com/pipelines/github/adelapena/cassandra/3143/workflows/66ca0ccc-f6a0-4ba6-9376-59b9b03b57fb

The tricky bit with this test is that it runs both queries that should succeeded and queries that should timeout. So, if we increase the timeout thresholds for the queries that should succeed, then the queries that should timeout make the test incredibly slow.

Maybe we can live with those 30s timeouts, even if they make the test quite expensive. However, it occurs to me that we could relax the conditions of the test to only verify the queries that should succeed with the expected consistency levels and down replicas. We could simply skip testing the cases where the query should timeout. After all, those expected timeouts due to down replicas seem indistinguishable from the timeouts produced by an overloaded CI environment. wdyt?

bq. We could simply skip testing the cases where the query should timeout. After all, those expected timeouts due to down replicas seem indistinguishable from the timeouts produced by an overloaded CI environment. wdyt?

That seems reasonable to me.

This patch makes the test survive 500 runs:
||PR||CI||
|[5.0 |https://github.com/apache/cassandra/pull/2657]|[j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/3194/workflows/d7abb1fe-0954-4b11-8f15-0e9696d43f8d]|
|[trunk|https://github.com/apache/cassandra/pull/2658]|[j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/3195/workflows/1c6be32a-ac4d-44f8-91a4-238997418c28]|

I have changed the test to skip testing the cases where the query should timeout, as mentioned above. This allows us to increase the timeouts without making the test prohibitively slow, which might cause a JUnit or CircleCI timeout.

I have also grouped the testing of different consistency level combinations, so they can all be tested into the same cluster run. This means that we only need to run a third of the clusters. That doesn't make the test less prone to fail, but it reduces the total CI load.

I have also split the test class by whether the coordinator node is upgraded or not, so we don't have to run two full sets of upgrade paths in the same JVM.

However, I'm not sure I understand why the test was more prone to fail in 5.0 and trunk than in 4.0 and 4.1. 5.0 and trunk don't support direct upgrades from 3.0 or 3.x. So I think the number of tested upgrade paths is the same. There should be something in 5.0 and trunk that makes the db, the test or the CI environment slower. Could it be due to the fact that we are running with J11 instead of J8?

LGTM +1 on the code. Do we have some explanation as to why trunk CI takes an extra hour? I can't seem to find one :(

Thanks for looking into this. {{trunk}} takes longer because it needs to test an additional upgrade path, {{5.0-alpha1 -> 5.1}}.

Committed to 5.0 as [c9b934df67ed4ea0779e1f07cf762b67ba6362c5|https://github.com/apache/cassandra/commit/c9b934df67ed4ea0779e1f07cf762b67ba6362c5] and merged to [{{trunk}}|https://github.com/apache/cassandra/commit/a67f4904f7cfda90239d285333903c42ec1596b2].

