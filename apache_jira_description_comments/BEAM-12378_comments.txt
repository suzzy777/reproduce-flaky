@reuvenlax This is quite interesting, I was working on something similar with one key difference:
 This PR assumes that a new record will not modify accumulated records size. 
 e.g lets say I'm accumulating records created by flattening of nested-repeated field and computing a table, 
 record 1- contains 3 fields, whereas record2 contained 4 (due to difference in repeated/ array elements)
 so the system will should recompute the size of accumulated batch again, how do we handle such situation with this PR?

I am using something like: [BatchBySize,Â |https://github.com/GoogleCloudPlatform/auto-data-tokenize/blob/main/dlp/src/main/java/com/google/cloud/solutions/autotokenize/pipeline/dlp/GroupByBatchSize.java]that allows accepting a [BatchAccumulator|https://github.com/GoogleCloudPlatform/auto-data-tokenize/blob/main/dlp/src/main/java/com/google/cloud/solutions/autotokenize/pipeline/dlp/BatchAccumulator.java] that offloads the batch size computation.

Does this sound interesting? Happy to provide commits if you feel my approach makes sense.

I'm not sure I quite understand - the output off GroupIntoBatches is an Iterable<ElementT>, so size logically accumulates. Are you saying that you have a downstream function that does some additional accumulation, and you want to feed that logic back into the GroupIntoBatches transform?

This issue is P2 but has been unassigned without any comment for 60 days so it has been labeled "stale-P2". If this issue is still affecting you, we care! Please comment and remove the label. Otherwise, in 14 days the issue will be moved to P3.

Please see https://beam.apache.org/contribute/jira-priorities/ for a detailed explanation of what these priorities mean.


This issue was marked "stale-P2" and has not received a public comment in 14 days. It is now automatically moved to P3. If you are still affected by it, you can comment and move it back to P2.

Closing as this seems to have been addressed in https://github.com/apache/beam/pull/14852, first available in 2.31.0.

Please re-open if there's more work to do here.

