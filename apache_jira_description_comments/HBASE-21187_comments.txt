How does the machine info compare for the two? Maybe we have a thrashing neighbor on the node?

I believe our Yetus version is new enough that it should have the [Process Reaper|http://yetus.apache.org/documentation/0.7.0/precommit-advanced/#process-reaper] functionality. IIRC, it came with some underlying functionality to monitor processes (e.g. the "process+thread count" in current reports).  we could make a plugin that uses the same thing to e.g. measure CPU or memory use as precommit goes.

dang! our flaky jobs don't run the [machine env capture tool|https://github.com/apache/hbase/blob/master/dev-support/gather_machine_environment.sh]

{quote}
I believe our Yetus version is new enough that it should have the Process Reaper functionality. IIRC, it came with some underlying functionality to monitor processes (e.g. the "process+thread count" in current reports). we could make a plugin that uses the same thing to e.g. measure CPU or memory use as precommit goes.
{quote}

We'd have to convert our flaky job to rely on yetus for this to help. Might be useful anyways since it would mean we could easily run in docker w/ all the protection against wild tests that we already have there. Shouldn't be too hard given that our personality already supports a filter for "only run these tests" and Yetus 0.8.0 adds a "only run tests named in this file" cli option for maven builds.

So let's execute the tool when running flaky jobs? Open a sub task for it?

yeah let me do it in a subtask.

Thanks [~busbey], you expert.

We can always success on H4, but on other machines we are likely to fail... And there are no big difference between the machines... Strange...

This is build 993
{noformat}
07:41:03 up 81 days, 20 min,  0 users,  load average: 0.92, 0.51, 0.66
{noformat}
We passed.

994
{noformat}
10:09:37 up 81 days,  2:28,  0 users,  load average: 14.13, 12.51, 14.56
{noformat}
Lots of tests failed.

995
{noformat}
10:51:42 up 81 days,  3:15,  0 users,  load average: 3.84, 4.95, 7.31
{noformat}
Only TestRSGroups failed.

996
{noformat}
11:16:46 up 36 days, 13:35,  0 users,  load average: 13.16, 11.57, 11.42
{noformat}
Lots of tests failed.

997
{noformat}
11:46:21 up 81 days,  4:25,  0 users,  load average: 2.34, 3.17, 6.23
{noformat}
Only TestCompactingToCellFlatMapMemStore failed. And it is not because of timeout, just an assertion error, so this one is truly flaky...

So I think the problem is that, for TRSP, we will have one more procedure as we use a sub procedure to schedule the remote procedure to simplify the logic, so on a already loaded machine, and if there are lots of regions to assign/unassign, it will be slower as there are extra context switches, and lead to the timeout...


