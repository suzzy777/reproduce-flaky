Instead of using random seed , using the  cluster centers returned by kmeans++ for initializing the means in GMM would be good strategy as implemented in scikit-learn  http://scikit-learn.org/stable/modules/mixture.html#mixture. What is your opinion ?

It's hard to say without running tests.  The current method (random) is simple (and maybe faster), but kmeans++ might be better for accuracy (and maybe fast if it reduces the number of EM iterations).  It would definitely be worth seeing if one or the other (a) gave better clusterings and (b) ran faster in general.

We would try to experiment with both the initialization methods and come up with a comparison on cluster quality and running time.

I'm upping this to critical since it makes the test suite flaky.  I'll submit a PR right away.

User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/3981

Issue resolved by pull request 3981
[https://github.com/apache/spark/pull/3981]

