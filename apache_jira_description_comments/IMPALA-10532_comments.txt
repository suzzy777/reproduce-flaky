The test failed in tasn build.

A fix to this problem has been merged to up stream on 2/26/2021. 

Commit 16493c0416d5de9c02e70c9b786e980a739020da in impala's branch refs/heads/master from Qifan Chen
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=16493c0 ]

IMPALA-10532 TestOverlapMinMaxFilters.test_overlap_min_max_filters seems flaky

This patch addresses the flakiness seen with a particular test within
overlap_min_max_filters by allowing the sum of NumRuntimeFilteredPages
to be greater than an expected value. Previously, such a sum can only
be equal to the expected value and is not sufficient for various test
conditions in which the scan of the parquet data files can start
before the arrival of a runtime filter.

The extension in test_result_verifier.py allows '>' and '<' condition
to be expressed for aggregation(SUM, <counter>), such as
aggregation(SUM, NumRuntimeFilteredPages)> 80.

Testing:
 - Ran TestOverlapMinMaxFilters.

Change-Id: I93940a104bfb2d68cb1d41d7e303348190fd5972
Reviewed-on: http://gerrit.cloudera.org:8080/17111
Reviewed-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


This issue still happened for impala-asf-master-core-erasure-coding nightly test job after the fixing was merged.

Saw this test failed a impala-asf-master-core-erasure-coding build: 
{code}
query_test/test_runtime_filters.py:288: in test_overlap_min_max_filters
    test_file_vars={'$RUNTIME_FILTER_WAIT_TIME_MS': str(WAIT_TIME_MS)})
common/impala_test_suite.py:721: in run_test_case
    update_section=pytest.config.option.update_results)
common/test_result_verifier.py:659: in verify_runtime_profile
    % (function, field, expected_value, actual_value, op, actual))
E   AssertionError: Aggregation of SUM over NumRuntimeFilteredPages did not match expected results.
E   EXPECTED VALUE:
E   200
E   
E   
E   ACTUAL VALUE:
E   0
E   
E   OP:
E   >
{code}
Attached details in ErrorMsg2.txt

Ran the following script to create a table lineitem_orderkey_only_ec on EC directory

{code:java}
hdfs dfs -rmr -skipTrash /ec_tmp_rs_3_2 
hdfs dfs -mkdir /ec_tmp_rs_3_2 
~/Impala/bin/impala-shell.sh --protocol=beeswax -f ddl.lineitem_orderkey_only_ec
hdfs ec -setPolicy -path /ec_tmp_rs_3_2  -policy XOR-2-1-1024k
hdfs ec -getPolicy -path  /ec_tmp_rs_3_2
~/Impala/bin/impala-shell.sh --protocol=beeswax -f load.lineitem_orderkey_only_ec
hdfs ec -getPolicy -path  /ec_tmp_rs_3_2
{code}

Failed to reproduce with the following dml, as the total counter NumStatsFilteredPages is at 281. 

{code:java}
set minmax_filter_threshold=0.5;
set MINMAX_FILTERING_LEVEL=PAGE;
set ALLOW_ERASURE_CODED_FILES=true;    
set num_nodes=2;
select straight_join a.l_orderkey from                                                        
lineitem_orderkey_only_ec a join [SHUFFLE] tpch_parquet.orders b                                 
where a.l_orderkey = b.o_orderkey                                                             
and b.o_custkey = 5 order by l_orderkey;  
profile;

Server version: impalad version 4.0.0-SNAPSHOT DEBUG (build df3cf5eb8b82e2d044af7009cf5f1e662b4c57ea)
MINMAX_FILTER_THRESHOLD set to 0.5
MINMAX_FILTERING_LEVEL set to PAGE
ALLOW_ERASURE_CODED_FILES set to true
NUM_NODES set to 2
EXPLAIN_LEVEL set to 3
Query: --explain
select straight_join a.l_orderkey from
lineitem_orderkey_only_ec a join [SHUFFLE] tpch_parquet.orders b
where a.l_orderkey = b.o_orderkey
and b.o_custkey = 5 order by l_orderkey
Query submitted at: 2021-03-30 14:43:28 (Coordinator: http://qifan-10229:25000)
Query progress can be monitored at: http://qifan-10229:25000/query_plan?query_id=cd4bb282d9e9df5d:4bda62d700000000
Fetched 19 row(s) in 0.55s
         - NumRuntimeFilteredPages: 93 (93)
         - NumStatsFilteredPages: 93 (93)
           - NumRuntimeFilteredPages: 0 (0)
           - NumStatsFilteredPages: 0 (0)
           - NumRuntimeFilteredPages: 130 (130)
           - NumStatsFilteredPages: 130 (130)
           - NumRuntimeFilteredPages: 151 (151)
           - NumStatsFilteredPages: 151 (151)
         - NumRuntimeFilteredPages: 0 (0)
         - NumStatsFilteredPages: 128 (128)
           - NumRuntimeFilteredPages: 0 (0)
           - NumStatsFilteredPages: 200 (200)
           - NumRuntimeFilteredPages: 0 (0)
           - NumStatsFilteredPages: 56 (56)
{code}




A comparison of the Final filter table between the failed official EC run and the local run reveals that the arrival time is about 250ms more with EC.

Local run:


{code:java}
199     Final filter table:                                                                    
 200  ID  Src. Node  Tgt. Node(s)  Target type  Partition filter  Pending (Expected)  First arrived  Completed  Enabled  Bloom Size   Est fpp  Min value   Max value
 201 ---------------------------------------------------------------------------------------------------------------------------------------------------------------
 202   1          2             0       REMOTE             false               0 (3)      278.449ms  292.193ms     true     MIN_MAX
 203   0          2             0       REMOTE             false               0 (3)      278.426ms  292.442ms     true     1.00 MB  1.75e-39
{code}

Failed EC run:


{code:java}
195 E       Final filter table:                                                                
 196 E    ID  Src. Node  Tgt. Node(s)  Target type  Partition filter  Pending (Expected)  First arrived  Completed  Enabled  Bloom Size   Est fpp  Min value   Max value
 197 E   ---------------------------------------------------------------------------------------------------------------------------------------------------------------
 198 E     1          2             0       REMOTE             false               0 (2)      526.000ms  569.000ms     true     MIN_MAX
 199 E     0          2             0       REMOTE             false               0 (2)      526.000ms  569.000ms     true     1.00 MB  1.75e-39
{code}



I can reproduce this locally. Start at creating the lineitem text table and load files in EC mode:
{code:sql}
create database tpch_ec;
create table tpch_ec.lineitem_text_ec like tpch.lineitem;
{code}
In shell:
{code:java}
hdfs ec -enablePolicy -policy RS-3-2-1024k
hdfs ec -setPolicy -policy RS-3-2-1024k -path hdfs://localhost:20500/test-warehouse/tpch_ec.db
hdfs dfs -cp hdfs://localhost:20500/test-warehouse/tpch.lineitem/* hdfs://localhost:20500/test-warehouse/tpch_ec.db/lineitem_text_ec
{code}
Then the file of {{tpch_ec.lineitem_text_ec}} table will be in EC mode and has {color:#FF0000}*2*{color} block groups.
 Using this table to create the parquet format table and the key_only table:
{code:sql}
refresh tpch_ec.lineitem_text_ec;
create table tpch_ec.lineitem_parq_ec like tpch.lineitem stored as parquet;

set ALLOW_ERASURE_CODED_FILES=true;
insert overwrite tpch_ec.lineitem_parq_ec select * from tpch_ec.lineitem_text_ec;

set PARQUET_PAGE_ROW_COUNT_LIMIT=24000;
drop table if exists tpch_ec.lineitem_orderkey_only;
CREATE TABLE tpch_ec.lineitem_orderkey_only(l_orderkey bigint) sort by (l_orderkey) STORED AS PARQUET;
insert into tpch_ec.lineitem_orderkey_only select l_orderkey from tpch_ec.lineitem_parq_ec;
show files in tpch_ec.lineitem_orderkey_only
+---------------------------------------------------------------------------------------------------------------------------------+--------+-----------+
| Path                                                                                                                            | Size   | Partition |
+---------------------------------------------------------------------------------------------------------------------------------+--------+-----------+
| hdfs://localhost:20500/test-warehouse/tpch_ec.db/lineitem_orderkey_only/de4292af20506645-d5353f3f00000000_464899205_data.0.parq | 4.92MB |           |
| hdfs://localhost:20500/test-warehouse/tpch_ec.db/lineitem_orderkey_only/de4292af20506645-d5353f3f00000001_436827258_data.0.parq | 4.30MB |           |
+---------------------------------------------------------------------------------------------------------------------------------+--------+-----------+
Fetched 2 row(s) in 0.02s
{code}
The {{tpch_ec.lineitem_orderkey_only}} table now has *{color:#FF0000}2{color}* files. Run the query
{code:sql}
SET RUNTIME_FILTER_WAIT_TIME_MS=60000;
SET MINMAX_FILTERING_LEVEL=PAGE;
SET MINMAX_FILTER_THRESHOLD=0.5;
select straight_join a.l_orderkey from
tpch_ec.lineitem_orderkey_only a join [SHUFFLE] tpch_parquet.orders b
where a.l_orderkey = b.o_orderkey
and b.o_custkey = 5 order by l_orderkey;
{code}
In the profile, all "NumRuntimeFilteredPages" are 0. Uploaded the profile.

The bottom line of the test failure is that only 2 nodes are involved in the EC plan and the split is such that in instance 1, the overlap ratio (0.749748) is higher than the threshold (0.5) which implies no min/max filtering at the page and row level. In instance 2, the overlap ration is 0 which implies no min/max filtering. 

Combined, no page level filtering is done and thus the total pages filtered is 0. 

Executor instance 1:

1732 E             RowGroup Debug: Try to filter out a rowgroup via overlap predicate: MonotonicMillis=13987739, fid=1, SchemaNode=optional int64 l_orderkey      [i:0 d:1 r:0], columnType=BIGINT, overlap ratio=0.749748, threshold=0.5, worthiness=0, enabled for page=0, enabled for row=0, data min=1, data max=3209607, content=BigIntMinMaxFilter(min=224167, max=2630562, always_false=false), always_true=false)

1778 E              - NumRuntimeFilteredPages: 0 (0)                                            
1779 E              - NumRuntimeFilteredRowGroups: 0 (0)


Executor instance 2

1919 E             RowGroup Debug: Try to filter out a rowgroup via overlap predicate: MonotonicMillis=13987773, fid=1, SchemaNode=optional int64 l_orderkey      [i:0 d:1 r:0], columnType=BIGINT, overlap ratio=0, threshold=0.5, worthiness=1, enabled for page=1, enabled for row=0, data min=3209632, data max=6000000, content=BigIntMinMaxFilter(min=224167, max=2630562, always_false=false), always_true=false)

1965 E              - NumRuntimeFilteredPages: 0 (0)                                            
1966 E              - NumRuntimeFilteredRowGroups: 1 (1)

Commit fdb6a4e264ad3f156a76c4fc01d4c1b8f9b39189 in impala's branch refs/heads/master from Qifan Chen
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=fdb6a4e ]

IMPALA-10532: TestOverlapMinMaxFilters.test_overlap_min_max_filters seems flaky

This change disables the overlap min/max filter test for hdfs in
erasure coding, due to the query plan change (from 3-node scan to
2-node scan) which splits the row groups among scan nodes differently.

The SkipIfEC class in test harness skip.py is enhanced with a new
skip reason 'different_scan_split' to facilitate this action.

Testing:
  1. Ran unit tests;
  2. Ran core tests.

Change-Id: I527de530f7db1ce959e7ef2ae3ced18677221c9f
Reviewed-on: http://gerrit.cloudera.org:8080/17289
Reviewed-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


