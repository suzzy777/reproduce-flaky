There is also an NPE in the logs concerning the alerting service:

{noformat}
java.lang.NullPointerException
[vm0] 	at org.apache.geode.distributed.internal.InternalDistributedSystem.disconnect(InternalDistributedSystem.java:1629)
[vm0] 	at org.apache.geode.distributed.internal.InternalDistributedSystem.disconnect(InternalDistributedSystem.java:1245)
[vm0] 	at org.apache.geode.distributed.internal.ClusterDistributionManager$DMListener.membershipFailure(ClusterDistributionManager.java:2311)
[vm0] 	at org.apache.geode.distributed.internal.membership.gms.GMSMembership.uncleanShutdown(GMSMembership.java:1283)
[vm0] 	at org.apache.geode.distributed.internal.membership.gms.GMSMembership$ManagerImpl.lambda$forceDisconnect$0(GMSMembership.java:2027)
[vm0] 	at java.lang.Thread.run(Thread.java:748)
{noformat}

This is from this code in ClusterDistributionManager:
{code}
        try {
          removeSystem(this);
          if (!attemptingToReconnect) {
            loggingSession.shutdown();
          }
          alertingService.useAlertMessaging(new NullAlertMessaging());
This line --->>> clusterAlertMessaging.get().close();
          alertingSession.shutdown();
          // Close the config object
          config.close();
        } finally {
          // Finally, mark ourselves as disconnected
          setDisconnected();
          SystemFailure.stopThreads();
        }
{code}

The root cause of the failure mentions loss of a member having the name "myName".  This is from a previously run test.  The test is testPartialIDInMessageReplacedWithFullID.  In that test a member sent a LeaveRequest to the dunit Locator but the locator went on to perform suspect processing on that member afterward and then sent out a view announcing the crash of that member.

{noformat}
  [vm0] [info 2020/01/15 02:48:57.405 GMT <unicast receiver,d777b7e439d8-47277> tid=0x80] received new view: View[172.17.0.21(105:locator)<ec><v0>:41000|13] members: [172.17.0.21(105:locator)<ec><v0>:41000, 172.17.0.21(myName:1)<v12>:41001{lead}, 172.17.0.21(name0:241)<v13>:41001]
  [vm0] old view is: null
...
 [vm0] [info 2020/01/15 02:48:57.888 GMT <unicast receiver,d777b7e439d8-47277> tid=0x80] received new view: View[172.17.0.21(105:locator)<ec><v0>:41000|14] members: [172.17.0.21(105:locator)<ec><v0>:41000, 172.17.0.21(name0:241)<v13>:41001{lead}]  crashed: [172.17.0.21(myName:1)<v12>:41001]
  [vm0] old view is: View[172.17.0.21(105:locator)<ec><v0>:41000|13] members: [172.17.0.21(105:locator)<ec><v0>:41000, 172.17.0.21(myName:1)<v12>:41001{lead}, 172.17.0.21(name0:241)<v13>:41001]

  [vm0] [warn 2020/01/15 02:48:57.889 GMT <unicast receiver,d777b7e439d8-47277> tid=0x80] total weight lost in this view change is 15 of 28.  Quorum has been lost!
{noformat}

So there was something that caused this flaky behavior in the dunit locator.  After receiving a LeaveRequest from the node in the previous test it should not have performed suspect processing on it.

The testTwoMembersSameName test should probably also have enable-network-partition-detection disabled.  It doesn't need it and it would not have failed in this way if it was turned off.

duplicate of GEODE-7705

Saw this twice recently. Here are the log artifacts:

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  Test Results URI =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
http://files.apachegeode-ci.info/builds/apache-develop-pr/geode-pr-5042/test-results/distributedTest/1588912782/
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

Test report artifacts from this job are available at:

http://files.apachegeode-ci.info/builds/apache-develop-pr/geode-pr-5042/test-artifacts/1588912782/distributedtestfiles-geode-pr-5042.tgz


