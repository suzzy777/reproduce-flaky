[~tedyu@apache.org], this looks like test timeout. 780 secs. This is a very long test, I will try to shorten the execution time. 

It takes 235.472 sec on my MBP. 

I removed two steps in a test (incr backup and one of twos bulk loading). It reduced execution time by 15%. That is all I could do now.

Patch is attached. 

Yes, it seems that the reason for a failure is test's time out. I can confirm, that despite the optimization we have in TestBackupBase, HBase cluster starts up and shuts down in every unit test we run because, maven launches new JVM per test class. We need to reuse JVM to shorten execution time. 

Every test class starts cluster, because surefire was configured to run every class in a new JVM. This overhead accounts for ~ 25% of a total execution time (63 min total, 17 min cluster start up time). So it make sense to modify backup tests to be able to share single JVM. 

{quote}Â We need to reuse JVM to shorten execution time.
{quote}

the tests don't reuse JVMs because of a history of stability issues when we did. please test on a branch using the flaky infra to make sure reuse doesn't destabilize things.


For TestIncrementalBackupWithBulkLoad, we can keep the incremental backup in the test and see if the test exposes any race condition.

Thanks

We can configure only backup tests (hbase-backup), so other modules won't be affected. I tried reusing JVM for backup tests. It did not yield any substantial improvement, because now we have to truncate 9 tables before starting new test class and it turned out that this is as expensive as starting new cluster. Instead of reusing JVMs I tried to increase fork count and it worked out much better. With 8 parallel forks the execution time decreased from 63 min to 9 min. But this is for different JIRA:
https://issues.apache.org/jira/browse/HBASE-16458





From https://builds.apache.org/job/HBase%20Nightly/job/master/503//testReport/junit/org.apache.hadoop.hbase.backup/TestIncrementalBackupWithBulkLoad/health_checks___yetus_jdk8_hadoop3_checks___TestIncBackupDeleteTable/ :
{code}
java.io.IOException: Failed copy from hdfs://localhost:40355/user/jenkins/test-data/6d0386bb-d9d8-ae4e-e2bf-c423554ea898/data/default/test-1537528002870/711c2cc8826d7170be6051afbe0b5ca0/f/1afeb42e839a40afa6389ac80c82e1ed_SeqId_205_,hdfs://localhost:40355/user/jenkins/test-data/6d0386bb-d9d8-ae4e-e2bf-c423554ea898/data/default/test-1537528002870/711c2cc8826d7170be6051afbe0b5ca0/f/2087ea2ec81a449ea5d35c405767f970_SeqId_205_ to hdfs://localhost:40355/backupUT/backup_1537528036126
{code}
There were some output truncated.

I searched for 2087ea2ec81a449ea5d35c405767f970_SeqId_205_ in the output but didn't find much clue.

I reproduced test error running against hadoop 3.1.1 :
{code}
[ERROR] TestIncBackupDeleteTable(org.apache.hadoop.hbase.backup.TestIncrementalBackupWithBulkLoad)  Time elapsed: 105.11 s  <<< ERROR!
java.io.IOException: java.io.IOException: Failed copy from hdfs://localhost:41712/user/hbase/test-data/c0f6352c-cf39-bbd1-7d10-57a9c01e7ce9/data/default/test-1539022262249/be1bf5445faddb63e45726410a07917a/f/41b6cb64bae64cbcac47d1fd9aae59f4_SeqId_205_,hdfs://localhost:41712/user/hbase/test-data/c0f6352c-cf39-bbd1-7d10-57a9c01e7ce9/data/default/test-1539022262249/be1bf5445faddb63e45726410a07917a/f/f565f49046b04eecbf8d129eac7a7b88_SeqId_205_ to hdfs://localhost:41712/backupUT/backup_1539022312079
  at org.apache.hadoop.hbase.backup.TestIncrementalBackupWithBulkLoad.TestIncBackupDeleteTable(TestIncrementalBackupWithBulkLoad.java:104)
Caused by: java.io.IOException: Failed copy from hdfs://localhost:41712/user/hbase/test-data/c0f6352c-cf39-bbd1-7d10-57a9c01e7ce9/data/default/test-1539022262249/be1bf5445faddb63e45726410a07917a/f/41b6cb64bae64cbcac47d1fd9aae59f4_SeqId_205_,hdfs://localhost:41712/user/hbase/test-data/c0f6352c-cf39-bbd1-7d10-57a9c01e7ce9/data/default/test-1539022262249/be1bf5445faddb63e45726410a07917a/f/f565f49046b04eecbf8d129eac7a7b88_SeqId_205_ to hdfs://localhost:41712/backupUT/backup_1539022312079
  at org.apache.hadoop.hbase.backup.TestIncrementalBackupWithBulkLoad.TestIncBackupDeleteTable(TestIncrementalBackupWithBulkLoad.java:104)
{code}

[~tedyu@apache.org], any chances to get full log output?

See the attached test output above.

It seems the Warning was due to check during concatenation phase of CopyCommitter.
The concatenation would happen when input file listing is present.



For hadoop 3.1, when multiple files are included in one DistCp session (specified by the listing file), they (the chunks in DistCp's terminology) would be concatenated by CopyCommitter#concatFileChunks .

CopyCommitter#concatFileChunks would throw the following exception when trying to concatenate the two bulk loaded hfiles:
{code}
2018-10-13 14:09:25,351 WARN  [Thread-936] mapred.LocalJobRunner$Job(590): job_local1795473782_0004
java.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@bb8826ee{hdfs://localhost:42796/user/hbase/test-data/       160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_ length = 5100 aclEntries  = null, xAttrs = null} doesnt match prior entry org.apache.hadoop.tools.CopyListingFileStatus@243d544d{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-   2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_ length = 5142 aclEntries = null, xAttrs = null}
  at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)
  at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)
  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)
{code}
But the two bulk loaded hfiles are independent.
This results in -999 being returned by DistCp.

Patch v2 breaks multiple source files into multiple activations of DistCp - one source per activation.

TestIncrementalBackupWithBulkLoad passes with v2 against hadoop 3.1.1

I wouldn't blame distcp here, yet. This hints of a race condition in the distcp setup process: have you kicked off distcp while some of the source files were being written?

I tried to plugin custom OutputFormat :
{code}
+    @Override
+    public Job createAndSubmitJob() throws Exception {
+      Job job = super.createAndSubmitJob();
+      job.setOutputFormatClass(BackupOutputFormat.class);
{code}
But I saw the following error:
{code}
2018-10-15 14:03:21,017 ERROR [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(294): java.lang.IllegalStateException: Job in state RUNNING instead of DEFINE
java.lang.IllegalStateException: Job in state RUNNING instead of DEFINE
  at org.apache.hadoop.mapreduce.Job.ensureState(Job.java:300)
  at org.apache.hadoop.mapreduce.Job.setOutputFormatClass(Job.java:886)
  at org.apache.hadoop.hbase.backup.mapreduce.MapReduceBackupCopyJob$BackupDistCp.createAndSubmitJob(MapReduceBackupCopyJob.java:217)
  at org.apache.hadoop.tools.DistCp.execute(DistCp.java:182)
  at org.apache.hadoop.hbase.backup.mapreduce.MapReduceBackupCopyJob$BackupDistCp.execute(MapReduceBackupCopyJob.java:253)
  at org.apache.hadoop.tools.DistCp.run(DistCp.java:153)
  at org.apache.hadoop.hbase.backup.mapreduce.MapReduceBackupCopyJob.copy(MapReduceBackupCopyJob.java:469)
  at org.apache.hadoop.hbase.backup.impl.IncrementalTableBackupClient.incrementalCopyHFiles(IncrementalTableBackupClient.java:348)
{code}

bq. kicked off distcp while some of the source files were being written?

Here is rough description of how the test is conducted:

* compose hfiles to be bulk loaded
* the hfiles are moved to region dir
* bulk loaded hfiles are recorded in backup table
* perform backup copy

By the time #3 is reached, writing to hfiles has finished (for sometime).

| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 12s{color} | {color:blue} Docker mode activated. {color} |
| {color:blue}0{color} | {color:blue} patch {color} | {color:blue}  0m  2s{color} | {color:blue} The patch file was not named according to hbase's naming conventions. Please see https://yetus.apache.org/documentation/0.8.0/precommit-patchnames for instructions. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:orange}-0{color} | {color:orange} test4tests {color} | {color:orange}  0m  0s{color} | {color:orange} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m  9s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 30s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 14s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 31s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 37s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 15s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 36s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 10m 40s{color} | {color:green} Patch does not cause any errors with Hadoop 2.7.4 or 3.0.0. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 13m  1s{color} | {color:green} hbase-backup in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 11s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 48m 28s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:b002b0b |
| JIRA Issue | HBASE-21149 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12943859/21149.v2.txt |
| Optional Tests |  dupname  asflicense  javac  javadoc  unit  findbugs  shadedjars  hadoopcheck  hbaseanti  checkstyle  compile  |
| uname | Linux d32dd9a0d5a7 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/component/dev-support/hbase-personality.sh |
| git revision | master / 4a043126b1 |
| maven | version: Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T18:33:14Z) |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC3 |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/14705/testReport/ |
| Max. process+thread count | 4198 (vs. ulimit of 10000) |
| modules | C: hbase-backup U: hbase-backup |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/14705/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



{quote}
Patch v2 breaks multiple source files into multiple activations of DistCp - one source per activation.
{quote}

This is going to be *very* slow and inefficient. It is not unusual to have 1000s of bulk loaded files. [~yuzhihong@gmail.com], can you describe what is broken in a current version and how you patch fixes it. Is it hadoop 3.x specific issue?

See my comment above:

https://issues.apache.org/jira/browse/HBASE-21149?focusedCommentId=16649606&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16649606

This behavior of DistCp is for hadoop 3.

The fix is to avoid multiple files in one DistCp session.

I am open to suggestion on how to make the copy more efficient.
I tried with subclassing CopyOutputFormat / CopyCommitter but it seems CopyOutputFormat is hardcoded for DistCp.

Ran the test against hadoop 3.0.1 and got the same error:
{code}
2018-10-16 15:27:51,149 WARN  [Thread-943] mapred.LocalJobRunner$Job(590): job_local1548489402_0004
java.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@be86e9a8{hdfs://localhost:46049/user/hbase/test-data/9c9a54c5-aad7-13d5-bfda-69196b299a84/data/default/test-1539703612116/4d2ff817c98f234940f5db1c3bb1ed4a/f/babac32330014b64bebc546454c3e033_SeqId_205_ length = 5100 aclEntries = null, xAttrs = null} doesnt match prior entry org.apache.hadoop.tools.CopyListingFileStatus@a3de13f6{hdfs://localhost:46049/user/hbase/test-data/9c9a54c5-aad7-13d5-bfda-69196b299a84/data/default/test-1539703612116/4d2ff817c98f234940f5db1c3bb1ed4a/f/94e72f6f8375494fb6def9fdad243b1d_SeqId_205_ length = 5142 aclEntries = null, xAttrs = null}
  at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:269)
  at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:96)
  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)
{code}

{quote}
The fix is to avoid multiple files in one DistCp session.
{quote}

[~yuzhihong@gmail.com], I do not think that this is the fix. DistCp is supposed to copy multiple files and directories at once, not one by one. Either something is deeply broken in DistCp post 3.0 or we have B&R serious issue. We need RCA.

Added more DEBUG log:
{code}
2018-10-16 17:59:05,032 DEBUG [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(317): HdfsNamedFileStatus{path=hdfs://localhost:37318/user/hbase/test-data/f61cbd60-4f59-dea6-e789-213dedb22d55/data/default/test-1539712693301/db94181f831f30089fa1ff6d0973d754/f/d0512abaaf69408b9d0edcb4c3c4cb46_SeqId_205_; isDirectory=false; length=5100; replication=1; blocksize=134217728; modification_time=1539712730259; access_time=1539712729843; owner=hbase; group=supergroup; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}
2018-10-16 17:59:05,033 DEBUG [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(317): HdfsNamedFileStatus{path=hdfs://localhost:37318/user/hbase/test-data/f61cbd60-4f59-dea6-e789-213dedb22d55/data/default/test-1539712693301/db94181f831f30089fa1ff6d0973d754/f/fabb7b1dbe2845ca90c66d4e34987da3_SeqId_205_; isDirectory=false; length=5142; replication=1; blocksize=134217728; modification_time=1539712730686; access_time=1539712730274; owner=hbase; group=supergroup; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}

2018-10-16 17:59:06,222 WARN  [Thread-933] mapred.LocalJobRunner$Job(590): job_local1245512275_0004
java.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@2bf8b9e{hdfs://localhost:37318/user/hbase/test-data/      f61cbd60-4f59-dea6-e789-213dedb22d55/data/default/test-1539712693301/db94181f831f30089fa1ff6d0973d754/f/fabb7b1dbe2845ca90c66d4e34987da3_SeqId_205_ length = 5142           aclEntries = null, xAttrs = null} doesnt match prior entry org.apache.hadoop.tools.CopyListingFileStatus@70f30098{hdfs://localhost:37318/user/hbase/test-data/f61cbd60-4f59-dea6-e789-213dedb22d55/data/default/test-1539712693301/db94181f831f30089fa1ff6d0973d754/f/d0512abaaf69408b9d0edcb4c3c4cb46_SeqId_205_ length = 5100 aclEntries = null,      xAttrs = null}
  at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)
  at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)
  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)
{code}
We can see that the respective length of bulk loaded hfiles didn't change from when they were recorded in the listing file to when the exception was thrown.
We should find a way to skip the concatenation performed by DictCp.

Alternatively, we can merge the bulk loaded hfiles prior to copying - but that would require modifying the backup table which introduces more complexity in terms of maintaining consistency for backup images.

From feedback of two hadoop committers over HADOOP-15850, this seems to be bug in DistCp.

I have attached patch on HADOOP-15850 with which the test passes.

Once HADOOP-15850 is integrated, I will mark this dup of HADOOP-15850.

I upped the priority. Seems critical issue, since it brakes B&R with bulk loaded files completely. Overall, nice work, [~yuzhihong@gmail.com]. We need to expedite the HADOOP fix. 3.0,3.1 currently are not suited for B&R.

Given the severity here, probably worth either reopening this jira or making a B&R jira to update the ref guide section on hadoop versions to avoid. I get that B&R isn't in a shippable branch right now, but we'll lose the thread on things like this if we don't get them down while the failure is fresh.

Logged HBASE-21381 for refguide work.

Ted, should you disable the test for the time being? HADOOP-15850 isn't in any Hadoop releases yet and I haven't seen any one volunteering a Hadoop 3.1.2 release work (where HADOOP-15850 will be in)

The test would pass for hadoop 2 build:
{code}
    <hadoop-two.version>2.7.7</hadoop-two.version>
{code}
I think we can keep the test.
After HADOOP-15850 is in released 3.x version, we can upgrade the hadoop-three.version to catch up.

Yes, Hadoop 2.7/2.8 shouldn't be affected.

