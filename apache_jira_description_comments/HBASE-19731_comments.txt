I wrote a method to loop testCheckAndDeleteWithCompareOp and can make the test fail.

{code}
  @Test
  public void test() throws IOException {
    for (int i = 0; i < 100; i++) {
      testCheckAndDeleteWithCompareOp();
      TEST_UTIL.deleteTable(TableName.valueOf(name.getMethodName()));
    }
  }
{code}

Let me dig more.

[~stack] I think the problem is we assigned the same timestamp twice.

I added a static tsAssigned field in HRegion

{code}
  public static volatile List<Long> tsAssigned;
{code}

And in MutationBatchOperation.prepareMiniBatchOperations, I did this
{code}
      if (!region.getRegionInfo().isMetaRegion() && HRegion.tsAssigned != null) {
        HRegion.tsAssigned.add(timestamp);
      }
{code}

And I also modified the UT
{code}
  @Test
  public void test() throws IOException {
    try {
      for (int i = 0; i < 100; i++) {
        testCheckAndDeleteWithCompareOp();
        TEST_UTIL.deleteTable(TableName.valueOf(name.getMethodName()));
        HRegion.tsAssigned = null;
      }
    } catch (AssertionError e) {
      HRegion.tsAssigned.forEach(System.out::println);
      throw e;
    }
  }
{code}

Notice that I will create HRegion.tsAssigned in testCheckAndDeleteWithCompareOp after the creation of the test table.

And finally I got this output
{noformat}
1515397552529
1515397552533
1515397552535
1515397552537
1515397552539
1515397552541
1515397552543
1515397552546
1515397552547
1515397552548
1515397552549
1515397552550
1515397552551
1515397552554
1515397552555
1515397552556
1515397552556
{noformat}

You can see that the test fails immediately after we issue the same ts again.

This means we are doing faster mutation for beta1 so it is more easier to run into this situation? Maybe a good news...

Implement a NonRepeatedEnvironmentEdge which will never return the same time twice and inject it in TestFromClientSide.

Looped testCheckAndDeleteWithCompareOp 100 times, passed locally for me.

[~stack] Do you think we need to make this logic default? I mean, implement the same logic in each region, which will not assign the same timestamp twice?

This is great [~Apache9]. I've been running tests to see if this new since alpha4.... We used to have a means of not ensuring same ts on update.... Let me find that too. Will be back. Nice work sir.

Patch fixes the test for sure. +1.

On making this test timestamper default, we can't, right? Proper fix is HLC. Failing that, a timestamper like that in patch would be a limit of about 1k ops a second? And the checkAndSet for time is costly? We'd have to be parsimonious about checking time (currently we do it all over the code base w/o regard for cost).

It looks like the test fails in same place in alpha-4 so my thought that it new to beta-1 doesn't hold. Makes sense. I don't see it in the general flakies list: https://builds.apache.org/job/HBASE-Find-Flaky-Tests/lastSuccessfulBuild/artifact/dashboard.html  probably because apache jenkins is slow overall... slower than my local machine or JMS's (or yours).

Thanks for jumping in here [~Apache9] and confirming speculation on root issue (would have taken me way longer to figure...)



Pushed to master and branch-2. Resovling. [~Apache9] we can keep chatting on timestamping here or over in HBASE-14070 ... I'm all ears if you have idea for doing something in meantime till we deploy HLC...

FAILURE: Integrated in Jenkins build HBase-Trunk_matrix #4365 (See [https://builds.apache.org/job/HBase-Trunk_matrix/4365/])
HBASE-19731 TestFromClientSide#testCheckAndDeleteWithCompareOp and (stack: rev 2509a150c0792e914429264453510b9028250c29)
* (add) hbase-common/src/test/java/org/apache/hadoop/hbase/util/NonRepeatedEnvironmentEdge.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java


{quote}
Failing that, a timestamper like that in patch would be a limit of about 1k ops a second?
{quote}
It is region level, and is only for write, so 1k ops per second is fast enough? No? Since the problem only appears on row level, we can simply do a sharding? For example, using 1024 AtomicLongs, then the qps limit can be up to 1M.

What do you think [~stack]?

Thanks.

bq. What do you think stack?

Each edit having its own unique timestamp would solve a bunch of mysterious issues we see in tests but also in prod.

Your suggestion is nice and straightforward.

I hate having a running counter already that is close to what is needed here -- MVCC -- and then a system that is almost done -- HLC -- that would fix this issue in a manner that could be depended upon cluster-wide never mind inside region-scope only. HLC is so close... 

What you think [~Apache9]


So what is the plan for HLC? In which version will it land? If it is not too far away I think it is OK to keep the code as is.

bq. So what is the plan for HLC? In which version will it land?

Unfortunately no one working on it at mo. Was stalled by need to improve perf (crossing a synchronize getting unique timestamp) and then our intern who was working on it has moved on.

OK, then I think it is worth to make a temporary work around first. Will open a new issue for it.

