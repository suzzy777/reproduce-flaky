Why P2 and not P1?

Because there were several non-failing runs of the same test at the same commit.

My feeling is that all crashes are P1s, but I'm actually not sure if that reflects the way previous flaky crash issues are prioritized. I imagine that when you have filed flaky crashes from stress tests in the past, you have filed them at P2 and they have remained at that priority for their entire "Open" lifetime?

In the past, some stress crashes initially filed as P1s were then found to be flaky and downgraded to P2. I do not have any examples.

Note: I hit IMPALA-4226 when dealing with the minidump associated with the crash.

This crash happened again. [~dhecht], is P2 still a fair assessment? Should this be raised to P1? It's happened 2 of 7 runs so far.

[~dhecht] please take a look and re-assign/re-prioritize as necessary. This happened again over night.

Generally, we want to treat all crashes as P1s, so let's start them there and downgrade only if there is a good reason to do so (e.g. unsupported configuration).

Apparently {{Coordinator::TearDown()}} has already been called.

{code}
(gdb) f 9
#9  0x000000000199f596 in impala::Coordinator::GetNext (this=0x7fa7e9eb9b00, results=0x7fa193cef840, max_rows=1024, eos=0x7fa13e9b0f49) at /usr/src/debug/impala-2.7.0-cdh5.11.0-SNAPSHOT/be/src/runtime/coordinator.cc:1110
1110    in /usr/src/debug/impala-2.7.0-cdh5.11.0-SNAPSHOT/be/src/runtime/coordinator.cc
(gdb) p query_state_
$18 = (impala::QueryState *) 0x0
(gdb) p coord_sink_
$19 = (impala::PlanRootSink *) 0x0
(gdb) p torn_down_
$20 = true
{code}


I think it's likely a regression from this commit:
{code}
commit 85edc15fefe9ab9b2883b9c9f7451efc7e26ff94
Author: Tim Armstrong <tarmstrong@cloudera.com>
Date:   Tue Dec 20 09:02:31 2016 -0800
{code}

{{TearDown()}} is called from QueryExecState::Done() called from ImpalaServer::UnregisterQuery() called from CloseOperation(). I don't think we have any synchronization to make sure a Fetch RPC isn't still in progress at that point.

We've probably only seen this with COMPUTE STATS because sane clients wouldn't execute CloseOperation() and FetchResults() RPCs concurrently, but the ChildQuery callers do that.

FWIW, I've just seen a similar regression from that commit where {{filter_mem_tracker_}} is referred to in an RPC handler, but gets set to {{nullptr}} without synchronisation in {{Coordinator::TearDown()}}. That's on my KRPC branch, so not a bug in trunk, but it has exactly the same 'shape' as the issue here.

It also doesn't help that we have this nullptr checks which probably help hide this race.
{code}
RuntimeState* Coordinator::runtime_state() {
  return executor() == NULL ? NULL : executor()->runtime_state();
}
{code}

[~henryr], which RPC handler was it in your case?

{{UpdateFilter}}. But like I say, that on a branch with some heavy changes in that path, I'm not sure the race can happen on trunk (by luck more than judgment).

Yeah, just wanted to check.  On master, it does look like that {{FilterState::Disable()}} and {{filter_lock_}} would act as enough of a barrier to prevent the bug with {{filter_mem_tracker_}}.

Note: the stress test has been running nightly, but this crash has not been reproduced since Feb 11. This bug has been found a total of 4 times since Feb 6.

[~mikesbrown], [~marcelk], [~dhecht], what's the latest here?  Do we think this is fixed?  Can it affect 2.8 or earlier versions or have we ever seen it only on master after 2.8 was cut?

[~srus] The earliest I've seen this is master (future 2.9), on commit {{IMPALA-4876: Remove _test suffix from test names}}.

[~mikesbrown], have you seen this since?  If not, does it make sense to close as Cannot Repro?

[~srus], it shouldn't be closed. It's a real bug, has already been diagnosed, and needs to be fixed for the next release. It's not in 2.8.

What [~dhecht] said.

[~marcelk], are you planning to work on this for 2.9?

I am.

A variant of this happened in TestCancellationSerial.test_cancel_insert with query "compute stats lineitem" on asf master core. Verified that the core shows the state that Dan described above (query_state_ = null, coord_sink_ = null, torn_down_ = true). I uploaded the core to impala-desktop under the IMPALA-4890/impala_asf_master_core_1884 directory.

I can say the stress test hasn't hit this since approximately the end of March.

Got a new instance of this via the stress test. I'll report whether I can get it to reproduce more readily.

commit fcc3b9eded82953c04fd7510d724a2c9b7ff59a3
Author: Marcel Kornacker <marcel@cloudera.com>
Date:   Sun May 14 21:32:41 2017 -0700

    IMPALA-4890/5143: Coordinator race involving TearDown()
    
    TearDown() releases resources and destroys control
    structures (the QueryState reference), and it can be called
    while a concurrent thread executes Exec() or might call
    GetNext() in the future. The solution is not to destroy
    the control structures.
    
    This also releases resources automatically at the end
    of query execution.
    
    Change-Id: I457a6424a0255c137336c4bc01a6e7ed830d18c7


