I accidentally expired the AWS access keys which guard the S3 bucket used for testing.
Now keys are present (the condition which guards the tests), but the keys are not valid, which makes the tests fail.

Have a patch coming up introducing new encrypted access credentials...

GitHub user StephanEwen opened a pull request:

    https://github.com/apache/flink/pull/4892

     [FLINK-7905] [build] Update encrypted Travis S3 access keys

    This fixes the currently failing tests for S3 file systems by adding proper encrypted access credentials.
    
    The current credentials were deactivated.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/StephanEwen/incubator-flink hotfixes

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/4892.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4892
    
----
commit e78c345baad8ceace64815a46e2d9f918db12cc0
Author: Stephan Ewen <sewen@apache.org>
Date:   2017-10-15T18:54:01Z

    [hotfix] [tests] Increase stability of SavepointITCase

commit 5aafc1c11e337a7523852f046dbbda0804d599c2
Author: Stephan Ewen <sewen@apache.org>
Date:   2017-10-23T16:50:03Z

    [FLINK-7905] [build] Update encrypted Travis S3 access keys

----


[~sewen] This is now resolved, right?

Github user aljoscha commented on the issue:

    https://github.com/apache/flink/pull/4892
  
    @StephanEwen Can you please close the PR now that it's merged?


Github user StephanEwen commented on the issue:

    https://github.com/apache/flink/pull/4892
  
    Merged in 9b73a5a49812c206bf8c94e5e4c6f616b6d858b7


Github user StephanEwen closed the pull request at:

    https://github.com/apache/flink/pull/4892


Fixed in 9b73a5a49812c206bf8c94e5e4c6f616b6d858b7

This just occurred again with the latest master: https://travis-ci.org/zentol/flink/builds/292549997

{code}
Running org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase
Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 3.1 sec <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase
testDirectoryListing(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase)  Time elapsed: 0.257 sec  <<< ERROR!
java.nio.file.AccessDeniedException: s3://[secure]/tests-3d000b6f-1118-4f61-91a3-57f893fdd654/testdir: getFileStatus on s3://[secure]/tests-3d000b6f-1118-4f61-91a3-57f893fdd654/testdir: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 143C577462F68C24), S3 Extended Request ID: u96bfOdpa2cjC7DcrVLiS6Qol/5Y3gz1RlbBwUp85eG51PwtwWqEJy/r8Cy5iLF3u2hyMfJLx44=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1579)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1249)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1030)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:742)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:716)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4194)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4141)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1256)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1232)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.getFileStatus(HadoopFileSystem.java:77)
	at org.apache.flink.core.fs.FileSystem.exists(FileSystem.java:509)
	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.testDirectoryListing(HadoopS3FileSystemITCase.java:163)

testSimpleFileWriteAndRead(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase)  Time elapsed: 0.206 sec  <<< ERROR!
java.nio.file.AccessDeniedException: s3://[secure]/tests-3d000b6f-1118-4f61-91a3-57f893fdd654/test.txt: getFileStatus on s3://[secure]/tests-3d000b6f-1118-4f61-91a3-57f893fdd654/test.txt: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 24F178CAC9F6A2C4), S3 Extended Request ID: W8MWwBlF3UkvXw7VhuXGSiruZe+U/Gf/BpHPEMPr6/2ZLvKWfk/AXWthpkECcj35DYUAuV2l/HQ=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1579)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1249)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1030)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:742)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:716)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4194)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4141)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1256)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1232)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.delete(S3AFileSystem.java:1234)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.delete(HadoopFileSystem.java:134)
	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.testSimpleFileWriteAndRead(HadoopS3FileSystemITCase.java:147)
{code}

[~chesnay] did you remove your S3 tokens from your Travis config?

I didn't do anything to the config.

Then you have to (if you previously had S3 credentials set up). Here {{https://travis-ci.org/<your name>/flink/settings}} I had some S3 keys under "Environment Variables".

I'll try that and report back.

