Here are two logs that exhibit the problem. Just one repro took about 200 runs of TestRenameTableSync in ASAN, though I'm sure it can reproduce in other build types too.

Another point here is that it's probably not just an issue for alter table. If a client tried to read a table after a similar scenario, it would also get an error that there is no leader on the tablet, right? Or would our client "speculative guess for leader" stuff make it work OK?

bq. Another point here is that it's probably not just an issue for alter table. If a client tried to read a table after a similar scenario, it would also get an error that there is no leader on the tablet, right? Or would our client "speculative guess for leader" stuff make it work OK?

The "sketchy logic" in KUDU-759 works around that particular issue.

I've suggested another solution in the [multi-master design doc|http://gerrit.cloudera.org:8080/#/c/2527]: actually rebuild the per-tablet replica information from hard state when a new leader master is elected. It is this cache that is used in step 5 to figure out where to send the AlterTablet RPC. I think this is the best approach, because it acknowledges the fact that tablet replica information _is_ replicated, but not used on failover.

I'm changing the title of the JIRA to be more generic, as there's another related issue here: because the replica soft state isn't rebuilt on master failover,  DeleteTable() requests will succeed but won't delete any tablets from tservers until those tablets are present in tablet reports (i.e. via tablet role change).

Adar told me I can move it to 0.9.0.

test comment

Fixed in commit 5f7823f.

It was actually fixed by removing the soft state altogether, as we didn't see that much of a use for it.

