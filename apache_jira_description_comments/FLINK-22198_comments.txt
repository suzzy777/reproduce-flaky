https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17142&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6228

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17956&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7903

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18526&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6316

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18492&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6607

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18701&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6228

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18754&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7631

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18754&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=53f6305f-55e6-561c-8f1e-3a1dde2c77df&l=6413

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18894&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6646

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18941&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6592

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18941&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6680

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18952&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6680

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19085&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6625

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19084&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6701

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19153&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6316

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19154&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7903

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19438&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6261

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19563&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6625

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20034&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6900

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20035&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7543

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20120&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7469

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20122&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=e8fcc430-213e-5cce-59d4-6942acf09121&l=6551

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20269&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6941

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20285&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=53f6305f-55e6-561c-8f1e-3a1dde2c77df&l=6849

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20346&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=e8fcc430-213e-5cce-59d4-6942acf09121&l=6564

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20455&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7081

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20456&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6857

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20523&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6638

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20588&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6636

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20619&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=7917

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20710&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7861

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20751&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6857

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20751&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=53f6305f-55e6-561c-8f1e-3a1dde2c77df&l=6890

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20905&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6636

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20926&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7081


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20997&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6547

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21130&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6272

I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it "stale-blocker". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21229&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6548

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21257&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6620

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21557&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6272

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21642&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7218

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21644&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=e8fcc430-213e-5cce-59d4-6942acf09121&l=6569

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21933&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7081

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21932&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=6927

[~renqs], [~jqin]
Could you help look into this? It seems recent instances are no longer caused by the Testcontainers framework.

I checked the log of latest 2 failure cases. I think this case has the same cause as FLINK-22416 as described in my comment. 

I think we need more information for debugging, since Kafka is running in docker so I have no clue about what happened on broker side. I can submit a patch to log more information for debugging if the test fails.

Thanks, [~renqs]. 
Adding more logs sounds good to me. I'm assigning you to this ticket for the moment.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22180&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7113

This is another instance from my own Azure pipeline, which proved my assumption before in FLINK-22146: [https://dev.azure.com/renqs/Apache%20Flink/_build/results?buildId=65&view=logs&j=d543d572-9428-5803-a30c-e8e09bf70915&t=4e4199a3-fbbb-5d5b-a2be-802955ffb013]

Basically I add a timer to print Kafka topic partition status if the test hangs for 5 minutes. And the log showed that the starting offset of the partition is not 0, so test hanged because it couldn't receive expected messages with offset 0-3.  
{code:java}
19:54:22,603 [ Debug Logging Timer] INFO  org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase [] - TopicPartition "key_partial_value_topic_avro-0": starting offset: 4, stopping offset: 4
{code}
I'm still investigating why this could happen on broker side. Here is a PR for redirecting KafkaContainer's output to log4j and printing log if test hangs: [https://github.com/apache/flink/pull/16860] 

 

The blocked stack of this issue is the same with FLINK-22387, I think they have same root cause. Thanks [~renqs] for the investigation. 

{code}
"main" #1 prio=5 os_prio=0 tid=0x00007fc90000b000 nid=0x47de waiting on condition [0x00007fc90a4a2000]
Aug 14 11:06:02    java.lang.Thread.State: TIMED_WAITING (sleeping)
Aug 14 11:06:02 	at java.lang.Thread.sleep(Native Method)
Aug 14 11:06:02 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sleepBeforeRetry(CollectResultFetcher.java:237)
Aug 14 11:06:02 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:113)
Aug 14 11:06:02 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Aug 14 11:06:02 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Aug 14 11:06:02 	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
Aug 14 11:06:02 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.collectRows(KafkaTableTestUtils.java:52)
Aug 14 11:06:02 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.testKafkaSourceSinkWithKeyAndFullValue(KafkaTableITCase.java:485)
Aug 14 11:06:02 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 14 11:06:02 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 14 11:06:02 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 14 11:06:02 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 14 11:06:02 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 14 11:06:02 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 14 11:06:02 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 14 11:06:02 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 14 11:06:02 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 14 11:06:02 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
{code}



https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22396&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7380

Add logs in master e8bd5e7875f7ef02fb138385f8774bbbff6ad989

I have taken a look at the test code and related logs (not including the Kafka Server log). 

Here are the findings:
- According to the test code and the log, 5 minutes after the test starts, the logStartOffset=4 and the logEndOffset=4.
- According to the Kafka Source code, the logStartOffset will increase from 0 to 4 only in the following cases:
  - The Admin::deleteRecords(...) is invoked by test code
  - The log rolling happens because the current_time - timestamp_of_produced_records >= retention_time
  - The log rolling happens because the size of the log > max_log_segment_size.

My hypothesis is that the log rolling happens because the current_time - timestamp_of_produced_records >= retention_time. This might happen if the clock on the host that produces messages is several days earlier than the clock on the host that runs the Kafka Server container. In general this difference in clock is unlikely, because I find it even less likely to believe other reasons that could have causes this logStartOffset=4.

In order to validate the hypothesis and move forward in the debugging, we have updated Flink code to get Kafka Server info level logging and will check the following:
- What is the timestamp of the producer and Kafka Server in the log?
- Does Kafka Server actually roll the log, and what even has triggered the log rolling?






https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22447&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7121

[~renqs] [~lindong]
I think the latest instance occurred after the log changes being merged. Could you help take another look?


[~renqs],
BTW, please move the ticket to in-progress as we're investigating it.

Thanks [~xtsong] for the reminder. I had a discussion with [~lindong] and we checked the log of the latest instance. These logs caught our attention: 

 
{code:java}
13:27:11,089 INFO [Log partition=key_full_value_topic_avro-0, dir=/var/lib/kafka/data] Found deletable segments with base offsets [0] due to retention time 604800000ms breach (kafka.log.Log)
13:27:11,101 INFO [ProducerStateManager partition=key_full_value_topic_avro-0] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
13:27:11,104 INFO [Log partition=key_full_value_topic_avro-0, dir=/var/lib/kafka/data] Rolled new log segment at offset 3 in 15 ms. (kafka.log.Log)
13:27:11,106 INFO [Log partition=key_full_value_topic_avro-0, dir=/var/lib/kafka/data] Scheduling segments for deletion List(LogSegment(baseOffset=0, size=233, lastModifiedTime=1629293231000, largestTime=1583845931123)) (kafka.log.Log)
13:27:11,107 INFO [Log partition=key_full_value_topic_avro-0, dir=/var/lib/kafka/data] Incrementing log start offset to 3 (kafka.log.Log)
{code}
Basically a retention time based log deletion was triggered, so messages written to Kafka is deleted, then the test case hanged because it cannot receive expected messages. These logs proves the hypothesis made by [~lindong] that the clock is skewed on producer side. We are still investigating the reason and will update the comment once we have progress. 

 

I don't have the permission to change the state of this ticket. [~xtsong] Could you help to change it to {{in-progress}}? Thanks~ 

Thanks for the updates.

bq. I don't have the permission to change the state of this ticket. Xintong Song Could you help to change it to in-progress? Thanks~ 

Only the assignee can do it. There is a problem in jira that a `user` can be assigned to the ticket but cannot move it to in-progress, unless he/she is a `contributor`. I've grant you the `contributor` permission. You should be able to do it now.


Here are my findings regarding why the timestamp of the produced record is more than 7 days earlier than the test time:

1) KafkaWriter will explicitly assign the timestamp to the produced record, rather than using the system current time for the produced request [1].
2) The timestamp could be from the StreamRecord that fed to the Input::processElement(..) method, which in turn is determined by the caller of those method (e.g. SQL related logic).
3) The code used to produce records in the testSourceSinkWithKeyAndPartialValue() explicitly assigns some timestamp with date such as 2020-03-08. This seems to be the only place where a non-current-system-time timestamp could be provided.

My hypothesis is this:
- The timestamp used in the test (e.g. 2020-03-08) has been used as the timestamp of the records sent to Kafka.
- The test will fail if and only if Kafka Server happens to trigger log retention logic (which is triggered at regularly time interval) after the records are produced but before the records have been consumed. This explains why the test is flaky.

I would suggest to involve the developer who knows the expected behavior of the SQL/Table API tested by this test to validate the hypothesis described above.

[~xtsong] [~renqs] Do you know who can help answer the SQL related questions?

[1] https://github.com/apache/flink/blob/6c9818323b41a84137c52822d2993df788dbc9bb/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java#L138


Thanks [~lindong] for the investigation! Yes there are indeed some Kafka table test cases explicitly assigning record timestamp, such as this one[1]. These cases tests metadata column feature of Kafka sink table so I think we have to keep them. 

I would suggest to disable log deletion by setting Kafka broker conf {{log.retention.ms}} to -1 in KafkaTableTestBase. This will prevent records from being delete. I can submit a patch to fix this. 

[1] [https://github.com/apache/flink/blob/706cd677888312d2f71b498bf02447c750634d52/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaTableITCase.java#L278]

Thanks for the investigation, [~lindong] and [~renqs].

[~renqs], I think disabling log deletion is indeed one way to go. As a non-expert in Kafka, my question is would our IT cases may fail to capture potential problems that only happens when there's a regular log deletion, which IIUC is the default Kafka behavior.

[~twalthr], [~fsk119], what do you think as the original authors?

What's the default value of {{log.retention.ms}}?  

[~jark] The default value is 7 days. The problem is that some cases hard-coded the record timestamp (e.g. 2020-03-08, which is definitely beyond the retention timeout), so it's possible that these records are deleted by Kafka broker after being produced but before being consumed as [~lindong] mentioned. 

Maybe a better solution is using current timestamp in tests instead of hard-coding one. Changing the {{log.retention.ms}} indeed changes the default behavior of Kafka. Thanks [~xtsong] and [~lindong] for the feedback.

Thanks for the investigation:).

In the UpsertKafkaTableCase, we use the hard-coding timestamp to simplify the test(e.g. testTemporalJoin). I think it's okay to just turn off the log retention because users will not send out-of-date records in the production environment and they will not meet the problem.

Hey [~fsk119], do you think it would be simpler to just change the timestamp used in the test to e.g. year 2030?

I personally think this approach is better than disabling the log retention time. The reason is that we in general prefer the test to simulate the production environment, which do have the log retention in Kafka. I share the concern with Xintong that, there might be potential problems that only happens when there's a regular log deletion.







https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22577&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=7123

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22620&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=e8fcc430-213e-5cce-59d4-6942acf09121&l=6569

Sounds great. I think it's much better than turn off the log retention =-=. 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22686&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=e8fcc430-213e-5cce-59d4-6942acf09121&l=6569

[~xtsong] [~renqs][~lindong], personally, I'm -1 to change the test data because

1) it's hard to maintain, it's very trick to know which timestamp data should use a future time. See the pull request changes 136 lines, but there still some other timestamp not changed, not sure whether it changes all the necessary places. 
2) it may still broken in the future, e.g. some one add a test but not use a long future timestamp (the test won't broken in a long time, and we need another round to investigate the problem when it fails). 
3) I don't see big difference between changing the timestamp data and disabling log retention time. Is it common to insert 100 years later timestamp in production environment? 
4) It seems we also changed some default configs for kafka tests, e.g. {{enable.auto.commit}}, {{auto.offset.reset}} in {{KafkaSinkITCase}} and {{KafkaWriterITCase}}.

[~jark] Thanks for pointing this out! To be honest I have no preference on these two options. Both of them have their pros and cons. I would suggest that we leave a descriptive comment around the change to make it easy to maintain for other developers.

Fixed in master: 7814ee257b526d52f80a17143e228fb936b03ff5

Please reopen it if the problem happens again.

Instance on 1.13:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22920&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6855

I've reopened the ticket and downgraded it to Critical. Please back port the fix to 1.12/1.13.

Fixed in 
 - release-1.13: 96f7d900d64c49faaec59fa970e519aedae5958d

The docker-based test is introduced in 1.13, so we may need another pull request for release-1.12. Could you help for that [~renqs]?

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22983&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=53f6305f-55e6-561c-8f1e-3a1dde2c77df&l=6567

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23441&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6551

Fixed in 
 - release-1.12: b9dc04eaf2b28b85b19fd3f53f8358a7947262c7

