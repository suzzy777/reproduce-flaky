The stacktrace is always seen for the test even when test passes.

The stacktrace shows that the file schema and requested schema differs while reading parquet. FileSchema is the schema from parquet file and requested schema is schema during read.

File schema:

 
{code:java}
message spark_schema {
optional binary _hoodie_commit_time (STRING);
optional binary _hoodie_commit_seqno (STRING);
optional binary _hoodie_record_key (STRING);
optional binary _hoodie_partition_path (STRING);
optional binary _hoodie_file_name (STRING);
required int64 timestamp;
required binary _row_key (STRING);
required binary partition_path (STRING);
required binary trip_type (STRING);
required binary rider (STRING);
required binary driver (STRING);
required double begin_lat;
required double begin_lon;
required double end_lat;
required double end_lon;
required int32 distance_in_meters;
required int64 seconds_since_epoch;
required float weight;
required binary nation;
required int32 current_date (DATE);
required int64 current_ts;
required int64 height (DECIMAL(10,6));
required group city_to_state (MAP) {
repeated group key_value
{ required binary key (STRING); required binary value (STRING); }
}
required group fare { required double amount; required binary currency (STRING); }
required group tip_history (LIST) {
repeated group list {
required group element { required double amount; required binary currency (STRING); }
}
}
required boolean _hoodie_is_deleted;
required double haversine_distance;
}{code}
 


Requested schema:
{code:java}
message triprec {
optional binary _hoodie_commit_time (STRING);
optional binary _hoodie_commit_seqno (STRING);
optional binary _hoodie_record_key (STRING);
optional binary _hoodie_partition_path (STRING);
optional binary _hoodie_file_name (STRING);
required int64 timestamp;
required binary _row_key (STRING);
required binary partition_path (STRING);
required binary trip_type (STRING);
required binary rider (STRING);
required binary driver (STRING);
required double begin_lat;
required double begin_lon;
required double end_lat;
required double end_lon;
required int32 distance_in_meters;
required int64 seconds_since_epoch;
required float weight;
required binary nation;
required int32 current_date (DATE);
required int64 current_ts;
required fixed_len_byte_array(5) height (DECIMAL(10,6));
required group city_to_state (MAP) {
repeated group key_value { required binary key (STRING); required binary value (STRING); }
}
required group fare
{ required double amount; required binary currency (STRING); }
required group tip_history (LIST) {
repeated group list {
required group element
{ required double amount; required binary currency (STRING); }
}
}
required boolean _hoodie_is_deleted;
required double haversine_distance;
}
{code}
 

`org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport#init`

creates a new parquet schema after converting the struct ype fields for 
(DECIMAL(10,6)
but org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport#init doesn't do a similar coversion while reading leading to read error.

