This also seems to have caused a postcommit failure:

https://builds.apache.org/job/beam_PostCommit_MavenVerify/1567/

From my experience those failures happen when a "bad" context is reused - since the context is reused in Spark tests, if it failed and closed for some reason, it will try to keep reusing that closed context, and that's how it manifests.
I fixed this while working on BEAM-259, this is the fix: https://github.com/apache/incubator-beam/pull/1055/files#diff-cdc52a99e44ad1b6d1950ecab0928ecd.
I could patch this up as part of this ticket, because BEAM-259 is still in review, but it will be tomorrow morning (IDT).

[~kenn] do you think PR#1055 will be ready for merge soon ?

I would generally fix such issue immediately, but it's flakey, only happens if something failed (unexpectedly) and will cause a "domino effect" on following tests, so I might wait for PR#1055 if we can merge it soon.


Merging BEAM-259 which should resolve this, I'll wait a day or two to see it does and then close this ticket as duplicate.

Looks like it has not been resolved yet. There are 3 more flakes today that include the fix referenced in BEAM-259. 

https://builds.apache.org/job/beam_PostCommit_MavenVerify/1586/
https://builds.apache.org/job/beam_PostCommit_MavenVerify/1587/
https://builds.apache.org/job/beam_PostCommit_MavenVerify/1588/

Can you take another look?

Yeah I've noticed, if you look closely you'll see that it's not NPE anymore.
The problem seems to be Jenkins being horribly slow; Spark streaming tests have a timeout, and if nothing was processed until a hard timeout was reached, it'll rightfully fail.

I'll change the ticket accordingly.

I can set a graceful stop - meaning that the test will finish any currently processing batches - but that'll make those tests run X2 or even X3 longer most of the time.

Thoughts ?


Got this ready: https://github.com/amitsela/incubator-beam/tree/BEAM-769 but it might require further attention after https://github.com/apache/incubator-beam/pull/1143 is merged so I'll hold off on it.

I would prefer a bigger timeout over having flaky tests if we couldn't make it deterministic in some way. If a test never flakes, people won't have to look at it.

[~lcwik] I agree, I'll commit and update https://github.com/apache/incubator-beam/pull/1143 accordingly.
Since this is hard to replicate locally, it's best to give it a try on Jenkins ASAP.

GitHub user amitsela opened a pull request:

    https://github.com/apache/incubator-beam/pull/1161

    [BEAM-769] Spark streaming tests fail on "nothing processed" if runti…

    Be sure to do all of the following to help us incorporate your contribution
    quickly and easily:
    
     - [ ] Make sure the PR title is formatted like:
       `[BEAM-<Jira issue #>] Description of pull request`
     - [ ] Make sure tests pass via `mvn clean verify`. (Even better, enable
           Travis-CI on your fork and ensure the whole test matrix passes).
     - [ ] Replace `<Jira issue #>` in the title with the actual Jira issue
           number, if there is one.
     - [ ] If this contribution is large, please file an Apache
           [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.txt).
    
    ---
    
    …me env. is slow because timeout
    
    is hit before processing is done.
    
    Make graceful stop the default.
    
    Keep "pumping-in" the last batch in a mocked stream to handle overflowing batches in case of a
    graceful stop.
    
    Change tests accordingly.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/amitsela/incubator-beam BEAM-769

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-beam/pull/1161.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1161
    
----
commit 43c9e57ad8c7af022ba2f46ce4b8d20731f0766e
Author: Sela <ansela@paypal.com>
Date:   2016-10-20T22:20:33Z

    [BEAM-769] Spark streaming tests fail on "nothing processed" if runtime env. is slow because timeout
    is hit before processing is done.
    
    Make graceful stop the default.
    
    Keep "pumping-in" the last batch in a mocked stream to handle overflowing batches in case of a
    graceful stop.
    
    Change tests accordingly.

----


Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-beam/pull/1161


This is generally resolved.
Flakes in streaming tests could still occur, but providing a truly robust solution is a part of implementing triggers.

