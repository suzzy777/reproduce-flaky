Haven't found the root cause yet but revert HBASE-25282 could get TestRSGroupsFallback pass. [~yuqi] [~zghao] PTAL?

Thanks.

[~zhangduo] Sorry to introduce this problem, i will figure out the cause and fix is as soon as possible

Results for branch master
	[build #146 on builds.a.o|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/master/146/]: (/) *{color:green}+1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/master/146/General_20Nightly_20Build_20Report/]






(/) {color:green}+1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/master/146/JDK8_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(/) {color:green}+1 jdk11 hadoop3 checks{color}
-- For more information [see jdk11 report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/master/146/JDK11_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


Don't know if it helps but this test fails for me on branch-2 sometimes (not just master). Thanks.

Thanks for pointing out this problem, [~stack].

Maybe this PR can be divided to two parts, one is for the UT itseft, the other is for fixing HBASE-25282.

Merged to master.

Thanks [~Xiaolin Ha] for contributing.

Keep the issue open for a while to see if the flaky dashboard can go back to normal.

Results for branch master
	[build #155 on builds.a.o|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/master/155/]: (/) *{color:green}+1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/master/155/General_20Nightly_20Build_20Report/]






(/) {color:green}+1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/master/155/JDK8_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(/) {color:green}+1 jdk11 hadoop3 checks{color}
-- For more information [see jdk11 report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/master/155/JDK11_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


Any chance of a version for banch-2 [~Xiaolin Ha] ? I see that it is failing there w/ some regularity. Is it the same issue? See here https://ci-hadoop.apache.org/view/HBase/job/HBase/job/HBase-Find-Flaky-Tests/job/branch-2.4/lastSuccessfulBuild/artifact/output/dashboard.html  I'm trying a hacked version of your patch as a backport locally in the meantime. Thanks.

Interesting perhaps is that this test does not show as flakey on branch-2 https://ci-hadoop.apache.org/view/HBase/job/HBase/job/HBase-Find-Flaky-Tests/job/branch-2/lastSuccessfulBuild/artifact/output/dashboard.html. There is just the one failure on branch-2.4 (but it fails regularly for me in local runs)

I looked through the branch-2 and branch-2.4 failed dashboard, but cannot find any useful running logs for the failed TestRSGroupsFallback.testFallback.

!image-2020-12-13-10-15-55-445.png|width=665,height=159!

Is it failed at assertRegionsInGroup() after balance()? If so, maybe because waiting time is less than region correction time.

The async and throttle in balance() making region movement intermittent, as a result, just waiting for TEST_UTIL.waitUntilAllRegionsAssigned is not enough.

 

 

Looks like I did a bad job backporting... this test still fails locally for me here...

[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   TestRSGroupsFallback.testFallback:98->assertRegionsInGroup:121->lambda$assertRegionsInGroup$0:124


I run branch-2 TestRSGroupsFallback.testFallback locally, it failed occasionally.

I added an assertion to verify my idea for balancer, it failed as I thought.

!1607918235175-image.png|width=569,height=281!

I think it fails root cause maybe that, the RSGroupBasedLoadBalaner corrects table assignments by group info which is cached, but when a server is online, the cached is updated by a listener thread.

I pushed a new PR for branch-2, passed locally 20 times.

[~stack] Could you help to review and test it? Thanks.

 

I think it is fine for master now. On branch-2 it is a different problem?

 

[~zhangduo] I ported the PR on master to branch-2, it failed occationally.

From the test result on branch-2, just like the image I uploaded before, after a new server is online, it may need a while for the group info manager to update the group info cache about this server. The logic of method ServerListener.serverAdded() in class RSGroupInfoManagerImpl is different between branch-2 and master. In branch-2 it need to notify the thread to update default group info.

And balancer can run before the cache updated, no locks or state marks here to tell balancer the group info cache is updating right now and may be not correct or partial.

According the codes of updating group info cache, get partial/expired group info is allowed expectedly. So in the UT, just waiting new online server to be in default group is enough. But in the process of balance, I think maybe balance() can return false like the logic of other pre balance judgements. 

Trying the patch. Thanks.

The failure is showing in the flakies list now.... FYI, https://ci-hadoop.apache.org/view/HBase/job/HBase/job/HBase-Find-Flaky-Tests/job/branch-2.4/lastSuccessfulBuild/artifact/output/dashboard.html

Oh, in the CI test result that [~stack] attached, it failed at 

java.lang.AssertionError at org.apache.hadoop.hbase.rsgroup.TestRSGroupsFallback.lambda$assertRegionsInGroup$0(TestRSGroupsFallback.java:123) at org.apache.hadoop.hbase.rsgroup.TestRSGroupsFallback.assertRegionsInGroup(TestRSGroupsFallback.java:120) at org.apache.hadoop.hbase.rsgroup.TestRSGroupsFallback.testFallback(TestRSGroupsFallback.java:100)

before line 100 it called crashRsInGroup(RSGroupInfo.DEFAULT_GROUP)......

Logs show that the assertion of correct table group is before the SCP is executed.

 

[~Xiaolin Ha] is the failure like that in HBASE-25389 ?  In HBASE-25389, we are trying to verify that the meta Region comes up in a new location after we kill its current server.  The test was failing because we were checking for the new location before the old location server had crashed down.

Thanks, [~stack] . Yes, they are the same reason. They all need the SCP be completed to make the table regions be reassigned.

I have updated the PR. Since AssignmentTestingUtil.crashRs has already waited the RS to be dead on Master, it need to wait no DeadServersInProgress now.

Running w/ your new PR. It looks good. Let me get some more runs in to be sure.

Pushed branch-2 PR to branch-2.4 and branch-2 (Another PR had already been applied to master). Resolving after testing locally against branch-2. This patch seems to have cured the failure I've been seeing running unit tests locally. Thanks [~Xiaolin Ha] for the gnarly fix.

Results for branch branch-2
	[build #130 on builds.a.o|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-2/130/]: (/) *{color:green}+1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-2/130/General_20Nightly_20Build_20Report/]




(/) {color:green}+1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-2/130/JDK8_20Nightly_20Build_20Report_20_28Hadoop2_29/]


(/) {color:green}+1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-2/130/JDK8_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(/) {color:green}+1 jdk11 hadoop3 checks{color}
-- For more information [see jdk11 report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-2/130/JDK11_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


Results for branch branch-2.4
	[build #10 on builds.a.o|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-2.4/10/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-2.4/10/General_20Nightly_20Build_20Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-2.4/10/JDK8_20Nightly_20Build_20Report_20_28Hadoop2_29/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-2.4/10/JDK8_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(/) {color:green}+1 jdk11 hadoop3 checks{color}
-- For more information [see jdk11 report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-2.4/10/JDK11_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


