trunk patch: https://github.com/krummas/cassandra/commits/marcuse/15160
dtests: https://github.com/krummas/cassandra-dtest/commits/marcuse/15160

circleci: https://circleci.com/workflow-run/4f0960b3-649d-4a35-a1bc-0cbb4b40b045

[~dcapwell] noticed that we should be setting repaired status to completed [here|https://github.com/krummas/cassandra/commit/a2d492bf6a0f203a50162cacf68497322f4614f8#diff-4bc513a60150419e20a9449c70a64a66R250], looks good otherwise

Interesting that the test was success and didn't keep waiting; in other cases I found I have to set

{code}ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.COMPLETED,
                                                       ImmutableList.of(message, completionMessage));{code}

yet I see your tests were success: https://app.circleci.com/jobs/github/krummas/cassandra/2081/artifacts

rebased the patch above and added a jvm dtest

new cci run: https://app.circleci.com/pipelines/github/krummas/cassandra/504/workflows/3502debf-72fb-43bf-92ff-544ccae87a8f

Thanks, should be able to look later today

Curious, if https://issues.apache.org/jira/browse/CASSANDRA-16120 lands would we need python dtest to implement these test?

left comments on the commits, but I am unable to replicate the defined behavior in 3.0; ran the below tests in 3.0 and the unreplicated case all failed for me.

{code}
package org.apache.cassandra.distributed.test;

import java.io.IOException;

import org.junit.Test;

import org.apache.cassandra.distributed.Cluster;
import org.apache.cassandra.distributed.api.ConsistencyLevel;
import org.apache.cassandra.distributed.api.IInvokableInstance;
import org.assertj.core.api.Assertions;

public class RepairFilteringTest extends TestBaseImpl
{
    @Test
    public void dcFilterOnEmptyDC() throws IOException
    {
        try (Cluster cluster = Cluster.build().withRacks(2, 1, 2).start())
        {
            // 1-2 : datacenter1
            // 3-4 : datacenter2
            cluster.schemaChange("CREATE KEYSPACE " + KEYSPACE + " WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1':2, 'datacenter2':0}");
            cluster.schemaChange("CREATE TABLE " + KEYSPACE + ".tbl (id int PRIMARY KEY, i int)");
            for (int i = 0; i < 10; i++)
                cluster.coordinator(1).execute("INSERT INTO " + KEYSPACE + ".tbl (id, i) VALUES (?, ?)", ConsistencyLevel.ALL, i, i);
            cluster.forEach(i -> i.flush(KEYSPACE));

            // choose a node in the DC that doesn't have any replicas
            IInvokableInstance node = cluster.get(3);
            Assertions.assertThat(node.config().localDatacenter()).isEqualTo("datacenter2");
            // fails with "the local data center must be part of the repair"
            node.nodetoolResult("repair", "-full", "-dc", "datacenter1", "-st", "0", "-et", "1000", KEYSPACE, "tbl")
                   .asserts().failure().errorContains("the local data center must be part of the repair");
        }
    }

    @Test
    public void hostFilterDifferentDC() throws IOException
    {
        try (Cluster cluster = Cluster.build().withRacks(2, 1, 2).start())
        {
            // 1-2 : datacenter1
            // 3-4 : datacenter2
            cluster.schemaChange("CREATE KEYSPACE " + KEYSPACE + " WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1':2, 'datacenter2':0}");
            cluster.schemaChange("CREATE TABLE " + KEYSPACE + ".tbl (id int PRIMARY KEY, i int)");
            for (int i = 0; i < 10; i++)
                cluster.coordinator(1).execute("INSERT INTO " + KEYSPACE + ".tbl (id, i) VALUES (?, ?)", ConsistencyLevel.ALL, i, i);
            cluster.forEach(i -> i.flush(KEYSPACE));

            // choose a node in the DC that doesn't have any replicas
            IInvokableInstance node = cluster.get(3);
            Assertions.assertThat(node.config().localDatacenter()).isEqualTo("datacenter2");
            // fails with "Specified hosts [127.0.0.3, 127.0.0.1] do not share range (0,1000] needed for repair. Either restrict repair ranges with -st/-et options, or specify one of the neighbors that share this range with this node: [].. Check the logs on the repair participants for further details"
            node.nodetoolResult("repair", "-full",
                                "-hosts", cluster.get(1).broadcastAddress().getAddress().getHostAddress(),
                                "-hosts", node.broadcastAddress().getAddress().getHostAddress(),
                                "-st", "0", "-et", "1000", KEYSPACE, "tbl")
                .asserts().failure().errorContains("do not share range (0,1000] needed for repair");
        }
    }

    @Test
    public void emptyDC() throws IOException
    {
        try (Cluster cluster = Cluster.build().withRacks(2, 1, 2).start())
        {
            // 1-2 : datacenter1
            // 3-4 : datacenter2
            cluster.schemaChange("CREATE KEYSPACE " + KEYSPACE + " WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1':2, 'datacenter2':0}");
            cluster.schemaChange("CREATE TABLE " + KEYSPACE + ".tbl (id int PRIMARY KEY, i int)");
            for (int i = 0; i < 10; i++)
                cluster.coordinator(1).execute("INSERT INTO " + KEYSPACE + ".tbl (id, i) VALUES (?, ?)", ConsistencyLevel.ALL, i, i);
            cluster.forEach(i -> i.flush(KEYSPACE));

            // choose a node in the DC that doesn't have any replicas
            IInvokableInstance node = cluster.get(3);
            Assertions.assertThat(node.config().localDatacenter()).isEqualTo("datacenter2");
            // fails with [2020-09-10 11:30:04,139] Repair command #1 failed with error Nothing to repair for (0,1000] in distributed_test_keyspace - aborting. Check the logs on the repair participants for further details
            node.nodetoolResult("repair", "-full",
                                "-st", "0", "-et", "1000", KEYSPACE, "tbl")
                .asserts().failure().errorContains("Nothing to repair for (0,1000] in " + KEYSPACE);
        }
    }

    @Test
    public void mainDC() throws IOException
    {
        try (Cluster cluster = Cluster.build().withRacks(2, 1, 2).start())
        {
            // 1-2 : datacenter1
            // 3-4 : datacenter2
            cluster.schemaChange("CREATE KEYSPACE " + KEYSPACE + " WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1':2, 'datacenter2':0}");
            cluster.schemaChange("CREATE TABLE " + KEYSPACE + ".tbl (id int PRIMARY KEY, i int)");
            for (int i = 0; i < 10; i++)
                cluster.coordinator(1).execute("INSERT INTO " + KEYSPACE + ".tbl (id, i) VALUES (?, ?)", ConsistencyLevel.ALL, i, i);
            cluster.forEach(i -> i.flush(KEYSPACE));

            // choose a node in the DC that doesn't have any replicas
            IInvokableInstance node = cluster.get(1);
            Assertions.assertThat(node.config().localDatacenter()).isEqualTo("datacenter1");
            node.nodetoolResult("repair", "-full",
                                "-st", "0", "-et", "1000", KEYSPACE, "tbl")
                .asserts().success();
        }
    }
}
{code}

when I run the tests on 4.0 emptyDC and hostFilterDifferentDC fail with a different message ("Repair command #1 failed with error Endpoints can not be empty"), but they still fail (behavior of 3.0).

speaking to Marcus more it turns out that this was a regression in 3.0+ as this behavior was valid in 2.1

{code}
package org.apache.cassandra.distributed.upgrade;

import java.io.IOException;

import org.junit.Test;

import org.apache.cassandra.distributed.Cluster;
import org.apache.cassandra.distributed.api.ConsistencyLevel;
import org.apache.cassandra.distributed.api.IInvokableInstance;
import org.apache.cassandra.distributed.shared.Versions;
import org.assertj.core.api.Assertions;

public class RepairFilteringTest extends UpgradeTestBase
{
    @Test
    public void emptyDC() throws IOException
    {
        Versions versions = Versions.find();
        Versions.Version version = versions.getLatest(Versions.Major.v22);
        try (Cluster cluster = Cluster.build().withVersion(version).withRacks(2, 1, 2).start())
        {
            // 1-2 : datacenter1re
            // 3-4 : datacenter2
            cluster.schemaChange("CREATE KEYSPACE " + KEYSPACE + " WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1':2, 'datacenter2':0}");
            cluster.schemaChange("CREATE TABLE " + KEYSPACE + ".tbl (id int PRIMARY KEY, i int)");
            for (int i = 0; i < 10; i++)
                cluster.coordinator(1).execute("INSERT INTO " + KEYSPACE + ".tbl (id, i) VALUES (?, ?)", ConsistencyLevel.ALL, i, i);
            cluster.forEach(i -> i.flush(KEYSPACE));

            // choose a node in the DC that doesn't have any replicas
            IInvokableInstance node = cluster.get(3);
            Assertions.assertThat(node.config().localDatacenter()).isEqualTo("datacenter2");
            // fails with [2020-09-10 11:30:04,139] Repair command #1 failed with error Nothing to repair for (0,1000] in distributed_test_keyspace - aborting. Check the logs on the repair participants for further details
            node.nodetoolResult("repair", "-full",
                                "-st", "0", "-et", "1000", KEYSPACE, "tbl")
                .asserts().success();
        }
    }
}
{code}

Finished testing.  What I see is that with the flag you can get the same behavior as 2.1 and without the flag (default) we keep the current behavior.  

LGTM +1.

It would be nice to add the tests below, as it tests the different conditions where the neighbors can be empty, also it would be nice if the Python dtests could be jvm dtests but I think we need https://issues.apache.org/jira/browse/CASSANDRA-16120 merged/released before we could so the Python dtests are fine by me.

{code}
package org.apache.cassandra.distributed.test;

import java.io.IOException;

import org.junit.Test;

import org.apache.cassandra.distributed.Cluster;
import org.apache.cassandra.distributed.api.ConsistencyLevel;
import org.apache.cassandra.distributed.api.IInvokableInstance;
import org.apache.cassandra.distributed.shared.Versions;
import org.assertj.core.api.Assertions;

public class RepairFilteringTest extends TestBaseImpl
{
    private static final Versions VERSIONS = Versions.find();
//    private static final Versions.Version VERSION = VERSIONS.getLatest(Versions.Major.v22);
    private static final Versions.Version VERSION = VERSIONS.getLatest(Versions.Major.v4);

    @Test
    public void dcFilterOnEmptyDC() throws IOException
    {
        try (Cluster cluster = Cluster.build().withVersion(VERSION).withRacks(2, 1, 2).start())
        {
            // 1-2 : datacenter1
            // 3-4 : datacenter2
            cluster.schemaChange("CREATE KEYSPACE " + KEYSPACE + " WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1':2, 'datacenter2':0}");
            cluster.schemaChange("CREATE TABLE " + KEYSPACE + ".tbl (id int PRIMARY KEY, i int)");
            for (int i = 0; i < 10; i++)
                cluster.coordinator(1).execute("INSERT INTO " + KEYSPACE + ".tbl (id, i) VALUES (?, ?)", ConsistencyLevel.ALL, i, i);
            cluster.forEach(i -> i.flush(KEYSPACE));

            // choose a node in the DC that doesn't have any replicas
            IInvokableInstance node = cluster.get(3);
            Assertions.assertThat(node.config().localDatacenter()).isEqualTo("datacenter2");
            // fails with "the local data center must be part of the repair"
            node.nodetoolResult("repair", "-full",
                                "-dc", "datacenter1", "-dc", "datacenter2",
                                "--ignore-unreplicated-keyspaces",
                                "-st", "0", "-et", "1000",
                                KEYSPACE, "tbl")
                .asserts().success();
        }
    }

    @Test
    public void hostFilterDifferentDC() throws IOException
    {
        try (Cluster cluster = Cluster.build().withVersion(VERSION).withRacks(2, 1, 2).start())
        {
            // 1-2 : datacenter1
            // 3-4 : datacenter2
            cluster.schemaChange("CREATE KEYSPACE " + KEYSPACE + " WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1':2, 'datacenter2':0}");
            cluster.schemaChange("CREATE TABLE " + KEYSPACE + ".tbl (id int PRIMARY KEY, i int)");
            for (int i = 0; i < 10; i++)
                cluster.coordinator(1).execute("INSERT INTO " + KEYSPACE + ".tbl (id, i) VALUES (?, ?)", ConsistencyLevel.ALL, i, i);
            cluster.forEach(i -> i.flush(KEYSPACE));

            // choose a node in the DC that doesn't have any replicas
            IInvokableInstance node = cluster.get(3);
            Assertions.assertThat(node.config().localDatacenter()).isEqualTo("datacenter2");
            // fails with "Specified hosts [127.0.0.3, 127.0.0.1] do not share range (0,1000] needed for repair. Either restrict repair ranges with -st/-et options, or specify one of the neighbors that share this range with this node: [].. Check the logs on the repair participants for further details"
            node.nodetoolResult("repair", "-full",
                                "-hosts", cluster.get(1).broadcastAddress().getAddress().getHostAddress(),
                                "-hosts", node.broadcastAddress().getAddress().getHostAddress(),
                                "--ignore-unreplicated-keyspaces",
                                "-st", "0", "-et", "1000",
                                KEYSPACE, "tbl")
                .asserts().success();
        }
    }

    @Test
    public void emptyDC() throws IOException
    {
        try (Cluster cluster = Cluster.build().withVersion(VERSION).withRacks(2, 1, 2).start())
        {
            // 1-2 : datacenter1
            // 3-4 : datacenter2
            cluster.schemaChange("CREATE KEYSPACE " + KEYSPACE + " WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1':2, 'datacenter2':0}");
            cluster.schemaChange("CREATE TABLE " + KEYSPACE + ".tbl (id int PRIMARY KEY, i int)");
            for (int i = 0; i < 10; i++)
                cluster.coordinator(1).execute("INSERT INTO " + KEYSPACE + ".tbl (id, i) VALUES (?, ?)", ConsistencyLevel.ALL, i, i);
            cluster.forEach(i -> i.flush(KEYSPACE));

            // choose a node in the DC that doesn't have any replicas
            IInvokableInstance node = cluster.get(3);
            Assertions.assertThat(node.config().localDatacenter()).isEqualTo("datacenter2");
            // fails with [2020-09-10 11:30:04,139] Repair command #1 failed with error Nothing to repair for (0,1000] in distributed_test_keyspace - aborting. Check the logs on the repair participants for further details
            node.nodetoolResult("repair", "-full",
                                "--ignore-unreplicated-keyspaces",
                                "-st", "0", "-et", "1000",
                                KEYSPACE, "tbl")
                .asserts().success();
        }
    }

    @Test
    public void mainDC() throws IOException
    {
        try (Cluster cluster = Cluster.build().withVersion(VERSION).withRacks(2, 1, 2).start())
        {
            // 1-2 : datacenter1
            // 3-4 : datacenter2
            cluster.schemaChange("CREATE KEYSPACE " + KEYSPACE + " WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1':2, 'datacenter2':0}");
            cluster.schemaChange("CREATE TABLE " + KEYSPACE + ".tbl (id int PRIMARY KEY, i int)");
            for (int i = 0; i < 10; i++)
                cluster.coordinator(1).execute("INSERT INTO " + KEYSPACE + ".tbl (id, i) VALUES (?, ?)", ConsistencyLevel.ALL, i, i);
            cluster.forEach(i -> i.flush(KEYSPACE));

            // choose a node in the DC that doesn't have any replicas
            IInvokableInstance node = cluster.get(1);
            Assertions.assertThat(node.config().localDatacenter()).isEqualTo("datacenter1");
            node.nodetoolResult("repair", "-full",
                                "--ignore-unreplicated-keyspaces",
                                "-st", "0", "-et", "1000",
                                KEYSPACE, "tbl")
                .asserts().success();
        }
    }
}
{code}

3.0: [patch|https://github.com/krummas/cassandra/commits/marcuse/15160-3.0], [cci|https://app.circleci.com/pipelines/github/krummas/cassandra/514/workflows/df085bfb-d6df-434f-a6a6-846df6b9c9e0]
3.11: [patch|https://github.com/krummas/cassandra/commits/marcuse/15160-3.11], [cci|https://app.circleci.com/pipelines/github/krummas/cassandra/513/workflows/ad7c154b-6463-433c-8008-4cfb09f75458]
trunk: [patch|https://github.com/krummas/cassandra/commits/marcuse/15160], [cci|https://app.circleci.com/pipelines/github/krummas/cassandra/510/workflows/655d5f75-0193-4e61-8555-89f56f26cecd]

[~dcapwell] added your tests, could you have a quick look at the 3.0 patch as it is slightly different from the trunk one? (the 3.11 is basically the same as 3.0)

checking now

Left comment for 3.0 https://github.com/krummas/cassandra/commit/8bfdb0b6eed1e74ad6a0c8f4fac94980db19fd49#r42335793. Small change to properly complete, once that is in then +1 from me.

Sorry, I was confusing 3.0 and trunk...  In 3.0 we ONLY have notifications, trunk has notifications + polling... given this 3.0 and 3.11 LGTM

+1

small bug in org.apache.cassandra.distributed.test.RepairOperationalTest#mainDC, it chooses a node in the wrong DC so the test fails; switched to node 3 rather than node 1 (test passes).

+1

Ok I think I figured out the python dtest issues; https://github.com/krummas/cassandra/commit/8bfdb0b6eed1e74ad6a0c8f4fac94980db19fd49#diff-ed04b58f62a097d7f25c755d487cb1a3R334. The new flag was added to the toString, and the tests search for the toString, so this regex fails https://github.com/apache/cassandra-dtest/blob/master/repair_tests/deprecated_repair_test.py#L168.

3.x jvm-dtests fail since they check the wrong DC; all branches should have (the original test tests dc2, so mainDC only needs to test dc1)

{code}
 // choose a node in the DC that has any replicas
IInvokableInstance node = cluster.get(1);
Assertions.assertThat(node.config().localDatacenter()).isEqualTo("datacenter1");
{code}

Held off merge on the Python dtests.

ah nice catches, pushed fixes to the branches above + rebased the old dtest branch with the log greping fix.

running cci with that dtest branch here:
[3.0|https://app.circleci.com/pipelines/github/krummas/cassandra/538/workflows/c3cc5064-7c70-4873-8af6-268560883825]
[3.11|https://app.circleci.com/pipelines/github/krummas/cassandra/540/workflows/83c6341a-b386-473d-ae71-bbf292fbc0c1]
[trunk|https://app.circleci.com/pipelines/github/krummas/cassandra/539/workflows/a2cd8cc2-2072-49b7-a63e-5a99063a0ec9]

Overall LGTM +1.  

I would need to look closer to make sure the python dtest change doesn't break 2.2, '(, ignore unreplicated keyspaces: (?P<ignoreunrepl>true|false))?' I think this should be ignored if not present, so should be fine?

ran 2.2 cci against the dtest branch [here|https://app.circleci.com/pipelines/github/krummas/cassandra/542/workflows/1a3f6d7c-53ad-4683-81b9-8a9f2ffc4259] - unclear what is going on with TestCqlsh, but no failures in deprecated_repair_test.py

thats a known issue [~djoshi] said he would look into; a test runs in 2.2 but uses a config added in 3.0, these tests have been failing for a long time now.

[~marcuse] thanks for running, can commit or ill commit it later today.

Committed, only known flaky tests and a issue with python installing packages

CI

3.0
Jenkins: https://ci-cassandra.apache.org/job/Cassandra-devbranch/41/
Circle CI: https://app.circleci.com/pipelines/github/dcapwell/cassandra?branch=commit_remote_branch%2FCASSANDRA-15160-cassandra-3.0-FE679B1E-281C-44F2-8C62-9EDA039FE4C3

3.11
Jenkins: https://ci-cassandra.apache.org/job/Cassandra-devbranch/42/
Circle CI: https://app.circleci.com/pipelines/github/dcapwell/cassandra?branch=commit_remote_branch%2FCASSANDRA-15160-cassandra-3.11-FE679B1E-281C-44F2-8C62-9EDA039FE4C3

trunk
Jenkins: https://ci-cassandra.apache.org/job/Cassandra-devbranch/43/
Circle CI: https://app.circleci.com/pipelines/github/dcapwell/cassandra?branch=commit_remote_branch%2FCASSANDRA-15160-trunk-FE679B1E-281C-44F2-8C62-9EDA039FE4C3

