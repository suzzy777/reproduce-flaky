I took a brief look at this today and have a few thoughts on why this might be hanging.  I noticed that we first bind a socket in Java then release it and re-use that port number for the Thriftserver; this probably works most of the time, but it's prone to races.  What happens if the ThriftServer fails to bind to a port or dies during startup?  Another potential cause of flakiness is how we search for the "Thriftserver started" message in its log: because we "tail -f" the log, it's possible that we might miss the message if a bunch of output is printed after the message we're looking for, causing it to not appear in the initial "tail" output.

Maybe we should make several attempts to bind on different ports in case the first attempt fails.  To do this, we probably shouldn't lower the timeout because that might introduce a new source of flakiness if the startup is slow.  Instead, what do you think about using a second future / promise to detect whether the ThriftServer process died then Awaiting on whichever future completes first?  I think we can detect unclean subprocess exit by wrapping a blocking call to Process.exitCode in a future.

I haven't dug in super-deeply, but these were just my initial thoughts after a quick pass through this code today.

Hey [~joshrosen], thanks for the investigation! I came to a similar conclusion about the port contention issue. Also observed port conflicts between core Spark services and HiveThriftServer2. 

The {{tail -f}} part is a great advice, didn't notice that before. I think we can fix this part by using {{tail -n +0 -f}}, which always show all lines of the log file.

User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4720

Issue resolved by pull request 4720
[https://github.com/apache/spark/pull/4720]

User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4758

