Following these steps, I received:

{noformat}
2022-03-23 20:36:12,259 - target-dc3 - __main__ - INFO - Got 0/552 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:36:12,265 - target-dc1 - __main__ - INFO - Got 0/553 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:36:12,348 - target-dc2 - __main__ - INFO - Got 0/552 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:37:12,276 - target-dc3 - __main__ - INFO - Got 0/569 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:37:12,337 - target-dc1 - __main__ - INFO - Got 0/569 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:37:12,432 - target-dc2 - __main__ - INFO - Got 0/569 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:38:12,282 - target-dc3 - __main__ - INFO - Got 0/575 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:38:12,415 - target-dc1 - __main__ - INFO - Got 0/575 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:38:12,456 - target-dc2 - __main__ - INFO - Got 0/575 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:39:12,373 - target-dc3 - __main__ - INFO - Got 0/577 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:39:12,556 - target-dc2 - __main__ - INFO - Got 0/578 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:39:16,284 - target-dc1 - __main__ - INFO - Got 1/565 (0.176991) timeouts/total_rqs in the last 1 minute
2022-03-23 20:39:42,083 - target-dc3 - cassandra.connection - WARNING - Heartbeat failed for connection (139909259222464) to 127.0.0.1:9042
2022-03-23 20:39:42,106 - target-dc1 - cassandra.connection - WARNING - Heartbeat failed for connection (139909259218800) to 127.0.0.1:9042
2022-03-23 20:39:42,149 - target-dc2 - cassandra.connection - WARNING - Heartbeat failed for connection (139909259222224) to 127.0.0.1:9042
2022-03-23 20:39:47,113 - target-dc1 - cassandra.cluster - WARNING - [control connection] Error connecting to 127.0.0.1:9042:
Traceback (most recent call last):
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/cluster.py", line 3522, in _reconnect_internal
    return self._try_connect(host)
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/cluster.py", line 3544, in _try_connect
    connection = self._cluster.connection_factory(host.endpoint, is_control_connection=True)
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/cluster.py", line 1620, in connection_factory
    return self.connection_class.factory(endpoint, self.connect_timeout, *args, **kwargs)
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/connection.py", line 831, in factory
    conn = cls(endpoint, *args, **kwargs)
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py", line 344, in __init__
    self._connect_socket()
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/connection.py", line 898, in _connect_socket
    raise socket.error(sockerr.errno, "Tried connecting to %s. Last error: %s" %
OSError: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-23 20:40:12,117 - target-dc1 - cassandra.connection - WARNING - Heartbeat failed for connection (139909241902704) to 127.0.0.1:9042
2022-03-23 20:40:12,117 - target-dc1 - cassandra.cluster - WARNING - Host 127.0.0.1:9042 has been marked down
2022-03-23 20:40:12,381 - target-dc3 - __main__ - INFO - Got 0/574 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:40:12,594 - target-dc2 - __main__ - INFO - Got 0/574 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-23 20:40:17,316 - target-dc1 - __main__ - INFO - Got 7/9 (77.777778) timeouts/total_rqs in the last 1 minute
2022-03-23 20:40:18,326 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 2.28 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-23 20:40:25,636 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 4.2 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-23 20:40:34,850 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 8.08 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-23 20:40:47,976 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 16.32 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
{noformat}

Which seems correct?

Hey [~brandon.williams], 

I am going to do a bit of an assumption with my comment, please correct me if I'm wrong.

If you kept the app running for a few more minutes you should have seen other "got x/y ... timeouts/total_rqs" where (x>0) entries, highlighting the client still sends requests to the DOWN node which didn't happen in c3. In c3 after in flight requests are concluded, no more traffic is sent to the DOWN node until it goes back UP

This behavior (sending queries to a DOWN node) is specially bad when counters are involved as those do not seem trigger speculative retries and always result in query timeouts for the client.


Here it is with more time:

{noformat}
2022-03-30 18:57:07,763 - target-dc1 - cassandra.connection - WARNING - Heartbeat failed for connection (139719619682112) to 127.0.0.1:9042
2022-03-30 18:57:07,776 - target-dc3 - cassandra.connection - WARNING - Heartbeat failed for connection (139719619681680) to 127.0.0.1:9042
2022-03-30 18:57:07,980 - target-dc2 - __main__ - INFO - Got 0/579 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 18:57:08,094 - target-dc3 - __main__ - INFO - Got 0/579 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 18:57:12,768 - target-dc1 - cassandra.cluster - WARNING - [control connection] Error connecting to 127.0.0.1:9042:
Traceback (most recent call last):
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/cluster.py", line 3522, in _reconnect_internal
    return self._try_connect(host)
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/cluster.py", line 3544, in _try_connect
    connection = self._cluster.connection_factory(host.endpoint, is_control_connection=True)
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/cluster.py", line 1620, in connection_factory
    return self.connection_class.factory(endpoint, self.connect_timeout, *args, **kwargs)
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/connection.py", line 831, in factory
    conn = cls(endpoint, *args, **kwargs)
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py", line 344, in __init__
    self._connect_socket()
  File "/home/drift/cassandra-dtest/venv/src/cassandra-driver/cassandra/connection.py", line 898, in _connect_socket
    raise socket.error(sockerr.errno, "Tried connecting to %s. Last error: %s" %
OSError: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-30 18:57:12,831 - target-dc1 - __main__ - INFO - Got 5/190 (2.631579) timeouts/total_rqs in the last 1 minute
2022-03-30 18:57:37,774 - target-dc1 - cassandra.connection - WARNING - Heartbeat failed for connection (139719602362976) to 127.0.0.1:9042
2022-03-30 18:57:37,775 - target-dc1 - cassandra.cluster - WARNING - Host 127.0.0.1:9042 has been marked down
2022-03-30 18:57:43,983 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 1.74 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-30 18:57:50,790 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 4.52 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-30 18:58:00,403 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 7.04 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-30 18:58:08,006 - target-dc2 - __main__ - INFO - Got 0/582 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 18:58:08,144 - target-dc3 - __main__ - INFO - Got 0/582 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 18:58:12,519 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 16.0 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-30 18:58:14,102 - target-dc1 - __main__ - INFO - Got 9/15 (60.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 18:58:33,550 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 33.28 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-30 18:59:08,102 - target-dc2 - __main__ - INFO - Got 0/584 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 18:59:08,221 - target-dc3 - __main__ - INFO - Got 0/584 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 18:59:11,914 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 55.04 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-30 18:59:16,479 - target-dc1 - __main__ - INFO - Got 12/23 (52.173913) timeouts/total_rqs in the last 1 minute
2022-03-30 19:00:08,126 - target-dc2 - __main__ - INFO - Got 0/586 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 19:00:08,246 - target-dc3 - __main__ - INFO - Got 0/586 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 19:00:12,003 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 125.44 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-30 19:00:18,924 - target-dc1 - __main__ - INFO - Got 12/24 (50.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 19:01:08,141 - target-dc2 - __main__ - INFO - Got 0/586 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 19:01:08,302 - target-dc3 - __main__ - INFO - Got 0/586 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 19:01:20,657 - target-dc1 - __main__ - INFO - Got 12/17 (70.588235) timeouts/total_rqs in the last 1 minute
2022-03-30 19:02:08,164 - target-dc2 - __main__ - INFO - Got 0/587 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 19:02:08,374 - target-dc3 - __main__ - INFO - Got 0/587 (0.000000) timeouts/total_rqs in the last 1 minute
2022-03-30 19:02:22,510 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 225.28 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-03-30 19:02:23,033 - target-dc1 - __main__ - INFO - Got 12/23 (52.173913) timeouts/total_rqs in the last 1 minute
{noformat}

node1 appears down to others:
{noformat}
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack
UN  127.0.0.2  91.08 KiB  1       100.0%            b79a56f9-74bb-488a-ae7a-def71b9d2e32  r1
DN  127.0.0.1  91.15 KiB  1       100.0%            f90307f3-fda3-47c9-9f78-e6f570a67790  r1

Datacenter: dc2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack
UN  127.0.0.4  91.09 KiB  1       100.0%            d0b5d218-544f-4d06-802f-f526c8c7c106  r1
UN  127.0.0.3  91.09 KiB  1       100.0%            d8212283-807c-4ca2-a866-91320faaa9d2  r1

Datacenter: dc3
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack
UN  127.0.0.5  91.11 KiB  1       100.0%            82b6222b-79fc-42e6-b1d9-8a3710bb033b  r1
UN  127.0.0.6  91.09 KiB  1       100.0%            ca8c4b9b-d470-43f6-82cc-f6d910f282b3  r1
{noformat}

and everything appears down to node1:

{noformat}
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack
DN  127.0.0.2  91.08 KiB  1       100.0%            b79a56f9-74bb-488a-ae7a-def71b9d2e32  r1  
UN  127.0.0.1  91.15 KiB  1       100.0%            f90307f3-fda3-47c9-9f78-e6f570a67790  r1  

Datacenter: dc2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack
DN  127.0.0.4  91.09 KiB  1       100.0%            d0b5d218-544f-4d06-802f-f526c8c7c106  r1  
DN  127.0.0.3  91.09 KiB  1       100.0%            d8212283-807c-4ca2-a866-91320faaa9d2  r1  

Datacenter: dc3
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack
DN  127.0.0.5  91.11 KiB  1       100.0%            82b6222b-79fc-42e6-b1d9-8a3710bb033b  r1  
DN  127.0.0.6  91.09 KiB  1       100.0%            ca8c4b9b-d470-43f6-82cc-f6d910f282b3  r1 
{noformat}

All behavior from the server side looks as expected, so I guess this is indeed a driver problem.


Ah now I get where your brain was going towards, I thought you had problems reproducing the issue.

Fair enough, it is a bit "odd" the exact behavior happens in at least 2 different drivers (java & python) which is why I opened it here but anyway, I'll open tickets in their queues and reference this one for completeness if that seems ok to you.

Sure, I think that makes sense.  What I can confirm here is that the _coordinator_ isn't sending requests to the dead node, so there's nothing we can do on the server side for this behavior.

Hey [~brandon.williams] , I had to dig into this a bit for a related issue with the Python driver and I came up with some additional info which might be useful.  After looking into this it sure looks like this might be a server-side issue after all... but obviously I'll defer to you on that part. :)

 

Based on my testing I confirmed the following things about the LBPs in use by app.py when running against Cassandra 4.0.3:

 
 * When iptables is used to shut down access to one of the IP addresses the load balancing policy (LBP) in use by the client is notified that the node has gone down

 

 

 * As expected that LBP then removes the down node from it’s collection of “live” nodes for the given DC

 

 * The LBP no longer returns the down node when make_query_plan() is called

 

Great, but by itself that doesn't prove anything.  So I decided to take a look at the exceptions that were actually getting thrown in this case.  With the following change to app.py:

 

 
{code:java}
55c55,59
<         except (cassandra.OperationTimedOut, cassandra.WriteTimeout):
---
>         except cassandra.OperationTimedOut as exc:
>             LOG.info(exc)
>             timeout_counter += 1
>         except cassandra.WriteTimeout as exc:
>             LOG.info(exc) {code}
 

 

I see a sequence of two different error messages when I drop the iptables hammer.  The first is a set of "Client request timeout" messages (which is entirely expected since the LBP doesn't know about the down node yet) while the second looks to be a server-side issue:

 

 
{code:java}
2022-05-27 22:01:38,629 - target-dc1 - __main__ - INFO - errors={'127.0.0.1:9042': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.1:9042
2022-05-27 22:01:48,734 - target-dc1 - __main__ - INFO - errors={'127.0.0.1:9042': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.1:9042
2022-05-27 22:01:58,845 - target-dc1 - __main__ - INFO - errors={'127.0.0.1:9042': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.1:9042
2022-05-27 22:02:03,954 - target-dc1 - __main__ - INFO - Error from server: code=1100 [Coordinator node timed out waiting for replica nodes' responses] message="Operation timed out - received only 0 responses." info={'consistency': 'ONE', 'required_responses': 1, 'received_responses': 0, 'write_type': 'COUNTER'}
2022-05-27 22:02:14,172 - target-dc1 - __main__ - INFO - errors={'127.0.0.1:9042': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.1:9042
2022-05-27 22:02:22,545 - target-dc3 - __main__ - INFO - Got 0/579 (0.000000) timeouts/total_rqs in the last 1 minute
2022-05-27 22:02:22,548 - target-dc2 - __main__ - INFO - Got 0/579 (0.000000) timeouts/total_rqs in the last 1 minute
2022-05-27 22:02:24,275 - target-dc1 - __main__ - INFO - errors={'127.0.0.1:9042': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.1:9042
2022-05-27 22:02:24,376 - target-dc1 - __main__ - INFO - Got 6/66 (9.090909) timeouts/total_rqs in the last 1 minute
2022-05-27 22:02:34,387 - target-dc1 - __main__ - INFO - errors={'127.0.0.1:9042': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.1:9042
2022-05-27 22:02:44,695 - target-dc1 - __main__ - INFO - errors={'127.0.0.1:9042': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.1:9042 {code}
 

 

When the LBP gets notified of the down node things change; the "Client request timeout" disappears and all I see is the server-side issue:

 
{code:java}
2022-05-27 22:02:52,482 - target-dc1 - cassandra.cluster - WARNING - Host 127.0.0.1:9042 has been marked down                                                                     
2022-05-27 22:02:52,483 - target-dc1 - cassandra.policies - INFO - on_down for host 127.0.0.1:9042                                                                                
2022-05-27 22:02:52,485 - target-dc1 - cassandra.policies - INFO - Setting hosts for dc dc1 to (<Host: 127.0.0.2:9042 dc1>,)                                                      
2022-05-27 22:02:52,486 - target-dc1 - cassandra.policies - INFO - on_down for host 127.0.0.1:9042                                                                                
2022-05-27 22:02:52,488 - target-dc1 - cassandra.policies - INFO - on_down for host 127.0.0.1:9042                                                                                
2022-05-27 22:02:52,488 - target-dc1 - cassandra.policies - INFO - on_down for host 127.0.0.1:9042                                                                                
2022-05-27 22:02:57,507 - target-dc1 - cassandra.cluster - WARNING - [control connection] Error connecting to 127.0.0.1:9042:                                                     
Traceback (most recent call last):                                                                                                                                                
  File "cassandra/cluster.py", line 3522, in cassandra.cluster.ControlConnection._reconnect_internal                                                                              
  File "cassandra/cluster.py", line 3544, in cassandra.cluster.ControlConnection._try_connect                                                                                     
  File "cassandra/cluster.py", line 1620, in cassandra.cluster.Cluster.connection_factory                                              
  File "cassandra/connection.py", line 831, in cassandra.connection.Connection.factory                                                                                            
  File "/home/automaton/les23/lib/python3.6/site-packages/cassandra/io/libevreactor.py", line 267, in __init__
    self._connect_socket()                                                                                                                                                        
  File "cassandra/connection.py", line 898, in cassandra.connection.Connection._connect_socket                                         
OSError: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out
2022-05-27 22:02:57,609 - target-dc1 - __main__ - INFO - Error from server: code=1100 [Coordinator node timed out waiting for replica nodes' responses] message="Operation timed out - received only 0 responses." info={'consistency': 'ONE', 'required_responses': 1, 'received_responses': 0, 'write_type': 'COUNTER'}                                           
2022-05-27 22:02:58,615 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 2.22 seconds: [Errno None] Tried connecting to [('127.0.0.1', 9042)]. Last error: timed out                                                                                                                                  
2022-05-27 22:03:02,712 - target-dc1 - __main__ - INFO - Error from server: code=1100 [Coordinator node timed out waiting for replica nodes' responses] message="Operation timed out - received only 0 responses." info={'consistency': 'ONE', 'required_responses': 1, 'received_responses': 0, 'write_type': 'COUNTER'}                                           
2022-05-27 22:03:05,925 - target-dc1 - cassandra.pool - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 4.24 seconds: [Errno None] Tried connectin$
 to [('127.0.0.1', 9042)]. Last error: timed out
2022-05-27 22:03:07,917 - target-dc1 - __main__ - INFO - Error from server: code=1100 [Coordinator node timed out waiting for replica nodes' responses] message="Operation timed out - received only 0 responses." info={'consistency': 'ONE', 'required_responses': 1, 'received_responses': 0, 'write_type': 'COUNTER'}{code}
It appears that it's these server-side exceptions which are accounting for the non-zero error counts returned by the app.

 

If I can do anything to help repro or debug this further please let me know!

 

 

I've simplified the reproduction in a dtest [here|https://github.com/driftx/cassandra-dtest/tree/CASSANDRA-17411] and updated the fix versions accordingly while I continue to investigate.

I believe transient replication is what broke this [in this commit|https://github.com/apache/cassandra/commit/f7431b432875e334170ccdb19934d05545d2cebd] and never filtered out dead replicas when selecting one for counter writes.

||Branch||CI||
|[4.0|https://github.com/driftx/cassandra/tree/CASSANDRA-17411-4.0]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/516/workflows/c19c3e37-5fd4-4057-8ed1-70a8594fe025], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/516/workflows/421da77e-42d7-4219-9f53-a1fae299a360]|
|[4.1|https://github.com/driftx/cassandra/tree/CASSANDRA-17411-4.1]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/518/workflows/a1fe8fa9-5cdf-41c7-8f86-7088d5261a12], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/518/workflows/8f60ed4f-269d-4c36-9bf6-ec171408420e]|
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-17411-trunk]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/517/workflows/51115c65-7f25-49f8-89e2-8fbb9cee5698], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/517/workflows/16011fa8-96e1-481d-9c02-d6c286c39d1f]|

[500 runs|https://app.circleci.com/pipelines/github/driftx/cassandra/519/workflows/b509e5dd-ba9b-4aee-b2fa-74f1a6d4275e] on the new test against 4.1.

Looks good to me, +1. The new dtest reproduces the issue in the most clear way.

Maybe we can combine the new filter and the one right before into a single one to save us a copy of the collection of replicas:
{code:java}
replicas = replicas.filter(replica -> FailureDetector.instance.isAlive(replica.endpoint()) &&
                                      StorageService.instance.isRpcReady(replica.endpoint()));
{code}
However I doubt it would be worth the loss of readability.

The test failure of {{HostReplacementTest}} on 4.0 seems to be CASSANDRA-16953, and the failure of {{RepairErrorsTest}} on 4.1 looks like the same timeout on {{{}Cluster#close(){}}}. Both can be reproduced without the patch, so CI looks clean to me.

Committed as is for readability as suggested, thanks for the review!

Looks like a bit of infra pressure or flaky CI run; the "1st failure" tests below don't seem to have any rhyme or reason.

[CI Results]
Branch: 4.1, build number: 71
butler url: [https://butler.cassandra.apache.org/#/ci/upstream/compare/Cassandra-4.1/Cassandra-4.1]
jenkins url: [https://ci-cassandra.apache.org/job/Cassandra-4.1/71/]
JIRA: CASSANDRA-17411
commit url: [https://gitbox.apache.org/repos/asf?p=cassandra.git;a=commit;h=51ce71881c87df754969809b5f6b2a466abc1e35]
affected paths:
 * CHANGES.txt
 * src/java/org/apache/cassandra/service/StorageProxy.java

Build Result: UNSTABLE
Passing Tests: 47328
Failing Tests: 21
||Test|Failures|JIRA|
|dtest-offheap.repair_tests.repair_test.TestRepair.test_dc_repair|1 of 68|[Multiple JIRAs found|https://issues.apache.org/jira/issues/?jql=project%20%3D%20CASSANDRA%20and%20resolution%20%3D%20unresolved%20and%20summary%20~%20%22*TestRepair*%22]|
|org.apache.cassandra.distributed.upgrade.CompactStoragePagingTest.testPagingWithCompactStorage|4 of 68|[No JIRA found|https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252]|
|org.apache.cassandra.cql3.validation.operations.InsertUpdateIfConditionCollectionsTest.testFrozenWithNullValues[0: clusterMinVersion=3.0]-cdc|1 of 68|[No JIRA found|https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252]|
|org.apache.cassandra.cql3.validation.entities.SecondaryIndexTest.testWriteOnlyIndex|5 of 68|[No JIRA found|https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252]|
|dtest-novnode.bootstrap_test.TestBootstrap.test_simultaneous_bootstrap|1 of 68|[Multiple JIRAs found|https://issues.apache.org/jira/issues/?jql=project%20%3D%20CASSANDRA%20and%20resolution%20%3D%20unresolved%20and%20summary%20~%20%22*TestBootstrap*%22]|
|org.apache.cassandra.cql3.validation.operations.SelectTest.testRangeQuery|4 of 68|[No JIRA found|https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252]|
|dtest-upgrade.upgrade_tests.drop_compact_storage_upgrade_test.TestDropCompactStorage.test_drop_compact_storage_mixed_cluster|8 of 68|CASSANDRA-17674?|
|org.apache.cassandra.cql3.validation.entities.TupleTypeTest.tuplePartitionReadWrite|2 of 68|[No JIRA found|https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252]|
|org.apache.cassandra.distributed.upgrade.BatchUpgradeTest.batchTest|19 of 68|CASSANDRA-17651?|
|org.apache.cassandra.cql3.ViewComplexTTLTest.testUpdateColumnInViewPKWithTTLWithFlush[3]|11 of 68|[No JIRA found|https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252]|
|dtest-upgrade.upgrade_tests.upgrade_through_versions_test.TestProtoV3Upgrade_AllVersions_RandomPartitioner_EndsAt_3_11_X_HEAD.test_parallel_upgrade_with_internode_ssl|3 of 68|CASSANDRA-17610?|
|org.apache.cassandra.net.ConnectionTest.testMessageDeliveryOnReconnect-compression|4 of 68|CASSANDRA-16677?|
|dtest-upgrade.upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD.test_parallel_upgrade_with_internode_ssl|4 of 68|CASSANDRA-17296?|
|org.apache.cassandra.distributed.test.CASTest.testSuccessfulWriteBeforeRangeMovement|6 of 68|CASSANDRA-17461?|
|org.apache.cassandra.cql3.validation.entities.SecondaryIndexTest.testIndexOnPartitionKeyInsertExpiringColumn|4 of 68|[No JIRA found|https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252]|
|org.apache.cassandra.cql3.validation.entities.SecondaryIndexTest.testIndexesOnNonStaticColumnsWhereSchemaIncludesStaticColumns|1 of 68|[No JIRA found|https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252]|
|org.apache.cassandra.db.commitlog.GroupCommitLogTest.testOutOfOrderFlushRecoveryWithCompaction[3]-compression|1 of 68|CASSANDRA-17232?|
|org.apache.cassandra.distributed.test.CASTest.testSucccessfulWriteDuringRangeMovementFollowedByRead|10 of 68|CASSANDRA-17461?|
|org.apache.cassandra.distributed.upgrade.MixedModeFrom3UnloggedBatchTest.testSimpleStrategy|4 of 68|[No JIRA found|https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252]|
|org.apache.cassandra.cql3.validation.entities.TupleTypeTest.testNestedTuple|9 of 68|[No JIRA found|https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252]|
|dtest-upgrade.upgrade_tests.upgrade_through_versions_test.TestUpgrade_indev_4_0_x_To_indev_4_1_x.test_bootstrap_multidc|3 of 68|[Multiple JIRAs found|https://issues.apache.org/jira/issues/?jql=project%20%3D%20CASSANDRA%20and%20resolution%20%3D%20unresolved%20and%20summary%20~%20%22*TestUpgrade*%22]|

Yep, the failures that are seen for the first time in the run for this patch seem timeouts due to a bad CI run and, if I'm not missing anything, they don't fail in later runs.

