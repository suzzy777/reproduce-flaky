This update will remove the {{@Nullable}} attribute from jsr305, which may propagate through the code

-though as the only place @Nullable is used in the entire Hadoop codebase is apparently {{TestMetricsSystemImpl}}, HADOOP-10067 will have fixed it.

hdfs stops compiling with this patch
{code}
hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java:[68,32] error: cannot find symbol
[ERROR] symbol:   class Ranges

{code}

problem HBase had that also arose in {{hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferKeepalive.java}}

patch fixing two compilation errors after Guava dropped classes

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12613889/HADOOP-10101-002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1565 javac compiler warnings (more than the trunk's current 1546 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3287//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/3287//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3287//console

This message is automatically generated.

Link to HDFS-5518: the test/review side of the HDFS patch

Deprecation warnings of
* com.google.common.collect.ComparisonChain
* com.google.common.base.Stopwatch

Replacing them with their successors would change more code but reduce the effort on the next guava update, when perhaps these files go away completely
{code}
291a292
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionUtil.java:[112,6] [deprecation] compare(boolean,boolean) in com.google.common.collect.ComparisonChain has been deprecated
293a295,296
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/JvmPauseMonitor.java:[156,21] [deprecation] Stopwatch() in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/JvmPauseMonitor.java:[165,32] [deprecation] elapsedMillis() in com.google.common.base.Stopwatch has been deprecated
432a436,439
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java:[372,19] [deprecation] Stopwatch() in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java:[377,22] [deprecation] elapsedTime(java.util.concurrent.TimeUnit) in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java:[378,10] [deprecation] elapsedTime(java.util.concurrent.TimeUnit) in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java:[380,28] [deprecation] elapsedTime(java.util.concurrent.TimeUnit) in com.google.common.base.Stopwatch has been deprecated
435a443,444
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java:[133,45] [deprecation] Stopwatch() in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java:[437,30] [deprecation] elapsedMillis() in com.google.common.base.Stopwatch has been deprecated
437a447,448
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNode.java:[330,19] [deprecation] Stopwatch() in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNode.java:[334,18] [deprecation] elapsedMillis() in com.google.common.base.Stopwatch has been deprecated
438a450,453
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMultiThreadedHflush.java:[103,21] [deprecation] Stopwatch() in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMultiThreadedHflush.java:[106,22] [deprecation] elapsedTime(java.util.concurrent.TimeUnit) in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMultiThreadedHflush.java:[279,21] [deprecation] Stopwatch() in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMultiThreadedHflush.java:[284,44] [deprecation] elapsedMillis() in com.google.common.base.Stopwatch has been deprecated
447a463,466
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestChunkedArrayList.java:[72,23] [deprecation] Stopwatch() in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestChunkedArrayList.java:[77,51] [deprecation] elapsedMillis() in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestChunkedArrayList.java:[84,23] [deprecation] Stopwatch() in com.google.common.base.Stopwatch has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestChunkedArrayList.java:[89,51] [deprecation] elapsedMillis() in com.google.common.base.Stopwatch has been deprecated
{code}

Any update on this Jira? 
If no objection, I would like to take-up and replace all deprecated with their successors..

I'm happy for you to take it up!

Attaching the same patch as in HDFS-5518, with Hadoop QA overall +1.

Hi [~stevel@apache.org], Could you take a look at changes...?
Thanks,

This is the previous patch resynced with trunk and all trailing CR's stripped. 

Vinay, can you make sure that your editor/OS/SCM tool isn't adding the wrong line endings?

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12617772/HADOOP-10101.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3367//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3367//console

This message is automatically generated.

{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12619314/HADOOP-10101-004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3368//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3368//console

This message is automatically generated.

Because 3rd party Hadoop dependencies leak to the client and apps, this is an incompatible change (Guava 15 removed some classes). This may affect applications that use those classes avail in older versions of Guava. 

IMO, we have to wait till Hadoop 3 for this change. Plus, we should add this to the compat guidelines.

# marked the patch as targeting 3.0
# are there any specific applications which you know will break! or is this just a theoretical concern?
# I did write a section on dependency compatibility for the docs, precisely baca use they are visible. It comes down to "there are no guarantees"

Pint #3 doesn't mean that we should push this in to branch 2, just that we explicitly state that we don't make any guarantees about dependency compatibility -I'd we did we would never be able to upgrade anything

bq. #2, if somebody is using the same Guava classes that break HDFS.

bq. #3, we should figure this out somehow, either using classloader magic or shading to hide all 3rd party dep JARs from the user space. But this is a 3.x timeframe thing.

-yes, but which apps are they? As I don't want us to be constrained fir theoretical reasons.
-MR jobs can already override classloaders
-there's some OSGi JIRAs, which won't break compatibility as they only declare what is exported. Once we have them in, downstream apps than want isolation can use OSGi classloaders with having to try to roll their own solutions


You'll only know when you release with Guava 15 and users apps start breaking.

MR jobs can use classloading isolation, yes. But only on the task side and by default is OFF.

OSGi requires changes in existing apps.

My point is, and I think you agree, we cannot do this in 2.x timeframe.

# yes, this must be a 3.x patch. I'm only putting in low/no risk updates into branch-2.
# I would like to identify apps at risk! as otherwise they only know of the problem at ship time.
# I mentioned OSGi as you mentioned 'classpath magic'. We don't need more magic, and OSGi enabling branch 2 would be transparent to existing apps, while isolating new ones from change.
# Shading has its own flaws! including stopping ops teams from auditing and updating an installation from a security perspective.



Guava dependency injection pre v16 has been broken by Java 7u51; as/when google backport the fix to the 15.x branch we will need to upgrade -as downstream clients will need to do their own fix.

This patch needs to be rebased, and it is probably okay to move guava towards v17.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12619314/HADOOP-10101-004.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4096//console

This message is automatically generated.

[~wheat9], if we make this change, per prior discussion in this JIRA (se above), it will live only in trunk, correct?

Correct me if I'm wrong, this patch should only be committed to trunk based on [~stevel@apache.org]'s comments.

ok, gr8 we are sync on this going into trunk.

There is another JIRA going on to bump up trunk to jdk1.7.

I think that is related to this as well and we should revive the thread we had in dev@ a while ago to come up with a balance on how to move to newer components without making it too difficult for backporting things to stable releases which cannot move to those new components.

+1 on going to guava 17 on the trunk. If we stuck with 15, it would be obsolete from the moment when hadoop 3 is released.

It would have to be on trunk, guava is a troublespot all round. I'm almost tempted to see if we can repimplement the most common uses (checkArguments, visibleForTesting, and some of the collections) and pull it out. 

Some of the fixes for 7u55+ changes are only in more recent versions of Guava, so the update should follow a java 7 migrate in trunk

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12619314/HADOOP-10101-004.patch
  against trunk revision 4be9517.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4698//console

This message is automatically generated.

Could Hadoop start using the maven shade plugin to relocate the guava classes it uses into a private package? Then downstream projects that make use of Guava wouldn't be impacted by whatever version Hadoop chooses.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12619314/HADOOP-10101-004.patch
  against trunk revision 5bd048e.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5105//console

This message is automatically generated.

The patch needs to be updated because HADOOP-11032 is committed. BTW, guava v18 removes Enums#valueOfFunction. This patch includes the fix.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12695059/HADOOP-10101-005.patch
  against trunk revision 9850e15.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

      {color:red}-1 javac{color}.  The applied patch generated 1199 javac compiler warnings (more than the trunk's current 1187 warnings).

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.ipc.TestRPCWaitForProxy
                  org.apache.hadoop.fs.shell.TestXAttrCommands

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5524//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5524//artifact/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5524//console

This message is automatically generated.

The failure of TestXAttrCommands is related. We need to make error handling consistent before and after the change. Attaching new patch to address the issue.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12695092/HADOOP-10101-006.patch
  against trunk revision caf7298.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5527//console

This message is automatically generated.

We need to replace LimitInpuStream with ByteStreams.limit(java.io.InputStream, long) instead in addition to Enums#valueOfFunction.

I think we shouldn't use functions annotated as Beta or Deprecated in upstream Guava.

Attaching patch for fixing it.

[~ozawa], you might want to take a look at HADOOP-11286 and HADOOP-11470.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12695199/HADOOP-10101-007.patch
  against trunk revision 7882bc0.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

      {color:red}-1 javac{color}.  The applied patch generated 1199 javac compiler warnings (more than the trunk's current 1187 warnings).

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5529//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5529//artifact/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5529//console

This message is automatically generated.

# without this patch, what is the max guava version Hadoop 2.6+ runs against?
# can this patch (without the actual POM change) go into branch-2
# ...so then the switch to 18.0 would be the branch-3 change

also, minor, please set a LOCALE_EN in {{en.toUpperCase()}} for consistent behaviour round the world. That's a bug in the existing code that could be fixed here

Along that line, if we can maximize the guava compatibility within the hadoop code (which several previous HADOOP JIRAs achieved), that is already a win for users. +1 for making code changes to make things compatible.

As for the patch, note that HADOOP-11286 introduced the Hadoop version of LimitInputStream. It'd be good to use it to remove the dependency on guava's LimitInputStream.

Thanks Steven and Sangjin for your review.

We can upgrade Guava to 14.0.1 without the patch because: 
1. LimitInputStream is removed from 15.0: http://docs.guava-libraries.googlecode.com/git-history/5e074af6a526af39a4410a836517e57c72aa50bb/javadoc/com/google/common/io/LimitInputStream.html
2. Enums.valueOfFunction is removed from 18.0: http://docs.guava-libraries.googlecode.com/git-history/v16.0.1/javadoc/com/google/common/base/Enums.html

My understand is as follows:
1. For branch-2, we replace deprecated methods and en.toUpperCase() without pom.xml change(, or should we upgrade the version to 14.0.1?).
2. For trunk, we replace deprecated methods and en.toUpperCase() with pom.xml change(guava will be 18.0?).

Please let me know if I'm wrong. I'll update the patches soon.

YARN-3029 addresses locale problem partially. Linking to the issue.

HADOOP-11286 replaced all use of guava's LimitInputStream with Hadoop's own. It appears a new reference to guava's LimitInputStream has crept in since then.

Since we introduced Hadoop's LimitInputStream, we might as well stick with that approach. I understand it is not perfect (as people can still introduce new references without enforcement), but it still gives us value.

Could you fix that to use Hadoop's LimitInputStream rather than using ByteStreams.limit()? That way it can be committed to branch-2 as well.

If we fix that one, then the compatibility is expanded to prior to 18.0, correct?

[~sjlee0], Good point, you're correct. We can upgrade guava to 17.0 with hadoop's LimitInputStream instead of ByteStreams.limit.

Am I missing something here? If we push this to branch-2 that would be an incompatible change, we may break applications that that use ByteStreams.limit.


Sorry what I said may sound confusing.

I understand that we are *NOT* upgrading guava in branch-2. As such, any change that upgrades guava cannot go into branch-2 as is. But code changes that make it compatible with different versions of guava can go into branch-2. I was suggesting pushing those code changes to branch-2. On trunk, we can also upgrade guava.

I noticed that Enums#stringConverter is supported from 16. This change cannot be applied to branch-2 since Guava is not updated in branch-2.

got it, thx

Attaching a patch for trunk:

1. Updating Guava to 18.0.
2. Using org.apache.hadoop.util.LimitInputStream instead of Guava's one.
3. Using Enums.stringConverter instead of Enums.valueOfFunction.
4. Using only Locale.ENGLISH for String#toUpperCase/toLowerCase. 

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12695405/HADOOP-10101-008.patch
  against trunk revision e36ef3b.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 17 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1199 javac compiler warnings (more than the trunk's current 1187 warnings).

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 26 new Findbugs (version 2.0.3) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-annotations hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-nfs hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-maven-plugins hadoop-tools/hadoop-azure hadoop-tools/hadoop-distcp hadoop-tools/hadoop-extras hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-rumen hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.conf.TestJobConf
                  org.apache.hadoop.mapreduce.TestLargeSort
                  org.apache.hadoop.mapred.gridmix.TestHighRamJob
                  org.apache.hadoop.hdfs.server.balancer.TestBalancer

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5542//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5542//artifact/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5542//artifact/patchprocess/newPatchFindbugsWarningshadoop-rumen.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5542//artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5542//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs-httpfs.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5542//artifact/patchprocess/newPatchFindbugsWarningshadoop-maven-plugins.html
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5542//artifact/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5542//console

This message is automatically generated.

* findbugs warnings all seem unrelated
* javac warnings are a mix of spurious and more deprecated warnings; presumably some are guava 18 telling hadoop of for having source compatible with 11.x. Those warnings should be viewed as a sign that those methods will be going away at some point, which will matter for Hadoop-on-Guava 11.

I wouldn't worry about them; just the tests

bq.findbugs warnings all seem unrelated

Raised HDFS-7709 to address the findbug warnings

Thanks Steve for your review. I'll check the javac warnings.

Attaching a patch for trunk. Replaced deprecated methods with proper methods:
* Objects.toStringHelper-> MoreObjects.toStringHelper
* MoreExecutors.sameThreadExecutor -> MoreExecutors.newDirectExecutorService
* Iterators.emptyIterator -> Collections.emptyIterator

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12696476/HADOOP-10101-009.patch
  against trunk revision 42548f4.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 18 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1151 javac compiler warnings (more than the trunk's current 1150 warnings).

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 26 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-annotations hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-nfs hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-maven-plugins hadoop-tools/hadoop-azure hadoop-tools/hadoop-distcp hadoop-tools/hadoop-extras hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-rumen hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.ipc.TestRPCWaitForProxy
                  org.apache.hadoop.mapreduce.TestLargeSort
                  org.apache.hadoop.conf.TestJobConf
                  org.apache.hadoop.mapred.gridmix.TestHighRamJob

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5579//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5579//artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5579//artifact/patchprocess/newPatchFindbugsWarningshadoop-maven-plugins.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5579//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs-httpfs.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5579//artifact/patchprocess/newPatchFindbugsWarningshadoop-rumen.html
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5579//artifact/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5579//console

This message is automatically generated.

[~stevel@apache.org] I checked Javac warnings: these warnings look not related to the patch. I also checked findbugs are not related. Could you take a look at 009 patch?

bq. I also checked findbugs are not related.
I've attached patch to fix HDFS-7709, kindly review it. Thanks!

Rakesh's patch was merged on HDFS-7709.

Attaching v9 patch again.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12698468/HADOOP-10101-009.patch
  against trunk revision 3826277.

    {color:red}-1 patch{color}.  Trunk compilation may be broken.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5673//console

This message is automatically generated.

Refreshed a patch.

Refreshed a patch.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12698904/HADOOP-10101-010.patch
  against trunk revision 6804d68.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 18 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1152 javac compiler warnings (more than the trunk's current 1151 warnings).

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 19 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-annotations hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-nfs hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-maven-plugins hadoop-tools/hadoop-azure hadoop-tools/hadoop-distcp hadoop-tools/hadoop-extras hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-rumen hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.conf.TestJobConf
                  org.apache.hadoop.mapred.gridmix.TestHighRamJob

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5696//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5696//artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5696//artifact/patchprocess/newPatchFindbugsWarningshadoop-maven-plugins.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5696//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5696//artifact/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5696//console

This message is automatically generated.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12698912/HADOOP-10101-010.patch
  against trunk revision ef950ea.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 18 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1152 javac compiler warnings (more than the trunk's current 1151 warnings).

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 19 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-annotations hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-nfs hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-maven-plugins hadoop-tools/hadoop-azure hadoop-tools/hadoop-distcp hadoop-tools/hadoop-extras hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-rumen hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.conf.TestJobConf
                  org.apache.hadoop.mapred.gridmix.TestHighRamJob
                  org.apache.hadoop.fs.http.client.TestHttpFSFWithWebhdfsFileSystem

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5698//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5698//artifact/patchprocess/newPatchFindbugsWarningshadoop-maven-plugins.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5698//artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5698//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5698//artifact/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5698//console

This message is automatically generated.

The test failures are not related to the patch. [~stevel@apache.org], do you mind taking a look? I'll create a patches for branch-2 after trunk code.

Talked with Steve offline. The patch is now very large, so we should split them:

1. Fix up usage of Guava's methods for both branch-2 and trunk on HADOOP-11600.
2. Locale bug is addressed on HADOOP-11602.
3. trunk-specific change to upgrade Guava 18.0 on this issue(HADOOP-10101).

3 depends on 1, so I'll submit a patch after 1 is committed.

This patch is only for trunk and can be applied after HADOOP-11600 is committed.

Submitting v11 patch since HADOOP-11600 was committed.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12699092/HADOOP-10101-011.patch
  against trunk revision f0412de.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1152 javac compiler warnings (more than the trunk's current 1151 warnings).

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5727//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5727//artifact/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5727//console

This message is automatically generated.

The javac warning looks not related to the patch. [~stevel@apache.org], could you take a look at v11 patch?

I think [~busbey] made a good point. I'm sure there are some practical concerns, but as more libraries start to depend on more recent guava versions it becomes more of a problem.
For now I've taken the reverse approach for our integration tests (shading dropwizard 0.8.1 w. relocated guava classes) to allow both hadoop and dropwizard services to run in one jvm.

I hit something similar in tez recently and looking at this jira, I am wondering as to why shading ( relocating all classes ) the guava jar is not an option?

Consider the following approach: 
   - introduce a new hadoop-guava module which depends on guava version X.
      - hadoop-guava relocates all classes in guava to org.apache.hadoop.shaded.guava 
   - all code in hadoop depends on hadoop-guava and excludes the google-guava dependency

I did try a quick prototype approach on branch-2 and it seems to function without directly pulling in guava ( except of course - the curator guava dependency problem ) 

\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  18m 56s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:red}-1{color} | javac |   7m 35s | The applied patch generated  1  additional warning messages. |
| {color:green}+1{color} | javadoc |   9m 37s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 43s | The applied patch generated  3 new checkstyle issues (total was 83, now 86). |
| {color:red}-1{color} | checkstyle |   2m 23s | The applied patch generated  2 new checkstyle issues (total was 43, now 44). |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 20s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   4m 19s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | common tests |  22m  2s | Tests passed in hadoop-common. |
| {color:red}-1{color} | hdfs tests | 160m 45s | Tests failed in hadoop-hdfs. |
| | | 227m 59s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.hdfs.TestDistributedFileSystem |
|   | hadoop.hdfs.TestAppendSnapshotTruncate |
|   | hadoop.hdfs.TestLeaseRecovery2 |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12699092/HADOOP-10101-011.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 419c51d |
| javac | https://builds.apache.org/job/PreCommit-HADOOP-Build/7298/artifact/patchprocess/diffJavacWarnings.txt |
| checkstyle |  https://builds.apache.org/job/PreCommit-HADOOP-Build/7298/artifact/patchprocess/diffcheckstylehadoop-common.txt https://builds.apache.org/job/PreCommit-HADOOP-Build/7298/artifact/patchprocess/diffcheckstylehadoop-hdfs.txt |
| hadoop-common test log | https://builds.apache.org/job/PreCommit-HADOOP-Build/7298/artifact/patchprocess/testrun_hadoop-common.txt |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HADOOP-Build/7298/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/7298/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf906.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/7298/console |


This message was automatically generated.

Github user afs commented on the pull request:

    https://github.com/apache/jena/pull/87#issuecomment-123468596
  
    org.apache.hadoop:hadoop-common:jar:2.6.0 depends on com.google.guava:guava:jar:11.0.2 (provided).  
    
    Just because Jena tests work isn't evidence Guava 16.0.1 works unfortunately.  Jena is a library and may be co-resident with other code.  See HADOOP-10101 including the initial attempt at migration to 15.0 which did not go in. At this very late stage, it needs a strong advantage for users for change.
    
    (and this is why we shade guava for Jena!)


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  5s{color} | {color:red} HADOOP-10101 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12699092/HADOOP-10101-011.patch |
| JIRA Issue | HADOOP-10101 |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/9842/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Ping, any plan on upgrading Guava in this thread? btw, Guava is on 19.0.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  6s{color} | {color:red} HADOOP-10101 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HADOOP-10101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12699092/HADOOP-10101-011.patch |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/10648/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



I think it is high time we bump up the dependency however we should look to hiding it too i.e. shading it as needed so that downstream users can use whatever version of guava they want to use.  

Some questions:
  - why version 18 and not 19? Guava 19 was released in 2015 or so 
  - have you confirmed that no deprecated and/or beta apis are being used?
  - I believe there are some places where guava objects are being used in public facing APIs. This might not be something to address in this jira but should definitely be looked at as a blocker before the next release in addition to my initial comment.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  7s{color} | {color:red} HADOOP-10101 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HADOOP-10101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12699092/HADOOP-10101-011.patch |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11244/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



whatever the latest version goes hitesh, that's what the title says. the Hadoop code is designed to work with later versions (i.e we've moved off bits that were taken away), but still ship with an older one. I'd love to bump us up to a new one

We hit one problem with Guava version 18 for HDFS, The application we built is based on grpc, guava version 18 is minimum required

Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Enums.valueOfFunction(Ljava/lang/Class;)Lcom/google/common/base/Function;
        at org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand.<clinit>(XAttrCommands.java:69)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:132)
        at org.apache.hadoop.fs.shell.CommandFactory.getInstance(CommandFactory.java:125)
        at org.apache.hadoop.fs.shell.CommandFactory.getInstance(CommandFactory.java:109)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:290)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:352)


> I think it is high time we bump up the dependency however we should look to hiding it too i.e. shading it as needed so that downstream users can use whatever version of guava they want to use. 

I agree with [~hitesh] that we should shade guava.  Then, this JIRA is no longer an incompatible change.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  9s{color} | {color:red} HADOOP-10101 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HADOOP-10101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12699092/HADOOP-10101-011.patch |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11815/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Need to make sure that Guava objects aren't in use in "public" APIs, where public includes those bits of the private APIs which are actually needed to produce working code. There's a lot in YARN, and historically large bits of Hadoop common

[~grey] based on protobuf experience, I woudn't trust any google lib to have backwards compatibility. Think very hard about that before adopting grpc in any of your own code

I would like to resume working on this jira, based on shading approach as Nicholas and Hitesh mentioned. Please give me to try the approach for a few days.

{quote}
why version 18 and not 19? Guava 19 was released in 2015 or so
{quote}

It's because Guava 19 had more incompatible change. Now, 21.0, which requires JDK-8 or upper, has been released. I would like to create a patch to update Guava's version to 21.0, targeting branch-3.

{quote}
have you confirmed that no deprecated and/or beta apis are being used?
{quote}

Not yet. 

{quote}
I believe there are some places where guava objects are being used in public facing APIs. This might not be something to address in this jira but should definitely be looked at as a blocker before the next release in addition to my initial comment.
{quote}

Yeah, let's do this on another jira. I think we should create this wrapper class instead of exposing Guava's objects directly.

{quote}
I did try a quick prototype approach on branch-2 and it seems to function without directly pulling in guava ( except of course - the curator guava dependency problem )
{quote}

Curator have also shared Guava. If we can move the version to up to date version, we can avoid the problem. 

bq. Need to make sure that Guava objects aren't in use in "public" APIs

Are there any tools that analyze public vs. private usage of dependencies? Seems important but I've not seen any.

bq. where public includes those bits of the private APIs which are actually needed to produce working code.

Can you elaborate on this? My (naive) impression was that anything not "leaked" in method-signatures could be shaded.

Is this addressed by using the shaded client introduced by HADOOP-11804 for Hadoop 3? It shades Guava also.

The issue for APIs is that public APIs can't shade things, as then only callers with matching shade declarations can use them (personal experience there with hive & spark & kryo). It's ok privately, but the issue is defining "private" is a hard one, given that things like UGI have for a long time been tagged as private. Essentially: anything down as @LimitedPrivate for MapReduce has to be assumed as "needed by YARN apps" and touched very, very carefully

Thanks Andrew for sharing HADOOP-11804. Now, we can focus on upgrading guava here. The exposed objects of Guava may be a problem as Steve mentioned. I will create a following jira to address the issue.

Attaching a patch to upgrade guava to run tests.

I chose version 21.0, the latest version of Guava. It requires jdk 8 or later and fits in hadoop 3.0.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 16s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 18s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 11s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 10s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m  6s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m  6s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m  6s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m  6s{color} | {color:green} hadoop-project in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 15s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 14m 53s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-10101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12858811/HADOOP-10101.012.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  xml  |
| uname | Linux 905876c9c205 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / cc1292e |
| Default Java | 1.8.0_121 |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11827/testReport/ |
| modules | C: hadoop-project U: hadoop-project |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11827/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



On my local, compilation failed with the patch by changes of Guava's interface. Updating it soon.

Uploading v13 patch.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 16s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 48s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 24s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 10m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  4m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  2m  1s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m 16s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m 21s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 19s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 43s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 10m 20s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red} 10m 20s{color} | {color:red} root generated 8 new + 754 unchanged - 1 fixed = 762 total (was 755) {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  5s{color} | {color:orange} root: The patch generated 3 new + 220 unchanged - 1 fixed = 223 total (was 221) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  4m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  2m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  7m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 26s{color} | {color:green} hadoop-project in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  7m 56s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 13s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 64m  3s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 38m 46s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 52s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}214m  6s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.security.TestKDiag |
|   | hadoop.security.token.delegation.TestZKDelegationTokenSecretManager |
|   | hadoop.util.curator.TestChildReaper |
|   | hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer |
|   | hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA |
|   | hadoop.yarn.server.resourcemanager.TestLeaderElectorService |
|   | hadoop.yarn.server.resourcemanager.TestRMHAForNodeLabels |
|   | hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStorePerf |
|   | hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA |
|   | hadoop.yarn.server.resourcemanager.TestRMStoreCommands |
|   | hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStore |
|   | hadoop.yarn.server.resourcemanager.TestRMRestart |
|   | hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStoreZKClientConnections |
|   | hadoop.yarn.server.resourcemanager.TestReservationSystemWithRMHA |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-10101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12858877/HADOOP-10101.013.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 9522f9ab4b66 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / bb6a214 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| javac | https://builds.apache.org/job/PreCommit-HADOOP-Build/11828/artifact/patchprocess/diff-compile-javac-root.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11828/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11828/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11828/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11828/testReport/ |
| modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11828/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



IIRC, YarnClient has a guava object being used in one of its public APIs. 

OIC, we need to upgrade Curator on YARN-3774 at first. I would like to take Curator's update before solving this issue.

After working to update Curator for a while, I found that Curator 3.x only supports ZooKeeper 3.5.x. On HADOOP-14187, avoiding the drastic change(YARN-3774) now, I would like to suggest updating ZooKeeper dependency to 3.4.9 and Curator dependency to 2.12.0, which supports ZooKeeper 3.4.x. It's because our current focus here is to update Guava - the blocker of this issue is to introduce Curator which includes shaded Guava. 

I tested with v13 patch on HADOOP-14187 on my local:

{code}
mvn install test -Dtest=TestZKDelegationTokenSecretManager,TestChildReaper,TestKDiag,TestDelegationTokenRenewer,TestSubmitApplicationWithRMHA,TestLeaderElectorService,TestRMHAForNodeLabels,TestZKRMStateStorePerf,TestKillApplicationWithRMHA,TestRMStoreCommands,TestZKRMStateStore,TestRMRestart,TestZKRMStateStoreZKClientConnections,TestReservationSystemWithRMHA
{code}

All test passes on my local basically. Flaky tests sometimes fail, but they're not ZK-related. The reason seems to be YARN-5548.

Since HADOOP-14187 is merged, I'm resubmitting Jenkins CI again.

[~hitesh] thanks for sharing the information. IIUC, is the problem method AMRMClient#waitFor? It takes a com.google.common.base.Supplier instance as an argument.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 16s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 13s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 21m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 37s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 56s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 25s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 52s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red} 16m 52s{color} | {color:red} root generated 8 new + 777 unchanged - 1 fixed = 785 total (was 778) {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  1s{color} | {color:orange} root: The patch generated 3 new + 220 unchanged - 1 fixed = 223 total (was 221) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 20s{color} | {color:green} hadoop-project in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m  8s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m  9s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 65m  1s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 39m 58s{color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 42s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}228m 57s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.security.TestKDiag |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-10101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12859873/HADOOP-10101.014.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 1dee64063c26 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / f462e1f |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| javac | https://builds.apache.org/job/PreCommit-HADOOP-Build/11873/artifact/patchprocess/diff-compile-javac-root.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11873/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11873/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11873/testReport/ |
| modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11873/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Uploading v15 patch. It fixes checkstyle warning. javac warnings and TestKDiag failure are not related to the patch: javac warnings are by using deprecated methods which are not related to the patch, and TestKDiag passes on my local.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 17s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 14s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 47s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 21m 20s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 45s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 37s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m 31s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m 13s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  3m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 17m 24s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red} 17m 24s{color} | {color:red} root generated 8 new + 777 unchanged - 1 fixed = 785 total (was 778) {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  1s{color} | {color:orange} root: The patch generated 1 new + 220 unchanged - 1 fixed = 221 total (was 221) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 21s{color} | {color:green} hadoop-project in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m 25s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m  8s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 70m  5s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 40m 14s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 43s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}237m 13s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.ha.TestZKFailoverController |
|   | hadoop.security.TestKDiag |
|   | hadoop.hdfs.server.namenode.TestUpgradeDomainBlockPlacementPolicy |
|   | hadoop.hdfs.server.balancer.TestBalancer |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery |
|   | hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer |
|   | hadoop.yarn.server.resourcemanager.TestRMRestart |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-10101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12859942/HADOOP-10101.015.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 41034f791bc1 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / f462e1f |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| javac | https://builds.apache.org/job/PreCommit-HADOOP-Build/11878/artifact/patchprocess/diff-compile-javac-root.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11878/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11878/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11878/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11878/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11878/testReport/ |
| modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11878/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Patch looks good.  Thanks for working on this well known hard issue -- Hadoop is stuck on Guava 11.

Please take a look the javac/checkstyle warnings.  The test failures seem unrelated.  Please take a look as well.  Thanks.

Fixing checkstyle warning again.

[~szetszwo] thanks for taking a look! More taking a look at deeper, javac warnings were caused by deprecated warnings after upgrading Guava, as you mentioned.I will update it soon. Canceling a patch.

Fixing javac warnings.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 58s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m  6s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 36s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m  6s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 59s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 18s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  3m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 46s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red} 13m 46s{color} | {color:red} root generated 8 new + 776 unchanged - 1 fixed = 784 total (was 777) {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  0s{color} | {color:green} root: The patch generated 0 new + 220 unchanged - 1 fixed = 220 total (was 221) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 16s{color} | {color:green} hadoop-project in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  9m 12s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 15s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 69m  4s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 40m 40s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 41s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}230m 32s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.ha.TestZKFailoverController |
|   | hadoop.security.TestKDiag |
|   | hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation |
|   | hadoop.yarn.server.resourcemanager.TestRMRestart |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-10101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12860260/HADOOP-10101.016.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 3743f55b7a99 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 1280155 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| javac | https://builds.apache.org/job/PreCommit-HADOOP-Build/11904/artifact/patchprocess/diff-compile-javac-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11904/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11904/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11904/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11904/testReport/ |
| modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11904/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



The above report is with v16, not with v17. v17 includes fixes against javac warnings, so please wait for a report with v17 patch.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 58s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m  6s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  4m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 57s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m 29s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 20s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  3m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 48s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 13m 48s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  3s{color} | {color:orange} root: The patch generated 2 new + 388 unchanged - 2 fixed = 390 total (was 390) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  4m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  2s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  8m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m 44s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 18s{color} | {color:green} hadoop-project in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m  4s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m  6s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 65m 50s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  2m 44s{color} | {color:green} hadoop-yarn-common in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 40m 38s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 42s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}233m  2s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.net.TestDNS |
|   | hadoop.yarn.server.resourcemanager.TestRMRestart |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-10101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12860272/HADOOP-10101.017.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux db90677ab43a 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 1280155 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11905/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11905/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11905/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11905/testReport/ |
| modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11905/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Fixing an additional checkstyle warning - removing unused import. 

Another warning checkstyle raised looks checkstyle's bug - indentation should be 6 at the line, and it is now with the patch. https://github.com/checkstyle/checkstyle/issues/1174

{quote}
./hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java:66:      return ComparisonChain.start().compareFalseFirst(!elis1.isLocalLog(),: 'method def' child have incorrect indentation level 6, expected level should be one of the following: 8, 10, 12. [Indentation]
{quote}


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 14s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 17m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  6s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  4m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 58s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  8m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m 54s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 21s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  3m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 20s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 16m 20s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  5s{color} | {color:orange} root: The patch generated 1 new + 388 unchanged - 2 fixed = 389 total (was 390) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  4m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  2m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  8m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  3m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 18s{color} | {color:green} hadoop-project in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m 33s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m  8s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 66m 39s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  2m 37s{color} | {color:green} hadoop-yarn-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 40m 23s{color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 42s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}240m 37s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.security.TestKDiag |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-10101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12860315/HADOOP-10101.018.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 84d71d01fb1b 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / ab759e9 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11908/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11908/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/11908/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11908/testReport/ |
| modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11908/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



+1 the 018 patch is perfect, thanks!

Thanks Nicholas for taking a look. Checking this in within 2 hours.

Committed this to trunk. Thanks Nicholas and Steve for your review, and thanks people who joined this issue for your comments.

SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11461 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11461/])
HADOOP-10101. Update guava dependency to the latest version. (ozawa) (ozawa: rev 84ddedc0b2d58257d45c16ee5e83b15f94a7ba3a)
* (edit) hadoop-project/pom.xml
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/curator/ChildReaper.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MsInfo.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQJMWithFaults.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/MetricsCache.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInotifyEventInputStream.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/WebApp.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/AbstractMetric.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManager.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueueManager.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsTag.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AclTransformation.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ApplicationSubmissionContextPBImpl.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/XAttrCommands.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AllocationFileLoaderService.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/source/JvmMetricsInfo.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MetricsInfoImpl.java


please, please, please write a release note for this. it is very likely to be disruptive to downstream.

It would be good to proactively reach out to downstreams who aren't using the shaded clients about this change. I looked at CDH, and found these that seem to consume hadoop-hdfs directly:

{noformat}
-> % ag -l -G pom.xml "<artifactId>hadoop-hdfs</artifactId>" | cut -d/ -f 1 | sort -u
crunch
flume-ng
hadoop
hbase
hbase-indexer
hive
impala
kite
oozie
pig
sentry
sqoop
{noformat}

This guava change is a potential concern for yarn timeline service v2 as well. It actually broke the tests in org.apache.hadoop.yarn.server.timelineservice since timeline service v2 in yarn uses hbase (which uses hadoop 2.5). YARN-6414 .  

FYI that I've filed HADOOP-14284 to shade Guava everywhere. Shading isn't required for all dep upgrades, but given how many projects are consuming our private artifacts and also use Guava, this is almost sure to break a lot of users.

If we don't finish the shading in time for 3.0.0-alpha3, I'm going to revert this JIRA (HADOOP-10101) and related out of trunk. As a reminder, alpha3 is currently planned for May 15th.

I'm going to create a JIRA so that we can build Hadoop trunk against older versions of Guava; that way andrew can build the release with an older version and we can see what breaks. 

I'm concluding that even if we ship with a newer guava v., we should build with something older (18?), so that the signatures of methods compiled against all match there. When you build with 20, overloads of checkArgument() are enough to stop existing code linking to older guava versions, *even if the method call hasn't changed*

Like my comments in HADOOP-14284, shading guava everywhere failed to convince me that that is a right approach. I think we should revert this patch and stay the same version with downstream projects unless someone convince me why we have to upgrade guava (the poor backward compatibility jar) here.

I've been lurking on these Guava tickets for while a while, and I just want to chime in to say I'm going to be incredibly disappointed if this gets reverted.  The number of watchers on this ticket should be an indicator in itself, and I'm sure for every person who bothered creating a jira account, there are a couple hundred who were bitten by this.

Has anyone contacted the maintainers on the major affected downstream projects like Spark and HBase?  My guess is that they would be more than happy to help work around any breakages this upgrade causes -- if they are like us, they would be overjoyed at being able to finally upgrade. 

Hadoop's ancient Guava dependency is the single largest issue we run into when putting any other third party jar on our client or task classpath.  There are many libraries developed outside the Hadoop ecosystem which (rightfully) assume they can use a newer version of Guava, and we regularly either have to do horrible hacks to get these onto the client classpath, or tell developers not to use them.  This is an incredible time-waster that shouldn't exist.

I understand the concerns about stability, but this is a major upgrade, no?  I don't think it's acceptable to say that Hadoop will be running Guava 11 until Hadoop 4 comes out in 2025.

bq. Has anyone contacted the maintainers on the major affected downstream projects like Spark and HBase? My guess is that they would be more than happy to help work around any breakages this upgrade causes – if they are like us, they would be overjoyed at being able to finally upgrade.

There is an open jira in HBase to upgrade to guava 21 HBASE-17908 and I just noticed that Stack mentioned they are looking to shade out guava in a prebuild step.

HBASE-13517 provided shaded client and server jars. But these shaded jars can't be used inside of coprocessors (which timeline service in yarn currently makes use of). 

For Spark it does look like their users have run into issues as noted in SPARK-16725 and they are hoping hadoop 3.x will upgrade guava. 

Thanks [~bpodgursky] for sharing your thoughts. We are glad to hear voices from hadoop users as well as developers from hadoop and other related projects.

bq. The number of watchers on this ticket should be an indicator in itself, and I'm sure for every person who bothered creating a jira account, there are a couple hundred who were bitten by this.
The number of watchers doesn't indicate they are supporting this patch but just they pay attention to. They have to because this patch break everything. We can bring this topic to hadoop-user alias to get more audience if needed.

bq. Has anyone contacted the maintainers on the major affected downstream projects like Spark and HBase? My guess is that they would be more than happy to help work around any breakages this upgrade causes – if they are like us, they would be overjoyed at being able to finally upgrade.
I doubt this. I talked with several guys from HBase and Tez community offline - none of them are happy with doing the same change. That means our change are pushing them to create/maintain branches for different version of hadoop (if no shading work in HADOOP-14284 which has side effect though). It also means the releases need to get synchronized, otherwise no downstream project release can work with Hadoop GA release will be a bigger problem.

bq. Hadoop's ancient Guava dependency is the single largest issue we run into when putting any other third party jar on our client or task classpath.
Please don't blame hadoop for this. The real problem here is the poor incompatibility of Guava across different versions and they released 21 major versions across 7 years. The ancient guava dependency are also suffering from this.

bq. There are many libraries developed outside the Hadoop ecosystem which (rightfully) assume they can use a newer version of Guava, and we regularly either have to do horrible hacks to get these onto the client classpath, or tell developers not to use them. This is an incredible time-waster that shouldn't exist.
Is there any protocol for these libraries outside of hadoop to choose an uniform version of guava (like version 21)? If not, bump up guava version here does't help as it will still break these libraries which are using different version. Also, there are also many apps developed within hadoop ecosystem which should be seen as first class citizenship that get totally break by this change. As a system software, we should have some better solution like: application classpath isolation, etc. instead of simply keeping dependency updated.

bq. I understand the concerns about stability, but this is a major upgrade, no? I don't think it's acceptable to say that Hadoop will be running Guava 11 until Hadoop 4 comes out in 2025.
Guava is not Java, I don't think any hadoop releases should bind with specific guava version. It is just a third party library - with poor incompatibility - that's it!

As a committer for Apache Jena (who voted for this ticket and has been watching it hopefully :) ), I can tell you that the archaic version of Guava in Hadoop is the _only_ reason we must maintain a shaded (modern) version of the same dependency in our project. This is confusing at best for both developers and users. Hadoop moving forward on this would be a consummation devoutly to be wished.

Thanks [~junping_du].  Sorry if my reply came off as criticizing Hadoop, I totally recognize that this is a difficult problem.  I understand the problems are fundamentally caused by Guava breaking changes, not by Hadoop.  But I don't think I'm going to be able to convince Guava to stop doing major releases, and at the end of the day it's a super widely used library, so I figured I should try here instead :)

This isn't really a scientific analysis, but my anecdotal experience is that there aren't * that * many breaking changes between each Guava release, and the odds are that you can run Guava 21 against something compiled against 19, and you'll likely be fine.  There are some specific interface-to-abstract-class changes between 11 and 13 which are the big problem (IIRC, might be wrong on the versions).

I agree that classpath isolation is the "real" fix here.  I've been following HADOOP-11656 as well, ("Classpath isolation for downstream clients") but there's a ton of debate there about the costs of shading etc, so I'm not sure how close that is to being pushed. 

Anyway, thanks again for your work on this.  I'm really hoping we can get a newer version, but I understand the release synchronization costs.  Just wanted to mention that this causes user pain that might not be reflected on JIRA too often, since it's something we can generally work around.

{quote}
I doubt this. I talked with several guys from HBase and Tez community offline - none of them are happy with doing the same change. That means our change are pushing them to create/maintain branches for different version of hadoop (if no shading work in HADOOP-14284 which has side effect though). It also means the releases need to get synchronized, otherwise no downstream project release can work with Hadoop GA release will be a bigger problem.
{quote}

Offline conversations don't count for much in ASF projects. Speaking here as a member of the HBase PMC I will be super happy to upgrade our Guava dependency. FWIW I'm pretty sure we'll just switch to Hadoop's shaded artifacts to allow us to ignore the version used here.

bq. Offline conversations don't count for much in ASF projects.
Yes. But at least it convince me that not everyone from every project are welcome this guava upgrade.

bq. Speaking here as a member of the HBase PMC I will be super happy to upgrade our Guava dependency.
That's still count as your individual opinion. Any public jira or email discussion in HBase community to reflect any consensus on "happy upgrade"?

bq. FWIW I'm pretty sure we'll just switch to Hadoop's shaded artifacts to allow us to ignore the version used here.
I am -1 on shading guava everywhere approach (HADOOP-14284) unless someone convince me that is right approach. 

I am not familiar with the code base, so i do not know, what exactly is exposed to the public (and also used by downstream versions like Spark). But it is possible to expose Guava stuff, so that is compatible to the old guava version, but internally using the latest, but shaded version. For example, internally you can use ListenableFuture from the shaded version, but having a delegate to the ListenableFuture from the public one.

[~ajs6f] [~bpodgursky] : you should know that even though Hadoop ships a woefully out of date Guava version, it does run against later ones, up to 18, AFAIK; every time they pull something we've had to jump though hoops (HADOOP-10961) to implicitly keep up. 

Shading is going to be the Java 8 solution, until java 9 gives us {{jlink}} and we can adopt it (how? when?); IMO we'll have to shade not just guava, but if we can include Avro, then, along with protobuf (done) we've got the high-brittleness and IPC-critical serialization libs sorted

As far as we are aware, there are no APIs marked as public which export guava classes. This is a good thing, and does at least give us some freedom.

