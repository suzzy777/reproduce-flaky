This is the whole jstack 

And when hbase client timeout begin,  i found some WARN in  regionserver's log like below:
{code}
2015-08-02 11:22:31,091 WARN  [B.DefaultRpcServer.handler=10,queue=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2003001 service: ClientService methodName: Multi size: 606 connection: 10.11.11.151:39022: output error
2015-08-02 11:22:31,092 WARN  [B.DefaultRpcServer.handler=10,queue=1,port=60020] ipc.RpcServer: B.DefaultRpcServer.handler=10,queue=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2015-08-02 11:22:39,728 DEBUG [LruStats #0] hfile.LruBlockCache: Total=2.98 GB, free=160.88 MB, max=3.14 GB, blocks=3373164288, accesses=72835847, hits=45919402, hitRatio=63.05%, , cachingAccesses=67264519, cachingHits=45352036, cachingHitsRatio=67.42%, evictions=289532, evicted=1080418, evictedPerRun=3.7316012382507324
2015-08-02 11:24:19,922 WARN  [B.DefaultRpcServer.handler=0,queue=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2005736 service: ClientService methodName: Multi size: 606 connection: 10.11.11.152:2419: output error
2015-08-02 11:24:19,924 WARN  [B.DefaultRpcServer.handler=0,queue=0,port=60020] ipc.RpcServer: B.DefaultRpcServer.handler=0,queue=0,port=60020: caught: java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.writev0(Native Method)
        at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:51)
        at sun.nio.ch.IOUtil.write(IOUtil.java:148)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:503)
        at org.apache.hadoop.hbase.ipc.BufferChain.write(BufferChain.java:106)
        at org.apache.hadoop.hbase.ipc.RpcServer.channelWrite(RpcServer.java:2224)
        at org.apache.hadoop.hbase.ipc.RpcServer$Responder.processResponse(RpcServer.java:1012)
        at org.apache.hadoop.hbase.ipc.RpcServer$Responder.doRespond(RpcServer.java:1089)
        at org.apache.hadoop.hbase.ipc.RpcServer$Call.sendResponseIfReady(RpcServer.java:503)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
        at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:114)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:94)
        at java.lang.Thread.run(Thread.java:745)

2015-08-02 11:25:09,832 WARN  [RpcServer.reader=2,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
        at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2244)
        at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1423)
        at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:798)
        at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:589)
        at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:564)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

{code}

I check the code,  and found that every time call readBlock,  it will acqurie offsetLock no matter the blockcache is on or off. 
I think it is a bug.  Only the blockcache on,  it is necessary to recheck the cache.   I will submit a patch

This is a patch

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12748347/1.patch
  against master branch at commit 4b6598e394bae67b54d6f741dd262afe03b2c133.
  ATTACHMENT ID: 12748347

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14955//console

This message is automatically generated.

After check the code, i think the RS was blocked due to 
If some clients get some large row simultaneously, if the table's block cache is closed,  all get request will try to acquire the offsetLock 
and only one lock successfully, the others will wait before lock is released. All the handlers will be exhausted, and RS will be blocked!

rename the patch

I think the problem here is we only allow one thread read a HFileBlock at the same time even if we disable the BlockCache.
If BlockCache is enabled, this is useful since other threads can get the block directly from BlockCache after one thread successfully read the block and put it into BlockCache.
This is a needless overhead if we disable BlockCache although I think one should always turn on BlockCache if there are many requests to the same HFileBlock. We should just read the HFileBlock from HDFS.

[~chenheng], could you please prepare a patch for master first? Thanks.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12748356/HBASE-14178-0.98.patch
  against 0.98 branch at commit 4b6598e394bae67b54d6f741dd262afe03b2c133.
  ATTACHMENT ID: 12748356

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 21 warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14957//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14957//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14957//artifact/patchprocess/checkstyle-aggregate.html

  Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14957//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14957//console

This message is automatically generated.

Add patch for master

when read block read cache, it needs:
1. blockCache is open
2. the table's column family's blockCache is open
3. read request cache is true

So I modify the 'IF condition' to read cache

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12748380/HBASE-14178.patch
  against master branch at commit 4b6598e394bae67b54d6f741dd262afe03b2c133.
  ATTACHMENT ID: 12748380

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14958//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14958//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14958//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14958//console

This message is automatically generated.

Just moving the below if check inside the if block if (cacheConf.isBlockCacheEnabled())   Will do the work no?
{code}
 if (!useLock) {
          // check cache again with lock
          useLock = true;
          continue;
        }
{code}
When the block cache is enabled, we should check for the requested block in it. (This current scan may or may not be saying the block should be cached after read)

sorry, I forget one import thing that if the block catetory is index or bloom, we should skip check the table column family's cache and request's cache configuration

i update the patch, now the logic to read cache is:

1. blockCache is open
2. if expectedBlockType is null
           just read cache
     else
           2.1 the request cache is open
           2.2 table's column family cache is open
           2.3 check the blockType.category

The outer if check  if (cacheConf.isBlockCacheEnabled())  should include the table CF block cache enabled flag also then.

{quote}
Just moving the below if check inside the if block if (cacheConf.isBlockCacheEnabled()) Will do the work no?
{code}
 if (!useLock) {
          // check cache again with lock
          useLock = true;
          continue;
        }
{code}
When the block cache is enabled, we should check for the requested block in it. (This current scan may or may not be saying the block should be cached after read)
{quote}

thanks for your reply!

just move the if block " if (!useLock)" into if (cacheConf.isBlockCacheEnabled())  will not solve the problem！
because if the blockCache be enabled, but the table's family cache is closed,  it still acquire the lock.  So i update the 'if' condition of read cache, and submit the patch v2 already. 




bq.just move the if block " if (!useLock)" into if (cacheConf.isBlockCacheEnabled()) will not solve the problem！
because if the blockCache be enabled, but the table's family cache is closed, it still acquire the lock.
Agree.  That is why I said
bq.The outer if check if (cacheConf.isBlockCacheEnabled()) should include the table CF block cache enabled flag also then.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12748389/HBASE-14178_v2.patch
  against master branch at commit 4b6598e394bae67b54d6f741dd262afe03b2c133.
  ATTACHMENT ID: 12748389

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.client.TestFromClientSide
                  org.apache.hadoop.hbase.client.TestFromClientSideWithCoprocessor

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14960//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14960//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14960//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14960//console

This message is automatically generated.

Testcase failed calculate hitCount of cacheStats after regionServer do Compaction
{code}
      System.out.println("Compacting");
      assertEquals(2, store.getStorefilesCount());
      store.triggerMajorCompaction();
      region.compact(true);
      waitForStoreFileCount(store, 1, 10000); // wait 10 seconds max
      assertEquals(1, store.getStorefilesCount());
      expectedBlockCount -= 2; // evicted two blocks, cached none
      assertEquals(expectedBlockCount, cache.getBlockCount());
      expectedBlockHits += 2;
      assertEquals(expectedBlockMiss, cache.getStats().getMissCount());
      assertEquals(expectedBlockHits, cache.getStats().getHitCount());     {color:red} //Failed here,  cache.getStats().getHitCount() return 2, but expectedBlockHits equals 4 {color}
{code}

I check the code of compaction,  and found that the compaction will use scanner with no blockcache flag, and I will check this flag before read block from BlockCache
So as new logic, the hitcount will be two after compaction.  And I think during compaction, scan request will not use blockcache.

So i think the expectedBlockHits should be two in testcase











Update patch,   modify TestFromClientSide.testCacheOnWriteEvictOnClose:
After compaction, we don't modify the expectedBlockHits

This means when the current scan comes in which says cache read blocks as false, we wont even consult block for reading that?  IMO that is wrong. When block is available, we should try read from that. Agree.. when the table CF is set to be not to cache data from that CF at all, there is no point in looking into the cache. So in cases of BC is disabled as well as the read CF is set to be cache block = false, no need to obtain lock at all.  But the code change seems not just this much..

So IMO there can be 2 changes in code
1. Move the below piece of code inside the check  if (cacheConf.isBlockCacheEnabled())
    {code}
if (!useLock) {
          // check cache again with lock
          useLock = true;
          continue;
        }
{code}
2. The outer if check , (ie.  if (cacheConf.isBlockCacheEnabled()) ) itself should be changed to include the check for the CF level cache setting.  (HCD#setBlockCacheEnabled)

{quote}
This means when the current scan comes in which says cache read blocks as false, we wont even consult block for reading that? IMO that is wrong. When block is available, we should try read from that. Agree.. when the table CF is set to be not to cache data from that CF at all, there is no point in looking into the cache. So in cases of BC is disabled as well as the read CF is set to be cache block = false, no need to obtain lock at all. But the code change seems not just this much..
So IMO there can be 2 changes in code
1. Move the below piece of code inside the check if (cacheConf.isBlockCacheEnabled())
{code}
if (!useLock) {
          // check cache again with lock
          useLock = true;
          continue;
        }
{code}
2. The outer if check , (ie. if (cacheConf.isBlockCacheEnabled()) ) itself should be changed to include the check for the CF level cache setting. (HCD#setBlockCacheEnabled)
Reply
{quote}

Thanks for your reply!  I agree with you!  I has thought that the "cacheBlock" in request means the client could skip the blockCache to read HFileBlock from hdfs directly. I am wrong!
I will not check the cacheBlock before read in BC.


{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12748421/HBASE-14178_v3.patch
  against master branch at commit 4b6598e394bae67b54d6f741dd262afe03b2c133.
  ATTACHMENT ID: 12748421

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14961//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14961//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14961//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14961//console

This message is automatically generated.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12748436/HBASE-14178_v4.patch
  against master branch at commit 4b6598e394bae67b54d6f741dd262afe03b2c133.
  ATTACHMENT ID: 12748436

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

     {color:red}-1 core zombie tests{color}.  There are 2 zombie test(s): 	at org.apache.hadoop.mapred.TestMRIntermediateDataEncryption.testMultipleReducers(TestMRIntermediateDataEncryption.java:70)

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14962//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14962//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14962//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14962//console

This message is automatically generated.

{code}
public boolean shouldCacheBlockOnRead(BlockCategory category) {
    return isBlockCacheEnabled()
        && (cacheDataOnRead ||
            category == BlockCategory.INDEX ||
            category == BlockCategory.BLOOM ||
            (prefetchOnOpen &&
                (category != BlockCategory.META &&
                 category != BlockCategory.UNKNOWN)));
  }
{code}
You can see the call to cacheConf.shouldCacheBlockOnRead(expectedBlockType.getCategory()) checks wrt whether the read request says the block to be cached after this read.  It is not telling abt the CF level setting of whether data to be cached at all or not.   We have to make that info available here in HFileReader.  And when we read the index or meta blocks we have to consult BC.(which u do already)

{quote}
You can see the call to cacheConf.shouldCacheBlockOnRead(expectedBlockType.getCategory()) checks wrt whether the read request says the block to be cached after this read. It is not telling abt the CF level setting of whether data to be cached at all or not.
{quote}

cacheDataOnRead  represents for CF Level cache setting. You can see cacheDataOnRead is initialized in CacheConfig constructor 
{code}
  CacheConfig(final BlockCache blockCache,
      final boolean cacheDataOnRead, final boolean inMemory,
      final boolean cacheDataOnWrite, final boolean cacheIndexesOnWrite,
      final boolean cacheBloomsOnWrite, final boolean evictOnClose,
      final boolean cacheDataCompressed, final boolean prefetchOnOpen,
      final boolean cacheDataInL1) {
    this.blockCache = blockCache;
    this.cacheDataOnRead = cacheDataOnRead;
    this.inMemory = inMemory;
    this.cacheDataOnWrite = cacheDataOnWrite;
    this.cacheIndexesOnWrite = cacheIndexesOnWrite;
    this.cacheBloomsOnWrite = cacheBloomsOnWrite;
    this.evictOnClose = evictOnClose;
    this.cacheDataCompressed = cacheDataCompressed;
    this.prefetchOnOpen = prefetchOnOpen;
    this.cacheDataInL1 = cacheDataInL1;
    LOG.info(this);
  }
{code}

And this constructor is called by another constructor 
{code}
  public CacheConfig(Configuration conf, HColumnDescriptor family) {
    this(CacheConfig.instantiateBlockCache(conf),
        family.isBlockCacheEnabled(),
        family.isInMemory(),
        // For the following flags we enable them regardless of per-schema settings
        // if they are enabled in the global configuration.
        conf.getBoolean(CACHE_BLOCKS_ON_WRITE_KEY,
            DEFAULT_CACHE_DATA_ON_WRITE) || family.isCacheDataOnWrite(),
        conf.getBoolean(CACHE_INDEX_BLOCKS_ON_WRITE_KEY,
            DEFAULT_CACHE_INDEXES_ON_WRITE) || family.isCacheIndexesOnWrite(),
        conf.getBoolean(CACHE_BLOOM_BLOCKS_ON_WRITE_KEY,
            DEFAULT_CACHE_BLOOMS_ON_WRITE) || family.isCacheBloomsOnWrite(),
        conf.getBoolean(EVICT_BLOCKS_ON_CLOSE_KEY,
            DEFAULT_EVICT_ON_CLOSE) || family.isEvictBlocksOnClose(),
        conf.getBoolean(CACHE_DATA_BLOCKS_COMPRESSED_KEY, DEFAULT_CACHE_DATA_COMPRESSED),
        conf.getBoolean(PREFETCH_BLOCKS_ON_OPEN_KEY,
            DEFAULT_PREFETCH_ON_OPEN) || family.isPrefetchBlocksOnOpen(),
        conf.getBoolean(HColumnDescriptor.CACHE_DATA_IN_L1,
            HColumnDescriptor.DEFAULT_CACHE_DATA_IN_L1) || family.isCacheDataInL1()
     );
  }
{code}



upload patch for branch 0.98

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12748601/HBASE-14178-0.98.patch
  against 0.98 branch at commit 931e77d4507e1650c452cefadda450e0bf3f0528.
  ATTACHMENT ID: 12748601

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 21 warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14965//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14965//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14965//artifact/patchprocess/checkstyle-aggregate.html

  Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14965//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14965//console

This message is automatically generated.

[~anoopsamjohn]

{{CacheConfig}} is a bit confusing I think. {{family.isBlockCacheEnabled}} is only equal to {{cacheDataOnRead}}, and we still have chance to put data into {{BlockCache}} if we set {{cacheDataOnWrite}} or {{prefetchOnOpen}} to {{true}} even if we set  {{cacheDataOnRead}} to {{false}}?

So I suggest here we make a new method called {{shouldReadBlockFromCache}}, and check all the possibility that we may put a block into {{BlockCache}}?

Thanks.

I see.. I didnt not check much like how this variable is getting initialized in CacheConfig..  Ya we better do some cleanup there. So much confusing stuff.
bq.and we still have chance to put data into BlockCache if we set cacheDataOnWrite or prefetchOnOpen to true even if we set cacheDataOnRead to false?
I did not test it.  Nice to test with some UTs.  If at CF level we set like never cache the data from this CF into BC, we should NOT cache it at all.  Whatever be value of cacheDataOnWrite or prefetchOnOpen.  If we are not doing so, then those are bugs to be addressed.

bq.So I suggest here we make a new method called shouldReadBlockFromCache, and check all the possibility that we may put a block into BlockCache
Ideally, when the BC is enabled and CF level there is no setting like NOT to cache data into BC, we should try read it from the BC. Also even if the CF level setting is there and we are not reading back Data blocks, then also we have to consult BC.  Still it will be much cleaner to do ur suggestion of adding the new method to CacheConfig. It will look much cleaner.

{quote}
Ideally, when the BC is enabled and CF level there is no setting like NOT to cache data into BC, we should try read it from the BC. Also even if the CF level setting is there and we are not reading back Data blocks, then also we have to consult BC. Still it will be much cleaner to do ur suggestion of adding the new method to CacheConfig. It will look much cleaner.
{quote}

I agree with both of you, I will write a function named shouldReadBlockFromCache in CacheConfig to check all the situations we should read from BC.

But there is one problem.  we acquire lock to ensure next request could read block from BC.  
If cacheDataOnRead is false but cacheDataOnWrite is true, as we discuss, we still read from BC, and acquire the lock.
But after read block from hdfs, we use another condition to decide whether we should cache the block, 
and it will not cache the block when cacheDataOnRead is false and cacheDataOnWrite is true。 
In this situation, the lock is useless.

So i think we will use another 'If' to check whether we should acquire the lock. Do you think so?





Yes, the problem here is the lock, not when to read from cache...So if we can make sure the block will not be put into cache after we fetch it from HDFS, then we can bypass the locking step.

Upload patch 
changes blow:
1.  add function to check all situations we should read BC
2.  add function to check if we should acquire the lock

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12748640/HBASE-14178_v5.patch
  against master branch at commit 931e77d4507e1650c452cefadda450e0bf3f0528.
  ATTACHMENT ID: 12748640

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.client.TestMultiParallel
                  org.apache.hadoop.hbase.trace.TestHTraceHooks
                  org.apache.hadoop.hbase.client.TestScannersFromClientSide
                  org.apache.hadoop.hbase.TestLocalHBaseCluster
                  org.apache.hadoop.hbase.TestMetaTableAccessor
                  org.apache.hadoop.hbase.snapshot.TestRestoreFlushSnapshotFromClient
                  org.apache.hadoop.hbase.client.TestScannerTimeout
                  org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClientWithRegionReplicas
                  org.apache.hadoop.hbase.client.TestMetaWithReplicas
                  org.apache.hadoop.hbase.namespace.TestNamespaceAuditor
                  org.apache.hadoop.hbase.client.TestHCM
                  org.apache.hadoop.hbase.snapshot.TestMobRestoreFlushSnapshotFromClient
                  org.apache.hadoop.hbase.backup.TestHFileArchiving
                  org.apache.hadoop.hbase.client.TestSnapshotFromClientWithRegionReplicas
                  org.apache.hadoop.hbase.client.TestClientPushback
                  org.apache.hadoop.hbase.TestIOFencing
                  org.apache.hadoop.hbase.client.TestClientTimeouts
                  org.apache.hadoop.hbase.client.TestMobSnapshotFromClient
                  org.apache.hadoop.hbase.snapshot.TestFlushSnapshotFromClient
                  org.apache.hadoop.hbase.client.TestCloneSnapshotFromClient
                  org.apache.hadoop.hbase.TestMultiVersions

     {color:red}-1 core zombie tests{color}.  There are 7 zombie test(s): 	at org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge(TestNamespaceAuditor.java:316)

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14966//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14966//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14966//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14966//console

This message is automatically generated.

changes:

1. modify some comments

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12748653/HBASE-14178_v6.patch
  against master branch at commit 931e77d4507e1650c452cefadda450e0bf3f0528.
  ATTACHMENT ID: 12748653

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

     {color:red}-1 core zombie tests{color}.  There are 3 zombie test(s): 	at org.apache.hadoop.hbase.client.TestReplicasClient.testSmallScanWithReplicas(TestReplicasClient.java:606)

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14967//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14967//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14967//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14967//console

This message is automatically generated.

{code}
450	    if (blockType == null) {
451	      return true;
452	    }
{code}
Should false be returned in above condition ?

{quote}
{code}
450	    if (blockType == null) {
451	      return true;
452	    }
{code}
Should false be returned in above condition ?
{quote}

The code in shouldReadBlockFromCache is used to check whether we should read from BC.
When blockType is null, we don't know the blockType.category is Index, bloom or data, So we
just read from BC when BC is enabled. 



[~tedyu] If {{blockType}} is {{null}} then we can not determine if we could cache the block until we actually read it from HDFS. So I think it is appropriate to return false here? We should always acquire the lock unless we can make sure the block will not be cached.

Thanks.

Oh, I looked into wrong ling number...

It is the same reason, if {{blockType}} is {{null}}, the safe way is to read it from {{BlockCache}} first. Maybe it is an {{INDEX}} or {{BLOOM}} block.

Thanks.

Can we get a successful QA run ?

Thanks

Retry.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12748782/HBASE-14178_v6.patch
  against master branch at commit 18c9bb8b54a5bb101f793e555272cbc74b8288cb.
  ATTACHMENT ID: 12748782

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14981//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14981//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14981//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14981//console

This message is automatically generated.

A flaky test(TestFastFail) failed in pre-commit. Retry.

Retry.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12748841/HBASE-14178_v6.patch
  against master branch at commit 18c9bb8b54a5bb101f793e555272cbc74b8288cb.
  ATTACHMENT ID: 12748841

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14983//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14983//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14983//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14983//console

This message is automatically generated.

+1

[~anoopsamjohn] Any concerns of patch v6? Thanks.

Why we need different condition for reading from the cache with and with out lock?  with out lock we do at first , as an optimistic approach.  If block is not there by then, we are doing one more round of check for a possible another concurrent thread doing the caching of this block.  So we use lock then. So am not sure whether to read from cache why it has to have different condition.   Looks like the patch tend to make an impression that we will read from cache *with lock* in order to cache that block into the BC.  But it is not the case.   Sorry if I missed some other discussion parts in above comments

Yes, we doing more round of check with lock because maybe another thread has already cache the block for us. Things happen here is we disable BC for the given family, so it is impossible that another thread will do the work for us, so we just read from HDFS and bypass the second checking BC round.

And as I mentioned above, there are lots of configurations for BC, and {{family.isBlockCacheEnabled()}} is treated as {{cacheDataOnRead}} (You can see the code pasted by [~chenheng], maybe it is a mistake but it is not important for this issue I think, we could open another issue for it). So the safe way to determine if we need the second 'read BC with lock' round is to check if we will put the block back to BC after we read it from HDFS. This is why we introduce a {{shouldLockOnCacheMiss}} method here. Maybe we cound change the name to {{shouldReadAgainWithLockOnCacheMiss}}?

Thanks.

{quote}
Why we need different condition for reading from the cache with and with out lock? with out lock we do at first , as an optimistic approach. If block is not there by then, we are doing one more round of check for a possible another concurrent thread doing the caching of this block. So we use lock then. So am not sure whether to read from cache why it has to have different condition. Looks like the patch tend to make an impression that we will read from cache with lock in order to cache that block into the BC. But it is not the case. Sorry if I missed some other discussion parts in above comments
{quote}

Thanks for your reply!

The lock purpose is to improve the perf when other thread read same block.  If we make sure the block is not in BC,  there is no need to lock. So in {{shouldLockOnCacheMiss}} , we will check if the block will be cached after read from hdfs.



Reading thro the comments and the current code in CacheConfig, I think
{code}
 if (cacheDataOnWrite) {
448	      return true;
449	    }
{code}
These conditions may really be not needed. As you are saying isBlockCacheEnabled() wouild mean cacheDataOnRead is also true.  (I think this setting is going to be implemented per family only).

I see.. I get you now..  Basically we need it as 2 new methods in CacheConfig so as to accommodate some of the issues in today's impl.  Like we discussed, when the CF level BC usage is explicitly disabled, we should honour that. In such a case whatever be the value of cache on write or prefetch option, we should not cache those data blocks..   Here in this patch, we try to stick with the current behaviour. FIne..  We can correct them as part of another jira. 
+1
Just add a TODO with some comments as we discussed here in these new methods and well as old methods in CacheConfig. Which we need change as part of the above fix.

[~ram_krish] A bit strange but now, {{family.isBlockCacheEnabled}} only means {{cacheDataOnRead}} ... {{cacheDataOnWrite}} and some other configurations such as {{prefetchOnOpen}} are separated which means you can set them to {{true}} and will take effect even if you explicitly call {{family.setBlockCacheEnabled(false)}}. So theoretically even if you disable BC at family level, it is still possible that we could find the block in BC...

Okie. Agree for a TODO. +1 for adding a comment regarding this. +1.

changes:
1. Add Todo comments for shouldReadBlockFromCache

[~anoopsamjohn] [~ram_krish] Any concerns on patch v7 then? Thanks.

+1
Pls raise a Jira to track the TODO item in this.  Would be great to see a patch there as well :-)

Nice work.

[~chenheng] Could you please also prepare patch for branches other than master and 0.98? Thanks.

upload patch for 0.98, branch-1, master

Thanks for [~Apache9] , [~anoopsamjohn], [~ram_krish] review,  as we discuss, i create a new issue 14189 

https://issues.apache.org/jira/browse/HBASE-14189

reupload patch for 0.98, branch-1,   the last patch is incorrect....

Pushed to all branches.

Thanks [~chenheng] for the patch(and the quick turn around at last although I have done some revert commits...).

Thanks all guys who helped review the patch.

FAILURE: Integrated in HBase-1.3 #91 (See [https://builds.apache.org/job/HBase-1.3/91/])
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev fe9de40e6c16b6e030a89a759fa278f0e27722aa)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
Revert "HBASE-14178 regionserver blocks because of waiting for offsetLock" (zhangduo: rev 4623c843c137888d606578ed1bc579272a5ab2c2)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev 5c0c389b7a1b32a045e4bc1557b96a56291ab2ab)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java


{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12749008/HBASE-14178_v7.patch
  against master branch at commit 5d2708f628d4718f6267e9da6c8cbafeda66f4fb.
  ATTACHMENT ID: 12749008

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

     {color:red}-1 core zombie tests{color}.  There are 1 zombie test(s): 	at org.apache.hadoop.hbase.procedure2.store.TestProcedureStoreTracker.testRandLoad(TestProcedureStoreTracker.java:186)

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14990//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14990//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14990//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14990//console

This message is automatically generated.

FAILURE: Integrated in HBase-TRUNK #6700 (See [https://builds.apache.org/job/HBase-TRUNK/6700/])
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev 75a6cb2be6ae95654561213a247aa7ba62505072)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java


FAILURE: Integrated in HBase-1.0 #1002 (See [https://builds.apache.org/job/HBase-1.0/1002/])
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev 949092004379c7f2e8f08896c90b59d3a9272fbb)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java


SUCCESS: Integrated in HBase-1.1 #599 (See [https://builds.apache.org/job/HBase-1.1/599/])
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev ade125a4ce7825735ee99da4999ec290509d92c8)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java


{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12749022/HBASE-14178-branch-1_v8.patch
  against branch-1 branch at commit 5d2708f628d4718f6267e9da6c8cbafeda66f4fb.
  ATTACHMENT ID: 12749022

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14991//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14991//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14991//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14991//console

This message is automatically generated.

FAILURE: Integrated in HBase-1.2 #93 (See [https://builds.apache.org/job/HBase-1.2/93/])
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev 922c3ba554eeb13c2390cdd1140b26006bb8a7e9)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
Revert "HBASE-14178 regionserver blocks because of waiting for offsetLock" (zhangduo: rev a4092444e6eba39e7523c118e80b3fb726485984)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev 7a45596b40e9a6a011a7854d684fc69013b83e73)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java


SUCCESS: Integrated in HBase-0.98 #1068 (See [https://builds.apache.org/job/HBase-0.98/1068/])
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev 0dfd2cea4a6be3d98bbf5b0ef9769a584dd1986c)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java


FAILURE: Integrated in HBase-1.2-IT #76 (See [https://builds.apache.org/job/HBase-1.2-IT/76/])
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev 922c3ba554eeb13c2390cdd1140b26006bb8a7e9)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
Revert "HBASE-14178 regionserver blocks because of waiting for offsetLock" (zhangduo: rev a4092444e6eba39e7523c118e80b3fb726485984)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev 7a45596b40e9a6a011a7854d684fc69013b83e73)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java


FAILURE: Integrated in HBase-0.98-on-Hadoop-1.1 #1021 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/1021/])
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev 0dfd2cea4a6be3d98bbf5b0ef9769a584dd1986c)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java


FAILURE: Integrated in HBase-1.3-IT #73 (See [https://builds.apache.org/job/HBase-1.3-IT/73/])
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev fe9de40e6c16b6e030a89a759fa278f0e27722aa)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
Revert "HBASE-14178 regionserver blocks because of waiting for offsetLock" (zhangduo: rev 4623c843c137888d606578ed1bc579272a5ab2c2)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
HBASE-14178 regionserver blocks because of waiting for offsetLock (zhangduo: rev 5c0c389b7a1b32a045e4bc1557b96a56291ab2ab)
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java


Closing this issue after 1.0.2 release.

In case of cache miss, if missed row is accessed  simultaneously, then clients have to access cache one by one. does this also block RS ?



First we try to get the block from cache with out getting any lock.  In case of miss, it may try again with cache with acquired offset lock if the reading blocks as to be cached. (This is as per configs at RS level as well as config set in the read req).  So then ya, the reading handlers in RS might get blocked for acquiring the locks.  But this will avoid the need for every readers to go to HDFS and fetch the block.

