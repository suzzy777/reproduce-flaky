The underlying reason for this failure is found in MESOS-2354.

Added workaround, so the test won't break even if underlying cause still present.

https://reviews.apache.org/r/31084/ (discarded patch)

So, after further investigation. The problem was not the master assigning two slaves the same id, but the slave sending a second registration request, which can happen according to [{{slave.cpp}}|https://github.com/apache/mesos/blob/master/src/slave/slave.cpp#L1087] and even [{{master.cpp}}|https://github.com/apache/mesos/blob/master/src/master/master.cpp#L2912] has code to deal with this situation.

The second register call of the first slave happens before the register call of the second slave which set the future of the message intended for the second slave with information intended for the first slave, this causing the test to fail.

The behaviour occurs randomly (there is a random timeout involved) which is the reason the test had to be run repeatedly to be reproducible. A patch for the test is available:

https://reviews.apache.org/r/31114

Great results, thanks for the update [~arojas]. 

I do still wonder why that randomness (https://github.com/mesosphere/mesos/blob/master/src/slave/slave.cpp#L1040) was done this way (allowing a {{next}} of 0 => immediate retry).

Labelling this as flaky. Just had it fail on our OS X Buildbot:
{code}
[ RUN      ] MasterTest.SlavesEndpointTwoSlaves
../../src/tests/master_tests.cpp:1674: Failure
Value of: array.get().values.size()
  Actual: 1
Expected: 2u
Which is: 2
[  FAILED  ] MasterTest.SlavesEndpointTwoSlaves (111 ms)
{code}

You mean the current state or the patch here? because we are discussing how to fix just that problem.

commit c7c185e3829c4f73ae8ebeba1ae521fd3211bb19
Author: Alexander Rojas <alexander@mesosphere.io>
Date:   Wed Feb 18 10:26:09 2015 -0800

    Fixed flaky MasterTest.SlavesEndpointTwoSlaves caused by incorrect
    FUTURE_PROTOBUF usage.
    
    Review: https://reviews.apache.org/r/31114


