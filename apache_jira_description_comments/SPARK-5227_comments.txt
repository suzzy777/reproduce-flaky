I've bumped this up to a 1.2.1 blocker to see if we can find a fix, since this is preventing the unit tests from running in SBT under certain Hadoop configurations.

function of computing split size in hadoop's  FileInputFormat is:
protected long computeSplitSize(long goalSize, long minSize,
                                       long blockSize) {
    return Math.max(minSize, Math.min(goalSize, blockSize));
  }
long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
and minSize = 1
blockSize = getConf().getLong("fs.local.block.size", 32 * 1024 * 1024);
so when numSplits=2 and totalSize is very large, then goalSize > blockSize. if it is true, actual splitSize is blockSize, not goalSize.
In this situation, the error in this PR will be happened.

I think this might be caused by HADOOP-8490: the test code might be getting a cached FileSystem instance that was created by an earlier test run, causing the configuration from the earlier test to be re-used here.  We could try to completely disable this caching, but this could have a large negative performance impact on Hadoop library code which assumes that FileSystem creation is cheap.  I wonder if there's a way that we can clear this cache in between our test runs, which would at least address the test-flakiness issues.

User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4599

