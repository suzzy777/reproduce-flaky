You can set the logging level for the statuslogger to warn to avoid those error messages.
{code}nodetool setlogginglevel org.apache.cassandra.utils.StatusLogger WARN{code}

or you can set the equivalent in logback.xml. Maybe INFO is noisy but I think replacing the log messages with StatusLogger is busy would somewhat defeat the purpose. 

-edit: Having said that, maybe INFO is too high, and this detail needs to drop to debug. From experience managing a lot of clusters the actual GCInspector message is the useful output in this case. However StatusLogger is also triggered from other events. Although those are all paired with other log messages, which as in this case, are probably more useful than the statuslogger output. StatusLogger seems to me to be for much more specific debugging of a problem.

[~KurtG] Personally I don't have much issue with the current logging level for StatusLogger. When the node is suffering, either because it's dropping non-TRACE messages, or because it's exceeding the 1sec gc_warn_threshold_in_ms threshold, I'd like to see StatusLogger to give me more information for post-mortem analysis and I don't want to change to DEBUG level to see it. Note "post-mortem" is the key here as you won't know when this will happen and if you have to switch to DEBUG level to see the message likely it will be too late.

bq. I think replacing the log messages with StatusLogger is busy would somewhat defeat the purpose.
Can you elaborate why you think avoiding duplicate StatusLogger printing "defeat the purpose"? The StatusLogger usually only takes 100-200ms to finish printing its state. If at the time StatusLogger is printing, another StatusLogger gets triggered and print again, it mostly just adds duplicate information which makes log messages crowded without adding any more useful insight.

Sorry that was my misunderstanding, I didn't realise you were just targetting "duplicate" entries. In that case that makes sense. I suppose a cooldown period for StatusLogger messages is what you're looking for then? In that case I'd probably say something like restricting messages to once every 10 seconds or so would be reasonable. Just my 2c.

[~mychal] LGTM. I know it's very basic but wouldn't hurt to have a test that at least checks that it will be "busy".

Thank you [~KurtG] for the feedback. I've attached patch with a testcase.

Thanks [~mychal] I can RIP now that I know no one will ever break the status logger again. I ran the test a few hundred times to confirm there is no flakiness and seems like it works perfectly so props to you.

I noticed that the "StatusLogger is busy" message obviously interleaves with the actual StatusLogger dump, but I think this is fine as unless it happens a ludicrous amount it doesn't really interfere with the printout, and if it's happening that much to cause problems well the dumps probably aren't really a major concern. I've pinged IRC for someone to doubly review/commit.

Nice patch, [~mychal].

wrt to the {{"StatusLogger is busy"}} message, i don't think we should bother logging that at {{info}}, or actually at any level visible to an operator. When someone is digging into a prod problem, the last thing we want to do is throw more information at them that is potentially confusing. Hence, at best we should log at {{trace}}. I updated the class and test here: 

||12182||
|[branch|https://github.com/jasobrown/cassandra/tree/12182]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/12182]|

I also fixed a naming thing in the test. wdyt?

I was worried about the thread safety and visibility effects of {{StatusLoggerTest.InMemoryAppender#events}}, but since the parent class ({{AppenderBase}}) calls {{#append}} from within a {{synchronized}} method, we should be fine. (I also ran this test a few hundred times on my laptop.)

[~jasobrown] Thank you for the feedback, however I'm afraid I can't see your updates in the branch you've linked. I agree {{trace}} level is better option. I think you've actually spotted visibility issue, because I did not perform anything that guarantees visibility of {{InMemoryAppender#events}} in the thread that test is running on (and only reads {{events}}). I'm going to adjust test and submit a new patch.

well, it always help to commit the changes before you push up a branch (facepalm).  My branch is updated now.

I do not believe you necessarily need to anything wrt visibility as that is handled by {{AppenderBase#doAppend}} being {{synchronized}}. 

But {{AppenderBase#doAppend}} is called only from 2 threads created by {{ExecutorService}} in {{submitTwoLogRequestsConcurrently}}, essentially leaving thread that test is executed on without using {{InMemoryAppender}} instance as a monitor to read {{InMemoryAppender#events}}, so unless I'm totally missing it we're not guaranteed {{AppenderBase#doAppend}} happens before read of {{events}}.

{{#shutdown()}} guarantees that (from the javadoc):

{blockquote}
     * Initiates an orderly shutdown in which previously submitted
     * tasks are executed, but no new tasks will be accepted.
     * Invocation has no additional effect if already shut down.
     *
     * <p>This method does not wait for previously submitted tasks to
     * complete execution.  Use {@link #awaitTermination awaitTermination}
     * to do that.
{blockquote}

You are calling {{#awaitTermination}} which ensures the tasks complete, so that acts as a block for the test thread. I'm pretty sure the entries in {{InMemoryAppender#events}} a) will have been written to (because we've waited until the tasks are complete) and b) will be visible (because of {{AppenderBase#doAppend}} being synchronized). wdyt?



You're right (obviously). Feel free to remove {{InMemoryAppender#getEvents}} from my latest patch and replace it with direct field access. But please don't use previous patch, because test would incorrectly fail when time ranges are not connected ({{Range#intersect}} would throw {{IllegalArgumentException}} in such case). utests for the latest patch: [https://circleci.com/gh/mszczygiel/cassandra/4] 

committed as sha {{5b09543f64eafb1344f7814a80b73d312d5bbc37}}

Thanks for the patch, and thanks for indulging me on concurrency issues :)

bq. You're right (obviously).

Actually, your code was right, I was just pointing out how it was correct ;)

bq. But please don't use previous patch, because test would incorrectly fail when time ranges are not connected (Range#intersect would throw IllegalArgumentException in such case)
Good catch... I actually found this as well late on Friday arvo after pinging IRC and wrote out a comment but got distracted and never sent it -_-". Glad you found it!

