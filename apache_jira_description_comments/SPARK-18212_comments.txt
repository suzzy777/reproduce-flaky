cc [~zsxwing]

/cc [~cody@koeninger.org]

So here's a heavily excerpted version of what I see happening in that log:

{code}
16/11/01 14:08:46.593 pool-1-thread-1-ScalaTest-running-KafkaSourceSuite INFO KafkaTestUtils:   Sent 34 to partition 2, offset 3
16/11/01 14:08:46.593 pool-1-thread-1-ScalaTest-running-KafkaSourceSuite INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
16/11/01 14:08:46.596 pool-1-thread-1-ScalaTest-running-KafkaSourceSuite INFO KafkaTestUtils: Created consumer to get latest offsets


16/11/01 14:08:47.833 Executor task launch worker-2 ERROR Executor: Exception in task 1.0 in stage 29.0 (TID 142)
java.lang.AssertionError: assertion failed: Failed to get records for spark-kafka-source-a9485cc4-c83d-4e97-a20e-3960565b3fdb-335403166-execut\
or topic-5-2 3 after polling for 512


16/11/01 14:08:49.252 pool-1-thread-1-ScalaTest-running-KafkaSourceSuite INFO KafkaTestUtils: Closed consumer to get latest offsets
16/11/01 14:08:49.252 pool-1-thread-1-ScalaTest-running-KafkaSourceSuite INFO KafkaSourceSuite: Added data, expected offset [(topic-5-0,4), (topic-5-1,4), (topic-5-2,4), (topic-5-3,4), (topic-5-4,4)]
{code}


We're waiting on the producer's send future for up to 10 seconds; it takes almost 3 seconds between when the producer send finishes and the consumer that's being used to verify the post-send offsets finishes; but in the meantime we're only waiting half a second for executor fetches.

It's really ugly, but probably the easiest way to make this less flaky is to increase the value of kafkaConsumer.pollTimeoutMs to the same order of magnitude being used for the other test waits.

[~zsxwing] unless you see anything else wrong in the log or have a better idea, I can put in a pr tomorrow to increase that poll timeout in tests.


+1 to upping the timeout. We run with {{.option("kafkaConsumer.pollTimeoutMs", 10 * 1000)}}

User 'koeninger' has created a pull request for this issue:
https://github.com/apache/spark/pull/15737

[~cody@koeninger.org] Sounds good to me.

Issue resolved by pull request 15737
[https://github.com/apache/spark/pull/15737]

