Here is another one. The direct cause differs, but fwict, the issue here is flakiness with S3. If these are truly separate issues that go beyond a flaky test, these can be separated/replaced. The first issue above is from an immediate failure when renaming. This issue below is on the subsequent query where a file is not found. 
{noformat}
query_test/test_insert_parquet.py:113: in test_insert_parquet
    self.run_test_case('insert_parquet', vector, unique_database, multiple_impalad=True)
common/impala_test_suite.py:467: in run_test_case
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:688: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:170: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:182: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:356: in __execute_query
    self.wait_for_finished(handle)
beeswax/impala_beeswax.py:377: in wait_for_finished
    raise ImpalaBeeswaxException("Query aborted:" + error_log, None)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    Query aborted:Disk I/O error: Failed to open HDFS file s3a://<removed>/test-warehouse/test_insert_parquet_1332ae40.db/orders_insert_table/e44c72680633256a-6851b4d300000000_457705041_data.0.parq
E   Error(2): No such file or directory
E   Root cause: FileNotFoundException: No such file or directory: s3a://<removed>/test-warehouse/test_insert_parquet_1332ae40.db/orders_insert_table/e44c72680633256a-6851b4d300000000_457705041_data.0.parq{noformat}

[~tianyiwang], pls have a look. Assigned randomly, so pls re-assign if you can't get to it.

This looks like you are hitting S3 inconsistency; rename is usually the place where it surfaces

* you shouldn't be using any commit algorithm which relies on rename; see HADOOP-13786
* unless you can implement resilience to inconsistency (e.g. spinning), you are going to have to embrace S3Guard with some consistency metadata store.

You can't just view this as a flaky test: this is probably a symptom of a problem which surfaces in production: *this test has successfully found it*

Thanks for the background, so yes, agreed that this can't be labeled as flaky from the testing side.

It could be your assertions are just brittle to change, in which case spinning briefly until the listings are consistent is a tactic...but it is a symptom of a problem

Also saw this on TestTableWriters.test_seq_writer on an older branch

Also seen here:
{noformat}
custom_cluster.test_parquet_page_index.TestHdfsParquetTableIndexWriter.test_ctas_tables[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': None, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] (from pytest)
Failing for the past 1 build (Since Failed#226 )
Took 1 min 19 sec.
add description
Error Message

CalledProcessError: Command '['hdfs', 'dfs', '-get', '/test-warehouse/test_ctas_tables_7a125d10.db/test_hdfs_parquet_table_writer/', '/tmp/pytest-of-jenkins/pytest-2/test_ctas_tables_protocol__bee0/functional_parquet.widetable_1000_cols']' returned non-zero exit status 1

Stacktrace

custom_cluster/test_parquet_page_index.py:330: in test_ctas_tables
    "functional_parquet.widetable_1000_cols", tmpdir)
custom_cluster/test_parquet_page_index.py:266: in _ctas_table_and_verify_index
    self._validate_parquet_page_index(hdfs_path, tmpdir.join(source_table))
custom_cluster/test_parquet_page_index.py:227: in _validate_parquet_page_index
    row_group_indexes = self._get_row_groups_from_hdfs_folder(hdfs_path, tmpdir)
custom_cluster/test_parquet_page_index.py:108: in _get_row_groups_from_hdfs_folder
    check_call(['hdfs', 'dfs', '-get', hdfs_path, tmpdir.strpath])
/usr/lib64/python2.7/subprocess.py:542: in check_call
    raise CalledProcessError(retcode, cmd)
E   CalledProcessError: Command '['hdfs', 'dfs', '-get', '/test-warehouse/test_ctas_tables_7a125d10.db/test_hdfs_parquet_table_writer/', '/tmp/pytest-of-jenkins/pytest-2/test_ctas_tables_protocol__bee0/functional_parquet.widetable_1000_cols']' returned non-zero exit status 1
{noformat}

A recent instance when running {{query_test/test_tpcds_queries.py::TestTpcdsInsert}}:

{noformat}
query_test/test_tpcds_queries.py:521: in test_tpcds_partitioned_insert
    self.run_test_case('partitioned-insert', vector)
common/impala_test_suite.py:563: in run_test_case
    result = exec_fn(query, user=test_section.get('USER', '').strip() or None)
common/impala_test_suite.py:500: in __exec_in_impala
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:798: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:184: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:187: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:364: in __execute_query
    self.wait_for_finished(handle)
beeswax/impala_beeswax.py:385: in wait_for_finished
    raise ImpalaBeeswaxException("Query aborted:" + error_log, None)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    Query aborted:Error(s) moving partition files. First error (of 1) was: Hdfs op (RENAME s3a://<redacted>/test-warehouse/tpcds_parquet.db/store_sales_insert/_impala_insert_staging/834b0c158076d9d0_015f77df00000000/.834b0c158076d9d0-015f77df00000004_337386663_dir/ss_sold_date_sk=2451539/834b0c158076d9d0-015f77df00000004_1260764580_data.0.parq TO s3a://<redacted>/test-warehouse/tpcds_parquet.db/store_sales_insert/ss_sold_date_sk=2451539/834b0c158076d9d0-015f77df00000004_1260764580_data.0.parq) failed, error was: s3a://<redacted>/test-warehouse/tpcds_parquet.db/store_sales_insert/_impala_insert_staging/834b0c158076d9d0_015f77df00000000/.834b0c158076d9d0-015f77df00000004_337386663_dir/ss_sold_date_sk=2451539/834b0c158076d9d0-015f77df00000004_1260764580_data.0.parq
E   Error(5): Input/output error
{noformat}

Impala 3.3 added support for running the tests with s3guard (IMPALA-8344), so I'm resolving this.

