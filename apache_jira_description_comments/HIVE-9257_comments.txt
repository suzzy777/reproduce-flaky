This patch is totally 13mb and too big for JIRA.  Adding in two parts- one is untracked (new) files, and other is modified files.

Actually this is not going to work, the other patch is still larger.  Need to think of another way.

Adding review request: [https://reviews.apache.org/r/29787/|https://reviews.apache.org/r/29787/].

As the entire patch is too big, this shows the modified files.  These have been cleanuped as part of HIVE-9319, HIVE-9306, HIVE-9305.

The new files can be found here:  [http://svn.apache.org/repos/asf/hive/branches/spark/|http://svn.apache.org/repos/asf/hive/branches/spark/] or [https://github.com/apache/hive/tree/spark|https://github.com/apache/hive/tree/spark] under:

data/conf/spark/
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithLocalClusterSpark.java
itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestMultiSessionsHS2WithLocalClusterSpark.java
itests/qtest-spark/
ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java
ql/src/java/org/apache/hadoop/hive/ql/exec/spark/
ql/src/java/org/apache/hadoop/hive/ql/lib/TypeRule.java
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SparkMapJoinProcessor.java
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenSparkSkewJoinProcessor.java
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SparkCrossProductCheck.java
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SparkMapJoinResolver.java
ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/
ql/src/java/org/apache/hadoop/hive/ql/parse/spark/
ql/src/java/org/apache/hadoop/hive/ql/plan/SparkBucketMapJoinContext.java
ql/src/java/org/apache/hadoop/hive/ql/plan/SparkEdgeProperty.java
ql/src/java/org/apache/hadoop/hive/ql/plan/SparkHashTableSinkDesc.java
ql/src/java/org/apache/hadoop/hive/ql/plan/SparkWork.java
ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregatorSpark.java
ql/src/test/org/apache/hadoop/hive/ql/exec/spark/
ql/src/test/queries/clientpositive/auto_join_stats.q
ql/src/test/queries/clientpositive/auto_join_stats2.q
ql/src/test/queries/clientpositive/bucket_map_join_spark1.q
ql/src/test/queries/clientpositive/bucket_map_join_spark2.q
ql/src/test/queries/clientpositive/bucket_map_join_spark3.q
ql/src/test/queries/clientpositive/bucket_map_join_spark4.q
ql/src/test/queries/clientpositive/multi_insert_mixed.q
ql/src/test/queries/clientpositive/multi_insert_union_src.q
ql/src/test/queries/clientpositive/parallel_join0.q
ql/src/test/queries/clientpositive/parallel_join1.q
ql/src/test/queries/clientpositive/spark_test.q
ql/src/test/queries/clientpositive/udf_example_add.q
ql/src/test/results/clientpositive/auto_join_stats.q.out
ql/src/test/results/clientpositive/auto_join_stats2.q.out
ql/src/test/results/clientpositive/bucket_map_join_spark1.q.out
ql/src/test/results/clientpositive/bucket_map_join_spark2.q.out
ql/src/test/results/clientpositive/bucket_map_join_spark3.q.out
ql/src/test/results/clientpositive/bucket_map_join_spark4.q.out
ql/src/test/results/clientpositive/multi_insert_mixed.q.out
ql/src/test/results/clientpositive/multi_insert_union_src.q.out
ql/src/test/results/clientpositive/parallel_join0.q.out
ql/src/test/results/clientpositive/parallel_join1.q.out
ql/src/test/results/clientpositive/spark/
ql/src/test/results/clientpositive/spark_test.q.out
ql/src/test/results/clientpositive/udf_example_add.q.out
spark-client/

Cleanup and review of those have been done as part of HIVE-9281 and HIVE-9288.

For reference, patch is uploaded here:  [http://ec2-50-18-79-139.us-west-1.compute.amazonaws.com/data/spark-to-trunk-merge/HIVE-9257.patch|http://ec2-50-18-79-139.us-west-1.compute.amazonaws.com/data/spark-to-trunk-merge/HIVE-9257.patch] to get around the JIRA 10MB restriction.

Ran a separate build that uses this patch, and the existing tests are passing:  [http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build-Upload/lastCompletedBuild/testReport/|http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build-Upload/lastCompletedBuild/testReport/].  

But spark tests are not running as ptest-master is missing spark test properties.  Going to reconfigure the build machine with sparkCliDriver properties and give it another spin.

Nice, only seven failed tests:

http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build-Upload/5/



I created HIVE-9335 and put a patch up for the review items I saw.

I am very familiar with this work. We have some follow-on jiras like HIVE-9335 but that shouldn't hold the merge as we've done in the past.

+1 pending tests to merge this to trunk

Yea to get [http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build-Upload/5/|http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build-Upload/5/] to run spark tests, I had to create a new build profile (trunk-mr2-spark-merge) with the spark test-properties.  Its a chicken-egg problem (see HIVE-7801), as I tried changing the main build properties (trunk-mr2) but that screwed up all the other precommit builds 

Anyway the last step post merge will be putting the spark properties in trunk-mr2.

Analyzing the test failures, the orc file sizes are reported different per OS, so regenerating some golden files on Ubuntu and trying again.  This should probably be looked at.

For reference, URL of new patch.  Only difference is that the new spark.out files have updated file sizes: alter_merge_orc.q.out, alter_merge_stats_orc.q.out, and vectorized_ptf.q.out.

 [http://ec2-50-18-79-139.us-west-1.compute.amazonaws.com/data/spark-to-trunk-merge/HIVE-9257.3.patch|http://ec2-50-18-79-139.us-west-1.compute.amazonaws.com/data/spark-to-trunk-merge/HIVE-9257.3.patch]

I reviewed most of patches in Spark branch during the months, and also produced some. I reviewed the mega patch here, and left a couple of comments on RB. However, these can be addressed as followups.

+1 pending on test.




Latest test results looks much better:  [http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build-Upload/6/|http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build-Upload/6/]

windowing.q has been failing for spark for some time, and is tracked to be fixed (HIVE-9104), should be soon.  The other test failure does not look related.

Committed to trunk.  Thanks Brock and Xuefu for detailed review!  Thanks also to all the contributors to the spark branch for this milestone!

Also modified build machine's default properties (trunk-mr2) to the new properties attached (trunk-mr2-spark-merge) which has configurations to run SparkCliDriver tests.

Follow-ups will be taken care of in HIVE-9335, and subsequent JIRA's.

In terms of doc, there is one property added to HiveConf (hive.spark.client.future.timeout), but there are potentially more (see HIVE-9337).  These will have to be added in : [https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-HiveServer2|https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-HiveServer2]

Actually my comments on RB were not covered by HIVE-9335, which already has +1 pending. We may need a separate JIRA to cover them.

Thanks I noticed, was collecting those as well as some other items I saw: HIVE-9340

FYI - I am running a build here: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/HIVE-TRUNK-HADOOP-2/1/ to ensure all the tests are passing post merge. If there is any failing tests, due to the merge, we should address them ASAP tomorrow.

The only test which failed and is not flaky is was the windowing test. I created HIVE-9343 to track the issue.

I also created HIVE-9344 to fix that optimize_nullscan tests that are so flaky.

Thanks Brock.  FYI, I am planning another merge in next few days to incorporate the review fixes, and also to get rid of the spark-snapshot dependency.  It will hopefully be a more managable patch that can uploaded this time and tested the normal way :)

Thanks for the doc notes, [~szehon].

Besides documenting configuration properties, the preliminary doc "Hive on Spark: Getting Started" should be updated and moved from Developer Docs to ... where?  Admin Docs, User Docs, or some of each?

* [Hive Home | https://cwiki.apache.org/confluence/display/Hive/Home]
* [Hive on Spark: Getting Started | https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started]

It's not very nice to do such a large branch merge over the weekend with no advance warning.
This appears to depend on spark build snapshot; this is a bad practice in general, and I don't think it would be acceptable to have a Hive release with such dependency.
Is it possible to change to a released version of Spark?

Hi Sergey,

The change was created a many days ago. It's exceedingly hard to do a merge during the week due to commit volume. W.r.t Spark SNAPSHOT, we have depended on Calcite SNAPSHOT for a long time, even sadly in the 0.14.0 release. However, I agree a SNAPSHOT dep in trunk is not ideal and we actually have a change to resolve this committed to spark and we'll be merging that soon. I actually thought that was getting merged this time.

Brock

Also, obviously, if there is any additional feedback on the work, we'd be happy discuss.

Yea sorry thats the way it went about the weekend.  We made the JIRA on Monday for people to know the intention.  The code cleanup took all week, longer than I anticipated due to state of spark-branch, and first patch was linked early on Friday during the workday.  And as Brock said, it is really hard to merge such a big branch during the week when trunk is in constant motion (tons of conflicts), making the weekend the only viable choice to do it.

Also its my fault about missing the snapshot as part of the patch, it is fixed already in HIVE 7674 last week and will be merged in next merge (I missed it not being in this merge).  I could fix it immediately too, but was going to collect some more fixes and then merge by tomorrow/Wed.

Thanks, +1 on moving the link to under AdminDocs.  

There might be some parts that should be in UserDocs, but at this point I am not sure which (if any) to put in there.  Maybe some of the recommended configuration, as other configurations are under "UserDocs".

Can you remove all the EC2 random references from the poms? My builds are trying to pull hive JARs from some random EC2 hosts.

{code}
Downloading: http://ec2-50-18-79-139.us-west-1.compute.amazonaws.com/data/spark_2.10-1.2-SNAPSHOT/org/apache/hive/hive-service/0.15.0-SNAPSHOT/maven-metadata.xml
{code}

Yea that will be removed in next merge.  It's already checked in for HIVE-7674 and will be done in HIVE-9352

[~szehon] - do you think we could merge just that one change? (Sooner than the next merge.)

Thanks, [~brocknoland].

I traced the ec2 fetching to the root pom.xml, which has

{code}
 <repository>
       <id>spark-snapshot</id>
       <url>http://ec2-50-18-79-139.us-west-1.compute.amazonaws.com/data/spark_2.10-1.2-SNAPSHOT/</url>
       <releases>
...
{code}

I don't see that removed with HIVE-7674 (or in the branch) - https://github.com/apache/hive/blob/spark/pom.xml#L217

Yea I was going to reply earlier to Brock.  I submitted it for removal it now in HIVE-9340, it is one of the other cleanups, thats why I wanted to collect them and then merge.  I might be able to post a patch this evening.

Ahh yes, we discussed today that I forgot that in my patch. Nevermind me.

[~gopalv] [~brocknoland] tests passed for HIVE-9340 and I submitted a new merge to trunk in HIVE-9352.  Will merge as soon as possible.  Sorry for inconvenience.

Thank you cleaning up my mess! :)

