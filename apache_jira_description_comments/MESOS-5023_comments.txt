This failure comes from a race that the containers_[containerId] is erased accidentally. I am attaching the log which is the same flaky by comment out the check `CHECK(containers_.contains(containerId));` in _launch():

{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from MesosContainerizerProvisionerTest
[ RUN      ] MesosContainerizerProvisionerTest.DestroyWhileProvisioning
I0325 19:01:02.686094 19729 containerizer.cpp:666] Starting container '63a1b3d5-dd19-453f-9953-439d4126c488' for executor 'executor' of framework ''
I0325 19:01:02.687070 19732 containerizer.cpp:1421] Destroying container '63a1b3d5-dd19-453f-9953-439d4126c488'
I0325 19:01:02.687110 19732 containerizer.cpp:1424] Waiting for the provisioner to complete for container '63a1b3d5-dd19-453f-9953-439d4126c488'
F0325 19:01:02.688181 19731 owned.hpp:110] Check failed: 'get()' Must be non NULL 
*** Check failure stack trace: ***
    @     0x7f00ae6fe84d  google::LogMessage::Fail()
    @     0x7f00ae6fdc2e  google::LogMessage::SendToLog()
    @     0x7f00ae6fe51d  google::LogMessage::Flush()
    @     0x7f00ae701968  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f00adce4df4  google::CheckNotNull<>()
    @     0x7f00adc83760  process::Owned<>::operator->()
    @     0x7f00adc7154d  mesos::internal::slave::MesosContainerizerProcess::_launch()
    @     0x7f00adcec610  _ZZN7process8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS1_11ContainerIDERK6OptionINS1_8TaskInfoEERKNS1_12ExecutorInfoERKSsRKS8_ISsERKNS1_7SlaveIDERKNS_3PIDINS3_5SlaveEEEbRKS8_INS3_13ProvisionInfoEES5_SA_SD_SsSI_SL_SQ_bSU_EENS_6FutureIT_EERKNSO_IT0_EEMS10_FSZ_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_ENKUlPNS_11ProcessBaseEE_clES1P_
    @     0x7f00adcebf32  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS5_11ContainerIDERK6OptionINS5_8TaskInfoEERKNS5_12ExecutorInfoERKSsRKSC_ISsERKNS5_7SlaveIDERKNS0_3PIDINS7_5SlaveEEEbRKSC_INS7_13ProvisionInfoEES9_SE_SH_SsSM_SP_SU_bSY_EENS0_6FutureIT_EERKNSS_IT0_EEMS14_FS13_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x7f00ae67c188  std::function<>::operator()()
    @     0x7f00ae666494  process::ProcessBase::visit()
    @     0x7f00ae6bdbde  process::DispatchEvent::visit()
    @           0x857a71  process::ProcessBase::serve()
    @     0x7f00ae6641bd  process::ProcessManager::resume()
    @     0x7f00ae66c325  process::ProcessManager::init_threads()::$_1::operator()()
    @     0x7f00ae66c263  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvE3$_1St17reference_wrapperIKSt11atomic_boolEEE6__callIvJEJLm0EEEET_OSt5tupleIJDpT0_EESt12_Index_tupleIJXspT1_EEE
    @     0x7f00ae66c216  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvE3$_1St17reference_wrapperIKSt11atomic_boolEEEclIJEvEET0_DpOT_
    @     0x7f00ae66c1c5  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvE3$_1St17reference_wrapperIKSt11atomic_boolEEEvEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
    @     0x7f00ae66c195  std::_Bind_simple<>::operator()()
    @     0x7f00ae66c16c  std::thread::_Impl<>::_M_run()
    @     0x7f00a9742a60  (unknown)
    @     0x7f00a8f5f182  start_thread
    @     0x7f00a8c8c47d  (unknown)
{noformat}

The bug arises from `await()` in destroy(), which creates another future object. Thanks [~anandmazumdar] for investigating it together.

Hey [~klaus1982], sorry to ask that do you mind letting us to resolve this bug? Since it comes from a commit we merged two days ago, I should have it fixed asap. 

sure:)

Sorry, I don't get the bug. onAny callbacks registered on the 'future' will be invoked in serial order when the promise is set. I don't understand why this can go out of order. Can someone show more context here?

commit c9eece94a5d26502b26d8a90f0cdb52fa038ff6c
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Mar 29 17:51:52 2016 -0700

    Fix container destroy provisioning race.
    
    Review: https://reviews.apache.org/r/45386/

