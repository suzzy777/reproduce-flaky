David's the main guy working on this stuff and currently out due to immigration issues. Should be back on 8/14 or so and get back on this.

David's back in action this week and should be working on this.

David has pretty much finished getting rid of replicating COMMIT messages. Adopting the Raft paper way of getting replicas back up to speed when they're temporarily down.

Three main patches:
- no commit messages in replicated log
- using raft paper's way of getting guys up to speed
- asynchronously starting tablet peer

Once those patches are done, pretty close to config change.

This week hoping to finish the tablet peer patch and the "no commit messages" patch. The raft way of catchup is a stretch goal for this week depending on the other stuff.

Progress report: David is making config change more generic, so that the initial config change uses the same code path as later config changes. He's working through failing and flaky tests on his branch.

Regarding leader election, David and Mike are discussing some details on how to split up this work. In particular:
- how do we store the persistent metadata for consensus (a file? multiple files? a log?) likely we can just use a synchronous file replace.
- should we move the Quorum metadata out of the tablet metadata into the consensus metadata file?

Concrete next steps:
1. Mike to add this new consensus metadata file and associated PB
2. Mike to add basic algo for leader election
-- There is some discussion whether we need to use raft's idea of "absolute log indexes" to properly implement this. Mike is going to think about it, but David and Todd are unconvinced. Mike to discuss w/ David and Todd.

- Change config transaction patch is up, David is running tests on the "no replicated commits" patch
- David also started working on using the "preceding OpId" thing to make replicas refuse updates and get them caught up

These things get us on the way to changing config of a running quorum.

- Mike is going to review the above-mentioned patches

- Mike: has a kinda-working patch which moves the consensus metadata to its own file, but it's failing some CHECKs. He's trying to figure out if these CHECKs are wrong or not - maybe some weird circular dependency issues.
-- David thinks this should be better once his patches are committed, so first priority is reviewing David's change-config patch, and then Mike will rebase on top, making stuff smoother

Mike's also going to start focusing on the leader election algorithm and some unit-testable RPC based thing to do the votes/persistence of votes. Then David and Mike will integrate their stuff later.

David says:
- just pushed the patch which makes the initial ChangeConfiguration act like other transactions
- worked over the weekend on the patch to remove the replica CommitMsgs and the patch to catch up replicas to the leader when they're delayed
-- hoping to push these patches to gerrit in the next few days
-- need to work on a patch that tests that consensus can change leaders (with a quiesced quorum)
- should be all separate from Mike's leader election stuff

- Some of Mike's in-flight patches had tests break due to David's recent change - David and Mike to figure that out (probably just involves waiting for tablet to go from CANDIDATE->LEADER state)


*Mike's stuff*:
- catch-up (remote bootstrap) stuff is almost in
-- the actual integration stuff is on hold for M4.5 (it's a major feature in M5)
- working on leader election protocol and some unit tests for that protocol
-- then will do some more integration pieces

- last week, David did some patches to revamp log to work without commit IDs (turned out to be more work)
- has a WIP patch but has some test failures that need to be addressed
-- makes LogReader track an index over the segments, but we now potentially have segments that have no operations with IDs. We used to be able to index log segments by their first opid, but no longer works because of the afore-mentioned issue. Now the indexing is sparse - it'll give you a segment but you may have to continue reading past it.
-- this ought to clean up some stuff like GC too
-- some complexity around reading the same segment you're currently writing to -- still needs to be addressed
-- today this should be merged with the no-replicated-commits patch, needs to be split up into pieces for review

David also found some potential bugs around log anchoring (maybe a durability issue)

----

- Mike started a bit on the leader voting stuff, but didn't get too far yet. Working on a test to collect votes and make sure people only vote once, terms are monotonic, etc. Once the basic logic is working he'll start plumbing it in (like getting opids to the right part of the code)

- Moving quorum config to its own file: some complexity around pending state vs accepted state, working on splitting some of that stuff up in the state. This stuff is up for review



Progress report:
- "no replicated commits" patch mostly done. Missing: handling pending transactions. David has a patch in flight which handles already-committed pending transactions, and working on a patch to deal with uncommitted ones.
-- already pushed a patch which CHECK()s that there are no pending transactions. He will push the no-replicated-commits patch on top of this, which may be flaky, but at least can start the code review.

- pushed some changes for the log, nearly done (waiting on reviews from Mike, who's looking at it today).
- started working on a patch for leader promotion, which fails reliably because of above missing pieces

- next need to make the replicas reject requests that don't match their position in the log and leader needs to go backward to the right position

How many "unknowns" are left? The only one still in question is whether we need to implement operation abort for this milestone.

David predicts that with a quiesced quorum, we could do a leader switch by the end of the week. For a running quorum, we need more of the above stuff (dependent on reviews). He'll prioritize getting tihs working before moving on to the non-quiesced quorum case.

----

*Mike's stuff:*

- merged the on-disk format changes for consensus metadata, and some container class for where this will be tied in
-- not merged yet: responding to David's feedback on how to integrate this
- made a bit more progress on the leader election implementation and some tests, but still some ways to go (splitting time with the in-flight stuff)
- also working on reviewing David's stuff

goal for end of this week: unit tests up for leader election protocol (not hooked into the running system)




plan for the next few weeks:

9/29-10/3:
- David to work on leader change of a quiesced quorum, should be working with manual initiation of a leader change by end of week
- Mike working on leader election algorithm (without integration)
- goal for EOW:
-- hook up the "BecomeLeader" function to a remote RPC, add a simple tool to change leader on a running tablet (when quiesced)
-- demo for next week meeting: insert some data on a cluster. stop writing. manually move the leaders off one of the nodes, shut it down, continue writing


10/6-10/10: (more fuzzy plan, to be nailed down more next week)
- David to work on leader change of *non*-quiesced quorum
- Mike working on integrating leader election
- goal for EOW: explicit tool to trigger a leader election
-- demo: while a workload is running, we can run a command-line "re-elect leader" against a tablet

Another action item: Todd to produce a doc explaining what he thinks happens during a leader re-election. David will correct him where he is wrong.


We got the stuff done last week that we were aiming for (David to demo today)

This week plan:
- Todd to update the design doc with some details about this week's goal
- Mike to integrate the leader election stuff, hook up to an RPC
- David to work on dealing with pending changes (leader change while workload is running, *not* quiesced)
-- Explicitly *not* dealing with the case of the old leader coming back up or staying online. We'll kill -9 the old leader and let him be dead, but the new leaders have to run a correct election and commit any pending replicates left over from the old leader.
-- Next week (10/13 week), we'll deal with the old leader coming back up or staying online (eg kill -9 and restart, or kill -STOP for 30 seconds)



- Made some progress on leader election, but not integrated. Still working on that this week.
- Support for dealing with pending changes:
-- addressed when we start the cluster (immediately apply those that are committed, start the ones that aren't. if we're the leader, append to the queue, etc)
-- log matching property negotiation thing: queue actually loads old entries from disk as necessary, etc
-- tests now passing without any sleeps for the most part. need to tweak the linked_list-test one a little bit.
- seeing a couple other new bugs that we hadn't seen before
-- should add tests which have 5 members in a quorum instead of just 3

this week:
- merging all the above changes in (some stuff is still just in gerrit)
- working on failure detector
- reviewing Mike's patches

realistic goal for next week:
- all the patches for loading the queue, etc, committed
- the leader election stuff committed
- failure detector in review and minimally working for a demo (and review/revise/commit during next week)


Updates from last week:

Mike:
- Mike lost a day to recruiting at UCSC
- Lots of time spent last week finding weird bugs and fixing tests, tracking down issues, etc
-- the "mock RPC"system in consensus-test-util cost a lot of time, since it doesn't really act the same as RPC. Spent a lot of time trying to make it look closer/hacking around it. We should remove this stuff ASAP.

- As of last night, almost got leader election working on startup, but fails in the integration test. Need to fix the tablet layer stuff to allow votes in CONFIGURING tablets.
-- hacked that in, but still some tests failing

Discussion of the testing framework stuff ensued:
- some consensus that we shouldn't build more stuff on the consensus-test-util framework. Instead we should build (a) proper unit tests, and (b) proper integration tests. The existing raft-consensus-test is some kind of neither-here-nor-there which doesn't accurately represent the real world.


----

What do we actually think we can get done this week?

- hoping to commit the queue-loader and pending patches
- adding integration and tests for leader election
- op abort: before we work on that, todd/david to sync on it (not started yet)
- failure detector: David to post the design for this ASAP
-leader elector: Mike waiting on review from Todd for main election patch



David got pending stuff done and up for review.
Mike got leader election stuff committed, integration in progress (debugging some issues)

David working on a stress test and other "hardening" tasks. Will be helping Todd on dealing with abort/truncate.

Mike:
- has an integration test for leader election which worked until he rebased this morning (woops). He thinks he may have to disable elections on some particular tests.
-- the test starts a quorum with 5 nodes in external minicluster, then uses some new client APIs to find the leader, kill -9 the leader, let a new election happen, kill -9 again. Between each kill, writes data. All with automatic election.
-- may take a couple days to get this all merged (lots of little fixes separated out, etc)
-- David and Mike to discuss where we need specific test coverage (currently it's all itest-based) - maybe we can do more fine grained stuff on component level.
-- Mike deleted a bunch of code which does validation CHECKs, which he thinks weren't valid, but need to carefully evaluate these.

David to look at Mike's patch series today. Main issue to discuss is around transitions and pending/committed quorum stuff.

----

David:
- dealing with pending txns is done
- working on specific tests for different cases against consensus internals - they don't prove that the stuff works, but ensures that we cover various code paths
-- eg: turn to follower and become leader without aborting, with pending at different stages
- working on a patch for LMP negotiation between follower and leader when there is stuff to abort - fixing dedup to use log indexes instead of op ids, adding abort of operations


major remaining items:
- P1: Working on test flakiness
-- memory leak in client-test, due to leader election (Mike's working on this)
-- TSAN race in log (David fixing this currently)
-- timeout creating a table - just need to increase some timeouts we think (David)
-- countdown-latch-test: just a timing issue (David)

- log work so that replaced operations don't get returned (todd to post a WIP ASAP)
- david's code to abort the ops in flight on switchover
- mike's integration of leader election (discussed above)
- plenty of test coverage


David:
- integration test has been running for a while (the one that pauses leaders and brings them back)
-- on a gerrit review -- not yet committed
- two patches ready for review about aborting txns when leader switches (Mike reviewing)
- patch for the client (needs Adar's help)
-- client failover needs to fail over for non-NetworkErrors (eg illegal state when trying to write to a non-leader). Adar and David to talk on Tuesday.
- LogCache bug fix patch
-- weren't handling correctly searching for index 0

David's generally splitting stuff up, making good commit messages, cleaning up patches as he goes, etc, getting jenkins runs in the meanwhile.

David's next work will be around including integration testing coverage (eg verifying logs, mix of crash types) but first priority is committing everything that's in review.

JD hit some leader election issues in the cluster -- he had smoe paused leaders which triggered leader election, but the master branch doesn't handle that yet, so he had to disable leader election on the cluster.

Mike:
- not a lot to add to above
- working on reviewing David's stuff
- removing EmulateElection now that we have real election
- some other cleanup stuff, plus may help with some of the testing stuff David mentioned above

Todd:
- going to wait until David gets all of the above in, and then go at it with a refactoring chainsaw to try to clean up class interactions


David:
- the test which pauses and revives leaders is still in flight - merged in with the step-down patch. (the old leader when it comes back needs to step down when it sees the new term)
- some small patches which need to happen before that itest which are required for it to pass
-- spinning RPCs between leader and follower after LMP
- tried everything all together and is mostly working. has an itest for this which flakes occasionally due to a couple known issues, but we'll merge this anyway so long as it's only a few percent flaky. Then we can look at the flaky test results and work through them.
-- some race when aborting a transactino which is in the middle of prepare (shows up during churny elections)
-- lower priority: on restart, shouldn't always start an election (only for the first term, where it might save us a second or two when creating a table) (Mike to do this)

Integration test which verifies logs still isn't started

Cluster testing plan: David to assemble all the patches in flight into a branch and get JD to run it on the real cluster. Crossing our fingers that this should work pretty well in a real cluster (where weird edge cases are less likely). but maybe we'll learn about other bugs!


- been running itest for a while with current set of patches, still a little flaky
- tried running leader election on the real cluster, hit a few problems:
-- only the first five or six elections seem to make progress, and then we get "stuck" - everyone's sending stuff to everyone and not snoozing enough. fixed that and able to complete more elections
-- for some reason triggering elections even when we're already the leader. If we lose that election, we don't become replica properly
- been working on merging write paths and making queue/cache reusable - makes some stuff simpler. ran many iterations of tests locally and seems pretty stable. Ready to split that up and put up for review
- have a patch for the issue where we were getting a SEGV aborting a txn which was in the middle of prepare/replicate

Mike's update:
- reviewing David's stuff
- trying to debug some existing itest flakiness
-- some case where the master isn't getting notified about a leader change (not sure if we aren't reporting it, or getting stuck on a lock somewhere, or what)
-- some confusion why the client isn't trying all of the replicas here anyway (despite the master not having up-to-date info)
-- pretty reproducible (some number of loops - a few minutes) but slow edit-compile-repro cycle making it take a while to debug
- Todd will point Mike at his WIP tracing stuff which might help debug


Sub issues are closed. automated election works on the real cluster, but proper testing requires the java client. doing that

David:
 - we're feature complete.
 - able to run integration test for 2 days with failover.
  - pushed a patch on Friday that removed unused methods and then tests started failing
 - java client need to work properly with failover, currently illegal state and we're not refreshing our TS list
 - this week: continuing to run tests on the cluster, fix the bugs. 

Mike:
 - filed a jira on Friday to support sending parallel RPCs like we do with the master, David says there's a problem with managing multiple configurations we could get back.
 - nailing down some failures last week, we weren't serializing the quorum changes properly. We weren't following the sequence ids.
 - this week: fixing this bug, add redirects to the RPC so the client could go to the correct leader, modify the C++ client to use this.

JD:
 - this week: fix the Java client for leader re-elections (illegal state error, transaction aborted error), YCSB fails since we enabled leader election. Doesn't support the failover (as mentioned above). Investigate why the numbers we do have look better once the job is fixed. 

David:
- sorted out a bunch of bugs with JD's help with cluster testing
- submitted an integration test which kills nodes instead of just pausing them, which reproduced a new bug when running on jenkins. Has a handle on what went wrong and working on fixing that today
- we'd like to support having holes in the LogCache, but David doesn't have bandwidth to do it in the next week or two. Todd may help out here.
-- this is a pretty important feature because when a server dies, we don't end up cascading memory requirements to other machines
- Some discussion about RPC errors coming back from the tserver - we need to have specific codes instead of the generic statuses. Requesting help to get this done holistically.
- One bug which we haven't yet addressed, which we saw only once and hasn't reproduced easily. We'll wait and see if it pops up again.
- Will be OOO for a couple days this week, working on some PhD stuff, but will try to get above-mentioned itest running today and work on fixing a watermark negotiation bug

Mike:
- on Friday committed the fix to make the quorum serialized based on the consensus index. Seems to have solved several problems and also unblocked a bunch of other patches that Mike had. Feeding those in on top now.
- linked_list-test is failing after his latest patch to the client. Trying to figure out whether it's a bug or a feature. Mike to talk to Adar about this issue.
- this week: get tablets to redirect the client when they aren't the leader
-- if extra time, potentially looking at KUDU-120


David:
- made linked-list-test use automated election and kill nodes. Ran it a few 10s of times and passing so far. Trying to get it to reproduce some of the issues that we've seen in the real cluster, which are potentially related to bootstrap. Always seems to happen right after bootstrap.
-- trying to make sure the "slow" test version writes enough data to actually flush some data, which makes bootstrap more realistic
-- haven't been able to repro the failures in the system test yet
- once this stuff above is stabilized, will bring back snapshot isolation so that we can actually do a better job of verifying that the replicas have the same data
-- eventually leading to fault-tolerant scans

Mike:
- fixed the linked-list-test issue mentioned in last week's update
- mid-week switched gears from what we mentioned last week: working on stability issues, in particular fixing data races and deadlock warnings exposed by clang 3.5.
- late last week decided it'd be helpful to work on a tablet verification mechanism
-- RPC which does a checksum on some data
-- integrate with kudu "check" program (ksck?)

Todd:
- working on the log index series which allows bounding memory usage in the log cache, faulting in only a small number fo messages when a peer is really laggy
- will post a patch series with all of these changes tomorrow or wed.

- JD has been testing on a cluster
- bug on bootstrap which is mostly fixed
- exposed a new bug on the java client which affected our long-running linked-list integration test
- bootstrap is taking longer and longer over time
-- our test currently waits a fixed amount of time after bootstrap finishes, but doesn't account for catch-up time
-- hard to figure out when the tablet is ready to be verified because of this effect
-- not 100% sure why bootstrap's taking so long... potentially we have an old anchor which is keeping old WAL segments around for too long. Need to sort this out.

*Todos to help debug why we have too many log segments retained*:
- bootstrap verbose info per segment on how many entries ([~tlipcon] to do this)
- anchor registry dump web page (or verbose logging during GC) ([~mpercy] to do this)
- change DMS anchor names to include the RS ID (not just DMS id) so we can determine which they refer to ([~mpercy])
- include timestamps in log anchor metadata ([~mpercy])

David's working on a couple patches:
- bootstrap using the correct timestamps (snapshot isolation-related)
- if we don't have anything to replay, we might issue the wrong transaction IDs after restart

Mike to ramp back up on the tablet verification/checksum tool.

Summary: major bugs we are finding aren't consensus correctness issues, but ancillary issues like bootstrapping, consistency during verify, client bugs, etc.


- Java bug: JD knows how to trigger the issue but doesn't know exactly what it is. Still working on trying to reproduce this and understand what's going on, but hasn't been able to get a unit test on a single cluster. He filed KUDU-590
- Bootstrap getting longer issue: the DMS wasn't flushing old stuff properly
-- Mike got patches in to monitor/expose log anchor stuff
-- JD has a patch in the works which more actively encourages flushing stuff which has been in the log for a long time. Has a POC patch (had to learn a lot of new code). Goal is to get this into a reviewable format.

David's patches:
- bootstrap using correct timestamps
-- reached the conclusion that we need this in order to solve various issues we've seen in the cluster testing
-- worked on it a bit. a number of patches:
--- 1) delta compaction filtering properly  (nearly done)
--- 2a and 2b) timestamp being part of replicate message, and making bootstrap use that timestamp  (need some testing and debugging)
--- 3) finish snapshot isolation by propagating safe time across nodes (but not part of this ticket)

Todd's stuff:
- updating log_cache patch which simplifies lots of stuff, David reviewing, JD cluster-testing
- Todd to keep debugging the issue we saw on the cluster






This is basically done. There are a couple patches we want to commit before this JIRA is closed. If we find further bugs, we can open them:

- the out-of-order replicate bug (KUDU-597)
- the log_cache refactor patch (which needs to be rebased and cluster-tested)

Stretch goal is to get these finished/committed by next Monday, pending test results on the cluster.

We're all done with this work! Party!

