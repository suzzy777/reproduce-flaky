Failed a second time.

And one more.

And again.

+1

Again.

Hey Matthias. Thanks for bringing this one up. I believe this is caused by a race condition where the alter log dirs operation completes before we can check the fetch state. One option is to increase the number of records in the partition, but I've tried that a few times and it hasn't seemed to work. Any other ideas are welcome.

Just to note, the error checkpointing appears to be a different test.

[~jolshan] if you look at some of the other tests, they use a CountDownLatch with their mock to control the timing of how things complete, e.g. this setup:


{noformat}
val countDownLatch = new CountDownLatch(1)

// Prepare the mocked components for the test
val (replicaManager, mockLogMgr) = prepareReplicaManagerAndLogManager(new MockTimer(time),
topicPartition, leaderEpoch + leaderEpochIncrement, followerBrokerId, leaderBrokerId, countDownLatch,
expectTruncation = expectTruncation, localLogOffset = Some(10), extraProps = extraProps, topicId = Some(topicId)){noformat}
That allows you do things like ensure that the fetcher thread blocks in a certain state where you can validate the state:
{code:java}
override def createFetcherThread(fetcherId: Int, sourceBroker: BrokerEndPoint): ReplicaFetcherThread = {
  new ReplicaFetcherThread(s"ReplicaFetcherThread-$fetcherId", fetcherId,
    sourceBroker, config, failedPartitions, replicaManager, metrics, time, quotaManager.follower, Some(blockingSend)) {

    override def doWork() = {
      // In case the thread starts before the partition is added by AbstractFetcherManager,
      // add it here (it's a no-op if already added)
      val initialOffset = InitialFetchState(
        topicId = topicId,
        leader = new BrokerEndPoint(0, "localhost", 9092),
        initOffset = 0L, currentLeaderEpoch = leaderEpochInLeaderAndIsr)
      addPartitions(Map(new TopicPartition(topic, topicPartition) -> initialOffset))
      super.doWork()

      // Shut the thread down after one iteration to avoid double-counting truncations
      initiateShutdown()
      countDownLatch.countDown()
    }
  }
}
 {code}
Maybe something like that would be helpful?

So I tried to simulate this test failure under normal circumstances and couldn't do so. I will retry the tests using docker to limit the resources of to see if I can fail the test under those circumstances.

Okay so I have done some testing with docker to see if I can reproduce the flakiness under constrained resources and I couldn't simulate although the progress of doing this is incredibly slow (for some reason when running tests under the Kafka core gradle project it takes aÂ *really* long amount of time per run, it takes ~5-10 minutes in the configuring stage before even starting to compile the sources).

[~mjsax] When is the last time you experienced this flaky test?

So I ran the tests inside a docker container to simulate limited resources and couldn't replicate the flakiness

I also can't reproduce the flakiness on latest trunk, FWIW. I've set up to run the failing test repeatedly and got bored after 4,381 successful runs.

Out of the last 333 runs on trunk, there were no failures or flakes  [(source)|https://ge.apache.org/scans/tests?search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.tags=trunk&tests.container=kafka.server.ReplicaManagerTest&tests.test=testReplicaAlterLogDirsWithAndWithoutIds(boolean)%5B1%5D].

Out of the last ~2930 runs on all branches, there were also no failures or flakes [(source)|https://ge.apache.org/scans/tests?search.relativeStartTime=P28D&search.rootProjectNames=kafka&tests.container=kafka.server.ReplicaManagerTest&tests.test=testReplicaAlterLogDirsWithAndWithoutIds(boolean)%5B1%5D].

