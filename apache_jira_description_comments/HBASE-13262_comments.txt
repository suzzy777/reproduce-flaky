Seems to be only a client-side issue (as running the client with hbase-client-1.0.0 and a 1.1.0-SNAPSHOT installation worked fine).

The ClientScanner appears to be jumping to the next region too early:

{noformat}
2015-03-17 11:43:35,459 DEBUG [main] client.ClientScanner(281): Advancing internal scanner to startKey at '1\x00'
2015-03-17 11:43:35,711 INFO  [main] hbase.HBaseTest(152): Saw row 108997
2015-03-17 11:43:35,847 DEBUG [main] client.ClientScanner(281): Advancing internal scanner to startKey at '2\x00'
2015-03-17 11:43:35,877 INFO  [main] hbase.HBaseTest(152): Saw row 117997
2015-03-17 11:43:35,960 INFO  [main] hbase.HBaseTest(152): Saw row 208270
2015-03-17 11:43:36,077 INFO  [main] hbase.HBaseTest(152): Saw row 217270
2015-03-17 11:43:36,088 DEBUG [main] client.ClientScanner(281): Advancing internal scanner to startKey at '3\x00'
2015-03-17 11:43:36,171 INFO  [main] hbase.HBaseTest(152): Saw row 306297
2015-03-17 11:43:36,236 INFO  [main] hbase.HBaseTest(152): Saw row 315297
2015-03-17 11:43:36,268 DEBUG [main] client.ClientScanner(281): Advancing internal scanner to startKey at '4\x00'
2015-03-17 11:43:36,333 INFO  [main] hbase.HBaseTest(152): Saw row 404322
2015-03-17 11:43:36,421 INFO  [main] hbase.HBaseTest(152): Saw row 413322
2015-03-17 11:43:36,493 DEBUG [main] client.ClientScanner(281): Advancing internal scanner to startKey at '5\x00'
2015-03-17 11:43:36,546 INFO  [main] hbase.HBaseTest(152): Saw row 502349
2015-03-17 11:43:36,656 INFO  [main] hbase.HBaseTest(152): Saw row 511349
2015-03-17 11:43:36,717 DEBUG [main] client.ClientScanner(281): Advancing internal scanner to startKey at '6\x00'
2015-03-17 11:43:36,732 INFO  [main] hbase.HBaseTest(152): Saw row 600375
2015-03-17 11:43:36,830 INFO  [main] hbase.HBaseTest(152): Saw row 609376
2015-03-17 11:43:36,894 INFO  [main] hbase.HBaseTest(152): Saw row 618376
2015-03-17 11:43:36,924 DEBUG [main] client.ClientScanner(281): Advancing internal scanner to startKey at '7\x00'
2015-03-17 11:43:37,035 INFO  [main] hbase.HBaseTest(152): Saw row 707401
2015-03-17 11:43:37,134 INFO  [main] hbase.HBaseTest(152): Saw row 716401
2015-03-17 11:43:37,146 DEBUG [main] client.ClientScanner(281): Advancing internal scanner to startKey at '8\x00'
2015-03-17 11:43:37,221 INFO  [main] hbase.HBaseTest(152): Saw row 805428
2015-03-17 11:43:37,327 INFO  [main] hbase.HBaseTest(152): Saw row 814428
2015-03-17 11:43:37,367 DEBUG [main] client.ClientScanner(281): Advancing internal scanner to startKey at '9\x00'
2015-03-17 11:43:37,417 INFO  [main] hbase.HBaseTest(152): Saw row 903454
2015-03-17 11:43:37,512 INFO  [main] hbase.HBaseTest(152): Saw row 912454
2015-03-17 11:43:37,574 INFO  [main] hbase.HBaseTest(159): Last row in Result 919972
{noformat}

The client should be logging each 10000th row (e.g. we should see 10 log messages between the ClientScanner log message).

Bisect'ed back to HBASE-11544 as the cause.

{noformat}
0c64a57e529d591a96d56721a1ae538167a03a11 is the first bad commit
commit 0c64a57e529d591a96d56721a1ae538167a03a11
Author: Jonathan Lawlor <jonathan.lawlor@cloudera.com>
Date:   Wed Feb 4 14:07:35 2015 -0800

    HBASE-11544: [Ergonomics] hbase.client.scanner.caching is dogged and will try to return batch even if it means OOME

    Signed-off-by: stack <stack@apache.org>
{noformat}

Going to try to investigate given the changeset.

Good catch! Took a quick look and it seems to be related to the change in the default caching value from HBASE-11544 when coupled with the change in the default max result size from HBASE-12976. I was actually able to reproduce this issue in branch-1.0 by tweeking the caching and max result size to be equal to the defaults that are present in branch-1+. Specifically, the following configuration also produced issues for me in branch-1.0:

{code}
Scan scan = new Scan();
scan.setMaxResultSize(2 * 1024 * 1024);
scan.setCaching(Integer.MAX_VALUE);
...
{code}

(Note that to run the scan with this configuraiton you must remove the ArrayList presizing that is performed in RSRpcServices on line 2055. The caching value is used to presize the array and thus you will likely run into OOME if you do not remove the presizing when running this configuration). 

As to why this error is occuring, I am not sure yet, but I thought this may shed some light on it.

Thanks. Extra information is much appreciated!

bq. Specifically, the following configuration also produced issues for me in branch-1.0
HBASE-11544 is not in 1.0 branch. Is this a different issue? Should we create a subtask and fix it in 1.0.1?

bq. Is this a different issue? Should we create a subtask and fix it in 1.0.1?

It appears as though this problem wasn't "caused" by HBASE-11544 but made more readily reproducible (i.e. the issue already existed, it's just that the default configuration changes introduced by HBASE-11544 make it occur without any additional configuration required). I believe this is the case because if you do the reverse and use the branch-1.0 default values in branch-1+ the issue does not occur:

{code}
Scan scan = new Scan();
// Configure scan to use branch-1.0 default values
scan.setMaxResultSize(2 * 1024 * 1024);
scan.setCaching(100);
...
{code}

Appears to "fix" the trivial example I was trying to write.

So, it sounds like you're saying the fix/affects should also include 1.0.0, but the true defect is still eluding us for the moment (I use "us" very loosely to include my newbie-self)?

I believe that 1.0.0 is also affected by the underlying issue (still not sure what that actual cause is). When I ran your test on branch-1.0 with the Scan configuration from branch-1+ (i.e. caching=Integer.Max_Value and maxResultSize=2MB) I saw this at the output:

{quote}
2015-03-17 12:51:59,123 INFO  [main] hbase.HBaseTest(167): Wrote 1000000 entries
2015-03-17 12:51:59,123 INFO  [main] hbase.HBaseTest(182): Wrote 1000000 entries in total
2015-03-17 12:51:59,123 INFO  [main] hbase.HBaseTest(185): Closing table used for writes
2015-03-17 12:51:59,127 DEBUG [main] client.ClientScanner(260): Advancing internal scanner to startKey at '1\x00'
2015-03-17 12:51:59,327 DEBUG [main] client.ClientScanner(260): Advancing internal scanner to startKey at '2\x00'
2015-03-17 12:51:59,412 DEBUG [main] client.ClientScanner(260): Advancing internal scanner to startKey at '3\x00'
2015-03-17 12:51:59,477 DEBUG [main] client.ClientScanner(260): Advancing internal scanner to startKey at '4\x00'
2015-03-17 12:51:59,528 DEBUG [main] client.ClientScanner(260): Advancing internal scanner to startKey at '5\x00'
2015-03-17 12:51:59,567 INFO  [main] hbase.HBaseTest(204): Saw row 500253
2015-03-17 12:51:59,597 DEBUG [main] client.ClientScanner(260): Advancing internal scanner to startKey at '6\x00'
2015-03-17 12:51:59,641 DEBUG [main] client.ClientScanner(260): Advancing internal scanner to startKey at '7\x00'
2015-03-17 12:51:59,683 DEBUG [main] client.ClientScanner(260): Advancing internal scanner to startKey at '8\x00'
2015-03-17 12:51:59,713 DEBUG [main] client.ClientScanner(260): Advancing internal scanner to startKey at '9\x00'
2015-03-17 12:51:59,729 INFO  [main] hbase.HBaseTest(204): Saw row 801761
2015-03-17 12:51:59,742 INFO  [main] hbase.HBaseTest(211): Last row in Result 902495
2015-03-17 12:51:59,742 INFO  [main] hbase.HBaseTest(214): Saw 23590 rows
2015-03-17 12:51:59,743 INFO  [main] hbase.HBaseTest(215): Saw 235900 cells
2015-03-17 12:51:59,881 INFO  [main] hbase.HBaseTest(220): Missing 976410 rows:...
....
{quote}

As [~elserj] called out above, it seems that we are jumping between regions too early. This hypothesis is also supported by the fact that if you remove the splits from table creation then the issue does not occur. Sorry, my intention is not to hijack this issue, just trying to provide some information that may lead us to the cause.

bq. Sorry, my intention is not to hijack this issue, just trying to provide some information that may lead us to the cause.

Worry not. I'm the blind feeling my way around here. Your insight is helpful and appreciated.

{quote}
So, it sounds like you're saying the fix/affects should also include 1.0.0, but the true defect is still eluding us for the moment (I use "us" very loosely to include my newbie-self)?
{quote}

+1, yeah I'd say the root cause is a 1.0.0 issue as well. what about earlier versions?

bq. I'd say the root cause is a 1.0.0 issue as well. what about earlier versions?

I just came back to my desk and am looking at this now with 0.98 out of concern.

It's not a problem in 0.98 it seems.

I tested with this: https://github.com/apurtell/hbase-hwhat/tree/0.98

{noformat}
2015-03-17 14:00:52,911 WARN  [main] util.NativeCodeLoader(62): Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-17 14:00:53,103 INFO  [main] zookeeper.RecoverableZooKeeper(121): Process identifier=hconnection-0x7afc920e connecting to ZooKeeper ensemble=localhost:2181
2015-03-17 14:00:53,167 DEBUG [main-EventThread] zookeeper.ZooKeeperWatcher(320): hconnection-0x7afc920e0x0, quorum=localhost:2181, baseZNode=/hbase Received ZooKeeper Event, type=None, state=SyncConnected, path=null
2015-03-17 14:00:53,171 DEBUG [main-EventThread] zookeeper.ZooKeeperWatcher(397): hconnection-0x7afc920e-0x14c2985e89a0008 connected
2015-03-17 14:00:53,566 INFO  [main] zookeeper.RecoverableZooKeeper(121): Process identifier=hconnection-0x64198527 connecting to ZooKeeper ensemble=localhost:2181
2015-03-17 14:00:53,574 DEBUG [main-EventThread] zookeeper.ZooKeeperWatcher(320): hconnection-0x641985270x0, quorum=localhost:2181, baseZNode=/hbase Received ZooKeeper Event, type=None, state=SyncConnected, path=null
2015-03-17 14:00:53,578 DEBUG [main-EventThread] zookeeper.ZooKeeperWatcher(397): hconnection-0x64198527-0x14c2985e89a0009 connected
2015-03-17 14:00:54,789 INFO  [main] zookeeper.RecoverableZooKeeper(121): Process identifier=catalogtracker-on-hconnection-0x64198527 connecting to ZooKeeper ensemble=localhost:2181
2015-03-17 14:00:54,790 DEBUG [main] catalog.CatalogTracker(198): Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@3559b630
2015-03-17 14:00:54,797 DEBUG [main-EventThread] zookeeper.ZooKeeperWatcher(320): catalogtracker-on-hconnection-0x641985270x0, quorum=localhost:2181, baseZNode=/hbase Received ZooKeeper Event, type=None, state=SyncConnected, path=null
2015-03-17 14:00:54,798 DEBUG [main-EventThread] zookeeper.ZooKeeperWatcher(397): catalogtracker-on-hconnection-0x64198527-0x14c2985e89a000a connected
2015-03-17 14:00:54,799 DEBUG [main] zookeeper.ZKUtil(430): catalogtracker-on-hconnection-0x64198527-0x14c2985e89a000a, quorum=localhost:2181, baseZNode=/hbase Set watcher on existing znode=/hbase/meta-region-server
2015-03-17 14:00:54,821 DEBUG [main] catalog.CatalogTracker(222): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@3559b630
2015-03-17 14:00:54,837 INFO  [main] client.HConnectionManager$HConnectionImplementation(2246): Closing master protocol: MasterService
2015-03-17 14:00:54,837 INFO  [main] client.HConnectionManager$HConnectionImplementation(1903): Closing zookeeper sessionid=0x14c2985e89a0009
Write buffer size: 52428800
2015-03-17 14:01:05,010 INFO  [main] hbase.HBaseTest(118): Wrote 50000 entries
2015-03-17 14:01:12,491 INFO  [main] hbase.HBaseTest(118): Wrote 100000 entries
2015-03-17 14:01:19,860 INFO  [main] hbase.HBaseTest(118): Wrote 150000 entries
2015-03-17 14:01:27,223 INFO  [main] hbase.HBaseTest(118): Wrote 200000 entries
2015-03-17 14:01:34,772 INFO  [main] hbase.HBaseTest(118): Wrote 250000 entries
2015-03-17 14:01:41,784 INFO  [main] hbase.HBaseTest(118): Wrote 300000 entries
2015-03-17 14:01:49,003 INFO  [main] hbase.HBaseTest(118): Wrote 350000 entries
2015-03-17 14:01:56,044 INFO  [main] hbase.HBaseTest(118): Wrote 400000 entries
2015-03-17 14:02:03,331 INFO  [main] hbase.HBaseTest(118): Wrote 450000 entries
2015-03-17 14:02:10,360 INFO  [main] hbase.HBaseTest(118): Wrote 500000 entries
2015-03-17 14:02:18,349 INFO  [main] hbase.HBaseTest(118): Wrote 550000 entries
2015-03-17 14:02:26,238 INFO  [main] hbase.HBaseTest(118): Wrote 600000 entries
2015-03-17 14:02:33,430 INFO  [main] hbase.HBaseTest(118): Wrote 650000 entries
2015-03-17 14:02:40,362 INFO  [main] hbase.HBaseTest(118): Wrote 700000 entries
2015-03-17 14:02:47,358 INFO  [main] hbase.HBaseTest(118): Wrote 750000 entries
2015-03-17 14:02:54,396 INFO  [main] hbase.HBaseTest(118): Wrote 800000 entries
2015-03-17 14:03:01,735 INFO  [main] hbase.HBaseTest(118): Wrote 850000 entries
2015-03-17 14:03:08,691 INFO  [main] hbase.HBaseTest(118): Wrote 900000 entries
2015-03-17 14:03:16,234 INFO  [main] hbase.HBaseTest(118): Wrote 950000 entries
2015-03-17 14:03:22,987 INFO  [main] hbase.HBaseTest(118): Wrote 1000000 entries
2015-03-17 14:03:22,987 INFO  [main] hbase.HBaseTest(133): Wrote 1000000 entries in total
2015-03-17 14:03:22,987 INFO  [main] hbase.HBaseTest(136): Closing table used for writes
2015-03-17 14:03:23,000 DEBUG [main] client.ClientScanner(277): Advancing internal scanner to startKey at '1\x00'
2015-03-17 14:03:23,667 INFO  [main] hbase.HBaseTest(156): Saw row 108997
2015-03-17 14:03:24,013 INFO  [main] hbase.HBaseTest(156): Saw row 117997
2015-03-17 14:03:24,303 INFO  [main] hbase.HBaseTest(156): Saw row 126997
2015-03-17 14:03:24,610 INFO  [main] hbase.HBaseTest(156): Saw row 135997
2015-03-17 14:03:24,911 INFO  [main] hbase.HBaseTest(156): Saw row 144997
2015-03-17 14:03:25,181 INFO  [main] hbase.HBaseTest(156): Saw row 153997
2015-03-17 14:03:25,442 INFO  [main] hbase.HBaseTest(156): Saw row 162997
2015-03-17 14:03:25,759 INFO  [main] hbase.HBaseTest(156): Saw row 171997
2015-03-17 14:03:26,053 INFO  [main] hbase.HBaseTest(156): Saw row 180997
2015-03-17 14:03:26,351 INFO  [main] hbase.HBaseTest(156): Saw row 189998
2015-03-17 14:03:26,692 INFO  [main] hbase.HBaseTest(156): Saw row 198998
2015-03-17 14:03:26,724 DEBUG [main] client.ClientScanner(277): Advancing internal scanner to startKey at '2\x00'
2015-03-17 14:03:27,026 INFO  [main] hbase.HBaseTest(156): Saw row 207997
2015-03-17 14:03:27,318 INFO  [main] hbase.HBaseTest(156): Saw row 216997
2015-03-17 14:03:27,616 INFO  [main] hbase.HBaseTest(156): Saw row 225997
2015-03-17 14:03:27,908 INFO  [main] hbase.HBaseTest(156): Saw row 234997
2015-03-17 14:03:28,203 INFO  [main] hbase.HBaseTest(156): Saw row 243997
2015-03-17 14:03:28,510 INFO  [main] hbase.HBaseTest(156): Saw row 252997
2015-03-17 14:03:28,785 INFO  [main] hbase.HBaseTest(156): Saw row 261997
2015-03-17 14:03:29,093 INFO  [main] hbase.HBaseTest(156): Saw row 270997
2015-03-17 14:03:29,362 INFO  [main] hbase.HBaseTest(156): Saw row 279998
2015-03-17 14:03:29,642 INFO  [main] hbase.HBaseTest(156): Saw row 288998
2015-03-17 14:03:29,924 INFO  [main] hbase.HBaseTest(156): Saw row 297998
2015-03-17 14:03:29,993 DEBUG [main] client.ClientScanner(277): Advancing internal scanner to startKey at '3\x00'
2015-03-17 14:03:30,270 INFO  [main] hbase.HBaseTest(156): Saw row 306997
2015-03-17 14:03:30,542 INFO  [main] hbase.HBaseTest(156): Saw row 315997
2015-03-17 14:03:30,838 INFO  [main] hbase.HBaseTest(156): Saw row 324997
2015-03-17 14:03:31,100 INFO  [main] hbase.HBaseTest(156): Saw row 333997
2015-03-17 14:03:31,365 INFO  [main] hbase.HBaseTest(156): Saw row 342997
2015-03-17 14:03:31,648 INFO  [main] hbase.HBaseTest(156): Saw row 351997
2015-03-17 14:03:31,902 INFO  [main] hbase.HBaseTest(156): Saw row 360997
2015-03-17 14:03:32,123 INFO  [main] hbase.HBaseTest(156): Saw row 369998
2015-03-17 14:03:32,345 INFO  [main] hbase.HBaseTest(156): Saw row 378998
2015-03-17 14:03:33,285 INFO  [main] hbase.HBaseTest(156): Saw row 387998
2015-03-17 14:03:33,485 INFO  [main] hbase.HBaseTest(156): Saw row 396998
2015-03-17 14:03:33,562 DEBUG [main] client.ClientScanner(277): Advancing internal scanner to startKey at '4\x00'
2015-03-17 14:03:33,750 INFO  [main] hbase.HBaseTest(156): Saw row 405997
2015-03-17 14:03:34,012 INFO  [main] hbase.HBaseTest(156): Saw row 414997
2015-03-17 14:03:34,270 INFO  [main] hbase.HBaseTest(156): Saw row 423997
2015-03-17 14:03:34,535 INFO  [main] hbase.HBaseTest(156): Saw row 432997
2015-03-17 14:03:35,658 INFO  [main] hbase.HBaseTest(156): Saw row 441997
2015-03-17 14:03:35,917 INFO  [main] hbase.HBaseTest(156): Saw row 450997
2015-03-17 14:03:36,174 INFO  [main] hbase.HBaseTest(156): Saw row 459998
2015-03-17 14:03:36,432 INFO  [main] hbase.HBaseTest(156): Saw row 468998
2015-03-17 14:03:36,674 INFO  [main] hbase.HBaseTest(156): Saw row 477998
2015-03-17 14:03:36,918 INFO  [main] hbase.HBaseTest(156): Saw row 486998
2015-03-17 14:03:37,184 INFO  [main] hbase.HBaseTest(156): Saw row 495998
2015-03-17 14:03:37,309 DEBUG [main] client.ClientScanner(277): Advancing internal scanner to startKey at '5\x00'
2015-03-17 14:03:37,470 INFO  [main] hbase.HBaseTest(156): Saw row 504997
2015-03-17 14:03:37,803 INFO  [main] hbase.HBaseTest(156): Saw row 513997
2015-03-17 14:03:38,057 INFO  [main] hbase.HBaseTest(156): Saw row 522997
2015-03-17 14:03:38,308 INFO  [main] hbase.HBaseTest(156): Saw row 531997
2015-03-17 14:03:38,571 INFO  [main] hbase.HBaseTest(156): Saw row 540997
2015-03-17 14:03:38,826 INFO  [main] hbase.HBaseTest(156): Saw row 549998
2015-03-17 14:03:39,061 INFO  [main] hbase.HBaseTest(156): Saw row 558998
2015-03-17 14:03:39,311 INFO  [main] hbase.HBaseTest(156): Saw row 567998
2015-03-17 14:03:39,560 INFO  [main] hbase.HBaseTest(156): Saw row 576998
2015-03-17 14:03:39,893 INFO  [main] hbase.HBaseTest(156): Saw row 585998
2015-03-17 14:03:40,135 INFO  [main] hbase.HBaseTest(156): Saw row 594998
2015-03-17 14:03:40,252 DEBUG [main] client.ClientScanner(277): Advancing internal scanner to startKey at '6\x00'
2015-03-17 14:03:40,367 INFO  [main] hbase.HBaseTest(156): Saw row 603997
2015-03-17 14:03:40,626 INFO  [main] hbase.HBaseTest(156): Saw row 612997
2015-03-17 14:03:40,826 INFO  [main] hbase.HBaseTest(156): Saw row 621997
2015-03-17 14:03:41,106 INFO  [main] hbase.HBaseTest(156): Saw row 630997
2015-03-17 14:03:41,346 INFO  [main] hbase.HBaseTest(156): Saw row 639998
2015-03-17 14:03:41,514 INFO  [main] hbase.HBaseTest(156): Saw row 648998
2015-03-17 14:03:41,730 INFO  [main] hbase.HBaseTest(156): Saw row 657998
2015-03-17 14:03:42,021 INFO  [main] hbase.HBaseTest(156): Saw row 666998
2015-03-17 14:03:42,273 INFO  [main] hbase.HBaseTest(156): Saw row 675998
2015-03-17 14:03:42,516 INFO  [main] hbase.HBaseTest(156): Saw row 684998
2015-03-17 14:03:42,708 INFO  [main] hbase.HBaseTest(156): Saw row 693998
2015-03-17 14:03:42,903 DEBUG [main] client.ClientScanner(277): Advancing internal scanner to startKey at '7\x00'
2015-03-17 14:03:42,979 INFO  [main] hbase.HBaseTest(156): Saw row 702997
2015-03-17 14:03:43,184 INFO  [main] hbase.HBaseTest(156): Saw row 711997
2015-03-17 14:03:43,510 INFO  [main] hbase.HBaseTest(156): Saw row 720997
2015-03-17 14:03:43,713 INFO  [main] hbase.HBaseTest(156): Saw row 729998
2015-03-17 14:03:44,057 INFO  [main] hbase.HBaseTest(156): Saw row 738998
2015-03-17 14:03:44,254 INFO  [main] hbase.HBaseTest(156): Saw row 747998
2015-03-17 14:03:44,608 INFO  [main] hbase.HBaseTest(156): Saw row 756998
2015-03-17 14:03:44,816 INFO  [main] hbase.HBaseTest(156): Saw row 765998
2015-03-17 14:03:45,054 INFO  [main] hbase.HBaseTest(156): Saw row 774998
2015-03-17 14:03:45,340 INFO  [main] hbase.HBaseTest(156): Saw row 783998
2015-03-17 14:03:45,482 INFO  [main] hbase.HBaseTest(156): Saw row 792998
2015-03-17 14:03:45,630 DEBUG [main] client.ClientScanner(277): Advancing internal scanner to startKey at '8\x00'
2015-03-17 14:03:45,680 INFO  [main] hbase.HBaseTest(156): Saw row 801997
2015-03-17 14:03:45,901 INFO  [main] hbase.HBaseTest(156): Saw row 810997
2015-03-17 14:03:46,101 INFO  [main] hbase.HBaseTest(156): Saw row 819998
2015-03-17 14:03:46,330 INFO  [main] hbase.HBaseTest(156): Saw row 828998
2015-03-17 14:03:46,647 INFO  [main] hbase.HBaseTest(156): Saw row 837998
2015-03-17 14:03:46,875 INFO  [main] hbase.HBaseTest(156): Saw row 846998
2015-03-17 14:03:47,109 INFO  [main] hbase.HBaseTest(156): Saw row 855998
2015-03-17 14:03:47,316 INFO  [main] hbase.HBaseTest(156): Saw row 864998
2015-03-17 14:03:47,534 INFO  [main] hbase.HBaseTest(156): Saw row 873998
2015-03-17 14:03:47,722 INFO  [main] hbase.HBaseTest(156): Saw row 882998
2015-03-17 14:03:47,978 INFO  [main] hbase.HBaseTest(156): Saw row 891998
2015-03-17 14:03:48,098 DEBUG [main] client.ClientScanner(277): Advancing internal scanner to startKey at '9\x00'
2015-03-17 14:03:48,122 INFO  [main] hbase.HBaseTest(156): Saw row 900997
2015-03-17 14:03:48,454 INFO  [main] hbase.HBaseTest(156): Saw row 909998
2015-03-17 14:03:48,664 INFO  [main] hbase.HBaseTest(156): Saw row 918998
2015-03-17 14:03:48,896 INFO  [main] hbase.HBaseTest(156): Saw row 927998
2015-03-17 14:03:49,157 INFO  [main] hbase.HBaseTest(156): Saw row 936998
2015-03-17 14:03:49,391 INFO  [main] hbase.HBaseTest(156): Saw row 945998
2015-03-17 14:03:49,579 INFO  [main] hbase.HBaseTest(156): Saw row 954998
2015-03-17 14:03:49,802 INFO  [main] hbase.HBaseTest(156): Saw row 963998
2015-03-17 14:03:50,164 INFO  [main] hbase.HBaseTest(156): Saw row 972998
2015-03-17 14:03:50,347 INFO  [main] hbase.HBaseTest(156): Saw row 981998
2015-03-17 14:03:50,617 INFO  [main] hbase.HBaseTest(156): Saw row 990998
2015-03-17 14:03:50,767 INFO  [main] hbase.HBaseTest(156): Saw row 999999
2015-03-17 14:03:50,768 INFO  [main] hbase.HBaseTest(163): Last row in Result 999999
2015-03-17 14:03:50,768 INFO  [main] hbase.HBaseTest(166): Saw 1000000 rows
2015-03-17 14:03:50,768 INFO  [main] hbase.HBaseTest(167): Saw 10000000 cells
2015-03-17 14:03:51,065 INFO  [main] hbase.HBaseTest(172): Missing 0 rows: []
{noformat}


I don't see where the reconfiguration is done like Jonathan did on branch-1.0. Is the cache behavior change not needed on 0.98?

I was also unable to reproduce the issue in 0.98; saw the same output as [~apurtell]

Ah, missed the comment on this issue where that was mentioned. Let me try it.

Same result after https://github.com/apurtell/hbase-hwhat/commit/13385ef and a similar hack to HRegionServer that Jonathan made to RSRpcServices on 1.0 to remove result size preallocation. 

2015-03-17 14:23:36,333 INFO  [main] hbase.HBaseTest(167): Saw 1000000 rows
2015-03-17 14:23:36,333 INFO  [main] hbase.HBaseTest(168): Saw 10000000 cells
2015-03-17 14:23:36,556 INFO  [main] hbase.HBaseTest(173): Missing 0 rows: []

Anything else I should try?

bq. Note that to run the scan with this configuraiton you must remove the ArrayList presizing that is performed in RSRpcServices on line 2055. The caching value is used to presize the array and thus you will likely run into OOME if you do not remove the presizing when running this configuration

That's an issue in 0.98 too, see HBASE-13269 to follow up

sounds good to me. [~elserj] and [~jonathan.lawlor]? (y'all have better depth on evaluating the issue)

May be worthwhile to do a sanity check to make sure that it breaks on branch-1.0 for you as well. Otherwise I believe we need to dig deeper into the internals. Currently investigating

I think I may have found the issue, and it looks like it is being caused by a mismatch of the result size estimate server side versus client side...

Some background:
* In a scan, the method Scan#setMaxResultSize is exposed to allow the user to restrict the size (in memory) of the Results returned from the server. 
* To enforce this size limit, the server calculates the size of each Result prior to adding it to the list of Results it will send back to the client.
** In 0.98, this size calculation is performed on line 3238 inside HRegionServer.java
** In branch-1.0 this size calculation is performed on line 2099 inside RSRpcServices.java ....
* When the client receives the Results back from the server, it repeats the size calculation. 
** The client does this so that it can infer whether or not this response was sent back because the size limit was reached (in which case there may be more Results within the current region that still need to be scanned). 
** If the client calculates the size of the Results and it sees that the size limit has NOT been hit, it assumes that the current region has been exhausted, and will move the scanner to the next region

The problem here is that there is an implied relationship between the size calculation on the server and the size calculation on the client: The two sizes MUST be equal. If the server reaches the size limit, it is implied that the client should also reach the size limit.... This is where the issue occurs.

In 0.98 I added some logs and it looks like this relationship is always true. Specifically, the size calculated by the server is always equal to the size calculated by the client. However, in this test case, this is not true in branch-1.0+. What I see instead is that the size calculated by the server is LARGER than the size calculated by the client. The net effect is that the client checks its size limit and sees that the limit has not been reached, so it assumes that the region has been exhausted and moves the scanner to the next region... so as [~elserj] predicted, the root cause is that we jump between regions too soon....

It looks like the root cause of this issue is due to the fact that the implementation of the method that is used to calculate the Result size changed between 0.98 and branch-1.0. 

I am attaching two test run outputs with some added logging. The way to interpret the output is as follows:
* A log was added on the server to log when the result size and number of rows being returned (this is seen as a log from HRegionServer in 0.98 and RSRpcServices in branch-1.0)
* A log was added on the client to log the remaining result size and also whether or not the size or caching limit has been reached (seen as a log from ClientScanner)

We expect the two result sizes to be equal.. in the 0.98 you can see they are equal, branch-1.0 the result size on the server is larger causing us to skip to the next region too early. The logging was a little rough, so if anything needs clarification please let me know.

attaching test run outputs during the scanning phase with added logging as described above

Kudos on tracking this down!

bq. The net effect is that the client checks its size limit and sees that the limit has not been reached, so it assumes that the region has been exhausted and moves the scanner to the next region... so as Josh Elser predicted, the root cause is that we jump between regions too soon....

Bingo. I just got to this point as well.

{panel:title=ClientScanner.java:482}
{code}
        } while (remainingResultSize > 0 && countdown > 0
            && (!partialResults.isEmpty() || possiblyNextScanner(countdown, values == null)));
{code}
{panel}

One important thing that I think I've convinced myself of is that this also only happens when there are no queued partial results in the client as well (as the presence of the partial will also force the client to talk to the same region again).

I'll take a look at the stuff you attached (again, much appreciated) and see if I can chow down on the rest of your analysis and merge that in with what I (think) I figured out.

I'm a little confused given what you suggested:

bq. The problem here is that there is an implied relationship between the size calculation on the server and the size calculation on the client: The two sizes MUST be equal. If the server reaches the size limit, it is implied that the client should also reach the size limit.... This is where the issue occurs.

Are you implying that this is specifically the problem? I'm not seeing where these sizes are used for anything more than metrics tracking (on client and server, {{ScannerCallable#updateResultsMetrics(Result[])}} and {{MetricsRegion#updateScanNext(long)}} (called via {{RSRpcServices#scan(..)}}, respectively).

To me, the larger issue seems to be that only a {{Result[]}} is returned from {{ScannerCallable}} back to {{ClientScanner}} as {{ScanResponse.getMoreResults()}} is hidden in the callable implementation. Thus, the scanner impl needs to some how reason about more results on being on the server even when we don't have any partial results cached.

That being said, I think I need to look at this with some fresh eyes :). Please correct me if I missed something.

bq. Are you implying that this is specifically the problem? I'm not seeing where these sizes are used for anything more than metrics tracking

So within {{RSRpcServices#scan(...)}} we keep a running tally of the size of the accumulated {{Result}} within the variable {{currentScanResultSize}}. We collect the {{Result}} in a while loop that loops while the caching limit hasn't been reached. At the beginning of each iteration of this loop, we check the running Result size limit against the {{maxResultSize}}. If the size limit has been reached, we break out of the loop and will end up returning whatever Results we have accumulated thus far back to the client. The problem is that we then expect the Client to realize that the Results they receive are larger than the {{maxResultSize}} -- if the client's size calculation is less than the server's then it's possible the client will misinterpret the response as meaning the region has been exhausted.

bq. To me, the larger issue seems to be that only a Result[] is returned from ScannerCallable

I agree completely. It is ugly that we return ONLY a {{Result[]}} to the client and then expect them to understand why those are the Results that were returned from the server. Was the size limit reached? Was the caching limit reached? Was a partial result formed? Are there more results on the server or is the region exhausted? There are too many things that the client needs to infer from the {{Result[]}} alone that the server already had the answer to. I think it would be great if this could be cleaned up.

bq. So within RSRpcServices#scan(...) we keep a running tally of the size of the accumulated Result within the variable currentScanResultSize. We collect the Result in a while loop that loops while the caching limit hasn't been reached. At the beginning of each iteration of this loop, we check the running Result size limit against the maxResultSize. If the size limit has been reached, we break out of the loop and will end up returning whatever Results we have accumulated thus far back to the client. The problem is that we then expect the Client to realize that the Results they receive are larger than the maxResultSize – if the client's size calculation is less than the server's then it's possible the client will misinterpret the response as meaning the region has been exhausted.

Thanks for the pointer. I will investigate.

We also need to keep in mind that a 0.98 client *must* work with a 1.0+ server.
This is very scary stuff.

The concern is definitely noted, but thank you for being clear.

Ok, been a while since I posted some progress, here's my current understanding of things and hopefully an easier to grok statement of the problem:

When clients request a batch of rows which is larger than the server is configured to return (often, when the client
does not explicitly set a limit to the results to be returned from the server), the client will incorrectly treat this
as all data in the current region has been exhausted. This goes back to what [~jonathan.lawlor] pointed out about
clients and servers needing to stay in sync WRT the size of a batch of {{Result}}s. The client ultimately requests the
server return a batch of size 'hbase.client.scanner.max.result.size' and then believe that the server returned less data
than that limit.

A client-side workaround to the problem is to reduce the number of rows requested on the {{Scan}} via
({{Scan#setCaching(int)}}. Setting this value sufficiently low enough (for my test code, anything less than 1000 seems
to do the trick) will cause the server to flush the results back to the client before the server gets close
to the size limit which would cause the client to do the wrong thing.

I still don't completely understand what is causing the difference on the server-side in the first place (over 0.98). I
need to dig more there to understand things. I'm not sure if I'm just missing somewhere that
{{CellUtil#estimatedHeapSizeOf(Cell)}} isn't being used, or if some size is bubbling up through the {{NextState}} via
the {{KeyValueHeap}} (and thus MemStores or StoreFiles), or something entirely different.

Ultimately, the underlying problem is likely best addressed from the stance that a scanner shouldn't be performing
special logic based on the size of the batch of data returned from a server. In other words, the
client should not be making logic decisions based solely on the size or length of the {{Result[]}} it receives.

The server already maintains a nice enum of the reason which it returns a batch of results to a client via
{{NextState$State}}. The server has the answer to our question when returns a batch: is this batch return due
a limitation on size of this batch (either length or bytes)?

I'm currently of the opinion that it's ideal to pass this information back to the client via the {{ScanResult}}.
Ignoring wire-version issues for the moment, this means that clients would rely on this new enum to determine when
there is more data to read from a Region and when a Region is exhausted (instead of the size and length checks of the
{{Result[]}}.

This approach wouldn't break 0.98 clients against 1.x; however, it also wouldn't address the underlying problem of the
client guessing at what to do based on the characteristics of the {{Result[]}} when it is unaware of the existence of
this new field in the protobuf. Given my understanding of the problem, 0.98 clients running against 1.x *could* see
this problem, although I have not tested that to confirm it happens.

Obviously, I need to do some more digging as to where the mismatch in size is coming from (unless I missed it from
Jonathan earlier on) before I get a patch. Thoughts/comments welcome meanwhile.

bq. This approach wouldn't break 0.98 clients against 1.x; however, it also wouldn't address the underlying problem of the client guessing at what to do based on the characteristics of the {{Result[]}} when it is unaware of the existence of this new field in the protobuf. Given my understanding of the problem, 0.98 clients running against 1.x *could* see this problem, although I have not tested that to confirm it happens.

Wire compatibility and a default configuration in 1.0.x that mitigates the problem until a rolling upgrade is completed could be good enough. Additional comment reserved until you come back with results from more digging. 

bq. The client ultimately requests the server return a batch of size 'hbase.client.scanner.max.result.size' and then believe that the server returned less data than that limit.

Exactly correct. The client looks at the Results returned from the server and from its point of view it sees that neither the maxResultSize or caching limit has been reached. The only explanation it can come up with as to why the server would return these Results is that it must have exhausted the region (otherwise it has no reason to stop accumulating Results). But the server stopped because from its PoV the size limit was reached. There is a miscommunication

bq. I still don't completely understand what is causing the difference on the server-side in the first place (over 0.98)

Ya, it's a little cryptic because the exact same function is used to calculate the size server side and client side. I would recommend adding some logs that allows you to see the estimatedHeapSize of a cell server side versus client side and see where they differ. My guess would be that somehow the Cell on the client side returns a slightly lower heap size estimation than the SAME Cell on the server (I don't believe it's related to the NextState size bubbling up since NextState is only in branch-1+ and the issue is branch-1.0+). Maybe the Cells/Results are serialized in such a way that these calculations are slightly different? Somehow the server's size calculation is larger than the client's size calculation.

However, even when we do understand why the server's size calculation is different from the client's it may not help (of course we can only know once the issue has been identified). Like you said, the underlying problem is that the client shouldn't even be performing a size calculation but rather being told by the server why the Results were returned. As long as there is a possibility for the server and client to disagree on why the Results were returned, it is possible to incorrectly jump between regions. Fixing the size calculation may be sufficient for resolving this issue, but going forward I think your idea of passing information back to the client in the ScanResult will be the best way to go.

bq. Ultimately, the underlying problem is likely best addressed from the stance that a scanner shouldn't be performing special logic based on the size of the batch of data returned from a server

Agreed

bq. The server already maintains a nice enum of the reason which it returns a batch of results to a client via NextState$State

Just a note: NextState was introduced with HBASE-11544 which has only been backported to branch-1+ at this point. Since this issue appears in branch-1.0+, returning the NextState$State enum would require backporting that feature further. 

bq. I'm currently of the opinion that it's ideal to pass this information back to the client via the ScanResult 

I agree that somehow we need to communicate the reasoning behind why these Results were returned to the client rather than looking at the Result[] and making an "educated" guess

bq. 0.98 clients running against 1.x could see this problem, although I have not tested that to confirm it happens.

I suspect you're correct


Turns out the ever-so-slight variance between sizes on server and client is there from {{KeyValue#heapSize()}}.

{panel:title=RegionServer}
{noformat}
206228/cf:0/1426716998482/Put/vlen=50/seqid=41, size=152, length=81
KeyLength=21, TagsLength=0, RowLength=6, FamilyLength=2, QualiferLength=1
{noformat}
{panel}

{panel:title=Client}
{noformat}
206228/cf:0/1426716998482/Put/vlen=50/seqid=0, Size=144, Length=79
KeyLength=21, TagsLength=0, RowLength=6, FamilyLength=2, QualiferLength=1
{noformat}
{panel}

Best as I understand, the seqId should be causing a difference in the calculation of {{heapSize()}}. Aside from that, I haven't been able to find a fundamental difference in the underlying {{byte[]}} of the {{KeyValue}} on client and server. My present hunch is that the length of the array happens to be 2-bytes longer than the data in the array and somehow those two extra bytes get lopped off in the process of being sent from server to client? Just a guess at this point -- will hook up a debugger again tmrw to see if I can figure out exactly why the length is longer but the objects appear to be functionally equivalent.

What exactly is 'size' above [~elserj]? (and why are we capitalized in one log and not in the other? Is it just how you inserted your logging strings?). If it is heapSize, I cannot explain the diff. Same JVM on client and server?  Or is it estimated size? Can we see your hacked in debug?

That client and server have to agree on size is broke, yeah, but before protobufs, it was the only hacky means of introducing size-based scanning. See explanation here on HBASE-1996 by searching "Erik Rozendaal added a comment - 31/Dec/09 02:14". Now we are up on pb and are passing flags in the result envelope, as per yours and [~jonathan.lawlor]'s suggestion above, could add flags to do this properly going forward (caveat the [~lhofhansl] note that it has to work for 0.98.. .ugh).

Thanks for the work on this. Nice digging.

Heh, yeah, the log message discrepancy is just me not doing things cleanly. In the above, size is {{CellUtils.estimatedHeapSizeOf(Cell)}} and length is {{KeyValue.getLength()}}. Sorry for the confusion:

{code}
KeyValue kv = (KeyValue) cell;
LOG.info(cell + ", size=" + CellUtil.estimatedHeapSizeOf(cell) + ", length=" + kv.getLength());
{code}

bq.  Same JVM on client and server

Yeah, I'm just running this on a single machine.

bq. Can we see your hacked in debug

Yeah, I'll post it in a little bit. I'm cleaning this up ATM so I can get back to a good, known working copy again.

bq. Just a note: NextState was introduced with HBASE-11544 which has only been backported to branch-1+ at this point. Since this issue appears in branch-1.0+, returning the NextState$State enum would require backporting that feature further. 

Oh, thank you for making me aware of this. I haven't been paying close enough attention to have noticed this on my own.

{quote}
bq. Just a note: NextState was introduced with HBASE-11544 which has only been backported to branch-1+ at this point. Since this issue appears in branch-1.0+, returning the NextState$State enum would require backporting that feature further.
Oh, thank you for making me aware of this. I haven't been paying close enough attention to have noticed this on my own.
{quote}

I don't like the implication that scanning only works in 0.98 by happy accident of KeyValue size estimation. I'm inclined to make a solution that comes out of this discussion a blocker for the next release. 

I think we can backport NextState. Additions to protobuf messages are compatible. Older clients won't understand them, and would by definition carry on with the older logic. Older servers won't send them, so newer clients would also have to fall back. We can have compensating code for these cases in the client.  Newer clients talking to newer servers won't risk missing data in edge cases now or going forward. Newer 0.98 clients talking with 1.0+ servers also would be in great shape. Am I missing anything?

Makes sense to me. The ultimate reason for this size discrepancy may influence the final resolution, but you hit the general path I'm leaning toward presently.

A quick patch which adds some extra debug into RSRpcServices.java. The same logging can be added in the client. Hopefully this is a little more clear. I can update my test code too if someone wishes for it.

For a single cell, here's what I actually saw: server is the top pair, client is the bottom:

{noformat}
206228/cf:0/1426716998482/Put/vlen=50/seqid=41, Estimated heap size=152, KeyValue length=81
206228/cf:0/1426716998482/Put/vlen=50/seqid=0, Estimated heap size=144, KeyValue length=79

ValueBytes=[0, 0, 0, 21, 0, 0, 0, 50, 0, 6, 50, 48, 54, 50, 50, 56, 2, 99, 102, 48, 0, 0, 1, 76, 46, -11, -69, 82, 4, 77, 48, -46, -65, -5, 11, -89, -64, -118, -77, -79, -117, -102, -63, 36, -102, -64, -15, 16, -57, 41, -74, -106, 31, 110, 33, -3, -71, -5, -93, -28, -33, 0, 5, -5, 28, 98, 70, 1, 5, 94, 81, 49, 28, -90, 79, 115, 96, -36, 82, 0, 0]
ValueBytes=[0, 0, 0, 21, 0, 0, 0, 50, 0, 6, 50, 48, 54, 50, 50, 56, 2, 99, 102, 48, 0, 0, 1, 76, 46, -11, -69, 82, 4, 77, 48, -46, -65, -5, 11, -89, -64, -118, -77, -79, -117, -102, -63, 36, -102, -64, -15, 16, -57, 41, -74, -106, 31, 110, 33, -3, -71, -5, -93, -28, -33, 0, 5, -5, 28, 98, 70, 1, 5, 94, 81, 49, 28, -90, 79, 115, 96, -36, 82]

KeyLength=21, TagsLength=0, RowLength=6, FamilyLength=2, QualiferLength=1, heapSize=152
KeyLength=21, TagsLength=0, RowLength=6, FamilyLength=2, QualiferLength=1, heapSize=144
{noformat}

As reported, the {{KeyValue}} on the server has an extra two bytes which get truncated before the client sees them. Not sure where they're coming from -- that's going to be the next thing I try to figure out (unless someone has a smart idea before I get there :D)

Only things I can think of are that we are doing the fork inside estimatedSerializedSizeOf (in one place it is a KeyValue, and in other it is a Cell)?

bq. I don't like the implication that scanning only works in 0.98 by happy accident of KeyValue size estimation. I'm inclined to make a solution that comes out of this discussion a blocker for the next release.
+1

This part needs to be 100% rock solid, or we could replace the memstore with /dev/null and scanners with /dev/zero :)

Also see HBASE-12976, I wonder whether I should undo that one. (seems it's _very_ bad when client and server do not agree on the size setting).

As a radical approach, we can fall through out of the while loop in ClientScanner as long as we get _anything_ from the server.
So if we get at least one KV back we can fall through and not go to the nextscanner(...). The implication is that we'd do an extra RPC per region.

Client and server agreeing with absolute certainty to the expected size clearly does not work.


If we could change the protocol we can send a flag along, obviously.

Here's a testpatch for what I meant. Do away with the size accounting and eat one extra RPC instead that would return an empty results array. Use that as indication to go to the next region instead.

Not ideal, because of the extra RPC, but it would work in 0.98 unchanged, and would relieve us from the result sizing.

Also attached a test that indicates the problems (server has size limit of 1, client has Long.MAX_VALUE). That would fail without this patch, which is bad.

In trunk we should fix with another attribute on the scan response.

I believe the extra two Bytes as seen on the server are due to the following code:

{panel:title=ScannerV3.java}
{noformat}
    @Override
    protected int getCellBufSize() {
      int kvBufSize = super.getCellBufSize();
      if (reader.hfileContext.isIncludesTags()) {
        kvBufSize += Bytes.SIZEOF_SHORT + currTagsLen;
      }
      return kvBufSize;
    }
{noformat}
{panel}

The KeyValue doesn't have any tags {{currTagsLen==0}}, but the length of the buffer is still extended 2 bytes which would explain the difference of 2bytes in the size of the (limits in the) backing array. Seems to me that the client and the server should be calculating this the same way. Next question is why they aren't.

[~elserj] Wow, nice digging, that looks like the cause!

It makes sense because when the cell block for the RPC response is being built within {{IPCUtil#buildCellBlock}}, the {{KeyValueEncoder}} that is used to write the cells from the cellScanner uses {{KeyValue#oswrite(final KeyValue kv, final OutputStream out, final boolean withTags)}} to write the cells to the output stream. Implementation pasted below:

{panel:title=KeyValue.java}
{noformat}
public static long oswrite(final KeyValue kv, final OutputStream out, final boolean withTags)
      throws IOException {
    // In KeyValueUtil#oswrite we do a Cell serialization as KeyValue. Any changes doing here, pls
    // check KeyValueUtil#oswrite also and do necessary changes.
    int length = kv.getLength();
    if (!withTags) {
      length = kv.getKeyLength() + kv.getValueLength() + KEYVALUE_INFRASTRUCTURE_SIZE;
    }
    // This does same as DataOuput#writeInt (big-endian, etc.)
    StreamUtils.writeInt(out, length);
    out.write(kv.getBuffer(), kv.getOffset(), length);
    return length + Bytes.SIZEOF_INT;
  }
{noformat}
{panel}

Notice that when withTags is false, we end up stripping those 2 bytes! The question now is why does 0.98 NOT have this problem. The implementation of ScannerV3#getCellBuffSize hasn't changed from 0.98 to branch-1.0 -- does that mean that in 0.98 we end up using ScannerV2#getCellBuffSize instead of V3... (I'm not too familiar with this area myself so maybe someone can confirm/deny)?

[~lhofhansl] I think you're right. That solution would ensure that there is no possibility for the client to preemptively move regions. While there is potentially some ambiguity behind why a particular {{Result[]}} was returned from the server when it is not empty (was the size limit reached? caching limit? end of region?), there is no ambiguity currently associated with seeing an empty {{Result[]}} (we know that the we must try to change regions). Having to eat an extra RPC certainly isn't ideal, but I don't know if there is a solution other than that one that would guarantee no chance of data loss.

Tentative patch which replaces hard-coded withTags parameter in KeyValueCodec$KeyValueEncoder.write(Cell) by checking tag length.

TestFromClientSide passes. Running more tests.

[~ted_yu] What are you doing in here? [~elserj] and [~jonathan.lawlor] are working on this. They do all the footwork and then you come in after all is (near) worked out with a patch?

[~elserj] Do we know when the change came in? What's git blame show? That'd allow us rule when the breakage started, do you think?

[~larsh]
bq. Not ideal, because of the extra RPC, but it would work in 0.98 unchanged, and would relieve us from the result sizing.

So 0.98 has the issue too? ([~jonathan.lawlor] seems to indicate not? Maybe I misread).  The plan is to add your patch to 0.98 only? Thanks.

[~tedyu] no dice with that patch. Still bails out early. It exhibits the same symptoms as previously, but I didn't dig further.

{noformat}
2015-03-19 23:34:44,269 INFO  [main] hbase.HBaseTest(184): Saw 198352 rows
2015-03-19 23:34:44,269 INFO  [main] hbase.HBaseTest(185): Saw 1983520 cells
{noformat}

[~jonathan.lawlor], thanks again for the confirmation on my findings! 

{quote}
Notice that when withTags is false, we end up stripping those 2 bytes! The question now is why does 0.98 NOT have this problem. The implementation of ScannerV3#getCellBuffSize hasn't changed from 0.98 to branch-1.0 – does that mean that in 0.98 we end up using ScannerV2#getCellBuffSize instead of V3... (I'm not too familiar with this area myself so maybe someone can confirm/deny)?
{quote}

Is there a reason to even do the following change of the length when there are no tags? It would result in slightly more data sent to the client, but is there a compatibility (or any other) reason too?

{code}
    if (!withTags) {
      length = kv.getKeyLength() + kv.getValueLength() + KEYVALUE_INFRASTRUCTURE_SIZE;
    }
{code}

[~lhofhansl], thanks for throwing up a patch for 0.98. It makes sense to me.

Anyone mind if I try to take a stab at adding the necessary changes to the RPC layer to push this extra logic into (as we've talked about a couple of times now)? Lars, you mentioned, trunk, would we also want to target any branch1 stuff with this?

bq. Do we know when the change came in? What's git blame show? That'd allow us rule when the breakage started, do you think?

Not sure yet. Despite knowing the reason why, it's not entirely obvious to me at where the change happened that caused it (yet). Specifically, HFileContext, FileInfo, KeyValueUtils, KeyValueCodec and/or KeyValue could all have changed and caused a different path to be taken. Need to actually track down if this has been like this since the introduction of HFileV3 or something more recent (post-0.98). Any pointers/hunches welcome as always.

[~elserj] hfilev3 is defaulted on in hbase 1.0 (https://issues.apache.org/jira/browse/HBASE-10855) Was off previous; e.g. 0.98.... if that helps.

bq. So 0.98 has the issue too? (Jonathan Lawlor seems to indicate not? Maybe I misread). 

[~stack] sorry, my comment above was misleading. What I meant to say is that we don't see this issue when a 0.98 client is interacting with a 0.98 server. However, we suspect that the issue may occur when a 0.98 is talking to a branch-1.0+ server (I don't think anyone has actually confirmed this yet?). [~lhofhansl]'s patch would eliminate any possibility of this issue occurring between a 0.98 client and a branch-1.0+ server (correct me if I'm wrong [~lhofhansl]).

bq. Is there a reason to even do the following change of the length when there are no tags?

[~elserj] Good question. I don't know myself but maybe somebody else has a little more context around that change? My guess would be that, as you suggested, it was put in for compatibility reasons so that the length would match the old length

bq. hfilev3 is defaulted on in hbase 1.0

This is likely why we see this issue appear in branch-1.0+. A good test to see if this is the case would be to run [~elserj]'s test in 0.98 with HFileV3 turned on.

Guys, I can help out in this area regarding the changes done. 

First of all the change that you see in ScannerV3 is purely on the reader side.  With HfileV3 on, we ensure that while flushing we always add the tag length even when we don't have a tag.  But while compaction we will remove that tag length.  So a KV coming out of a just flushed file and a KV coming out of a compacted file will have the difference in the Cell size.
Where as in the client side - we will never return tags.  This means that in the KeyValueCodec we will always strip the tags.  But the same is not applicable when we do the codec with the WALCodecs. Because there we need the Tags.
For the above test - are you sure all the store files are compacted?  


But as suggested in the above comments am not very sure why the problem is in 1.0+ and above.  If the same feature is in 0.98 also then we should have failed even there. I will once again check whats going on here based on the analysis above.

bq. So 0.98 has the issue too? (Jonathan Lawlor seems to indicate not? Maybe I misread)

[~stack] So the general issue is that the counting on client and server needs to match 100% in order for this to work .

There are multiple ways to make this fail:
* accidentally with incorrect code, which lead to this issue
* having client and server configured with different values for the max scanner result size in 0.98 or 1.0 (that I demonstrate in the test in patch)
* probably more

The sizing on the client is fickle and bad. It _has_ to go.
The patch I propose just does away with the sizing on the client for 0.98 and 1.0. That will cause an extra RPC if the scanner caching is set such that it would fire after the size limit, in that case we need the extra RPC to detect that we're done with a region.

In 1.1 and later we can do what has been proposed here in various forms and add some extra flag to the RPC to indicate whether we filled the batch rather than trying to derive this information for the size of the results array.

In either case the matching size calculation on the client is bad and should be removed in all cases.

Am I making sense? Maybe we should have two different jira...?

Edit: Fixed lot's of spelling errors...

[~lhofhansl] it makes sense to me. Do you want to make a ticket for the 0.98/1.0 patch you already have? [~jonathan.lawlor] and I can then either use this one (or make a new child) for the 1.1->later fix as described multiple times.

[~ram_krish] thanks for the offer. I'll address your questions tmrw and see what else I might be able to turn out.

Once the HFile V3 is enabled (using conf) and test, it might come in 0.98 also.

Oh ya.. That is right. I forgot that it is not enabled by default in 0.98.

bq. The sizing on the client is fickle and bad. It has to go.
+1. 
bq. That will cause an extra RPC if the scanner caching is set such that it would fire after the size limit, in that case we need the extra RPC to detect that we're done with a region.
Maybe we can add a flag indicating that the client "knows" about the flag in the openScanner() request. If the client knows about the flag, server sends the flag, if not, the client does an extra RPC. 

An extra RPC will be quite bad for performance, especially for clients like Phoenix, which does a lot of short scans using guideposts. We should backport the NextState (or similar) changes all the way back to 0.98 as I proposed above. I will do this if nobody else wants to do the work. I don't see any reason the changes cannot be both backwards and forwards compatible.

bq. Maybe we can add a flag indicating that the client "knows" about the flag in the openScanner() request. If the client knows about the flag, server sends the flag, if not, the client does an extra RPC.

This shouldn't be necessary. The server can just send NextState. If the client knows about it, it can do the right thing, avoiding unnecessary RPC. Otherwise we can fall back to the safest possible alternative, such as not estimating size on the client at all, therefore incurring costs like an extra RPC.

bq. We should backport the NextState (or similar) changes all the way back to 0.98

The NextState change from HBASE-11544 required a change in the RegionScanner interface so that this richer type could be returned. [~enis] recently pointed out in HBASE-11544 that this change to the RegionScanner interface would cause issues with coprocessors. [~stack] pointed out that this change is okay in 1.1+ due to the semver, but backporting beyond 1.1 may break this. 

If backporting the NextState return type change is not possible, backporting the NextState data structure may be sufficient, and we could construct a corresponding NextState within RSRpcServices#scan.

bq. For the above test - are you sure all the store files are compacted? 

Interesting, thanks for the note about difference between flushed and compacted files. That does make a difference. If I do a (full) major compaction to rewrite the files before running the query, suddenly the query starts operating as intended.

Is there a read that we always add the tag length when flushing?

Yes, during flushing we always write the tag length even when it is 0.

Yes, during flushing we always write the tag length even when it is 0.

Yes, during flushing we always write the tag length even when it is 0.

Yes, during flushing we always write the tag length even when it is 0.

Preliminary patch which fixes ClientScanner to adhere to moreResults on ScanResponse.

TestAssignmentManager is being problematic for me locally -- still debugging it. Attaching here to get the full test-patch report meanwhile.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12706573/HBASE-13262.patch
  against master branch at commit 845f5de121e92a99b41b30dc86cb3f2898e0254f.
  ATTACHMENT ID: 12706573

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 16 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13368//console

This message is automatically generated.

Same patch (sans conflicts), cherrypicked over to master instead of branch-1.

[~elserj] You can trigger a build vs a specific branch by including the branch name in the patch name. The branch names accepted are: https://github.com/apache/hbase/blob/master/dev-support/test-patch.properties#L28-L30

Ooo, thanks for the tip, Nick!

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12706579/HBASE-13262-v1.patch
  against master branch at commit 845f5de121e92a99b41b30dc86cb3f2898e0254f.
  ATTACHMENT ID: 12706579

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 16 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 1920 checkstyle errors (more than the master's current 1918 errors).

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +    KeyValue kv1 = new KeyValue("row".getBytes(), "cf".getBytes(), "cq".getBytes(), 1, Type.Maximum);
+      inOrder.verify(caller, Mockito.times(2)).callWithoutRetries(Mockito.any(RetryingCallable.class),
+    KeyValue kv1 = new KeyValue("row".getBytes(), "cf".getBytes(), "cq".getBytes(), 1, Type.Maximum);
+      inOrder.verify(caller, Mockito.times(2)).callWithoutRetries(Mockito.any(RetryingCallable.class),
+      inOrder.verify(caller, Mockito.times(2)).callWithoutRetries(Mockito.any(RetryingCallable.class),
+    KeyValue kv1 = new KeyValue("row".getBytes(), "cf".getBytes(), "cq".getBytes(), 1, Type.Maximum);
+      inOrder.verify(caller, Mockito.times(2)).callWithoutRetries(Mockito.any(RetryingCallable.class),
+    KeyValue kv1 = new KeyValue("row".getBytes(), "cf".getBytes(), "cq".getBytes(), 1, Type.Maximum);
+    KeyValue kv2 = new KeyValue("row2".getBytes(), "cf".getBytes(), "cq".getBytes(), 1, Type.Maximum);
+      inOrder.verify(caller, Mockito.times(2)).callWithoutRetries(Mockito.any(RetryingCallable.class),

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//artifact/patchprocess/checkstyle-aggregate.html

                Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13370//console

This message is automatically generated.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12706672/HBASE-13262-branch-1.patch
  against branch-1 branch at commit 65d3781897baf05ef583fce3b5e8e08cd6228f35.
  ATTACHMENT ID: 12706672

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 16 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 3811 checkstyle errors (more than the master's current 3809 errors).

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +    KeyValue kv1 = new KeyValue("row".getBytes(), "cf".getBytes(), "cq".getBytes(), 1, Type.Maximum);
+      inOrder.verify(caller, Mockito.times(2)).callWithoutRetries(Mockito.any(RetryingCallable.class),
+    KeyValue kv1 = new KeyValue("row".getBytes(), "cf".getBytes(), "cq".getBytes(), 1, Type.Maximum);
+      inOrder.verify(caller, Mockito.times(2)).callWithoutRetries(Mockito.any(RetryingCallable.class),
+      inOrder.verify(caller, Mockito.times(2)).callWithoutRetries(Mockito.any(RetryingCallable.class),
+    KeyValue kv1 = new KeyValue("row".getBytes(), "cf".getBytes(), "cq".getBytes(), 1, Type.Maximum);
+      inOrder.verify(caller, Mockito.times(2)).callWithoutRetries(Mockito.any(RetryingCallable.class),
+    KeyValue kv1 = new KeyValue("row".getBytes(), "cf".getBytes(), "cq".getBytes(), 1, Type.Maximum);
+    KeyValue kv2 = new KeyValue("row2".getBytes(), "cf".getBytes(), "cq".getBytes(), 1, Type.Maximum);
+      inOrder.verify(caller, Mockito.times(2)).callWithoutRetries(Mockito.any(RetryingCallable.class),

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.master.TestAssignmentManager

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//artifact/patchprocess/checkstyle-aggregate.html

                Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13371//console

This message is automatically generated.

New patches for master and branch-1. Fixed up the tests locally and the line length stuff.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12706813/HBASE-13262-v2.patch
  against master branch at commit 5459008ccdc83f09e6d8d7ec90144dc120e1a084.
  ATTACHMENT ID: 12706813

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 12 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 1919 checkstyle errors (more than the master's current 1917 errors).

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.client.TestFromClientSideWithCoprocessor
                  org.apache.hadoop.hbase.client.TestFromClientSide

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//artifact/patchprocess/checkstyle-aggregate.html

                Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13379//console

This message is automatically generated.

Missed running these tests locally. Looks like there was some implicit state built into a null ScanResponse which I broke in this last patch. Instead of a null {{Result[]}} coming back, it got an empty {{Result[]}} and {{getMoreResults=false}} -- I think I can just update the "done" condition in ClientScanner.

[~elserj] Nice patch, some review below

Nice tests, I especially like the new TestClientScanner

bq. // TODO Use the server's response about more results
Not sure what this line means, do we need to check the more results flag here?

I like the idea of using moreResults flag but I believe we need to actually introduce a new flag into the ScanResponse. Unfortunately, the name moreResults is a little misleading as it seems perfect for what we are trying to achieve. Looking into RSRpcServices to see when this moreResults flag is set to false, it looks like this happens only when scanner.isFilterDone() is true. Looking closer, RegionScannerImpl#isFilterDone is only true when the RegionScanner wants to indicate that the entire scan should stop (i.e. the client shouldn't even try to change regions, the whole scan is done).

So to be clear, it looks as though the moreResults flag is false ONLY when the entire scan needs to stop, NOT when a region is exhausted. The net effect is that moreResults will always appear to be true client side, even when the region is exhausted. Thus, I think we will still end up making that extra RPC that Lars mentioned above in order to see that the Result[] is empty and thus the region is exhausted, before the region change occurs.

Since moreResults is a flag that is used for global scan logic (logic not specific to a particular region), I think we need to introduce a new flag that is specific to the region's results. If the result size limit or caching limit is reached inside RSRpcServices, return true, else false.

bq. // Server didn't respond whether it has more results or not.
Is it possible here that we may inadvertently interpret the missing flag as meaning the region is exhausted? Probably fine because the limit logic is still in the ClientScanner while condition, just wondering.

Just attached some v3 patches which will hopefully fix all of the tests and checkstyle stuff.

Thanks for the review, Jonathan.

bq. Not sure what this line means, do we need to check the more results flag here?

Yeah, I only changed ClientScanner to start using {{ScanResponse#getMoreResults()}}. I assume that the reverse (small) scanner should also start using it in the long run. I left them as-is for the time just to not get more sidetracked with them and focus on getting ClientScanner correct.

{quote}
I like the idea of using moreResults flag but I believe we need to actually introduce a new flag into the ScanResponse. Unfortunately, the name moreResults is a little misleading as it seems perfect for what we are trying to achieve. Looking into RSRpcServices to see when this moreResults flag is set to false, it looks like this happens only when scanner.isFilterDone() is true. Looking closer, RegionScannerImpl#isFilterDone is only true when the RegionScanner wants to indicate that the entire scan should stop (i.e. the client shouldn't even try to change regions, the whole scan is done).

So to be clear, it looks as though the moreResults flag is false ONLY when the entire scan needs to stop, NOT when a region is exhausted. The net effect is that moreResults will always appear to be true client side, even when the region is exhausted. Thus, I think we will still end up making that extra RPC that Lars mentioned above in order to see that the Result[] is empty and thus the region is exhausted, before the region change occurs.
{quote}

Hrm, it appears you are correct once again. I had thought I had some changes in RSRpcServices for this already, but perhaps I lost them in the shuffle. Also a sign that my tests aren't sufficient, I suppose. Is there a reason you think that we need to introduce a new attribute on the message though? As long as the server fills in that attribute correctly in all cases, it would work w/o changing the message structure, no? That would hopefully make moreResults a bit less obtuse for client-use (not to mention the improvements already present WRT null results carrying some special logic in ClientScanner and ScannerCallable).

{quote}
bq. // Server didn't respond whether it has more results or not.

Is it possible here that we may inadvertently interpret the missing flag as meaning the region is exhausted? Probably fine because the limit logic is still in the ClientScanner while condition, just wondering.
{quote}

Yeah, that was a drawback to dealing with it like I did. I was trying to think through the case of what happens when talking to an older server (or one that just doesn't give the flag). I think the best we can do is fall back to the old logic and pray we don't run into the problem? I'd need to do more digging through older versions to be 100% certain though.

bq. Is there a reason you think that we need to introduce a new attribute on the message though
Good point. Perhaps the current moreResults flag could be modified to be relevant to the region. My concern would be that the flag has already been established as relevant to the scan as a whole rather than per region, so we would have to be sure that any changes to the way that the flag is set is understood globally and not misunderstood to mean that the whole scan should stop. Also, if it does become about the region rather than the scan as a whole, would there still be a mechanism for the server to indicate to the client that the whole scan is done (would it still be result == null)?

bq. I think the best we can do is fall back to the old logic and pray we don't run into the problem
Ya, I think you may be right

bq. would there still be a mechanism for the server to indicate to the client that the whole scan is done

Ah, that's a good point. So, as I understand it, we have the following cases which the RS will return to a client:

* {{Result}}s and more results on this region
* {{Result}}s and no more result on this region (this was the original "bug")
* No {{Result}}s and no more results on this region (but we need to check the other Regions)
* No {{Result}}s and the filter is done (this filter case)

It might be possible to differentiate between the 3 and 4th point if we knew when {{Result[]}} was null instead of just empty, specifically, for the filter case, if the client could get a ScanResponse in which the {{Result[]}} was actually null, we could still keep them separate. I'm not a big fan of that though, nor am I convinced it would even work. I don't quite understand how the non-protobuf serialization of {{Result}}s works presently.

That said, hasResults could be left solely for the filter-termination case, and I could introduce a "moreResultsInRegion" attribute after all. hasResults would still be a bit goofy WRT its name, but some comments can make it clear that it's purpose is explicitly for the filter case and nothing else.

Linking HBASE-13303 here as a reminder that commit should be reverted once this issue has a more comprehensive fix ready.
Edit: But to be clear, we'd need for that fix to include logic on the server side that works safely with older clients, or we can't revert it. (We'd risk skipping data with older clients if cells on the server carry tags.)

{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707001/HBASE-13262-v3.patch
  against master branch at commit 8cb4f89c013b254c6d3e9bee7b63137a391bcaba.
  ATTACHMENT ID: 12707001

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 16 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13396//console

This message is automatically generated.

bq. as I understand it, we have the following cases which the RS will return to a client 
Those cases make sense to me

bq. I'm not a big fan of that though, nor am I convinced it would even work
I agree. I think it may be best to introduce a new flag to communicate this information on a per region basis, but if anyone else has any ideas please chime in.

bq. But to be clear, we'd need for that fix to include logic on the server side that works safely with older clients, or we can't revert it
Ya, in the case that the client does not know to look for the flag, or in the case that the server does not send the flag, the fix from HBASE-13303 will be necessary. Because of this, I don't believe the fix currently in the works would allow us to revert HBASE-13303, correct me if I'm wrong [~elserj].

Without looking at the patch... I do not think this issue needs an extensive fix. We only need to do the following:
# add an "exhausted region" bit to the scan response
# on the server set that flag if we sent a response because we ran out of stuff to read from the region (i.e. we did *not* run into a size or row limit).
# on the client, if present, use this bit to decide how to continue in the loop. I.e. if set skip to the next region (else fall back to the existing logic, in all versions but trunk)

I wouldn't do more in this jira. Fix the basic stuff first, then file add-on jiras to improve.
It's also quite possible that I am missing something.


bq. I don't believe the fix currently in the works would allow us to revert HBASE-13303, correct me if I'm wrong Josh Elser.

Yeah, I think you're right. We'd need it in place as long as we support <1.1 clients to speak with >=1.1 servers.

That is essentially what the current patch does, Lars. The discussion today was that Jonathan pointed out that the "bit" I did use wouldn't work 100% -- I need to introduce a brand new one instead. I had thought I could get away with reusing the existing moreResults bit that was already on ScanResponse.

Let me know if I missed some concern you have.

I see the patch is big only because you factored out loadCache and added tests :)

Do you need ScanResultWithContext? Could also add a getter for the new bit (similar to the existing getHRegionInfo). That'd keep the change more manageable (but maybe not nicer).


I at least tried to add some tests :)

bq. Do you need ScanResultWithContext? Could also add a getter for the new bit (similar to the existing getHRegionInfo). That'd keep the change more manageable (but maybe not nicer).

We could, but I don't think that's quite as representative of the state we're encapsulating. It made sense to me to tie the results we got from the server to the extra information we got in the same call. We could make a getter on the Callable -- it's just separating state which ultimately came from the same RPC.

That said, I'm not extremely adamant one way or the other, just explaining the rationale behind why I did it that way. :)

My reasoning would be that result[] is the actual state we want, the other bits are used for internal management, and hence they feel separate to me (i.e. we might want to add more/different bits later, and not change the result classes around).

Not feeling strongly about this either.


I think this is important enough to get it into 0.98 sooner than later.

Concern noted. I think I have the changes made WRT the stuff Jonathan pointed out yesterday. Will put up a new patch shortly.

v4 which introduces a new bool on ScanResponse instead of incorrectly reusing moreResults. Didn't get rid of ScanResultWithContext (we can do that as a follow-on if desired). I tested branch-1 with the changes as well (will attach that patch if master test-patch comes back +1) -- will start testing against 0.98 locally.

[~elserj] This is looking good, I like the introduction of the new flag. Some quick review below:

When setting the flag inside RSRpcServices, I think we should be checking the size limit and the caching (i.e. row) limit. Currently the batch limit and size limit are being checked, but the batch limit should be replaced with a check against {{i >= rows}}. 

The reason behind replacing the batch limit check with a caching limit check is that the batch is a limit on the number of cells per Result. So reaching the batch limit doesn't necessarily mean we are done with the scan but rather that we have accumulated enough cells necessary for a Result to be formed. On the other hand, it would be worthwhile to add a check against the caching limit (i.e. {{i >= rows}}). I think your intention was probably to check the caching limit it's just that the terminology surrounding batch is a little misleading (even the javadoc on {{Scan#setBatch}} is misleading; "values" instead of "cells").

Also, just a note on backporting to 0.98 and branch-1.0. Because NextState is only in branch-1+, the corresponding check for size limit will simply be {{currentScanResultSize >= maxResultSize}} (looks like you have it covered in this patch actually) and the cache limit check will still be {{i>=rows}}.

:sigh: reran my standalone test and I'm seeing the original problem. Apparently my tests are missing something.

Oops. Caught it -- will add a new test and repost patch (plus Jonathan's concerns after I read them)

bq. Currently the batch limit and size limit are being checked, but the batch limit should be replaced with a check against i >= rows. 

Cool, I just noticed this was actually the reason things were broken. Checking only the sizeLimit and batchLimit was causing moreResultsInReason=false when we just had a normal moreValues. I think the positive case for moreResultsInRegion would then be:

{code}
null != state && (state.sizeLimitReached() || i >= rows || state.hasMoreValues())
{code}

bq. I think your intention was probably to check the caching limit it's just that the terminology surrounding batch is a little misleading (even the javadoc on Scan#setBatch is misleading; "values" instead of "cells").

You read my confusion like a book :). The differentiation between caching and batch is rather confusing. Thank you for clarifying.

bq. Also, just a note on backporting to 0.98 and branch-1.0. Because NextState is only in branch-1+, the corresponding check for size limit will simply be currentScanResultSize >= maxResultSize (looks like you have it covered in this patch actually) and the cache limit check will still be i>=rows.

Cool thank you for commenting on those, I've only started looking back there now (1.1 and 2.0 were enough to try to wrap my head around together). FWIW, [~lhofhansl]'s recommendation about removing ScanResultWithContext will likely make the 0.98 and 1.0 backport much easier (at least the cherry-pick was a bit painful locally). I am leaning towards that now as well.

bq. I think the positive case for moreResultsInRegion would then be...

That condition looks good to me. I would probably swap out {{state.sizeLimitReached()}} for {{currentScanResultSize >= maxResultSize}} just because it is the condition that is used to break out of the loop and so may make more sense to someone reading the code later (the two conditions would be equivalent, this is just a thought). I like the inclusion of state.hasMoreValues, smart :)

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707260/HBASE-13262-v4.patch
  against master branch at commit 5a58116bb531f0b256fdbfcabd3fad20663170fd.
  ATTACHMENT ID: 12707260

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 16 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +    KeyValue kv1 = new KeyValue("row1".getBytes(), "cf".getBytes(), "cq".getBytes(), 1, Type.Maximum);
+              new java.lang.String[] { "CellsPerResult", "ScannerId", "MoreResults", "Ttl", "Results", "Stale", "PartialFlagPerResult", "MoreResultsInRegion", });

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13423//console

This message is automatically generated.

Alright, v5 addresses Jonathan's latest feedback as well as Lars' feedback on ScanResultWithContext.

[~elserj] This patch looks great, just one nit below, may not be relevant:

Nit: The methods for checking moreResultsInRegion were placed inside the RegionServerCallable. I wonder if they would be better located inside ScannerCallable rather than overridden there. The flag is only really relevant when we're scanning, so seems weird that we have it inside RegionServerCallable. That being said, I do understand you were looking to match behavior of getHRegionInfo so probably why it is in RegionServerCallable

It seems as though the logic within ClientScanner will deal with this problem well. I think the only part left would be to expand that coverage into the ClientSmallScanner and ClientSmallReversed scanners and then backport it to 0.98 and branch-1.0 (I know above you mentioned your intention was to get ClientScanner covered first before worrying about the others).

Would be great to get another set of eyes on this as well if anyone else has some time

Great work [~elserj]!

Cool, thanks for taking a look.

{quote}
Nit: The methods for checking moreResultsInRegion were placed inside the RegionServerCallable. I wonder if they would be better located inside ScannerCallable rather than overridden there. The flag is only really relevant when we're scanning, so seems weird that we have it inside RegionServerCallable. That being said, I do understand you were looking to match behavior of getHRegionInfo so probably why it is in RegionServerCallable
{quote}

Yeah, that's right. I can lift it up to ScannerCallable if you think that makes more sense. Like you said, I was just trying to mimic what was already in place.

{quote}
It seems as though the logic within ClientScanner will deal with this problem well. I think the only part left would be to expand that coverage into the ClientSmallScanner and ClientSmallReversed scanners and then backport it to 0.98 and branch-1.0 (I know above you mentioned your intention was to get ClientScanner covered first before worrying about the others).
{quote}

I've been going under the assumption that it's much less likely to run into them (emphasis on "small"). For the sake of getting this out, I didn't want to make yet another change as we get close, but thought I would just update those as a follow-on (hopefully with some more tests). Really doesn't matter if we just do it at once.

bq. I can lift it up to ScannerCallable if you think that makes more sense. Like you said, I was just trying to mimic what was already in place.

I think they would be better located inside ScannerCallable because there are already a few methods defined there that are specific to scanning such as getCaching and getScan. Again, not a huge deal

bq. I've been going under the assumption that it's much less likely to run into them (emphasis on "small"). For the sake of getting this out, I didn't want to make yet another change as we get close

I see, makes sense to me; just wanted to make sure that we didn't forget about them :)

bq. I think they would be better located inside ScannerCallable because there are already a few methods defined there that are specific to scanning such as getCaching and getScan. Again, not a huge deal

Ok, I'll get these staged but wait for the current test-patch to fire before putting up a new patch. If v5 comes back clean, we can clean this up later.

bq. just wanted to make sure that we didn't forget about them

np, created HBASE-13335 to make sure :)

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707338/HBASE-13262-v5.patch
  against master branch at commit 134b95579c0d02d54f477e437f03ca1a825256f4.
  ATTACHMENT ID: 12707338

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 12 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13430//console

This message is automatically generated.

{{TestDistributedLogSplitting.testLogReplayTwoSequentialRSDown}} is what failed. Rerunning it locally, the test just seems flaky. 

v6 since I had the changes and didn't want PreCommit to be lonely overnight.

Moves the server context information out of RegionServerCallable into ScannerCallable.

{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707430/HBASE-13262-v6.patch
  against master branch at commit 948a6a63e8c1a347ff18903cba8fe6ffb6190d37.
  ATTACHMENT ID: 12707430

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 12 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any                  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13437//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13437//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13437//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13437//console

This message is automatically generated.

+1, lgtm

Anyone else have some time to review this nice patch?

Looks good to me.
{{+      if (null != values && values.length > 0 && callable.getServerHasMoreResultsContext()) {}}

I'd probably name this just {{callable.hasMoreResultsContext}}. So that will work right with an old client, right? It'll just not read the context and fall back to the old logic. That's what the code does, just making sure it's something we tested.


bq. I'd probably name this just callable.hasMoreResultsContext.

Happy to change that. Will do (below) testing first.

bq. So that will work right with an old client, right? It'll just not read the context and fall back to the old logic. That's what the code does, just making sure it's something we tested.

That's the intent, but, admittedly, I haven't explicitly tested this. I'll give it a quick go.

Updated the method name per Lars' request. Also removed a comment I had added to a test method when I was debugging things before.

Did some quick testing of my original code depending on 1.0.0 with the changes in v7 on top of 2.0.0-SNAPSHOT. The client could still read/write all of the data. It appears that table state querying has been broken (which, as I understand, is acceptable), but just creating an empty table for my test allowed my client to write then read successfully.

That was just a nit :) (thanks for changing it)
What's "table state querying"? Meaning to use a scanner to see if a table exists or is enabled/disabled? How would the patch affect that?

+1 on patch in any case.


For the record, 1.0.0 client with a patched 1.1.0-SNAPSHOT worked fine

Sorry, I was unclear. {{Admin#isTableEnabled}} and {{Admin#deleteTable}} don't appear to work with a 1.0.0 client and a 2.0.0-SNAP server, but such a breakage is allowed as I understand it. This happens with and without the patches here. I was just noting that I had to slightly change my program for the 1.0.0 and 2.0.0-SNAP test I ran (whereas testing 1.0.0 and 1.1.0-SNAP required no changes to the original program).

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707604/HBASE-13262-v7.patch
  against master branch at commit 18256fc9544866fc7182dfc5dd9d73b8cef02d66.
  ATTACHMENT ID: 12707604

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 8 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13450//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13450//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13450//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13450//console

This message is automatically generated.

Test failure looks unrelated to me (Jenkins goofed up, perhaps?).

I agree, that test failure looks unrelated to me; likely just flaky. Let's reattach the patch to trigger another QA run to see if it happens again

Reattaching [~elserj]'s patch to trigger another QA run

sorry for the drive-by question, what's the current set of target branches for this fix? jira says master, branch-1, and 0.98. There's a change that is disallowed in branch-1.0, I presume?

[~elserj] please correct me if I'm wrong, but I believe this change will go into 0.98+ (so branch-1.0 should be included in the target branches). In 0.98 and branch-1.0, The flag can be set based on the same checks from branch-1+ except that a check against the boolean {{moreRows}} would replace the check {{state.hasMoreValues}} since NextState is branch-1+

I was actually thinking that branch-1.0 would be omitted, but, on second look, I'm not seeing anything that would actually violate the patch update rules. I agree with everything Jonathan said WRT necessary patch alterations on 0.98 and branch-1.0.

I'm happy to apply the patch for those branches as well, but I'm kind of waiting on someone to tell/ask this of me rather than just do it on my own..

{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707651/HBASE-13262-v7.patch
  against master branch at commit 18256fc9544866fc7182dfc5dd9d73b8cef02d66.
  ATTACHMENT ID: 12707651

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 8 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13455//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13455//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13455//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13455//console

This message is automatically generated.

+1 on patch v7.

perfectly reasonable. I was only checking so I would know what to look for when reviewing.

I updated the targets to include branch-1.0.

Any last thoughts or updates?

Let's get this in so we can start releases on Monday with this included, e.g. 0.98.12. I will probably commit this in a couple of hours to let Jenkins chew on the results over the weekend. 



Unfortunately this isn't ready yet. This issue has fix versions for master, branch-1, branch-1.0, and 0.98. I was able to port back the v7 patch for master to branch-1 pretty easily but not so to branch-1.0. I stopped there and didn't even attempt 0.98. We can commit this to branch-1 and master only or wait for all necessary patches first. I'd rather commit everywhere at once if fixes for all targeted branches are forthcoming. 

The last time I had a patch targeted for multiple branches, I was told the committer would take care of the backporting. Your comment tells me otherwise. I'll get you some patches for branch-1.0 and 0.98.

There's no rule. Personally, I will try to backport if no branch patches are provided if its clear how to resolve the conflicts and I'm confident of what I am doing. In this case, git is giving me large conflict sections. In order to port, I'd have to look at the master patch and replay it onto branch by hand. Better let the person(s) deeply engaged in the original work do that. 

0.98 patch. Had to move some stuff from ScannerCallable up to RegionServerCallable because of SmallClientScanner using RegionServerCallable directly.

Great, thanks. Working on commits now. (Checking out locally first.)

So with this in place. Can we redo the server side size scanner size limit?
I assume this can still go wrong.
Maybe I'll make an 0.98 patch that drives the size from the client.

bq. Can we redo the server side size scanner size limit? I assume this can still go wrong.

You mean have the server adhere to the limit set in the {{Scan}}? Best as I can tell, the server does look at the size set on the scan. Perhaps I misunderstood your question though.

Not at problem, glad we got this in.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12708004/HBASE-13262-branch-1.0-v7.patch
  against branch-1.0 branch at commit 0967c6af29ba0546a167583535659a0c13de45b3.
  ATTACHMENT ID: 12708004

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 8 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 3759 checkstyle errors (more than the master's current 3758 errors).

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

     {color:red}-1 core zombie tests{color}.  There are 2 zombie test(s): 	at org.apache.camel.component.quartz2.SpringQuartzConsumerTwoAppsClusteredRecoveryTest.testQuartzPersistentStoreClusteredApp(SpringQuartzConsumerTwoAppsClusteredRecoveryTest.java:64)
	at org.apache.camel.component.jpa.JpaWithNamedQueryTest.testProducerInsertsIntoDatabaseThenConsumerFiresMessageExchange(JpaWithNamedQueryTest.java:112)
	at org.apache.camel.component.jpa.JpaWithNamedQueryTest.testProducerInsertsIntoDatabaseThenConsumerFiresMessageExchange(JpaWithNamedQueryTest.java:112)

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13477//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13477//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13477//artifact/patchprocess/checkstyle-aggregate.html

                Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13477//console

This message is automatically generated.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12708007/HBASE-13262-0.98-v7.patch
  against 0.98 branch at commit 0967c6af29ba0546a167583535659a0c13de45b3.
  ATTACHMENT ID: 12708007

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 24 warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/13479//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13479//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/13479//artifact/patchprocess/checkstyle-aggregate.html

  Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/13479//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/13479//console

This message is automatically generated.

Sorry wasn't clear. The max result size setting is available on the client only for 1.1 and up, right?
I was wondering whether we can just turn on that setting on the server again for 0.98 and 1.0. But it would still be an issue with an old client that does not have this fix.
So my next thought: Add the max size to 0.98 and 1.0 and remove the server side limit there. Then default the client size limit to 2mb.

Pushed to 0.98 and up.

The new client tests pass on all branches, other client tests all pass (e.g. TestFromClientSide*)

Changes in branch-1.0 and 0.98 are to private interfaces only and, even so, JavaACC says the private interface changes are binary compatible with previously compiled code.

Thanks, Andrew! Your legwork is much appreciated.

FAILURE: Integrated in HBase-1.0 #836 (See [https://builds.apache.org/job/HBase-1.0/836/])
HBASE-13262 Observe ScanResponse.moreResults in ClientScanner. (apurtell: rev 20189c258cbcf109de10f76f06c059681e438615)
* hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallReversedScanner.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallableWithReplicas.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallScanner.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSizeFailures.java
* hbase-protocol/src/main/protobuf/Client.proto
* hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientScanner.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java


FAILURE: Integrated in HBase-1.1 #336 (See [https://builds.apache.org/job/HBase-1.1/336/])
HBASE-13262 Observe ScanResponse.moreResults in ClientScanner. (apurtell: rev b0116398ffa6aec57d42739df40e22dc4edaaa0f)
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSizeFailures.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallScanner.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
* hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientScanner.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallableWithReplicas.java
* hbase-protocol/src/main/protobuf/Client.proto
* hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallReversedScanner.java


SUCCESS: Integrated in HBase-TRUNK #6319 (See [https://builds.apache.org/job/HBase-TRUNK/6319/])
HBASE-13262 Observe ScanResponse.moreResults in ClientScanner. (apurtell: rev ced0e324a190e75d8a93b8b09676781853819361)
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSizeFailures.java
* hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallableWithReplicas.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallReversedScanner.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallScanner.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java
* hbase-protocol/src/main/protobuf/Client.proto
* hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientScanner.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java


SUCCESS: Integrated in HBase-0.98 #924 (See [https://builds.apache.org/job/HBase-0.98/924/])
HBASE-13262 Observe ScanResponse.moreResults in ClientScanner. (apurtell: rev 1abf3aa0fd355d69cb3eed325b876a62065807b3)
* hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallReversedScanner.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallScanner.java
* hbase-protocol/src/main/protobuf/Client.proto
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionServerCallable.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSizeFailures.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java


FAILURE: Integrated in HBase-0.98-on-Hadoop-1.1 #878 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/878/])
HBASE-13262 Observe ScanResponse.moreResults in ClientScanner. (apurtell: rev 1abf3aa0fd355d69cb3eed325b876a62065807b3)
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionServerCallable.java
* hbase-protocol/src/main/protobuf/Client.proto
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallScanner.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallReversedScanner.java
* hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSizeFailures.java


Please ignore my comment above. 0.98 already has Scan.{get|set\}MaxResultSize.
What I was trying to say is that we should default that (i.e. on the client) to 2mb.


Are we confident that 0.98 is not affected by this?  Running some test jobs on a migrated cluster from 0.94 to a build of 0.98.11++ shows behavior that looks just like this.  Scanner caching at 30, max result size is the default of 2MB.  Most tasks pick up 10-15 rows, then terminate without scanning the rest of the region.  Setting scanner caching to 1 row fixes it.

bq. Are we confident that 0.98 is not affected by this

A patch did land in 0.98 for this. Are you seeing these symptoms with those changes as well (not sure what all your 0.98.11++ includes)?

You said 0.98.11 with patches [~davelatham]? Can you try with commit cddcc28 or later on branch 0.98? 

The build is 0.98.11 plus some patches that should be unrelated (HBASE-12890, HBASE-13276, HBASE-12814, and a couple of fixes for HBASE-12814).  It does not have the patch in this issue (yet).  

[~apurtell], you think it may be HBASE-13374?  I haven't picked up "small" scanners yet - but we're not explicitly setting anything as small scans.

We just tried a workaround of setting hbase.client.scanner.max.result.size=100000000000 and that did not seem to fix the issue - so our issue may indeed be something else.

bq. you think it may be HBASE-13374
No, I just wanted to cover the bases will all scanner related changes that will be in the next 0.98.12 release candidate.

[~davelatham] so with the workaround of setting hbase.client.scanner.max.result.size=100000000000, you receive a batch of 30 rows back from the server and then the scan terminates without scanning the rest of the region, is that right? Also, do you have HFileV3 turned on (i.e. what is the configured value for hfile.format.version)? When tests were run in 0.98 (details above), this issue wasn't readily producible (0.98 uses HFileV2 by default, and this issue was discovered to be a result of using HFileV3). If you do receive the full 30 rows back from the server, and you are *not* using HFileV3, I would be inclined to agree with you and say that this is in fact a different issue. Would you be able to provide any more details about the particular scan configuration?



[~davelatham], do client and server have the exact same config? Are client and server the same (minor) version of HBase (both 0.98.11)?


[~lhofhansl], thanks for asking that question!  I thought they were using the same jars, but turns out the client was using 0.98.10.1 while the server was on 0.98.11++.  Digging through differences, I of course found HBASE-12976.  I didn't realize the server used hbase.client.scanner.max.result.size directly as opposed to getting it from the client, so that seems to explain it - client had Long.MAX_VALUE and server had 2MB.

Sorry all for the distraction on this issue.

No worries, glad you asked :)

NP at all. We all didn't realize he fragility of this (at least I didn't). This just shows what a bad call HBASE-12976 was. Looked like such an innocuous change.
Fixing it the right way in HBASE-13362 (which drives it from the client, just like scanner caching).


Oh and to be clear, this scenario *only* happens with a 0.98.11 server and in earlier 0.98 client. With 0.98.12 this is fixed (I removed the limit at the server side again).


Should we put a warning somewhere (mailing list? book?) about this?  Something like:

IF (client OR server is <= 0.98.11/1.0.0) AND server has a smaller value for hbase.client.scanner.max.result.size than client does, THEN scan requests that reach the server's hbase.client.scanner.max.result.size are likely to miss data.  In particular, 0.98.11 defaults hbase.client.scanner.max.result.size to 2MB but other versions default to larger values, so be very careful using 0.98.11 servers with any other client version.

Yeah that might be good. Best on Dev and User lists, I think. I can take a stab at it.

How about we add a note in the ref guide for upgrades and for
troubleshooting?



bq. How about we add a note in the ref guide for upgrades and for troubleshooting

+1, that's probably the place more people will end up

filed HBASE-13446 to do the docs since we're trying to cut new RCs for versions that include this. 

Not sure we want this in the docs. It only affects 0.98.11 (not .10 and not .12) and only with clients before .11. A note on the mailing lists seems to be more appropriate. (Not feeling strongly, though).

If it only impacts 0.98.11 talking to earlier clients, how about amending
the release notes for 0.98.11?

(I thought it also impacted 1.0.0?)



Hmm... Yes, 1.0.0 as well. But not 1.0.1 (when it's out).

The book has a section on upgrading. We can put it there: https://hbase.apache.org/book.html#hbase.versioning. 

Let's continue the doc discussion at HBASE-13446

Closing this issue after 1.0.1 release.

FAILURE: Integrated in HBase-TRUNK #6708 (See [https://builds.apache.org/job/HBase-TRUNK/6708/])
HBASE-13446 Add docs warning about missing data for downstream on versions prior to HBASE-13262 (mstanleyjones: rev 0daae433423da12343d4a64fdc987ffa64d4785f)
* src/main/asciidoc/_chapters/upgrading.adoc
* src/main/asciidoc/_chapters/troubleshooting.adoc


