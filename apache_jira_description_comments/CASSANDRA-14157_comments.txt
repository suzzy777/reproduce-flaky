Hi [~samt], do you plan to work on this ticket?

I saw this failure again today:

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/349/workflows/04bccc52-4e3e-41e2-9c04-93501ea4ce77/jobs/2116

Hey [~e.dimitrova], I'm afraid I don't have capacity to pick it up right now, so feel free to take it if you like.

Thanks for confirming [~samt] . I left it unassigned In case someone is interested, otherwise, probably I will try to look into it next week.Â 

Running on a resource-constrained VM reproduces almost 100% for me.

I believe this is just a problem with the test. It looks like the read repairs are valid, occasionally being triggered by queries "under the hood" in cqlsh.

The test rightly expects the [insert and subsequent select|https://github.com/apache/cassandra-dtest/blob/b117565b8f0096a3ed2af05fdec6e014a05788a1/cql_tracing_test.py#L80-L98] at CL.ALL to produce no read repairs. However, running with trace in cqlsh [causes additional queries getting the trace data|https://github.com/apache/cassandra/blob/83e1e9e45193322f18f57aa7cc4ad31d9d5a152d/bin/cqlsh.py#L1044]. They will be run at the session level of CL.ALL, but the data is written at CL.ONE. Therefore these queries can cause RR.

Rather than try to work around this in-place, my suggested change relocates the test out of cql_tracing_tests and uses the driver directly (cqlsh is not the thing we're testing here).

[patch|https://github.com/apache/cassandra-dtest/compare/master...aholmberg:CASSANDRA-14157?expand=1]
[ci|https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-14157] (note that another read_repair_test is failing on trunk for a different reason CASSANDRA-16148)

The patch looks good to me.

Committed into master at 4b164488735fb13e31b21d7786acb7b477473e2e

