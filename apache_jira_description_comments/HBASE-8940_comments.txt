[~ted_yu] You going to fix this?

Yes.

Looks like the root cause of this is the same as that of HBASE-8941.  I think it is better to fix the root cause.


Closed HBASE-8940 as a duplicate of this one.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12592068/8940-v1.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/6315//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6315//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6315//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6315//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6315//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6315//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6315//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6315//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6315//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/6315//console

This message is automatically generated.

Another possible solution is to give some headroom in getRegionByEncodedName():
{code}
      Boolean isOpening = this.regionsInTransitionInRS.get(Bytes.toBytes(encodedRegionName));
      if (isOpening != null && isOpening.booleanValue()) {
        throw new RegionOpeningException("Region is being opened: " + encodedRegionName);
{code}
Instead of throwing RegionOpeningException immediately, we can wait for a short while.

I think this a test issue.  What you think [~jxiang]?  We are asking to merge a region that the regionserver is NotServering.   Merge should fail.

I think this issue will be fixed by the Enis's "HBASE-8934 Fix bunch of flaky tests" -- the region will be fully opened before the test starts.

Chatting w/ Jimmy friday afternoon, there is a little hole where we have published the region as available on regionserver X -- because it has been set in .META. and master has been informed the region has been opened via a transitioning of znode -- but the region may not be added to the regionserver online regions set just yet (it is done after we move region from OPENING to OPENED up in zk).  If a client comes in in the meantime, which is possible when all threads are running in the one jvm as we have in unit tests, client will get NotServingRegionException (or RegionOpeningException which is a subclass in this case)... which is the 'truth' in that we have not yet put up the region online.  The client will retry usually but in this case, in merge, there is no retry since it is the regionserver itself making a call on itself; there is no client.  Adding retries inside the regionserver seems wrong.  Regionserver knows its own state.

Talking w/ Jimmy, changing the order in which we online the region in the regionserver so we do it before we 'publish' via znode could open us up to races where region could be open in more than one place so it is safer to leave things as they are regards region onlining and instead just fix tests, or better, HBaseTestingUtility; i.e. not start the merge until for sure the regions are online.

Let me put up a patch.


I took a closer look at the logs.  Something else is going on here.  We load the table, scan the table to ensure row count is as expected (which can only succeed after all regions are online), we then print out the regions we think make up the table, and then immediately after, we choose to unassign a region.  The merge fails because this unexpected unassign followed by an open elsewhere has not completed by the time the merge goes to run.  I cannot see why we do the unassign.  I see that the balancer is set to off for these tests.  Any clues [~jxiang]?  Otherwise, I'm going to add in a 'why' string for unassigns, etc.

Here is us listing all regions after successful verification of row count followed by the immediate unassign:

{code}
...
2013-07-11 09:33:43,744 INFO  [pool-1-thread-1] regionserver.TestRegionMergeTransactionOnCluster(262): Regions after load: {{ENCODED => efcb10dcfa250e31bfd50dc6c7049f32, NAME => 'testWholesomeMerge,,1373535210124.efcb10dcfa250e31bfd50dc6c7049f32.', STARTKEY => '', ENDKEY => 'testRow0020'},ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884},{{ENCODED => 3ffefd878a234031675de6b2c70b2ead, NAME => 'testWholesomeMerge,testRow0020,1373535210125.3ffefd878a234031675de6b2c70b2ead.', STARTKEY => 'testRow0020', ENDKEY => 'testRow0040'},ip-10-174-118-204.us-west-1.compute.internal,60498,1373535184820},{{ENCODED => 96a2b66754838d21e88d31cbeb21ed41, NAME => 'testWholesomeMerge,testRow0040,1373535210125.96a2b66754838d21e88d31cbeb21ed41.', STARTKEY => 'testRow0040', ENDKEY => 'testRow0060'},ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884},{{ENCODED => 71eb1ffffd93a0d6a9f5529a45922519, NAME => 'testWholesomeMerge,testRow0060,1373535210125.71eb1ffffd93a0d6a9f5529a45922519.', STARTKEY => 'testRow0060', ENDKEY => 'testRow0080'},ip-10-174-118-204.us-west-1.compute.internal,60498,1373535184820},{{ENCODED => f0cc6807bbeb1b4731f81397710ead3d, NAME => 'testWholesomeMerge,testRow0080,1373535210125.f0cc6807bbeb1b4731f81397710ead3d.', STARTKEY => 'testRow0080', ENDKEY => 'testRow0100'},ip-10-174-118-204.us-west-1.compute.internal,58569,1373535184726},{{ENCODED => bc7ff4e217795751fd2a9fc671ad33a4, NAME => 'testWholesomeMerge,testRow0100,1373535210125.bc7ff4e217795751fd2a9fc671ad33a4.', STARTKEY => 'testRow0100', ENDKEY => 'testRow0120'},ip-10-174-118-204.us-west-1.compute.internal,58569,1373535184726},{{ENCODED => 7bd39de7c7cc8e46007e50d2e60e4e3d, NAME => 'testWholesomeMerge,testRow0120,1373535210125.7bd39de7c7cc8e46007e50d2e60e4e3d.', STARTKEY => 'testRow0120', ENDKEY => 'testRow0140'},ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884},{{ENCODED => f6ca1923a2636032360186166784a35c, NAME => 'testWholesomeMerge,testRow0140,1373535210125.f6ca1923a2636032360186166784a35c.', STARTKEY => 'testRow0140', ENDKEY => 'testRow0160'},ip-10-174-118-204.us-west-1.compute.internal,60498,1373535184820},{{ENCODED => cc4e5e073976cf056ea7b629f3eaeee6, NAME => 'testWholesomeMerge,testRow0160,1373535210125.cc4e5e073976cf056ea7b629f3eaeee6.', STARTKEY => 'testRow0160', ENDKEY => 'testRow0180'},ip-10-174-118-204.us-west-1.compute.internal,58569,1373535184726},{{ENCODED => 95559e718b7b1dc87635f3bd474b9d36, NAME => 'testWholesomeMerge,testRow0180,1373535210125.95559e718b7b1dc87635f3bd474b9d36.', STARTKEY => 'testRow0180', ENDKEY => ''},ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884}
2013-07-11 09:33:43,751 DEBUG [pool-1-thread-1] client.ClientScanner(195): Finished region={ENCODED => 1028785192, NAME => '.META.,,1', STARTKEY => '', ENDKEY => ''}
2013-07-11 09:33:43,767 DEBUG [MASTER_TABLE_OPERATIONS-ip-10-174-118-204:39405-0] master.AssignmentManager(2205): Starting unassign of testWholesomeMerge,testRow0020,1373535210125.3ffefd878a234031675de6b2c70b2ead. (offlining)

...
{code}



bq.I cannot see why we do the unassign. 
Before executing region merge, we will move merging regions to the same regionserver. Thus do a unassign there.

Here taking a retry should be accepted since HBASE-8941 has been fixed through improving the test case.

Making a patch with retry solution

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12592271/8940-trunk-v2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/6334//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6334//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6334//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6334//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6334//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6334//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6334//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6334//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/6334//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/6334//console

This message is automatically generated.

+1 on v2. Although the issue may be fixed by Stack's other patches, this patch is nice to have too.

Here is what I committed.  Only real difference is my adding logging of move of regions so they are adjacent; not having anything in the log confounded me.

Thanks for showing up w/ the why [~zjushch]

Just going to resolve this as committed to 0.95 and trunk.

It failed here: https://builds.apache.org/job/hbase-0.95/369/testReport/org.apache.hadoop.hbase.regionserver/TestRegionMergeTransactionOnCluster/testWholesomeMerge/

Want to take a look [~zjushch]?

The above failed test is caused by
{noformat}
2013-07-27 20:08:48,345 INFO  [MASTER_TABLE_OPERATIONS-quirinus:59057-0] handler.DispatchMergingRegionHandler(170): Cancel merging regions testWholesomeMerge,,1374955684854.8b452e80a9e15d54bd265c344f4ad953., testWholesomeMerge,testRow002
0,1374955684854.4a31455f2b0256853c41c52ba65bdc10., because can't move them together after 30003ms
{noformat}

Timeout of moving regions together on same RS is caused by closing region "4a31455f2b0256853c41c52ba65bdc10" take more time than 30s,

{noformat}
2013-07-27 20:08:18,753 DEBUG [RS_CLOSE_REGION-quirinus:57626-1] handler.CloseRegionHandler(125): Processing close of 
testWholesomeMerge,testRow0020,1374955684854.4a31455f2b0256853c41c52ba65bdc10.

2013-07-27 20:08:18,755 DEBUG [RS_CLOSE_REGION-quirinus:57626-1] regionserver.HRegion(1493): Started memstore flush for testWholesomeMerge,testRow0020,1374955684854.4a31455f2b0256853c41c52ba65bdc10., current region memstore size 3.4k

2013-07-27 20:09:03,914 INFO  [RS_CLOSE_REGION-quirinus:57626-1] regionserver.HRegion(1637): Finished memstore flush of ~3.4k/3520, currentsize=0.0/0 for region testWholesomeMerge,testRow0020,1374955684854.4a31455f2b0256853c41c52ba65bdc1
0. in 45159ms, sequenceid=186, compaction requested=false

2013-07-27 20:09:03,956 DEBUG [RS_CLOSE_REGION-quirinus:57626-1] handler.CloseRegionHandler(177): Closed region testWholesomeMerge,testRow0020,1374955684854.4a31455f2b0256853c41c52ba65bdc10.
{noformat}

From the above logs, closing region took 45s, it seems flushing is too slow, but can't get the reason from the current logs, maybe GC or high IO load at that time

[~zjushch] That the build box is loaded should be a given.  Would you suggest upping the timeouts on these tests?  I can do it no problem.  Just say (thanks for digging in).

Upping the timeouts from 30s to 120s in addendum patch.

I will commit it if no object

+1

Thanks.

Committed addendum to trunk and 0.95

