I had no idea S3 stored a content-type metadata about files. Can you also post a screenshot of the metadata for the boto3-created file?

[~apitrou] Uploaded.

Yes, it was also surprising for me. My current assumption is that the c++ aws sdk that pyarrow uses has different content-type inference logic than boto3?

I don't think the C++ AWS SDK uses any inference. "application/xml" seems to be a default value.

I am struggling to find this default value anywhere (in code as well as documentation of AWS).

Is pyarrow always using multipartUpload inside S3Filesystem or does it also use PutObject for small files?

If you grep for AMZN_XML_CONTENT_TYPE in the C++ AWS SDK you'll see that it's use as a default content-type value for S3 requests. I presume that's the reason for what you're observing, though it's only a guess.

That said, "binary/octet-stream" isn't correct for a text file either...

And to answer your other question, we always use multipart upload.

You are right, found it. Thanks for the hint with AMZN_XML_CONTENT_TYPE.

Now I am curious what it would set as content-type for parquet files and verified that it's also application/xml:

 
{code:java}
import pyarrow as pa
import pyarrow.parquet as pq
table = pa.table({'a': [1, 2], 'b': [.1, .2]})
pq.write_table(table, f"s3://{BUCKET}/test.parquet")

response = s3.get_object(Bucket=BUCKET, Key='test.parquet')
print(response['ContentType']) # application/xml
print(response['Body'].read())
{code}

[~npr] Do you have opinions on this?

If the aws-sdk-cpp has an API for setting content type, I guess we can expose it. Though as you say, it's not clear that boto is doing the right thing either, for what that's worth.

I was mainly surprised about this, I don't think that there is an immediate need to expose the content-type, at least for me. I can also switch to plain boto3 and create my own local / s3 filesystem abstraction.

Would it make sense to upstream this question to AWS and ask why the different sdk's behave differently in terms of content type inference? 

Just looked at pyfilesystem. They solved this by introducing extra_args when creating the filesystem
{code:java}
import fs, fs.mirror
s3fs = S3FS('example', upload_args={"CacheControl": "max-age=2592000", "ACL": "public-read"})
fs.mirror.mirror('/path/to/mirror', s3fs)
{code}
https://github.com/PyFilesystem/s3fs#extraargs

 

 

Issue resolved by pull request 10295
[https://github.com/apache/arrow/pull/10295]

This issue has been migrated to [issue #27070|https://github.com/apache/arrow/issues/27070] on GitHub. Please see the [migration documentation|https://github.com/apache/arrow/issues/14542] for further details.

