Hi [~daryn] thanks for reporting this.

I am aware that the fix HDFS-11160 could potential have the problem that you described. But choosing between correctness and performance, I would choose the former. That said,  HDFS-11187 is a possible optimization, perhaps you can consider it? I did not push HDFS-11187 forward, because HDFS-11160 has been running at our end internally in the past few months,  has been distributed in a number of CDH releases, and we've not yet received reports regarding performance anomaly.

For more context, there is a Spark Streaming use case that would see very frequent HDFS file corruption without HDFS-11160 (several corrupt files in a day). So I would hope you are not considering reverting HDFS-11160.

Normal serving has been greatly impaired.  Appending to a block while it's scanned is exceeding rare compared to the normal block sending rate, yet the fix impacted all serving.  There's a better way accomplished via:
* Entirely remove (revert) fetching of checksums for finalized blocks in the BlockSender ctor.  Reduces lock hold time by eliminating i/o in the dataset lock.
* If a checksum exception occurs during the scan, and the genstamp changed, mark the block as suspect for rescan.  This is the edge case.
* Recent suspect blocks considers genstamps.  Suspect blocks with a newer genstamp than last recorded are not skipped.
* Recent suspects expire 10 min after being added to the cache.  Prior behavior was 10 mins after last access - which could lead to indefinite postponement.

No test changes needed.  {{TestBlockScanner#testAppendWhileScanning}} proves this approach continues to work.

Only difference in trunk/branch-2 is context and a few log lines in code copied into a getStoredBlock method.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 19s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 47s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 39s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk has 10 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 38s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 82m 31s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 25s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}108m 39s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.TestNameNodeMetadataConsistency |
|   | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |
|   | hadoop.hdfs.TestDFSStripedInputStreamWithRandomECPolicy |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
| Timed out junit tests | org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:14b5c93 |
| JIRA Issue | HDFS-12136 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12877142/HDFS-12136.trunk.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux f92003b5745c 3.13.0-116-generic #163-Ubuntu SMP Fri Mar 31 14:13:22 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 945c095 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/20263/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/20263/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/20263/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/20263/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



The failed tests either aren't failing for me, or are failing with or w/o this patch.  The volume failure tests are very flaky.  Apparently races let volume references leak so the cluster can't shutdown and timeouts occur.  Will kick the build again to cross-compare test failures.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 14s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 39s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk has 10 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 40s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 39s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 65m 46s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 20s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 93m 15s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure010 |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:14b5c93 |
| JIRA Issue | HDFS-12136 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12877142/HDFS-12136.trunk.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 2376c0df5944 3.13.0-119-generic #166-Ubuntu SMP Wed May 3 12:18:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 75c0220 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/20277/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/20277/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/20277/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/20277/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



[~weichiu], we started seeing significant performance regression after increased I/O activities. Jstacking has revealed that DataXceiver threads are all waiting for the dataset impl lock. When the I/O load is reasonable, this might not be visible.

{noformat}
"org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer@61a9d939" #351184
 daemon prio=5 os_prio=0 tid=0x00007f94ddf0a000 nid=0xafef waiting on condition [0x00007f94c1d4f000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000d55efd28> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
        at org.apache.hadoop.hdfs.InstrumentedLock.lock(InstrumentedLock.java:102)
        at org.apache.hadoop.util.AutoCloseableLock.acquire(AutoCloseableLock.java:67)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.acquireDatasetLock(FsDatasetImpl.java:3274)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:252)
        at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2348)
        at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
        - None
{noformat}
{noformat}
"DataXceiver for client DFSClient_xxx [Sending block xxx]" #351183
 daemon prio=5 os_prio=0 tid=0x000000000409b000 nid=0xafee waiting on condition [0x00007f94c9f49000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000d55efd28> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
        at org.apache.hadoop.hdfs.InstrumentedLock.lock(InstrumentedLock.java:102)
        at org.apache.hadoop.util.AutoCloseableLock.acquire(AutoCloseableLock.java:67)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.acquireDatasetLock(FsDatasetImpl.java:3274)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:252)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:580)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:145)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:100)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:288)
        at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
        - None
{noformat}

Hi [~daryn] sorry come to this late. Thanks for the patch and thanks [~kihwal] for the stack trace. No doubt this is a serious performance regression.

I want to emphasis that the initial (false positive) corruption is due to a race condition between concurrent readers and writers. While HDFS-11160 made it seems like it only happen to VolumeScanner, the thing is that this can happen to any readers. When reader thinks it gets a checksum corruption, it reports to NN, which removes the block replica. This happens very frequent for a Spark Streaming application, and data is being read in real-time while data is being ingested.

If you want to go this route, please add the same check at dfsclient reader side. For example, when it receives a checksum error, read again to weed out false false positive caused by race condition.

[~weichiu], do you have a unit test that can reliably expose the issue?  We run lots of Spark apps and didn't see the problem in 2.7, or maybe it happens so infrequently that nobody notices.

The basic problem is that doing IO in the lock is completely unacceptable.  Basic things like a failing drive, a heavily utilized drive, high replication from decommissioning or a lost node, will jam up the DN.  Just one slow drive will ruin the DN.

Here's how that becomes catastrophic:  Under sufficiently high load, DNs congest with BlockSenders serially computing checksums.  Start decomming, heartbeat thread receives replication commands.  Instantiating BlockSenders contend with the backlogged xceiver threads.  Heartbeats are now delayed.  Meanwhile, clients timeout and pipelines collapse after 45s.  Clients reconstruct pipeline but the prior xceivers may are blocked creating the BlockSender – not knowing the client disconnected.  Blocked threads go up, eventually hitting the xceiver thread limit (4k for us).  Surprisingly the heartbeat thread can eventually receive such little share of the lock that 10mins elapse and the node goes "dead".  Now the replication monitor issues even more replications, causing even more congestion in other nodes.  In a real incident, we had ~6 random nodes flapping for hours causing sporadic missing blocks and jobs were taking forever.

Kihwal and I think we might know a few approaches to further fix the original issue.  However at this time I'd argue that undoing the cluster damaging performance is more important than completely fixing the reader/append race.



HDFS-6804 is another facet of the same race condition.

[~brahmareddy] made a unit test that reliably reproduces the bug if HDFS-11160 is reverted.

I think the performance impact is less severe after HDFS-12157, so we could target 2.8.3 for the fix. 

bq.so we could target 2.8.3 for the fix.
updated {{2.8.3}} as target version.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  5s{color} | {color:red} HDFS-12136 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HDFS-12136 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12877142/HDFS-12136.trunk.patch |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/22971/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



Hi [~daryn], I saw this issue has been quiet for a while. Can we move to next release given 2.8.4 is on releasing track?

I believe this issue is no more after HDFS-11187 (fixed in 2.8.4)

Thanks [~weichiu] for comments. I agree with you that previous lock issue get resolved in HDFS-11187. [~daryn], I will go ahead to resolve this as duplicated of HDFS-11187 if you also agree.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  6s{color} | {color:red} HDFS-12136 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HDFS-12136 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12877142/HDFS-12136.trunk.patch |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/24104/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.



I'll study other Jira to see if it's a dup.

Did a cursory scan of HDFS-11187 and I'm not sure if it supersedes this one.  We're still running with my patch internally.  In addition to eliminating i/o in the lock, this patch tweaked the volume scanner to intelligently handle checksum errors due to genstamp updates.  I didn't look that hard into the other patch.  Is that scenario an impossible condition now?  

looks like HDFS-11187 may not cover all fix here. [~weichiu], do you have further comments here?
Move it to 2.8.5 as we need more discussion and 2.8.4 is in RC stage.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  5s{color} | {color:red} HDFS-12136 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HDFS-12136 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12877142/HDFS-12136.trunk.patch |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/25007/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



Resolved by HDFS-11187 

