Merged to trunk. Thanks for reviewing it, [~iwasakims].

[~tasanuma], [~iwasakims] Thank you for creating that patch. I agree that it is important that the environment does not skip any tests.

However, IMHO that change was *never safe* to be merged into trunk without evaluating its side effect on all the test cases.
It blocked everyone else committing to Trunk.

Adding ISA-L triggered several failures listed under HDFS-15646 (i.e., HDFS-15643, HDFS-15654, HDFS-15461 ..etc)
Also, the testing run out of native memory, and it is possible that the bug was triggered as aside effect of this native library.

+I suggest that:+
* This change gets reverted.
* A thorough evaluation is performed on a local environment. This includes the native unit tests, and any test that might load the new library (EC, Stripedfiles..etc).
* When the failing units are fixed, then a PR that includes all the changes may be merged.
* keep an eye on the qbt-report to evaluate the environment post the merge.


CC: [~aajisaka], [~elgoiri], [~kihwal], [~daryn], [~weichiu], [~ayushtkn], [~ebadger]





Thanks for reporting it, [~ahussein].

Sorry that I didn't realize the side effects like HDFS-15643. But it seems that the bug was already there, and HADOOP-17224 is not the root cause.
{quote}Adding ISA-L triggered several failures listed under HDFS-15646
{quote}
If many failed after HADOOP-17224, I agree with reverting HADOOP-17224 once even if it is not the root cause. But does ISA-L really relate to them except for HDFS-15643?

{quote}
Adding ISA-L triggered several failures listed under HDFS-15646 (i.e., HDFS-15643, HDFS-15654, HDFS-15461 ..etc)
{quote}

[~ahussein] If the cause is in the tests, we should fix them or make them skipped if it takes time. If the cause is ISA-L feature itself, we should disable it first then fix or drop it.

I'm going to look into relevant OPEN issues under HDFS-15646.


{quote}Ahmed Hussein If the cause is in the tests, we should fix them or make them skipped if it takes time. If the cause is ISA-L feature itself, we should disable it first then fix or drop it.
I'm going to look into relevant OPEN issues under HDFS-15646.{quote}

Thanks [~iwasakims]. I certainly agree with you that the tests need to be fixed.

This change adds native libraries and it looks plausible that those JUnits in question are affected by loading the native library.
[~ayushtkn] and [~aajisaka] were able to narrow the reason of the failure (see [comment|https://issues.apache.org/jira/browse/HDFS-15643?focusedCommentId=17223204&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17223204]).
While this was a bug in the implementation, it is still does not seem right that those debugging are done after merging HADOOP-17224 .
If [~ayushtkn] and [~aajisaka] were not aware of that change, I would not figure out the problem on my own.

The closed jiras HDFS-15654, HDFS-15461  need to be re-investigated after the fix done in HDFS-15643.

Also, Yetus fails when the JVM fails to allocate memory to create new threads. It is unclear so far when that error was introduced in Hadoop. Yet, I can imagine that calling a new native library could lead to different memory behavior.
For Example, there could be a bug in the native code that leads to a memory leak, or the library reserves buffer that are not released back.

If you think HADOOP-17224 is unlikely to trigger the failures of Junits, then do you have suggestions what could be the code commit that change the behavior of those units and trigger OOM on Yetus?


bq. The closed jiras HDFS-15654, HDFS-15461 need to be re-investigated after the fix done in HDFS-15643.

Aren't they covered by the last jenkins result of HDFS-15643? Sorry if I'm missing something.

About the OOM error, I think it happened occasionally before that. HDFS-12711 is a similar issue and closed as incomplete. I'm not sure if the ISA-L has made that more likely to happen. We could revert HADOOP-17224 once and see the qbt results for a while.


{quote}Aren't they covered by the last jenkins result of HDFS-15643? Sorry if I'm missing something.{quote}

Hey [~tasanuma]. I put a patch to fix HDFS-15654, HDFS-15461  before we learned about the bug in HDFS-15643 .
Once HDFS-15643 got merged new errors started to show up in {{TestDFSClientRetries}} and {{TestBPOfferService}} (Example see [this comment|https://issues.apache.org/jira/browse/HDFS-15654?focusedCommentId=17226334&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17226334])

BTW, the reason I point to HADOOP-17224 to be the trigger of those failures is that the errors can only be reproduced with native flag. (See [Akira's comment on GitHub|https://github.com/apache/hadoop/pull/2408#discussion_r513963136])

[~tasanuma], If there are many test failures caused after the commit of this feature, it might be better to revert and rework, unless there is a simple change that will fix most failures quickly.  It does not matter which part is to blame. It could be faulty assumptions or designs in existing tests or unforeseen negative side-effect of the feature.  The point is, trunk build is in a broken state and more commits continue to come in with the justification like "I didn't break it. It was broken before".  This is a situation we really want to avoid.

If the test failures cannot be fixed quickly, we need to bring it back to the sane state first. Again, this has nothing to do with who is technically right or wrong.  Please review the test failures and determine whether they can be fixed quickly.  Feel free to ask if you need additional eyes.

[~ahussein] I understood. Thanks for your explanation.

[~kihwal] I agree with you to revert and rework. It's not going to be easy to fix them.

I will revert HADOOP-17224 and let's see how it goes.

Reverted it by [#2440|https://github.com/apache/hadoop/pull/2440].

[~tasanuma] Thank you.
Let's watch the qbt report after merging that change.

So far, there is a noticeable decrease in failed tests in HDFS. 

{quote}
So far, there is a noticeable decrease in failed tests in HDFS.
{quote}

[~ahussein] Could you give me the name of relevant test cases? I would like good example for digging the effect of ISA-L.

{quote}
Once HDFS-15643 got merged new errors started to show up in TestDFSClientRetries and TestBPOfferService (Example see this comment)
{quote}

TestDFSClientRetries looks fixed by HDFS-15461. TestBPOfferService is still flaky.


I don't think reverting this helped, It is still happening more or less in similar fashion.
[~tasanuma]/[~iwasakims] do you folks think ISA-L could bother other tests, It is something very much EC specific, and is actually running in lot of prod. environments as well. On the other side I remember seeing this "Unable to create native thread" failures, even last year as well or even before that as well, So this isn't something new pitching in.....

Any pointers, what you folks are thinking about this, Want to push this again, got something concerning, or still doubting this, or got something else, or stuck somewhere...

If nothing, we might even try taking help from Infra team as well, they might have some idea, may be something at their end?

[~ayushtkn], in addition to touching pom files, perhaps sending on the mailing list to stop commits for a day would be helpful .
After the latest qbt-report is generated, then we guarantee that the following qbt-report will be good to find all the side effects
of the ISA-L (if any).

I remember that the last time the ISA-L was in, the unit tests were failing with different stack traces.
The concern was that there were around hundred failing tests, many of which were EC specific.
I think that the ISA-L can be committed when it won't change the Yetus report outcome.

Once we commit ISLA-L, we might get different behavior and the opened Jira opened for failed tests will be confusing
because they have would have different description and stack trace. Then, someone doing bisect to find root cause of a
failure, would get stuck because he would have to consider the change of behavior before and after ISA-L.



The [latest qbt-report |https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/349/#showFailuresLink]points to usual suspects: 


{code:bash}
Test Result (17 failures / -207)
org.apache.hadoop.yarn.sls.appmaster.TestAMSimulator.testAMSimulatorWithNodeLabels[1]
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testUpdateAppStateXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testAppQueueXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testAppTimeoutsXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testGetContainersXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testUpdateAppPriorityXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testUpdateAppQueueXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testAppTimeoutXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testAppAttemptXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testGetAppAttemptXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testAppXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testAppStateXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testAppPriorityXML
org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testGetAppsMultiThread
org.apache.hadoop.tools.dynamometer.TestDynamometerInfra.org.apache.hadoop.tools.dynamometer.TestDynamometerInfra
org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithOpportunisticContainers
org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithEnforceExecutionType
{code}




I also think it's time to enable ISA-L. Otherwise, I fear that we will miss some bugs that relate to ISA-L, such as HDFS-15643.
To see the side effects, how about committing it this Friday or Saturday since the number of commits will be lower on weekends?

I could not reproduce the possibly relevant test failures on CentOS/RHEL on which I usually work. Same for Ubuntu VMs/containers on my local. I'm +1 to commit it again to see the difference.

Yahh, sounds good to me, we can commit over weekend and observe, if we get some EC related failures, or some suspicious test failures, or change in reasons of test failures, we can revert back by Monday. If there are failures, then that would be some bug itself in our code, which we need to fix. As of now, we don't know what to do here. Atleast we would have the traces and the names of the tests which are problematic, in case if they are...

[~ahussein] does that make sense?

[~tasanuma] will it be possible to run the entire test suite while raising the PR? May be just increasing the timeout in the Jenkinsfile or touching the hadoop-project/pom.xml should do the trick.

[~ayushtkn] Sure, I created the PR.
https://github.com/apache/hadoop/pull/2537

I didn't know the trick that editing Jenkinsfile and one java file run all unit tests. Thanks for letting me know. I'll try a few more times.

Merged the PR again. Thank you, everyone.

