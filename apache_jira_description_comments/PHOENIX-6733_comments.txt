[~vjasani] [~stoty] - this appears to be because of a check introduced in PHOENIX-5296 to insure that we don't leak scanners by checking to make sure that store files don't have any references to them when an IT test ends.

The check fails frequently (at least a few times in all CI runs), and it always appears to be in the ParallelStatsDisabled test runs. I don't recall ever seeing a failure in the NeedsOwnCluster test runs. That got me thinking: the difference between ParallelStats[Enabled|Disabled] and NeedsOwnCluster is of course that each of the latter gets its own dedicated minicluster, but for the former we share a minicluster between multiple IT tests. 

Is this check safe to run when two IT tests are using the same minicluster at the same time? Doesn't seem like it would be, because it's checking all store files, not just store files of tables created in a particular IT test. Seems like we'd either need to limit it to certain tables created in an IT test (which is hard to do generically), or limit it to only NeedsOwnCluster IT tests. Or have I missed something? 

I was also thinking about ParallelStats vs NeedsOwnCluster types and if the leaks might be relevant, but when I looked at the last successful build on 5.1 branch, it was on 21st Mar, 2022 [https://ci-hadoop.apache.org/job/Phoenix/job/Phoenix-mulitbranch/job/5.1/134/testReport/]

Perhaps that could be a starting point to see if we might have some commits checked-in during this time period. The only problem is that now a days, these test failures are not even flaky but rather consistent ones.

Similarly, for master branch, the last successful build was on 11th Jan, 2022 [https://ci-hadoop.apache.org/job/Phoenix/job/Phoenix-mulitbranch/job/master/375/] which seems quite far away, so perhaps we could start chasing 5.1 branch first?

Even in ParallelStatsDisabled tests only one IT is using a MiniCluster at the same time.
Surefire always waits for one test to finish before starting another one.

We've struggled with Minicluster restart issues in PHOENIX-6329, and we currently do not restart the MiniClusters, we just drop all HBase tables when we exceed a threshold after running the IT class.

I've worked har to make sure that dropping the tables is synchronous, and we don't start the new test untill it's finished.

I have touched the cluster restart  / reset code, and the leak detection code multiple times, but the commit times  do not correlate with the breakage times.

We could look into wether the leaks correlate with the minicluster table drops.
We could also play with the surefire test execution order to see if changing those has an effect on the leaks.

I checked the logging from the store ref check, and found that in the couple of examples I looked at it was always hbase:meta that was the region leaking. Since it's totally fine for some background process in a minicluster to be reading meta, I'm going to push up a PR with meta skipped in the check to see if that clears up the test failures. 

If not we may need to similarly exempt SYSTEM.CATALOG and SYSTEM.TASK. 

gjacoby126 commented on PR #1463:
URL: https://github.com/apache/phoenix/pull/1463#issuecomment-1189549922

   All tests passed (yay!) so it looks like that fixed it. Thanks for the reviews, @virajjasani and @stoty !




gjacoby126 merged PR #1463:
URL: https://github.com/apache/phoenix/pull/1463




Tests are passing now; merged to master. 

Reopening since this can't be cherry-picked back to 5.1 trivially (because the functionality moved from CompatUtil to BaseTest in 5.2 after we dropped support for HBase 2.1 and 2.2).

I'll create a new PR for 5.1 so it can get another test run. 

gjacoby126 commented on PR #1464:
URL: https://github.com/apache/phoenix/pull/1464#issuecomment-1189681021

   Tests pass (aside from test4tests). I'll commit based on the earlier review in the master branch. 




gjacoby126 merged PR #1464:
URL: https://github.com/apache/phoenix/pull/1464




