{quote}
the SQL engine does not seem to care about the fact that the node has already left.
{quote}

It's true. But SQL doesn't have to.

SQL engine uses TopologyService (TS) to get all live members of a cluster. So it's responsibility of TS to keep track of members of the cluster and provide valid state for those who asking for it.

I run mentioned test locally and found rather peculiar log sequence:


{noformat}
2023-06-06 12:25:50:581 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Node joined [node=ClusterNode [id=6f399661-6f3f-4d8a-ae79-ec2ad758c80a, name=itrst_sitdnbsiwsaiitf_1, address=192.168.8.104:3345, nodeMetadata=null]]
2023-06-06 12:25:50:582 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Topology snapshot [nodes=[itrst_sitdnbsiwsaiitf_1, itrst_sitdnbsiwsaiitf_0]]
2023-06-06 12:25:51:558 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Topology snapshot [nodes=[itrst_sitdnbsiwsaiitf_1, itrst_sitdnbsiwsaiitf_0]]
2023-06-06 12:25:51:904 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Node joined [node=ClusterNode [id=c015b0a9-aa4b-4a27-b42b-ba611b46978b, name=itrst_sitdnbsiwsaiitf_2, address=192.168.8.104:3346, nodeMetadata=null]]
2023-06-06 12:25:51:904 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Topology snapshot [nodes=[itrst_sitdnbsiwsaiitf_2, itrst_sitdnbsiwsaiitf_1, itrst_sitdnbsiwsaiitf_0]]
2023-06-06 12:25:52:870 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Topology snapshot [nodes=[itrst_sitdnbsiwsaiitf_2, itrst_sitdnbsiwsaiitf_1, itrst_sitdnbsiwsaiitf_0]]
2023-06-06 12:25:58:668 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Topology snapshot [nodes=[itrst_sitdnbsiwsaiitf_2, itrst_sitdnbsiwsaiitf_1, itrst_sitdnbsiwsaiitf_0]]
2023-06-06 12:26:06:408 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Node joined [node=ClusterNode [id=21182b57-f6fa-4129-94fe-c2f78a06296c, name=itrst_sitdnbsiwsaiitf_2, address=192.168.8.104:3346, nodeMetadata=null]]
2023-06-06 12:26:06:408 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Topology snapshot [nodes=[itrst_sitdnbsiwsaiitf_2, itrst_sitdnbsiwsaiitf_1, itrst_sitdnbsiwsaiitf_0]]
2023-06-06 12:26:07:204 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Topology snapshot [nodes=[itrst_sitdnbsiwsaiitf_2, itrst_sitdnbsiwsaiitf_1, itrst_sitdnbsiwsaiitf_0]]
2023-06-06 12:26:07:261 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Node left [member=ClusterNode [id=c015b0a9-aa4b-4a27-b42b-ba611b46978b, name=itrst_sitdnbsiwsaiitf_2, address=192.168.8.104:3346, nodeMetadata=null]]
2023-06-06 12:26:07:303 +0300 [INFO][sc-cluster-3344-1][ScaleCubeTopologyService] Topology snapshot [nodes=[itrst_sitdnbsiwsaiitf_2, itrst_sitdnbsiwsaiitf_1, itrst_sitdnbsiwsaiitf_0]]
2023-06-06 12:26:29:460 +0300 [INFO][sc-cluster-3344-1][Transport] Stopping [address=192.168.8.104:3344]
2023-06-06 12:26:29:461 +0300 [INFO][sc-cluster-3344-1][Transport] Stopped [address=192.168.8.104:3344]
{noformat}

In the log, the node {{itrst_sitdnbsiwsaiitf_2}} is joined twice: first time with ID={{c015...}}, and then with ID={{2118...}}. Then Node left was fired for this node with ID={{c015...}}. Since this node had been re-registered with new ID, this event just ignored leaving topology snapshot without change: {{Topology snapshot [nodes=[itrst_sitdnbsiwsaiitf_2, itrst_sitdnbsiwsaiitf_1, itrst_sitdnbsiwsaiitf_0]]}}.

I believe this is root cause of tests flakiness.


There are 2 weirdnesses:
 # Why does it take 10 seconds for a node to be removed from the physical topology if it left gracefully?
 # Why didn't the size of the topology shrink to 2 nodes when the node has actually left the topology?

The second weirdness is harmless, it just looks weird in the logs. The thing is that the node re-joined before it was considered as left, so the 'Node left' was about an outdated version of the node (that was already replaced with the new one on the second join). Logically, it's all ok: a node was 'detected' to be missing too late, so there was no time window when the cluster would know that the node was gone.

The first weirdness is really weird. I'm digging this, but we should probably remove the node from the topology as soon as it says 'good bye' (LEAVING event) as otherwise the suspicion check mechanism is used for it.

The first weirdness was fixed in IGNITE-19685. To make the situation with removal of an outdated node less confusing, I've changed logging a bit to make this case stand out and differ from the normal 'node left' case.

