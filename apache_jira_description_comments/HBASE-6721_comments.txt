Design document for HBase region server grouping feature.

More details should be added to the design.
Have you considered introducing interface for AssignmentManager so that existing and new managers can be easily swapped ?
Have you considered storing group information in zookeeper instead of on hdfs ?

Please explain more about RegionServerGroupProtocol.

Thanks for the initiative.

Another aspect is fault tolerance.
Say the smallest group consists of 6 region servers, the impact of majority of the 6 servers going down at the same time is much higher than 6 servers out of whole cluster going down where there is only one group.

> Have you considered introducing interface for AssignmentManager so that existing and new managers can be easily swapped?
Yes, part of the proposal is to make AssignmentManager pluggable. I'll add that as a subtask for this.

> Have you considered storing group information in zookeeper instead of on hdfs ?
Correct me if I'm wrong, but it seems the approach HBase has taken for it's usage of ZK is more towards storing temporal data for coordination and the real source of truth is on HDFS or Tables. And we decided to follow the same approach. 

> Please explain more about RegionServerGroupProtocol.
RegionServerGroupProtocol exposes APIs to manage Grouping (see API in doc). The currently plan is that these APIs will be used and exposed via the CLI commands. 

> Another aspect is fault tolerance.
> Say the smallest group consists of 6 region servers, the impact of majority of the 6 servers going down at the same time is much higher than 6 
> servers out of whole cluster going down where there is only one group.
This is similar to hbase cluster sizing for fault tolerance. Let's play around with it and later on document best practices.




Looking at current doc, GroupInfo would be passed to (new) AssignmentManager.
Do you plan to reference GroupInfo in AssignmentManager interface ?

GroupInfo may be retrieved internally through an instance of the GroupInfoManager. This shouldn't leak through the interface. On a similar note Stack was suggesting something different to making it pluggable in HBASE-6723.

bq. through an instance of the GroupInfoManager
If I understand correctly, GroupInfoManager singleton would be used inside new AssignmentManager.

bq. This is similar to hbase cluster sizing for fault tolerance.
I think what I wanted to say was that we may need to reserve some region servers from the Default pool for emergency scenario so that each group can have decent size all the time.

{quote}If I understand correctly, GroupInfoManager singleton would be used inside new AssignmentManager{quote}

Yep it will create an instance of it internally.

{quote}I think what I wanted to say was that we may need to reserve some region servers from the Default pool for emergency scenario so that each group can have decent size all the time.{quote}

I see, that could be something we could add later on. There are some scenarios we have to think more thoroughly about. A conservative group sizing and manual intervention should be good enough for now? Let's get the basics fully baked first.

At the recent HBase contributor meet up, the topic of making the assignment manager pluggable was discussed. There was agreement that the assignment manager is a core and complex component of HBase, and making it pluggable is not a good idea. Alternatively, suggestions were made to introduce the region placement logic  into the load balancer which is already pluggable. This idea works for this feature with making some changes to the LoadBalancer interface. 

I will update the design document with the changes I have mentioned. 

Lads, do you think this a 0.96 issue?  You think it will be done in time?  (W/i a couple of weeks or so)?  If not, can we move it out?  Thanks.

Stack, we'd like to get this into 0.96 (and if possible 0.94 as that's what we're planning on deploying). We should be have a usable patch in ~2 weeks. Does that sound acceptable?

That should work.

Will the group info persisted in the META table?

Stack, great

Ram, it'll be persisted in a file. Our thoughts were that the approach would be more invasive (and complex) if the information would be persisted in a table since we would have to know the group of a table before we can assign them to a region server and read it.

Ok, so if a region gets splitted then the daughter regions fall in the parents's group? So the group info file will be updated based on this info?
Also groups cannot be updated once created right?  

Group membership is based on tables, for a split no update will be necessary on the file. Groups can be updated. Each update will be updated in memory and flushed to the file for persistence.

0.94 patch for initial review

0.94 patch for initial review, We decided to combine the patch of two subtasks into one.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12550599/HBASE-6721_94.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3137//console

This message is automatically generated.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12550601/HBASE-6721_94.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 11 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3138//console

This message is automatically generated.

{code}
+  public LoadBalancer getBalancer() {
+    return balancer;
+  }
{code}
The above method can be package private.
{code}
+public class GroupAdminEndpoint extends BaseEndpointCoprocessor implements GroupAdminProtocol, EventHandler.EventHandlerListener {
+ private static final Log LOG = LogFactory.getLog(GroupAdminClient.class);
{code}
Please add javadoc for the class. The line is beyond 100 characters.
Log has wrong class.
{code}
+  private ConcurrentMap<String,String> serversInTransition =
{code}
What does the value in serversInTransition map represent ?
{code}
+   List<HRegionInfo> regions = new ArrayList<HRegionInfo>();
+   if (groupName == null) {
+      throw new NullPointerException("groupName can't be null");
{code}
nit: move ArrayList creation after the if statement.
{code}
+  public Collection<String> listTablesOfGroup(String groupName) throws IOException {
{code}
The return type is a collection, more generic than List that listOnlineRegionsOfGroup() returns. I guess there might be a reason.
{code}
+      HTableDescriptor[] tables = master.getTableDescriptors().getAll().values().toArray(new HTableDescriptor[0]);
{code}
nit: line too long.
{code}
+  public GroupInfo getGroup(String groupName) throws IOException {
{code}
Suggest renaming the method getGroupInfo(). getGroup() is kind of vague.

More reviews to follow.

In GroupAdminEndpoint:
{code}
+     throw new IOException(
+         "The region server or the target to move found to be null.");
{code}
It would be nice to point out which parameter is null.
{code}
+        throw new DoNotRetryIOException("Group must have no associated tables.");
{code}
Include group name in the exception message.
{code}
+  public Map<String, String> listServersInTransition() throws IOException {
{code}
Return type of Map includes additional information which is not used by callers. Suggest returning keySet.
Down in GroupAdminClient:
{code}
+      for(String server: proxy.listServersInTransition().keySet()) {
+        found = found || servers.contains(server);
+      }
{code}
Can you tell me what the body is supposed to achieve ?
Back to GroupAdminEndpoint:
{code}
+  private GroupInfoManager getGroupInfoManager() {
+    return ((GroupBasedLoadBalancer)menv.getMasterServices().getAssignmentManager().getBalancer()).getGroupInfoManager();
{code}
Does GroupInfoManager belong to balancer ? The above is probably the longest indirection I have ever seen :-)
{code}
+  private List<HRegionInfo> getOnlineRegions(String hostPort) throws IOException {
{code}
The above method is only called by listOnlineRegionsOfGroup() in a loop over online servers, resulting in nested loop.
Please consider collapsing the nested loop into one loop.
{code}
+      LOG.error("Failed to complete GroupMoveServer with of "+h.getPlan().getServers().size()+
{code}
nit: remove ' of ' in above sentence.

[~yuzhihong@gmail.com]

{quote}
What does the value in serversInTransition map represent?
{quote}

It represents servers that are being moved from one group to another.

{quote}
Can you tell me what the body is supposed to achieve ?
Back to GroupAdminEndpoint:
{quote}

Retrieveing the balancer during start() returns null. Thus I have to retrieve it lazily as needed.

{quote}
Does GroupInfoManager belong to balancer ? The above is probably the longest indirection I have ever seen
{quote}

We had to do this since we didn't want to touch AssignmentManager as much as possible :)



{quote}
We had to do this since we didn't want to touch AssignmentManager as much as possible
{quote}

As an alternative, we can add a getBalancer() Method to MasterServices. Thoughts?

{code}
+  public void beforeProcess(EventHandler event) {
{code}
I think normally the above should be called preProcess().
{code}
+  public void afterProcess(EventHandler event) {
{code}
Rename to postProcess().
{code}
+ * Copyright 2011 The Apache Software Foundation
{code}
The above is no longer needed in license header.
{code}
+public interface GroupAdminProtocol extends GroupAdmin, CoprocessorProtocol {
+}
{code}
I wasn't expecting a Protocol to not have methods in it :-)
{code}
+public class GroupBasedLoadBalancer implements LoadBalancer {
{code}
Add javadoc for GroupBasedLoadBalancer.
{code}
+      } catch (IOException e) {
+        LOG.warn("IOException while creating GroupInfoManagerImpl.", e);
+      }
{code}
I think if groupManager cannot be initialized, we should abort master because group policy wouldn't be enforced.
In correctAssignments():
{code}
+        if ((info == null) || (!info.containsServer(sName.getHostAndPort()))) {
+          // Misplaced region.
+          misplacedRegions.add(region);
{code}
Under what scenario would a region be misplaced at runtime ? I think rebalancing misplaced region(s) would affect normal operation of related groups.
{code}
+    //unassign misplaced regions, so that they are assigned to correct groups.
+    this.services.getAssignmentManager().unassign(misplacedRegions);
{code}

bq. Can you tell me what the body is supposed to achieve ?
I was asking about the following line of code:
{code}
found = found || servers.contains(server);
{code}
It seems to be condition checking.

bq. As an alternative, we can add a getBalancer() Method to MasterServices.
That would be better than the current form.

Just some initial thoughts:

* I couldn't seem to get it to compile for me on 0.94
* There seem to be a bunch of formatting changes that aren't needed.
* Passing in the preferred server into the load balancer on randomAssignment is messy.  If we know the preferred server why call this function at all ?
* The balancer is a public interface and we can't make changes to it in a minor release. And this patch won't apply to trunk.
* With this many interfaces and classes it might make sense to move them into a namespace.
* Why is GroupAdminClient in the master namespace and not in the client namespace.
* Why a co-processor and not build it in ?
** Security was done that was because it can be added or removed. As the patch is that's not really possible
** This makes a lot of changes in core code for something that is a co-processor. 
* Don't create a DefaultLoadBalancer in GroupBasedLoadBalancer.  The balancer was made pluggable and that feature shouldn't go away.
* Why return ArrayListMultimap from groupRegions in GroupBasedLoadBalancer? Why not the base class
* HTableDescriptor seems like the correct location for info about the table if you don't want to put that data into meta.
* putting things into the filesystem seems like the wrong way to do it. There are just so many different moving parts with getting things from hdfs with caching and cache invalidation, and edge cases on failure.
* There's a lot of logic about balancing bleeding into the AssignmentManager.  Right now assignment manager is already too complex.  I would much prefer a solution that had everything in the balancer.

bq. There's a lot of logic about balancing bleeding into the AssignmentManager.
Looking at the changes in AssignmentManager, they are mostly white space removal.
There is only one real change:
{code}
+  public LoadBalancer getBalancer() {
+    return balancer;
+  }
{code}
which Francis agrees to move out.

{quote}
I was asking about the following line of code:

found = found || servers.contains(server);

It seems to be condition checking.
{quote}

Yeah it's basically checking if the list of servers to be moved is already in the ServersInTransition list, meaning it is already being moved so we shouldn't allow that.

{quote}Looking at the changes in AssignmentManager, they are mostly white space removal.
There is only one real change:{quote}
You're missing the change where null plans are no longer queued which comes about because of this patch.

Thanks for the comments [~eclark]

{quote}
I couldn't seem to get it to compile for me on 0.94
{quote}
Did you apply the patches attached in the subtasks prior to apply this patch? If you'd like them all in one single patch I can do that as well.

{quote}
There seem to be a bunch of formatting changes that aren't needed.
{quote}
Will clean that up in the next update.

{quote}
Passing in the preferred server into the load balancer on randomAssignment is messy. If we know the preferred server why call this function at all ?
{quote}
Good point, will remove that argument.

{quote}
The balancer is a public interface and we can't make changes to it in a minor release. And this patch won't apply to trunk.
{quote}
I see, we can make it binary compatible at least by supporting both interfaces if you're amenable to that. We're planning on getting 0.94 into production and it'd be great if we didn't have a lot of custom patches on top of it.

{quote}
With this many interfaces and classes it might make sense to move them into a namespace.
{quote}
Will look into doing this, my main concern is if there any dependencies to package private methods.

{quote}
Why a co-processor and not build it in ?
    Security was done that was because it can be added or removed. As the patch is that's not really possible
    This makes a lot of changes in core code for something that is a co-processor.
{quote} 
As part of the design, HBase should run fine without the group based classes enabled (endpoint, balancer, etc). If it is not that case then that's a bug. As for some code changes in core code. Some may be unavoidable, but we could probably still make it less invasive (ie remove the EventHandler changes). Having said that, I don't mind if the community would like to have this fully integrated into HBase, just let us know.

{quote}
Don't create a DefaultLoadBalancer in GroupBasedLoadBalancer. The balancer was made pluggable and that feature shouldn't go away.
{quote}
The balancer is still pluggable it's just not pluggable for the GroupBasedLoadBalancer. Though should be ok to make that pluggable as well.

{quote}
HTableDescriptor seems like the correct location for info about the table if you don't want to put that data into meta.
{quote}
Yes, we have group affiliation store as a table property. Though group information is stored on hdfs.

{quote}
putting things into the filesystem seems like the wrong way to do it. There are just so many different moving parts with getting things from hdfs with caching and cache invalidation, and edge cases on failure.
{quote}
I see, were do you suggest we put it? Zookeeper? We mainly had it in HDFS since ZK, seemed to be the place to store only ephemeral data? Putting the data in tables would be a lot more complex and would require more core code change.



{quote}
You're missing the change where null plans are no longer queued which comes about because of this patch.
{quote}
We needed this change to prevent regions from being assigned to region servers they don't belong to. We can continue to recognize null, we just need another way to prevent regions from being assigned to the wrong group of region servers. One option is to have a dead/bogus server as part of the plan if no online servers are available for a given group, this way it eventually gets reassigned once a live server is up. Would that work?

{quote}Did you apply the patches attached in the subtasks prior to apply this patch? If you'd like them all in one single patch I can do that as well.{quote}
Nope I had missed those.

{quote}As part of the design, HBase should run fine without the group based classes enabled (endpoint, balancer, etc).{quote}
Then they should be a seperate module.  This half in half removable part is just a little worrisome.

{quote}The balancer is still pluggable it's just not pluggable for the GroupBasedLoadBalancer{quote}
If this is going to be a feature then it will need to work with all balancers.

{quote}I see, were do you suggest we put it? Zookeeper? We mainly had it in HDFS since ZK, seemed to be the place to store only ephemeral data? Putting the data in tables would be a lot more complex and would require more core code change.{quote}

I would prefer something like security has.  We have a perfectly usable key/value storage system no need to use a different storage path.



bq. Then they should be a seperate module.
In 0.94, there is no support for modules.
It would be nice to keep implementation roughly the same for 0.94 and 0.96. This way users can try this feature earlier and it is easier to maintain across major releases.

{quote}
I would prefer something like security has. We have a perfectly usable key/value storage system no need to use a different storage path.
{quote}
That was what we had originally in mind, but decided to punt on it due to the complexity. The main problem here is during startup before the catalog tables get assigned we would need to know what group the tables are members of. And since the "group" table isn't assigned yet we have to get creative. In my mind one way to do that is manually read the region on hdfs to retrieve the group information. Does that sound feasible?

bq. And since the "group" table isn't assigned yet we have to get creative. In my mind one way to do that is manually read the region on hdfs to retrieve the group information.

How do you handle META? Why not handle the "group" table the same way?

In the current implementation we handle META the same way as all the other tables. Since we just load the group information from hdfs. If the "group" information was stored on a table we won't be able to access the group information through normal means until after we assign ROOT, META and the "group" table itself.

bq. If the "group" information was stored on a table we won't be able to access the group information through normal means until after we assign ROOT, META and the "group" table itself.

Right, so you could assign ROOT, META, and the "group" table randomly at first, and then calculate placement and moves of the rest and perhaps also META and "group" after the "group" table becomes available, right? Worst case that would be 3 moves?

The current attached patch for 0.94 makes a ton of changes in o.a.h.h.master.

This stuff should all be pulled out into a separate package that can be put into a Maven module like done with security in 0.94.

What changes to core code remain after that?

Updated patch addressed most of the comments. Mainly:

- moved storage of group information from hdfs to zookeeper
- cleaned up whitespace and removed unnecessary changes into core master classes

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552071/HBASE-5498_94_4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 20 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3224//console

This message is automatically generated.

{quote}
This stuff should all be pulled out into a separate package that can be put into a Maven module like done with security in 0.94.
{quote}
Isn't that a bit too much overhead? Wasn't the reason security was a separate module was because it had to depend on a secure version of hadoop which the community didn't want?

{quote}
What changes to core code remain after that?
{quote}
I've updated the patch, there should be minimal and no invasive changes.

{quote}
Right, so you could assign ROOT, META, and the "group" table randomly at first, and then calculate placement and moves of the rest and perhaps also META and "group" after the "group" table becomes available, right? Worst case that would be 3 moves?
{quote}
Since we are not doing any changes to the assignment manager this actually is complicated since we won't really  know what state assignment is in based on just calls to the load balancer and information we can glean through the master services.

anyone know how to get a git patch onto review board? It seems it only works for trunk.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552073/HBASE-6721_94_2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 20 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3226//console

This message is automatically generated.

Looks like the HBASE-6721_94_2 patch contains the patch on HBASE-7042 too?

MasterExec and MasterExecResult are still in the HBASE-6721_94_2 patch, please use Exec and ExecResult and remove MasterExec and MasterExecResult and the change to HbaseObjectWritable.

bq. moved storage of group information from hdfs to zookeeper

Nice, this moves forward in the right direction IMHO. However, I don't see where this information is persisted into a table. Did I miss it? Because HBase state in ZooKeeper can be cleared by administrators in some recovery scenarios, we can't consider it persistent storage. See how the security coprocessor handles this.

{quote}
bq. This stuff should all be pulled out into a separate package that can be put into a Maven module like done with security in 0.94.
Isn't that a bit too much overhead? 
{quote}

The patch adds group-based assignment coprocessor specific classes to the main Master package and the main Client package. I should clarify that I didn't say your work must be put into a separate Maven module, but rather coprocessors are mini-applications, and should be organized as such. It is important to keep this set of changes, distinct optional functionality, grouped together apart from unrelated core code. Why not {{org.apache.hadoop.hbase.client.group}} and {{org.apache.hadoop.hbase.master.group}} etc.? I hope this feedback is clearer now. 

{quote}
bq. What changes to core code remain after that?
I've updated the patch, there should be minimal and no invasive changes.
{quote}

Thanks! Much better.

Still some possible whitepsace issues though, the patch may contain mixed tabs and spaces? Some of the formatting looks off when I view it. If you are using Eclipse, we have a HBase formatter in dev-support/ now that may be helpful. 


Regarding storing the group assignment information in ZooKeeper, I think this is a good strategy. You can optimize for the case where HBase (re)starts with valid group assignment data available in ZK. Therefore you can avoid some bootstrapping challenges. However, you must persist the group assignment information into a table, like how security does with {{_acl_}}. After starting up, if in the uncommon case there is group assignment information in the {{_group_}} (or similar) table that is not in sync with that in ZK (perhaps because ZK state was cleared), you should update ZK data accordingly. From there, there are a couple of options:
- WARN the administrator that the table assignments should be updated via disable and enable.
- Automatically trigger reassignment via disable and enable. 
- Region moves (if the assignment information is available - trunk only)

Edit: Fix incorrect use of markup.

@Francis:
On https://reviews.apache.org, you can select hbase-git repository and upload the patch for 0.94
Review board is able to generate proper diff.
{code}
+ * Copyright 2011 The Apache Software Foundation
{code}
The above line is no longer needed.

Thanks

Can this also colocate regions of different tables, maybe based on a key prefix?
In our case we'd tenants share tables, and each row key would be a prefixed by a tenant id.

In SecureGroupAdminEndpoint, the following code is called repeatedly:
{code}
+  private AccessController getAccessController() {
+    return (AccessController)menv.getMasterServices()
+        .getCoprocessorHost().findCoprocessor(AccessController.class.getName());
+  }
{code}
Should AccessController be stored in a field variable ?
For TestSecureGroupAdminEndpoint:
{code}
+  @Test
+  public void testGetAddRemove() throws Exception {
{code}
Would testGroupGetterAdditionRemoval() be better name for the above method ?
{code}
For GroupAdmin interface:

+   * List servers that are currently being moved to a new group
+   * @return
+   * @throws IOException
+   */
+  Map<String, String> listServersInTransition() throws IOException;
{code}
Please describe the meaning of key and value in the returned Map.
For GroupAdminClient.java:
{code}
+      for(String server: proxy.listServersInTransition().keySet()) {
+        found = found || servers.contains(server);
+      }
+      Thread.sleep(1000);
{code}
We can break out of the for loop once found becomes true, right ?
Would suggest using a shorter sleep interval above.
For DefaultLoadBalancer.java:
{code}
-  public ServerName randomAssignment(List<ServerName> servers) {
+  public ServerName randomAssignment(HRegionInfo region, List<ServerName> servers) {
{code}
The new parameter region is not used. We don't need to introduce this parameter, right ?
BTW StochasticLoadBalancer doesn't have randomAssignment().

Will provide more comments for next patch.

[~ted_yu] In your previous comment, the extra parameter (HRegionInfo) is used by the GroupBasedLoadBalancer class to determine the group information.

{quote}
Still some possible whitepsace issues though, the patch may contain mixed tabs and spaces? Some of the formatting looks off when I view it. If you are using Eclipse, we have a HBase formatter in dev-support/ now that may be helpful. 
{quote}
I'm using intellij and it doesn't seem to like the trailing whitespaces in the original file. I couldn't figure out a way to turn it off.

re: moving into another package - I see that sounds reasonable. Will do that but I will have to make some package private methods in assignment manager public.


{quote}
BTW StochasticLoadBalancer doesn't have randomAssignment().
{quote}
randomAssignment is part of the original interface StochasticLoadBalancer should have it. Also I don't see it that balancer in 0.94?


{quote}
Can this also colocate regions of different tables, maybe based on a key prefix?
In our case we'd tenants share tables, and each row key would be a prefixed by a tenant id.
{quote}
Currently it determines group membership based on a table property. We can make things more generic and have membership determined from region metadata (ie startKey). Would that work for you? Would each tenant become a group? Or can multiple tenants be part of the same group?

bq. re: moving into another package - I see that sounds reasonable. Will do that but I will have to make some package private methods in assignment manager public.

I don't see that as a problem if you could also kindly tag them with private interface annotations.

bq. Currently it determines group membership based on a table property. We can make things more generic and have membership determined from region metadata (ie startKey). Would that work for you? Would each tenant become a group? Or can multiple tenants be part of the same group?

That should work. I would need to think this through in a bit more detail. Might be a bit tricky, since many small tenants could be in the same region, in which case colocation might not be possible at all.

In trunk, BaseLoadBalancer has:
{code}
  public ServerName randomAssignment(HRegionInfo regionInfo, List<ServerName> servers) {
{code}
StochasticLoadBalancer extends BaseLoadBalancer.
so the addition of HRegionInfo region parameter in 0.94 should be fine.

First stab at using a table to store the group information. GroupBasedLoadBalancer now works in two modes online and offline. In offline, balance doesn't work and the rest does random assignment but only for the catalog table and group table the rest are given a null/bogus assignment. Random assignment will then need to be corrected with a call to balance() or just let the chore call balance eventually. The implementation is a bit clunky but probably the prolly the best choice if we want to keep the data in the tables. I've tested this on a distributed cluster and it seems to work. Let me know if this is ok so I can continue working on this and addressing the comments. Please call out other major concerns that I should be addressing. I'm hoping we can start working on a trunk patch soon :-).

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552446/HBASE-6721_94_3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 71 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3249//console

This message is automatically generated.

bq. The implementation is a bit clunky but probably the prolly the best choice if we want to keep the data in the tables.

[~toffer] Keeping the group assignment data in a table (at least the authoritative record of it) seems appropriate and consistent with feedback. Have you also considered adding a state mirror in ZK to avoid the need for random assignment of catalog tables and the group table if it is available on (re)start?

FYI, looks like the _3 patch picks up unrelated work.

Edit: See my above recent comment in this regard.

@Francis:
Please refresh your 0.94 workspace. Some changes in patch v3 were from HBASE-6796

{code}
+  public GroupMoveServerWorker(Server master, Map<String, String> serversInTransition,
...
+    this.master = (MasterServices)master;
{code}
master parameter should be declared as MasterServices.
{code}
+        if(sourceGroup != null && !tmpGroup.equals(sourceGroup)) {
+          throw new DoNotRetryIOException("Move server request should only come from one source group");
{code}
Consider including sourceGroup and tmpGroup in the exception message.
{code}
+      if(!sourceGroup.startsWith(GroupInfo.TRANSITION_GROUP_PREFIX)) {
+        transGroup = GroupInfo.TRANSITION_GROUP_PREFIX+
+            System.currentTimeMillis()+"_"+sourceGroup+"-"+plan.getTargetGroup();
+        groupManager.addGroup(new GroupInfo(transGroup, new TreeSet<String>()));
+      }
+    }
+    groupManager.moveServers(plan.getServers(), sourceGroup, transGroup!=null?transGroup:plan.getTargetGroup());
{code}
Why moving the servers to a transitional group ?
{code}
+        } catch (InterruptedException e) {
+          LOG.warn("Sleep interrupted", e);
{code}
Restore interrupt status.
In complete():
{code}
+      if(success) {
+        groupManager.moveServers(plan.getServers(), tmpSourceGroup, plan.getTargetGroup());
+        if(transGroup != null) {
+          groupManager.removeGroup(transGroup);
{code}
If early action in run() wasn't successful, the transitional group would hang around ?


In GroupInfoManager:
{code}
+  boolean moveServers(Set<String> hostPort, String srcGroup, String dstGroup) throws IOException;
{code}
Please add javadoc for moveServers() method.
For GroupInfoManagerImpl , indentation should be 2 spaces.
For moveServers:
{code}
+    for(String el: hostPort) {
+      foundOne = foundOne || src.removeServer(el);
+      dst.addServer(el);
{code}
If foundOne is true in the middle of the loop, would the call to src.removeServer(el) be skipped ? That would lead to server being in both groups.

clean patch, sorry for the confusion

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552520/HBASE-6721_94_3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 14 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3257//console

This message is automatically generated.

{quote}
Have you also considered adding a state mirror in ZK to avoid the need for random assignment of catalog tables and the group table if it is available on (re)start?
{quote}
I'm wondering if it's worth the added complexity it introduces. For security it was needed since consumers of acl data was distributed and zk could do the orchestration. But in this case the data is only read and written to by the active master which to me makes mirroring things in ZK more of an enhancement. I don't feel strongly about it but I'm more inclined to keep things simple for now and see how things go. Thoughts?

{quote}
Have you also considered adding a state mirror in ZK to avoid the need for random assignment of catalog tables and the group table if it is available on (re)start?
{quote}
Another approach is to mirror only the catalog and group table assignment information in ZK. This would add less complexity and minimizes the cost of having inconsistent data between the two stores.

Do you have a patch for 0.96 on review board?

[~jxiang] -- We have not started to work on the patch for trunk as we wanted the patch for branch-94 to address all the review comments. Hopefully after one more rounds of review, we will start working on the patch for the trunk.

In GroupBasedLoadBalancer.roundRobinAssignment():
{code}
+        for (HRegionInfo region : regions) {
...
+          assignments.put(ServerName.parseServerName("127.0.0.1:1"),regions);
{code}
Should regions for ROOT, .META. and group tables be excluded from the above put() ? These regions are handled earlier by the call to assignments.putAll().
Should we confine these special tables to default group ?


I went over GroupBasedLoadBalancer.
In correctAssignments():
{code}
+        }catch(IOException exp){
+          LOG.debug("Group information null for region of table " + region.getTableNameAsString(),
+              exp);
+        }
+        if ((info == null) || (!info.containsServer(sName.getHostAndPort()))) {
{code}
If groupManager.getGroup() fails to return group information, it is likely the exception would occur multiple times inside the loop. We should distinguish this from the case of !info.containsServer(sName.getHostAndPort()). Meaning, in case of exception, we don't conclude that the regions are misplaced.

In roundRobinAssignment():
{code}
+              regionGroup.get(groupKey), getServerToAssign(info, servers)));
{code}
Rename getServerToAssign() -> getServersToAssign()
{code}
+          assignments.put(ServerName.parseServerName("127.0.0.1:1"),regions);
{code}
The above statement would be executed many times. Consider creating a singleton for ServerName.parseServerName("127.0.0.1:1").
{code}
+      throw new IllegalStateException("Failed to access group store", e);
{code}
The above would cause master to exit, right ? If you don't want to change the signature of roundRobinAssignment(), consider returning null.
{code}
+        if(nulled.size() > 0) {
{code}
style: insert space between if and (.

offlineRetainAssignment() contains only one method call. Consider merging it into retainAssignment().
{code}
+    //chances are most are not, then we just use balance to correct
{code}
'are not': not correct or not incorrect ? Please clarify. It would be nice to end each line of comment with a period.
{code}
+      ListMultimap<String, HRegionInfo> rGroup = ArrayListMultimap.create();
{code}
Would groupToRegionMap be better name for the above variable ?

More comments about GroupBasedLoadBalancer.
In randomAssignment():
{code}
+      } else if(SPECIAL_TABLES.contains(region.getTableName())){
+        candidateList = servers;
{code}
If you limit special tables to default group, there is no need to move them after system becomes online.
For GroupStartupWorker, isOnline is set to true after waitForGroupTableOnline() returns. In waitForGroupTableOnline():
{code}
+          if(found.get()) {
+            ((GroupBasedLoadBalancer)masterServices.getLoadBalancer()).getGroupInfoManager();
+          }
{code}
Would the above call be effective (isOnline is false at this point) ?

{quote}
Should we confine these special tables to default group ?
{quote}
That's a good point, I just realized that it's not possible to add table properties to the catalog tables. Looks like I will have to move group membership into the group metadata table as well. It felt clunky that changing group membership would be considered as a schema change anyway.

Patch address most of the comments. Mainly:

-zookeeper as a temporary storage for special table group assignments.
-storing group table membership in the group table instead of the table descriptor

review board:
https://reviews.apache.org/r/8389/

I got a test failure in one of the newly added tests:
{code}
testOffline(org.apache.hadoop.hbase.master.TestGroupsOfflineMode)  Time elapsed: 1.283 sec  <<< ERROR!
java.lang.NullPointerException
  at org.apache.hadoop.hbase.master.TestGroupsOfflineMode.testOffline(TestGroupsOfflineMode.java:116)
{code}

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12559999/HBASE-6721_94_5.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 16 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3518//console

This message is automatically generated.

updated patch, also updated review board.

Will update documentation tomorrow.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12567985/HBASE-6721_94_6.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 28 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4329//console

This message is automatically generated.

Started reviewing, got as far as GroupInfoManagerImpl.java, this will probably take several go-s to go thru the entire patch.
My main question is - what is the state of the trunk patch? It looks like a huge feature so it should go into trunk first.
Another one may be (may be - due to not having seen the rest of the patch yet) - why is some JSON serialization used? Will it be different in trunk? We already have PB and writables, so I wonder how it works with them.

{quote}My main question is - what is the state of the trunk patch?{quote}
Our main focus was to get something released internally thus it's a bit behind. Now that we have things more or less stabilized we'll have time to get this done. 

{quote}Another one may be (may be - due to not having seen the rest of the patch yet) - why is some JSON serialization used?{quote}
It was an easy way of not having to deal with actually serializing the data. I'm open to suggestions. For trunk we can use PB.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12568784/HBASE-6721_94_7.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 28 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4409//console

This message is automatically generated.

With this, master, zk and META are still shared across different groups with little isolation guarantees or SLAs. I am wondering whether this is worth the effort without having proper multi-tenancy at all levels. Isn't it way more easier to install multiple clusters?

{quote}
With this, master, zk and META are still shared across different groups with little isolation guarantees or SLAs. I am wondering whether this is worth the effort without having proper multi-tenancy at all levels.
{quote}
The isolation it provides has been very useful for us. We already have this deployed on 2 clusters. I don't see sharing the master as an issue as interaction to it is normally administrative. As for ZK, there's an effort to make it highly scalable if the majority of clients are read-only. Regarding META, you can have an group which serves only internal tables. 

{quote}
I am wondering whether this is worth the effort without having proper multi-tenancy at all levels. Isn't it way more easier to install multiple clusters?
{quote}
This is much more easier to operate (installing, reallocating servers, tables, etc) while providing us the level of isolation we need. Interacting with it is easier as well, one single configuration file to access any of the tables (if ACLs permit). Keep in mind this does not preclude adding more multi-tenancy features (ie rate limiting, priorities, quotas, etc). This is just the first step, one which we felt provided the biggest gains as well as adding a new primitive we can build upon.


bq. With this, master, zk and META are still shared across different groups with little isolation guarantees or SLAs. I am wondering whether this is worth the effort without having proper multi-tenancy at all levels.

I agree with this, but in defense of Francis' approach here, HDFS is also shared (though it helps that writes prefer the local datanode, as do shortcut reads). Admission control / QoS is a hard problem that requires a full stack conversation and, in my opinion, calls for considering a framework for admission control spanning from Common outward, something similar to a security access token (perhaps even an attribute of same) with support at each layer for mapping a logical notion of priority or quota to the actual enforcement mechanism, whatever it is. That will be a difficult project to say the least and in the meantime we could make incremental improvements at various layers of the stack if there is a demonstrated benefit. 

[~toffer]
How about for this now? 
Is it available in your cluster?
When do you consider to apply in trunk?

Since we have all the tables' group info, could we remove GroupInfo.TABLEDESC_PROP_GROUP?

Could we only use hbase table to storage group information?

When we update the group information, why not update the memory the first?  Then we need't roll back if persistence failed




[~zjushch] 

We have the 0.94 running in our cluster for a while now.

I've uploaded the trunk patch on review board: https://reviews.apache.org/r/9680/diff/#index_header

{quote}
Since we have all the tables' group info, could we remove GroupInfo.TABLEDESC_PROP_GROUP?
{quote}
This is needed so we can create and assign a table to a group as part of a create table request.

{quote}
Could we only use hbase table to storage group information?
{quote}
Yes this is already done in the current implementation.

{quote}
When we update the group information, why not update the memory the first? Then we need't roll back if persistence failed
{quote}
Can you call out which section in GroupInfoManager you're referring to?





I need to updated the 0.94 patch to match changes made it trunk. Will post it soon.

Forgot to mention Vandana and I worked on this patch.

Included Trunk patch with Generated PB classes.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12571470/HBASE-6721_trunk.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 30 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 6 warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces lines longer than 100

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4611//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4611//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4611//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4611//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4611//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4611//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4611//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4611//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4611//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4611//console

This message is automatically generated.

Updated design doc

[~ted_yu] Why move the status to "open"?

Otherwise hadoop QA would treat pdf files as patches.

I apply the latest patch to our 0.94 cluster, it seems good. 

We will test it then, and feedback the usage

Other way, should we add group informations on masert web UI?

Upload the webUI patch if needed

Late to the game... but maybe below is of some help:

Design doc should have author, date, and reference to this issue.

Something is incomplete around here in the doc ... "configured to not use the region server grouping features,"

Should explain why table has such an odd-looking name “0group0”

When you refer to servers in the shell, should you use the full servername rather than just host and port?  e.g:

group_move_servers 'dest',['server1:port','server2:port']

Doc. looks good Francis.



[~saint.ack@gmail.com] I've updated the doc. Addressing your questions. Let me me know if it's missing anything else.



{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12580760/HBASE-6721_8.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 30 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 6 warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:red}-1 release audit{color}.  The applied patch generated 2 release audit warnings (more than the trunk's current 0 warnings).

    {color:red}-1 lineLengths{color}.  The patch introduces lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.backup.TestHFileArchiving

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/5469//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5469//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5469//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5469//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5469//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5469//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5469//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5469//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5469//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5469//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/5469//console

This message is automatically generated.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12580775/HBASE-6721_9.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 30 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 6 warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:red}-1 release audit{color}.  The applied patch generated 2 release audit warnings (more than the trunk's current 0 warnings).

    {color:red}-1 lineLengths{color}.  The patch introduces lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.TestFullLogReconstruction
                  org.apache.hadoop.hbase.group.TestRegionServerGroups

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/5470//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5470//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5470//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5470//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5470//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5470//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5470//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5470//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5470//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5470//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/5470//console

This message is automatically generated.

Looks like MetaScanner.allTableRegions() returned a map where some entry has null ServerName.
There is no null check:
{code}
        ServerName serverName = HRegionInfo.getServerName(rowResult);
        regions.put(new UnmodifyableHRegionInfo(info), serverName);
{code}
After adding null check in GroupAdminEndpoint#moveTables :
{code}
      for (Map.Entry<HRegionInfo, ServerName> entry: regionMap.entrySet()) {
        if (entry.getValue() == null) continue;
{code}
TestRegionServerGroups#testTableMoveAndDrop passed reliably.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12580943/HBASE-6721_9.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 32 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 6 warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:red}-1 release audit{color}.  The applied patch generated 2 release audit warnings (more than the trunk's current 0 warnings).

    {color:red}-1 lineLengths{color}.  The patch introduces lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.backup.TestHFileArchiving

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/5483//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5483//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5483//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5483//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5483//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5483//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5483//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5483//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5483//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5483//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/5483//console

This message is automatically generated.

fixed line length and missing apache header

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12581000/HBASE-6721_10.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 32 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 6 warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings (more than the trunk's current 0 warnings).

    {color:red}-1 lineLengths{color}.  The patch introduces lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.util.TestHBaseFsck

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/5485//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5485//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5485//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5485//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5485//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5485//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5485//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5485//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5485//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5485//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/5485//console

This message is automatically generated.

The current state of this issue is confusing. The parent is in 'Patch Available' state but seems stale. Subtask #3 has been committed to trunk/0.96 and 0.94. Other subtasks are resolved as 'Invalid'. We should at least have a release note describing what is going to be in 0.96 (and already in 0.94). What further work is going to be done here? What been abandoned? Has enough functionality been committed already to address the contributors' use case? Did something turn out to be problematic? Nothing more for 0.96 since [~stack] has frozen 0.95, correct? Nothing more need in 0.96 to support placing tables on certain regionservers only (will help with 0.96 to 0.98 migration story)? I can research the answers to my questions in commit history but maybe we can get an authoritative answer from [~avandana] or [~toffer]?

{quote}
The current state of this issue is confusing. The parent is in 'Patch Available' state but seems stale.
{quote}
The decision was to get the NS patch in and this can follow. Hence it got stale. 

{quote}
Subtask #3 has been committed to trunk/0.96 and 0.94. Other subtasks are resolved as 'Invalid'. We should at least have a release note describing what is going to be in 0.96 (and already in 0.94).
{quote}
Subtask #3, is a generic requirement, support for MasterCoprocessor Endpoints. It's an implementation specific to this jira. And so is the nature of subtask #6. Do we need release notes for this? I'm not familiar with what needs to get into release notes. Or how do I update them?

{quote}
What further work is going to be done here? What been abandoned? Has enough functionality been committed already to address the contributors' use case? Did something turn out to be problematic? 
{quote}
It essentially needs a rebase. Also I'd like to update it to make use of FavoredNodes. This jira and subtask #4 needs to be completed to get basic functionality.

{quote}
Nothing more for 0.96 since stack has frozen 0.95, correct? Nothing more need in 0.96 to support placing tables on certain regionservers only (will help with 0.96 to 0.98 migration story)? I can research the answers to my questions in commit history but maybe we can get an authoritative answer from Vandana Ayyalasomayajula or Francis Liu?
{quote}
In terms of backwards compatilibity issues the only piece missing was updating the LoadBalancer interface which is subtask #6. Tho I did a recheck. The LoadBalancer interface also needs a configure/initialize method (my bad), see LoadBalancer in the patch. Hopefully we can get that in 0.96. If not we'll have to write code to support old LoadBalancers which doesn't have this method.

My memory is a bit rusty but essentially the rest of the code resides in the custom balancer and the coprocessors. Maybe some internal code changes in AM. So no backwards compatibility issues apart from the one I mentioned. I'll double check early next week.



{quote}
It's an implementation specific to this jira.
{quote}
Sorry I meant "it's _not_ an implementation specific...."

This is to be finished post-0.96 then.  We should move the issue out [~toffer].  Adding an init/configure to LoadBalancer could come in into 0.96 but would have to be available real fast.

Yep post-0.96 is fine. It could be backported to minor versions if need be. Is getting the init/configure patch up tonight fast enough?

[~toffer] yes

Thanks for clearing it up guys.

Edit: Would be good if we can get this into 0.98 and also a 0.96 minor, then there's a great rolling upgrade story. Please let me know if I can be helpful there.

Moving out new feature.  Chatting w/ Francis Liu last night, could be good one for 0.98.

Well then we couldn't use this to stage a 0.96 to 0.98 upgrade on just one placement group. I think that would be a great test of our compatibility story. Any reason this can't come in on a minor 0.96.x release?

Its moved out because no dev going on and it a feature.

bq.  Any reason this can't come in on a minor 0.96.x release?

Because 0.98 will be weeks behind and groups+tags will be reason to upgrade

Sorry, should have said also that I do not see us needing groups to try out a 0.98 daemon in a 0.96 cluster.

Draft of trunk patch. I still need to move group apis from CP to part of core api. Everything else should remain the same so most of the patch shouldn't change pending review comments.

Review board here:

https://reviews.apache.org/r/27673/

@Francis:
I went through the updated patch but didn't seem to find utilization of FavoredNodes.
Would that be handled in separate JIRA ?

Getting Hadoop QA run on this patch is desirable.

[~ted_yu] Yes, that would be in a separate Jira. This is patch is already largish. How do I get Hadoop QA to run?

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679918/HBASE-6721_trunk2.patch
  against trunk revision .
  ATTACHMENT ID: 12679918

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 27 new or modified tests.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 7 warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 3841 checkstyle errors (more than the trunk's current 3788 errors).

    {color:red}-1 findbugs{color}.  The patch appears to introduce 8 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc ListTablesOfGroup(.ListTablesOfGroupRequest) returns (.ListTablesOfGroupResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.GetGroupInfoOfTableRequest) returns (.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupOfServer(.GetGroupOfServerRequest) returns (.GetGroupOfServerResponse);</code>
+       * <code>rpc ListServersInTransition(.ListServersInTransitionRequest) returns (.ListServersInTransitionResponse);</code>
+     * <code>rpc ListTablesOfGroup(.ListTablesOfGroupRequest) returns (.ListTablesOfGroupResponse);</code>
+     * <code>rpc GetGroupInfoOfTable(.GetGroupInfoOfTableRequest) returns (.GetGroupInfoOfTableResponse);</code>
+     * <code>rpc GetGroupOfServer(.GetGroupOfServerRequest) returns (.GetGroupOfServerResponse);</code>
+     * <code>rpc ListServersInTransition(.ListServersInTransitionRequest) returns (.ListServersInTransitionResponse);</code>
+  public void listTablesOfGroup(RpcController controller, RSGroupProtos.ListTablesOfGroupRequest request, RpcCallback<RSGroupProtos.ListTablesOfGroupResponse> done) {
+  public void getGroupInfo(RpcController controller, RSGroupProtos.GetGroupInfoRequest request, RpcCallback<RSGroupProtos.GetGroupInfoResponse> done) {

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/checkstyle-aggregate.html

                Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/11725//console

This message is automatically generated.

I left some comments in RB, but I wanted to comment on high level things here. 

I think this is generally a useful feature, since we are seeing more and more multi-tenant use cases, and without really good isolation rs groups allow the cluster to be shared more easily. 
 - I was reading up on the recent YARN label feature (YARN-796), since the ideas are very similar. However, in YARN labels, a node can have multiple labels, versus here, a server can only have one group. Will it be better to have one-to-many relationship from servers to groups? All servers will have the default group, as well as any more groups that it is assigned. We can do the same thing for tables as well. By default, tables will belong to group default, but can be added more. For assignment, we just look at all the cumulative list of servers that this region will be assignable to. Will this complicate assignment? 
 - Alternatively we can also name this labels (just a suggestion). 
 - For all new features, I think it is fair to request one single source of truth and also some more transactional guarantees for operations. If possible, we should not use zk at all (for caching as done in patch). Instead we can just open the region from master. This is different than the co-location discussion for master and meta. This table is tiny, and not query'able from clients. The data is just there for the master to access. If we do this, we do not even have to have a cache. 
 - I am ok with bringing this as a core functionality rather than CP + LB. The reasoning is that as clusters grow bigger, more users might be interested in this for better isolation.



Enis, thanks for the review:

{quote}
Will it be better to have one-to-many relationship from servers to groups? All servers will have the default group, as well as any more groups that it is assigned. We can do the same thing for tables as well. By default, tables will belong to group default, but can be added more. 
{quote}
I skimmed through YARN-796 doc. To me it seems the notion of yarn labels and rs groups are very different. Labels are intended to add attributes to certain resources (ie gpu, high-memory, etc) so you can request for specific attributes when requesting for resources. While groups mentioned in this jira is meant to identify a logical grouping of resources, meant to guarantee an amount of resources/capacity for a tenant. We have not seen a need for it to date. What do we gain from doing the same for tables? It seems to me the purpose of groups has no need for overlaps either way. Please correct me if I missed something. 

{quote}
For all new features, I think it is fair to request one single source of truth and also some more transactional guarantees for operations. If possible, we should not use zk at all (for caching as done in patch). Instead we can just open the region from master. This is different than the co-location discussion for master and meta. This table is tiny, and not query'able from clients. The data is just there for the master to access. If we do this, we do not even have to have a cache.
{quote}
I have thought of this as well, we can do this but it would require handling group table in a special way. ie make sure it is assigned after meta and before any other table. have a groupWAL, have it's one sets of handlers similar to meta, etc. As well as have a special case for making sure meta and root end up in their designated groups even when group table hasn't been assigned yet. So we're essentially trading one set of complexities for another. Having said that, I'm ok with either approach. Should write up a patch with this approach?

On a related note, region servers consume group information from ZK similar to security ACL. This is used in replication and other features we are working on. As mentioned it is possible to remove caching usage of ZK but it would be good to keep the information in there until the same facility security acl will eventually be migrating to is available. Thoughts?

{quote}
I am ok with bringing this as a core functionality rather than CP + LB. The reasoning is that as clusters grow bigger, more users might be interested in this for better isolation.
{quote}
Great with this we can do the approach mentioned earlier if need be.



I still don't see the need for the added complexity for most users. HBase is running the risk of being too big and too complex for what most users will use it for.

The real answer to get large clusters is to get multi-tenancy to just work.

The best answer is not to add a more different admin tools, config options, and things that add more operational overheard. As you add more and more machine to HBase the answer is not to make more and more special cases. It's instead to make the base case better. Get QOS, quotas, tiered storage, and better load balancing to work well and the whole idea of group based assignment is then useless. That's what should be solution for users of HBase. Not more things that need to be explained in the book for the default config/install. Things like:

* does locality based load balancing work with groups?
* What should hbck return when there's a region that can't be assigned due to groups?
* Does hbck know about groups? If it does then what's the message there when I haven't used that feature? If not then will it break things when I use hbck ?
* How does the open region command work with the groups ?
* Are the commands listed if the balancer isn't the group based balancer ?
* What's the default group?
* Is meta/namespace/acl in the default group?
* I had zookeeper get deleted/go corrupt/ get restored from backup why are all of my regions moving around?

All of those are added complexities that real users will run into, and in return only a very few will get any added benefit.

HBase can't provide the great qos and multi-tenancy right now, so the group based assignment might work well for a specific use case. However it adds complexity and layers of abstraction that hurt most users, while making the ideal solution harder. For that reason I'm against anything that's built into the the default install. 

HBase should be more like nginx and less like apache in this regard. If you need an added feature and it's not what most users want then compile/package it in. It should not be a kitchen sink with 10 million different xml configs needed. The first question asked on every posting to user@ mailing list shouldn't always be "can you post your configs?"

With all that said I do not want this to seem like I dislike the feature, the code, or the contribution. I really want to say thanks to [~toffer]. I would be a +1 if the code was a co-proc and a different module, something that was much less invasive on the main assignment, and master code.

Let me 2nd that. Many thanks to [~toffer], and also that some of the changes are very complex and pretty invasive.
Even at Salesforce we do not worry too much cross tenant impact, we rather throw enough resources at it and let HBase do the work, when we see issues we look into general fixes such as the ones that [~eclark] mentions above.

Now, Yahoo is a big place with very large clusters and many users on those clusters. If the need arises there for this kind of mulit tenancy, we'd better listen to them and to Francis, before we decide this it not the right path forward.


This is the reason why we have started the discussion thread on dev@ some time ago: http://search-hadoop.com/m/DHED47vIer1. 

RS group based assignment is already a plugin implementation, but we thought that bringing it as a "core" feature made sense because it will take some time to have full QOS and isolation. In the mean time, RS group based assignments are a reasonable solution that solves a real problem (although at Yahoo!'s scale). However, we are seeing more interest in this patch from other users as well. 

If there is strong objection for this being a core feature, I think we should still commit this as a co-proc based implementation. 

It could be useful to have a list of pros and cons for coprocessor implementation vs. core integration. Are we considering core integration for mainly code aesthetic reasons, for example, or are there significant obstacles imposed by a coprocessor implementations strategy? 

bq. If there is strong objection for this being a core feature
I'm not objecting. If this is needed we should chose the path of least future pain. Coprocessors lead to pain (as seen with Phoenix, where it is hard to implement core features without using some of our private interfaces), core changes lead to pain too.

bq. It could be useful to have a list of pros and cons for coprocessor implementation vs. core integration.
+1


Thanks for the interest and attention guys. I'll put up a seed list of pros and cons over the weekend. And we can hammer out concerns there.  

Similar to namespaces, regionserver groups can actually be enabled by default and existing users wouldn't notice any difference (except for the extra table that needs to be assigned). Unless they actually use it (similar to namespaces). 

FWIW I believe this feature is useful even for small clusters as long as you have a variation of tenants (or just misbehaving tenants). There are bunch of scenarios that need protection/isolation/QoS. ie cascading failures caused by a tenant, block cache thrashing, different configurations (ie GC configs), different requirements. This becomes even more important in a mixed workload environment (MR, storm, etc).  


Here's the doc as promised:

https://docs.google.com/document/d/1tUCDEEEFknn-HYlDqA72gtv02_nVsoKY_Er_8nBoW_0/edit

Comments are enabled ping me if anyone want's edit access to the doc.

I've also update the design doc. 

Fire away.



We discussed this during HBaseCon. The consensus was to get this into a feature branch to address any stability/integration issues as well as any other concerns. 

[~jmhsieh] [~eclark] [~enis] How to we go about creating the branch? Should we finish the review and merge it in? Should I rebase?

[~toffer].  Here's my suggestion: Let's finish the current review in review board.  Then instead of rebasing, let's use that patch from the point you branched at and start a 'hbase-6721' from there.   We can do periodic merges into the 'hbase-6721' branch afterwards and capture the changes due to the merges.

Works for me.

Looks like branch hbase-6721 has not been created.

Should we proceed with what Jon suggested above ?

Creating the branch is easy. Was the review on RB finished? 


Unless I'm missing something it seems the review is still pending. 

[~jmhsieh] would you still be able to complete the review? :-)

Sorry, I didn't realize I was blocking.  I can't commit to doing a review in the short term -- please proceed without me.  Let me suggest that we commit what we have with a cursory review to a branch and then make progress there.



bq.  Let me suggest that we commit what we have with a cursory review to a branch and then make progress there.

+1
Ping me for assistance when you're ready to commit to a branch [~toffer] 

Thanks [~apurtell] let me go through the patch and rebase it as well. Will put up an updated patch this week.

Rebased patch.

[~apurtell] I rebased the patch onto trunk. This should be good enough to commit to a branch. 

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12747704/HBASE-6721_11.patch
  against master branch at commit 05de2ec5801fbba4577fb363f858a6e6f282c104.
  ATTACHMENT ID: 12747704

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 30 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 6 warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 1908 checkstyle errors (more than the master's current 1864 errors).

		    {color:red}-1 InterfaceAudience{color}.  The patch appears to contain InterfaceAudience from hadoop rather than hbase:
             +import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceAudience;.

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings (more than the master's current 0 warnings).

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc getLastMajorCompactionTimestamp(.hbase.pb.MajorCompactionTimestampRequest) returns (.hbase.pb.MajorCompactionTimestampResponse);</code>
+       * <code>rpc getLastMajorCompactionTimestampForRegion(.hbase.pb.MajorCompactionTimestampForRegionRequest) returns (.hbase.pb.MajorCompactionTimestampResponse);</code>
+       * <code>rpc getProcedureResult(.hbase.pb.GetProcedureResultRequest) returns (.hbase.pb.GetProcedureResultResponse);</code>
+       * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupInfoOfServer(.hbase.pb.GetGroupInfoOfServerRequest) returns (.hbase.pb.GetGroupInfoOfServerResponse);</code>
+       * <code>rpc MoveServers(.hbase.pb.MoveServersRequest) returns (.hbase.pb.MoveServersResponse);</code>
+       * <code>rpc MoveTables(.hbase.pb.MoveTablesRequest) returns (.hbase.pb.MoveTablesResponse);</code>
+       * <code>rpc RemoveGroup(.hbase.pb.RemoveGroupRequest) returns (.hbase.pb.RemoveGroupResponse);</code>
+       * <code>rpc BalanceGroup(.hbase.pb.BalanceGroupRequest) returns (.hbase.pb.BalanceGroupResponse);</code>

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/14922//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14922//artifact/patchprocess/patchReleaseAuditWarnings.txt
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14922//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/14922//artifact/patchprocess/checkstyle-aggregate.html

                Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/14922//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/14922//console

This message is automatically generated.

Thanks [~toffer]. I applied your latest patch to master and pushed the result as new branch "hbase-6721". I checked that it compiled before pushing but didn't run tests.

Do you need this for 0.98? If so, we can do another branch "hbase-6721-0.98" with a backport and rebase it at every RC. Or branch-1.

Thanks [~apurtell], that'd be great let me work on one.

As I've been studying this code, in order to aid with the understanding of HBASE-6721, I’ve created some UML sequence diagrams of functions within GroupBasedLoadBalancer.java. This UML diagram only focuses on GroupBasedLoadBalancer, since GroupBasedLoadBalancer, along with GroupInfoManager, GroupInfoManagerImpl and GroupInfo as well as some way to configure groups represent the core functionality needed for this implementation. This represents the 1,500 core lines implementing the region server group functionality; this documentation does not cover the extensive configuration management via CLI stored in Zookeeper and an HBase table.

The goal of {{GroupBasedLoadBalancer}} is to separate servers and regions into groups, and to balance within groups using another load balancer (referred to as internal load balancer). The default load balancer used is {{StochasticLoadBalancer}}. Each group is stored as a {{GroupInfo}} object, and {{GroupInfoManagerImpl}} holds a collection of {{GroupInfo}} objects with APIs allowing for the manipulation of this collection. These groups can be created and configured via the CLI, and are stored within an HBase table as well as Zookeeper. As {{GroupBasedLoadBalancer}} implements the {{LoadBalancer}} interface, it has the following functions from the interface: 
As GroupBasedLoadBalancer implements the LoadBalancer interface, it has the following functions from the interface: 
•	balanceCluster
•	roundRobinAssignment
•	retainAssignment
•	immediateAssignment
•	randomAssignment
•	initialize
•	isStopped
•	setGroupInfoManager

Additionally, GroupBasedLoadBalancer has several helper functions:
•	offlineRetainAssignment
•	onlineRetainAssignment
•	generateGroupMaps
•	filterOfflineServers
•	filterServers
•	getMisplacedRegions
•	correctAssignments
•	isOnline
•	getGroupInfoManager

Through reading the code for the load balancer portion of HBASE-6721, I have a few questions:
1.	Within the interface GroupInfoManager.java, I noticed that the function getGroupOfServer returns a GroupInfo object, but the function getGroupOfTable returns a String object. Was there a performance consideration or some other reason for returning a string? (It seems the API would be more consistent to return a GroupInfo object.)
2.	For the function onlineRetainAssignment why are regions assigned to bogus so it ends up in RIT if a server is not available? (We would like to keep as few regions in RIT as possible in order to maximize our availability.)
3.	On the topic of onlineRetainAssignment, what is the objective for separating online and offline servers? I noticed that another balancer such as the StochasticLoadBalancer does not make such a distinction.

Lastly, the UML diagram I created can be edited by downloading the attached XML file and editing with http://draw.io.


draft of backported patch to 98.  need to run integration and unit tests then it should be good to go.

{quote}
1.	Within the interface GroupInfoManager.java, I noticed that the function getGroupOfServer returns a GroupInfo object, but the function getGroupOfTable returns a String object. Was there a performance consideration or some other reason for returning a string? (It seems the API would be more consistent to return a GroupInfo object.)
{quote}
Yeah performance. I didn't want to have to gather the information to generate the GroupInfo object since most internal calls won't need it. The external api call is consistent tho.

{quote}
2.	For the function onlineRetainAssignment why are regions assigned to bogus so it ends up in RIT if a server is not available? (We would like to keep as few regions in RIT as possible in order to maximize our availability.)
{quote}
This guarantees isolation. Which is one of the key features of Region Server Groups. ie you don't want the reason that the region from one group to run out of live servers to affect the regions in another group. So worst case you affect the availability of the group and not the entire cluster. What's your use case for region server groups?

{quote}
3.	On the topic of onlineRetainAssignment, what is the objective for separating online and offline servers? I noticed that another balancer such as the StochasticLoadBalancer does not make such a distinction.
{quote}
Yes that's inherent in GroupBaseLoadBalancer. Stochastic only looks at what's online, while Group needs to look at which member servers are online.


{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12750267/HBASE-6721_98_1.patch
  against master branch at commit 9c69bf766fcad024bef5760f242cae2bc609b374.
  ATTACHMENT ID: 12750267

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 33 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/15084//console

This message is automatically generated.

bq. draft of backported patch to 98. need to run integration and unit tests then it should be good to go.

Thanks [~toffer], let me know when you think it's ready to go in and I'll make a branch with its application

Tests passed tho I had to patch HBase/IntegrationTestingUtilitily to get the tests to run properly, there was a regression in an api that used to work in distributed mode and now it doesn't. Will deal with that issue in a separate jira. 

Reconciled some changes with internal implementation, rebased. 

[~apurtell] the patch should be good to commit. will need to create an addendum for trunk patch.

bq. the patch should be good to commit.

I made a branch 'hbase-6721-0.98' with the v2 0.98 patch applied and pushed it.

bq.  will need to create an addendum for trunk patch.

Please post the addendum here, I'll commit it to hbase-6721 branch.

When updating the feature branches do you prefer me to merge or rebase? I typically rebase in my own work but will do what you prefer, just say.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12750554/HBASE-6721_98_2.patch
  against master branch at commit 45aafb25b7911b5917ab47133e3e4268806e4c91.
  ATTACHMENT ID: 12750554

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 33 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/15104//console

This message is automatically generated.

Reattaching 'HBASE-6721_0.98_2.patch' with '0.98' in the name to match the branch-specific filter.

I ran the binary compatibility checker tool comparing hbase-6721-0.98 with 0.98.13. The tool says hbase-6721-0.98 is 100% binary compatible with 0.98.13, with some source level incompatibility of mild concern in the MasterObserver interface. 

Do we need GroupAdmin and GroupAdminClient now? A hold over from an initial coprocessor based implementation?



{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12750586/HBASE-6721_0.98_2.patch
  against 0.98 branch at commit 45aafb25b7911b5917ab47133e3e4268806e4c91.
  ATTACHMENT ID: 12750586

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 33 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 28 warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 3913 checkstyle errors (more than the master's current 3876 errors).

		    {color:red}-1 InterfaceAudience{color}.  The patch appears to contain InterfaceAudience from hadoop rather than hbase:
             +import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceAudience;.

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings (more than the master's current 0 warnings).

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +        public GetGroupInfoResponse getGroupInfo(RpcController controller, GetGroupInfoRequest request) throws ServiceException {
+        public GetGroupInfoOfTableResponse getGroupInfoOfTable(RpcController controller, GetGroupInfoOfTableRequest request) throws ServiceException {
+        public GetGroupInfoOfServerResponse getGroupInfoOfServer(RpcController controller, GetGroupInfoOfServerRequest request) throws ServiceException {
+        public MoveServersResponse moveServers(RpcController controller, MoveServersRequest request) throws ServiceException {
+        public MoveTablesResponse moveTables(RpcController controller, MoveTablesRequest request) throws ServiceException {
+        public AddGroupResponse addGroup(RpcController controller, AddGroupRequest request) throws ServiceException {
+        public RemoveGroupResponse removeGroup(RpcController controller, RemoveGroupRequest request) throws ServiceException {
+        public BalanceGroupResponse balanceGroup(RpcController controller, BalanceGroupRequest request) throws ServiceException {
+        public ListGroupInfosResponse listGroupInfos(RpcController controller, ListGroupInfosRequest request) throws ServiceException {
+       * <code>rpc GetGroupInfoOfTable(.GetGroupInfoOfTableRequest) returns (.GetGroupInfoOfTableResponse);</code>

    {color:red}-1 site{color}.  The patch appears to cause mvn post-site goal to fail.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

     {color:red}-1 core zombie tests{color}.  There are 5 zombie test(s): 	at org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.testLogRolling(TestLogRolling.java:219)
	at org.apache.hadoop.hbase.TestIOFencing.testFencingAroundCompaction(TestIOFencing.java:227)
	at org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.testRITStateForRollback(TestSplitTransactionOnCluster.java:292)
	at org.apache.hadoop.hbase.io.encoding.TestEncodedSeekers.testEncodedSeeker(TestEncodedSeekers.java:118)
	at org.apache.hadoop.hbase.TestClusterBootOrder.testBootRegionServerFirst(TestClusterBootOrder.java:104)

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/15105//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/15105//artifact/patchprocess/patchReleaseAuditWarnings.txt
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/15105//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/15105//artifact/patchprocess/checkstyle-aggregate.html

                Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/15105//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/15105//console

This message is automatically generated.

Thanks [~ndimiduk] wasn't aware of that feature.

{quote}
I ran the binary compatibility checker tool comparing hbase-6721-0.98 with 0.98.13. The tool says hbase-6721-0.98 is 100% binary compatible with 0.98.13, with some source level incompatibility of mild concern in the MasterObserver interface. 
{quote}
This is prolly because of the new methods this patch adds to the interface. This shouldn't be a concern then?

{quote}
Do we need GroupAdmin and GroupAdminClient now? A hold over from an initial coprocessor based implementation?
{quote}
This was based on review comments from [~enis] tho I'm not sure if the comment still stands post-cp implementation. I am fine either way. Should this be in HBaseAdmin then?

https://reviews.apache.org/r/27673/diff/2?file=751817#file751817line52
https://reviews.apache.org/r/27673/diff/2?file=751816#file751816line35



{quote}
bq. Do we need GroupAdmin and GroupAdminClient now? A hold over from an initial coprocessor based implementation?
This was based on review comments from Enis Soztutar tho I'm not sure if the comment still stands post-cp implementation. I am fine either way. Should this be in HBaseAdmin then?
{quote}
I opened HBASE-14226 to discuss merging them.

{quote}
bq. The tool says hbase-6721-0.98 is 100% binary compatible with 0.98.13, with some source level incompatibility of mild concern in the MasterObserver interface.
This is prolly because of the new methods this patch adds to the interface. This shouldn't be a concern then?
{quote}
Yes, it will be a concern for things like Apache Phoenix. (See their IndexMasterObserver, etc.) We can handle this by using compatibility helpers that won't attempt to invoke the new APIs on MasterObservers that do not implement them. We do something like this in WALCoprocessorHost. 

bq. This was based on review comments from Enis Soztutar tho I'm not sure if the comment still stands post-cp implementation. I am fine either way. Should this be in HBaseAdmin then?
This comes back to the discussion of whether RS groups are coprocessor based or core feature. If a core feature, then GroupAdmin methods should move to Admin interface. Otherwise, a GroupAdmin interface + a way to construct the GroupAdminClient would be the way to go. 

[~apurtell] Attached is the addedum patch which addresses three things: 1. Forward port some changes from 0.98 feature branch, 2. Address CP backward compatibility support, 3. Address removal of GroupAdmin and GroupAdminClient and moving the apis to Admin. Let me know if you'd like me to break these up into separate patches. Didn't do that as I thought it'd be more complicated.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12752029/HBASE-6721_hbase-6721_addendum.patch
  against master branch at commit 9334a47d4570f8adfc003f0fb2c5969a88c3bba0.
  ATTACHMENT ID: 12752029

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 21 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/15222//console

This message is automatically generated.

bq. Let me know if you'd like me to break these up into separate patches. 
Yes, let's do that. Now that we have feature branches with the previous patches posted here committed, all new work are addendums. It's best IMHO if the addendums have their own JIRA identifiers so we can collect patches and any discussion for each on separate tickets instead of all lumped in here. 
bq. 1. Forward port some changes from 0.98 feature branch
This needs committing to the master branch only. I suggest opening a subtask for it.
bq. 2. Address CP backward compatibility support
This needs committing to both master and 0.98 branches. Work should be done on HBASE-14232. Pretty sure we will need two patches, one for master, one for 0.98
bq. 3. Address removal of GroupAdmin and GroupAdminClient and moving the apis to Admin. 
This needs committing to both master and 0.98 branches. Work should be done on HBASE-14226. Pretty sure we will need two patches, one for master, one for 0.98

[~apurtell] created the patches in separate jiras. The patches are built on top of each other. The order is: HBASE-14312, HBASE-14232, HBASE-14226. Let me know if you'd like to rebase the succeeding patches once the one(s) prior are committed.

I'll create 98 whenver a trunk patch is committed.

bq. Let me know if you'd like to rebase the succeeding patches once the one(s) prior are committed.

I'm thinking we start carving out work as subtasks, applying patches from the subtasks to hbase-6721 branch, make the 0.98 versions of them and apply them to hbase-6721-0.98 branch. This will make development on these branches almost exactly like dev on a release branch.

On rebase vs. merge:
1. I can rebase, fix up for changes, then force push, for both hbase-6721 and hbase-6721-0.98
2. I can merge master into hbase-6721 (and 0.98 into hbase-6721-0.98), fix up for changes, commit the fixups in a merge commit, and then push the merged result.

There are pros and cons to either approach. What would work best for you [~toffer]?

Chatted with [~apurtell] at the powow. We'll be doing rebases when pulling in changes from master/0.98.

bq. Chatted with Andrew Purtell at the powow. We'll be doing rebases when pulling in changes from master/0.98.
So how are we gonna do reviews? I think only the master patch reviews are fine, but it would be good if we list the RBs here. 

I was thinking review patches on subtasks as normal. Once the work in total is ready to attempt a branch merge we can put up a single merge patch for here and RB. At any time it's possible to generate omnibus patches for RB if that would be helpful. 

Sounds good. I don't have any other changes pending so I'm going to update RB with the new patch on trunk. 

[~apurtell] thanks for taking care of the backport to 0.98 for the patches.

Here's a new RB request:

https://reviews.apache.org/r/27673/

Sorry it's not a new RB request I updated the old one. Was thinking wether I should just create a new RB request. 

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12753020/HBASE-6721_12.patch
  against master branch at commit cc1542828de93b8d54cc14497fd5937989ea1b6d.
  ATTACHMENT ID: 12753020

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 33 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/15317//console

This message is automatically generated.

I'm still officially -1 as long as this is built into the core. 99.99%(assuming 10k hbase users)  of HBase users should never ever run something like this. It will make an already very operationally complex system un-workable. Because of that anything that's in the default code, adds to the default admin, and is built in is something I can't see ever being ok with.

All of this has already been tried at FB and it was a mistake.  This ends up looking functionally very similar to 0.89-fb's favored nodes. ( Only assign regions to specific set of machines that's configured by the admin ). It's so bad that almost every time we try and solve an issue on a cluster with favored nodes, the first thing we do is turn off the balancer so that we don't have to worry about which nodes are configured to have with regions. That's literally step one of debugging. Turn off this feature.
We'll have a party when FB no longer has this operational nightmare. I won't sign anyone up for the same. I won't sign myself up for the same.

So I'm -1 on anything that I can't completely remove. RM -RF.

* Assignment manager is already too complex adding more complexity is awful
* region movement is already too stateful. Adding more is awful
* configuration of HBase is already way too complex. Multiplying that with multiple groups is awful.
* Admin already has way too many things for users to do that cause issue. Adding more ways for a cluster to be borked is awful.

{quote}
All of this has already been tried at FB and it was a mistake. This ends up looking functionally very similar to 0.89-fb's favored nodes. ( Only assign regions to specific set of machines that's configured by the admin ). It's so bad that almost every time we try and solve an issue on a cluster with favored nodes, the first thing we do is turn off the balancer so that we don't have to worry about which nodes are configured to have with regions. That's literally step one of debugging. Turn off this feature.
We'll have a party when FB no longer has this operational nightmare. I won't sign anyone up for the same. I won't sign myself up for the same.
{quote}
IMHO that's not an objective comparison. Favored Nodes and Region Server groups are very different. Their use cases are very different and their implementations are also very different.  

As for how useful it is for us (and potenitally for others), if we actually removed region server groups I'm pretty sure our HBase team and SEs would revolt :-). If we didn't have this feature we would be managing around 80 hbase clusters right now instead of the 6 multi-tenant cluster we are currently running. Step one of debugging is not turning of the balancer that would make things worse. In fact one of the useful features of region server groups is quickly isolating tables to a new group if they are misbehaving or their workload has changed. This can be done in a few minutes if not seconds. 

{quote}
Assignment manager is already too complex adding more complexity is awful
{quote}
If you look at the patch, the change in AM is an extra 20 lines of code. 6 lines are just bugfixes that should be done to AM anyway and the other 14 lines which is fairly straightforward we can even live without if that's what it takes.

{quote}
region movement is already too stateful. Adding more is awful
{quote}
Adding more states?

{quote}
Configuration of HBase is already way too complex. Multiplying that with multiple groups is awful.
{quote}
Not sure what the concern here is? That there's an option to configure a different balancer? 







Agreed we should evaluate these changes on their own merit. 

No core changes at all isn't a reasonable precondition because even with implementing as a coprocessor based feature we'd need new hooks. Those hooks would touch the same places. IMHO implementing as a coprocessor based feature makes little sense given the relatively small changes needed. it would unnecessarily pollute our space of admin APIs and shell commands with seperate APIs for optional feature like we have with security. 

Agreed the ops impact should be totally optional. Where these changes modify existing function it's reasonable to make sure the modified behavior all sits behind configuration based conditionals. 

I also find this a lot less impactful than MOB yet that went in. I think we need to be rational about the risks this change actually imposes. 

bq.I also find this a lot less impactful than MOB 
This is touching the code that empirically has been the worst part of hbase, and making it more complex. It's adding things that require user configuration. Adding things that require user interaction on failure (failed open is an awful thing to do to a table).

This adds another table that's required for the master to interact with for assignment and region movement. Right now master is really struggling because of the feedback loop of rit -> meta -> meta moves -> rit. Now we have double the number of tables.

On top of that it adds in more things to Zk while we're trying to remove as much data from there as possible.

This patch breaks ipv6.
This patch has Copy Paste code.
This patch doesn't have the correct headers.
This patch doesn't load meta's hri from disk on move.

bq. IMHO that's not an objective comparison. Favored Nodes and Region Server groups are very different. 
Not really they both are user configured places to put regions. Both features interact with the balancer and AM. Both features affect the cluster in disasters in basically the same way.

bq.Adding more states?
Failed open due to bogus servers. So an old state used in a new way. That means something different than it did before.

bq.That there's an option to configure a different balancer?
There are tons more configurations than that. There's namespace goups, table groups and groups for regionservers to be part of. Then there's all of the different configurations for the servers that are in different groups.

This is the worst possible way to do multi-tenancy. We're layering a hack on rather than do the real thing.
You're getting the same number of SPOF's and an added operational complexity.

If we want multi-tenancy, then lets do that right. We shouldn't accept hacks that don't help most users and make getting to the correct solution harder.



bq. This patch breaks ipv6.
bq. This patch has Copy Paste code.
bq. This patch doesn't have the correct headers.
bq. This patch doesn't load meta's hri from disk on move.
These are review comments that are not related to the discussion right now. 

bq. If we want multi-tenancy, then lets do that right. We shouldn't accept hacks that don't help most users and make getting to the correct solution harder.
I don't understand your reasoning. Yahoo has been running with this for >1 year, and we have other users that will run with RS groups once it is  in. The way I see it is not a hack, but a step towards better guarantees. Multi-tenancy and isolation will never be perfect and will always be an ongoing effort to improve it. For example, without making the coprocessors run in a colocated container, we cannot isolate one tenants co-processors from the others today. Saying that we should fix EVERY multitenancy/isolation problem which can take years is not realistic. 

bq. Agreed the ops impact should be totally optional.
+1 to this. I think with the patch it is already the case. If you are not using the feature, you should not be affected in any way. Yes, with RS groups, if you do not know what you are doing, it is more states and more things can go wrong, but we are not making it default or anything. Memcache based block cache is only used by FB and it is just added complexity for remaining users, but we have it regardless. 

bq.we have other users that will run with RS groups once it is in. 
We're failing all of those users then.

bq.Memcache based block cache is only used by FB and it is just added complexity for remaining users, but we have it regardless.
It's a pluggable thing with it off and no added complexity to the default path. If you think it should be in a different module I would be amenable to moving it.
As i've said before that's what I think we should do here. Make it pluggable with no changes to the core and I'm fine with it.

{quote}
It's adding things that require user configuration. Adding things that require user interaction on failure (failed open is an awful thing to do to a table).
{quote}
This is a key feature, it's better to penalize a misbehaving table than have it affect and potentially take down the entire cluster. This will only happen if all the servers are down analogous to a cluster being down. It's pretty intuitive. We can add a better message if need be. 

{quote}
This adds another table that's required for the master to interact with for assignment and region movement. Right now master is really struggling because of the feedback loop of rit -> meta -> meta moves -> rit. Now we have double the number of tables.
{quote}
It actually doesn't. The table is only read at startup and never again.

{quote}
On top of that it adds in more things to Zk while we're trying to remove as much data from there as possible.
{quote}
Yes, this will be addressed just like the other modules that currently use zk. 

{quote}
This patch breaks ipv6.
This patch has Copy Paste code.
This patch doesn't have the correct headers.
This patch doesn't load meta's hri from disk on move.
{quote}
Thanks for the review. I'll make sure to address them. Can you point out which code is copy paste. Also please elaborate on meta hri? Prolly post it on RB so we don't pollute this discussion?

{quote}
Not really they both are user configured places to put regions. Both features interact with the balancer and AM. Both features affect the cluster in disasters in basically the same way.
{quote}
IMHO this is a huge oversimplification. I agree with [~apurtell] let's evaluate regionserver group on it's own merits. ie User configures regionservers to put tables on not regions. Region servers can be any number while in FN it can be at most (or only?) 3. FN was designed to guarantee performance, RS was meant to guarantee isolation.

{quote}
There are tons more configurations than that. There's namespace goups, table groups and groups for regionservers to be part of.
{quote} 
Right now there's only group and it can have regionservers and tables as members.

{quote}
Then there's all of the different configurations for the servers that are in different groups.
{quote}
This is no different from configuring different clusters and is optional. A really good feature IMHO.

{quote}
This is the worst possible way to do multi-tenancy. We're layering a hack on rather than do the real thing.
You're getting the same number of SPOF's and an added operational complexity.
{quote}
IMHO the operational complexity is better than managing the same number of clusters. I honestly would not like to increase my number of SPOFs that increases the probability of one going down. Also isn't there NN HA already?   

{quote}
If we want multi-tenancy, then lets do that right. We shouldn't accept hacks that don't help most users and make getting to the correct solution harder.
{quote}
IMHO this will help a lot of users. This is not only a multi-tenancy solution but an isolation solution. It will help them isolate different workloads (ie CD/PROD, batch/realtime). It will also help them isolate system tables becoming unavailable because a user access pattern/data surfaced a bug which caused the RS to die. It also allows you to configure each group differently (ie GC settings). It also provides coprocessor isolation.











bq.IMHO this is a huge oversimplification.
All of the bad parts are the same. You still have to config it. You still have really weird behavior on cluster instability.

bq.Right now there's only group and it can have regionservers and tables as members.
The patch has a group attached to namespaces, and also provides the ability to move tables to different groups.

bq.IMHO this will help a lot of users.
Every user that's running this that doesn't have 10K machines is making a huge mistake. And we as the HBase developers should provide the better solution not the easiest solution. Not the one that works for one company.

bq.This is not only a multi-tenancy solution but an isolation solution. 
Isolation is the worse solution for getting multi-tenancy.

{quote}
bq.This is not only a multi-tenancy solution but an isolation solution.
Isolation is the worse solution for getting multi-tenancy.
{quote}

Maybe you can elaborate on this point? Seems we need to quarantine users from each other, whether that's physically as per this patch or via imposed resource controls within a single process. Either way, "quarantine" is the same as "isolation"; we're isolating users from each other to achieve fairness of service delivery in a multi-tenant environment.

Let's not let the perfect be the enemy of the good. A "proper" multi-layer admission control change isn't on the table, it isn't on anyone's roadmap, it isn't even something proposed on a JIRA and/or in a design document. Even if we have a proposal for HBase this will certainly be considered imperfect and incomplete by some without 100% agreement and a plan at the HDFS level, and getting that is as likely as finding a unicorn wandering around downtown SF. (Ok... maybe a horse dressed to look like a unicorn could be a thing...)  Meanwhile we have a patch on deck and we need to be evaluating it and its contributor's concerns on their merit.

This is something that one of our esteemed users runs in production, is persistent about getting in and responsive to feedback, and both of those things in my opinion should carry a lot of weight. The same kind of weight that previously proposed changes like HFileV2 or the 0.90 master rewrite (remember that?), or the memcache-based block cache carried, or pending IPv6 related changes.

That said, I don't think it's ready to be merged into master. We have it up in a feature branch. Let's continue that, address concerns, make sure it's totally optional for those who don't want it, measure its impact. 

bq.Maybe you can elaborate on this point?
Sure let me write up something more on that point.

bq.Let's not let the perfect be the enemy of the good.
I'm not asking for perfect. I'm asking not to take a half step back for most users so that one user can merge this feature and take a single step forward.

bq.Meanwhile we have a patch on deck and we need to be evaluating it and its contributor's concerns on their merit.
Yeah and I have evaluated it and used my knowledge and judgement and cast my vote on this patch and feature in accordance with the Apache Foundation rules (http://www.apache.org/foundation/voting.html#votes-on-code-modification)  . I have added my technical reasons. I have even outlined what I would need to vote a different way.

bq. the 0.90 master rewrite (remember that?)
Yeah that has since proved to be the wrong way to go. It put way too much in ZK and now we've spent years un-doing it. We would have been better served with asking for parts to be pluggable and not on by default.

bq.or the memcache-based block cache carried
That followed the exact same bar that I'm requesting here. No added complexity on the default use case. I've even moved it into a different module so that it will be exactly what I'm asking for here.

bq. No added complexity on the default use case.

With the group assignment logic changes behind a default off conditional that criteria is met here too. 

bq. Yeah and I have evaluated it and used my knowledge and judgement and cast my vote on this patch and feature in accordance with the Apache Foundation rules (http://www.apache.org/foundation/voting.html#votes-on-code-modification) . I have added my technical reasons

Let me collect them here:

- Don't break ipv6.
- Remove Copy Paste code.
- Add correct headers.
- Load meta's hri from disk on move.
- No added complexity on the default path

I'd like to add:

-  Ops impact should be totally optional: configurable, default off. (This is related to the last point above.)

If I've missed any items please let me know. 

In the interest of moving this feature forward I propose we change the current patch to use a coprocessor-based implementation. The broad strokes of the implementation are:

1. Group APIs will move to a coprocessor endpoint
2. Other hooks into core code will utilize coprocessor observers and possibly add more cp hooks in core as needed.
3. Security checks will be embedded directly into the endpoint, which will be activated if security is enabled.
4. Add new api/semantic in LoadBalancer so that it can tell callees when regions should not be assigned (ie bogus server name).
5. CLI will remain as-is (same as security)

[~eclark] does this work for you?

That works great for me and would address the concerns I had.

Great. 
We can in theory define another Observer type (RSGroupObserver, etc) which will be the endpoint that coprocessors might implement for example for learning about RS groups operations. But it will be a coprocessor having its own co-processors which seems is not needed at the moment.


[~enis] I can do that too and have security make use of it. Should be just as much effort as embedding security directly as I already have the RS group cp hooks in place as part of the current patch?

bq. But it will be a coprocessor having its own co-processors which seems is not needed at the moment.
Yeah, let's avoid that until/unless we really need it

Agreed, that let's keep it simple for now unless needed.

Latest patch attached. The patch makes the implementation cp-based. 

To avoid confusion I've created a new RB entry:

https://reviews.apache.org/r/38224/

[~apurtell] The new patch is based off of current master. Should we just replace the current branch with a new branch to go with the new patch? Also was wondering since this is now cp-based. Do we still need a branch? 

[~eclark] I believe I've addressed most of your concerns. The only thing I missed was "Load meta's hri from disk on move" can you elaborate more one what needs to be done on RB?




{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12754964/HBASE-6721_13.patch
  against master branch at commit 27d3ab43efeabe2a0e1173858b6994b17b5c355b.
  ATTACHMENT ID: 12754964

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 27 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0 2.7.1)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 checkstyle{color}.  The applied patch does not increase the total number of checkstyle errors

    {color:red}-1 findbugs{color}.  The patch appears to cause Findbugs (version 2.0.3) to fail.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupInfoOfServer(.hbase.pb.GetGroupInfoOfServerRequest) returns (.hbase.pb.GetGroupInfoOfServerResponse);</code>
+       * <code>rpc MoveServers(.hbase.pb.MoveServersRequest) returns (.hbase.pb.MoveServersResponse);</code>
+       * <code>rpc MoveTables(.hbase.pb.MoveTablesRequest) returns (.hbase.pb.MoveTablesResponse);</code>
+       * <code>rpc RemoveGroup(.hbase.pb.RemoveGroupRequest) returns (.hbase.pb.RemoveGroupResponse);</code>
+       * <code>rpc BalanceGroup(.hbase.pb.BalanceGroupRequest) returns (.hbase.pb.BalanceGroupResponse);</code>
+       * <code>rpc ListGroupInfos(.hbase.pb.ListGroupInfosRequest) returns (.hbase.pb.ListGroupInfosResponse);</code>
+     * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+     * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>

    {color:red}-1 site{color}.  The patch appears to cause mvn post-site goal to fail.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

     {color:red}-1 core zombie tests{color}.  There are 1 zombie test(s): 

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/15506//testReport/
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/15506//artifact/patchprocess/checkstyle-aggregate.html

  Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/15506//console

This message is automatically generated.

Commenting on RB now.

Addressed comments on RB.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12756192/HBASE-6721_14.patch
  against master branch at commit d2e338181800ae3cef55ddca491901b65259dc7f.
  ATTACHMENT ID: 12756192

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 27 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0 2.7.1)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 6 warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 1859 checkstyle errors (more than the master's current 1835 errors).

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupInfoOfServer(.hbase.pb.GetGroupInfoOfServerRequest) returns (.hbase.pb.GetGroupInfoOfServerResponse);</code>
+       * <code>rpc MoveServers(.hbase.pb.MoveServersRequest) returns (.hbase.pb.MoveServersResponse);</code>
+       * <code>rpc MoveTables(.hbase.pb.MoveTablesRequest) returns (.hbase.pb.MoveTablesResponse);</code>
+       * <code>rpc RemoveGroup(.hbase.pb.RemoveGroupRequest) returns (.hbase.pb.RemoveGroupResponse);</code>
+       * <code>rpc BalanceGroup(.hbase.pb.BalanceGroupRequest) returns (.hbase.pb.BalanceGroupResponse);</code>
+       * <code>rpc ListGroupInfos(.hbase.pb.ListGroupInfosRequest) returns (.hbase.pb.ListGroupInfosResponse);</code>
+     * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+     * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.client.TestAsyncProcess

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/15618//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/15618//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/15618//artifact/patchprocess/checkstyle-aggregate.html

                Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/15618//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/15618//console

This message is automatically generated.

bq. Andrew Purtell The new patch is based off of current master. Should we just replace the current branch with a new branch to go with the new patch?

I would say so if you want it. 

bq. Also was wondering since this is now cp-based. Do we still need a branch?

Up to you. We can rebase or delete, either way let me know.  

Some issues with the current security integration. Coprocessors can't call into the internals of other coprocessors. I understand why this was done, but we can't have it. Coprocessors calling into the internals of other coprocessors, this is a non-negotiable point for the sake of sanity in maintenance of separate optional extensions. It's a catch-22 imposed on this change by the requirement it be a coprocessor only implementation.

What I would suggest is introduce into the MasterObserver API hooks for the group admin APIs. Let the implementation of the group admin APIs and the authoritative security decisions both be separate mix-ins provided by different coprocessors. There needs to be common plumbing for the two. That belongs in MasterObserver. The plumbing could look like:
- MasterObserver support for pre/post group admin API action hooks
- In GroupAdminEndpoint, get the coprocessor host with getMasterCoprocessorHost()
- Invoke the public (technically, LimitedPrivate(COPROC)) APIs for pre/post group admin API actions.
- AccessController implements the new MasterObserver APIs to provide security for the group admin APIs.

This is much more in spirit with current interfaces and audience scoping. It decouples GroupAdminEndpoint from AccessController. (If the AC is not installed, no harm, no NPEs, no security checking (by intention), it's all good.) It also addresses concerns about zero impact in the default case. Those upcalls will never be made unless the GroupAdminEndpoint is installed.

Addressed comments on RB. 

[~apurtell] let me know if the changes I made for security is what you expected. I pretty much just ported the security/cp stuff from the non-cp patch.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12764454/HBASE-6721_15.patch
  against master branch at commit 24370547c500df0026a71944d8be88cd5b51b23e.
  ATTACHMENT ID: 12764454

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 33 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 3 warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 1811 checkstyle errors (more than the master's current 1781 errors).

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupInfoOfServer(.hbase.pb.GetGroupInfoOfServerRequest) returns (.hbase.pb.GetGroupInfoOfServerResponse);</code>
+       * <code>rpc MoveServers(.hbase.pb.MoveServersRequest) returns (.hbase.pb.MoveServersResponse);</code>
+       * <code>rpc MoveTables(.hbase.pb.MoveTablesRequest) returns (.hbase.pb.MoveTablesResponse);</code>
+       * <code>rpc RemoveGroup(.hbase.pb.RemoveGroupRequest) returns (.hbase.pb.RemoveGroupResponse);</code>
+       * <code>rpc BalanceGroup(.hbase.pb.BalanceGroupRequest) returns (.hbase.pb.BalanceGroupResponse);</code>
+       * <code>rpc ListGroupInfos(.hbase.pb.ListGroupInfosRequest) returns (.hbase.pb.ListGroupInfosResponse);</code>
+     * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+     * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.client.TestAsyncProcess

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/15832//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/15832//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/15832//artifact/patchprocess/checkstyle-aggregate.html

                Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/15832//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/15832//console

This message is automatically generated.

I backported the current v15 patch for testing it with 1.1 code base. Just parking it here in case somebody else needs it. I'll get back to the review soon. 

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12768882/hbase-6721-v15-branch-1.1.patch
  against branch-1.1 branch at commit 496d20cfca5a30bc72a29e4ef893424964f9fa91.
  ATTACHMENT ID: 12768882

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 33 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/16242//console

This message is automatically generated.

Finally got around testing the v15 patch on 1.1 code base with a 7 node cluster. Here are my test notes. Nothing too concerning, but we have to address some of these in the patch. This is the configuration to add to enable groups: 
{code}
    <property>
      <name>hbase.coprocessor.master.classes</name>
      <value>org.apache.hadoop.hbase.group.GroupAdminEndpoint</value>
    </property>
    <property>
      <name>hbase.master.loadbalancer.class</name>
      <value>org.apache.hadoop.hbase.group.GroupBasedLoadBalancer</value>
    </property>
{code}


1. Need to add this diff, so that new PB files get compiled with -Pcompile-protobuf command: 
{code}
diff --git hbase-protocol/pom.xml hbase-protocol/pom.xml
index 8034576..d352373 100644
--- hbase-protocol/pom.xml
+++ hbase-protocol/pom.xml
@@ -180,6 +180,8 @@
                           <include>ErrorHandling.proto</include>
                           <include>Filter.proto</include>
                           <include>FS.proto</include>
+                          <include>Group.proto</include>
+                          <include>GroupAdmin.proto</include>
                           <include>HBase.proto</include>
                           <include>HFile.proto</include>
                           <include>LoadBalancer.proto</include>
{code}

2. NPE in group shell commands with nonexisting groups: 
{code}
hbase(main):015:0* balance_group 'nonexisting' 

ERROR: java.io.IOException
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2156)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:101)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hbase.group.GroupAdminServer.groupGetRegionsInTransition(GroupAdminServer.java:412)
	at org.apache.hadoop.hbase.group.GroupAdminServer.balanceGroup(GroupAdminServer.java:348)
	at org.apache.hadoop.hbase.group.GroupAdminEndpoint.balanceGroup(GroupAdminEndpoint.java:229)
	at org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos$GroupAdminService.callMethod(GroupAdminProtos.java:11156)
	at org.apache.hadoop.hbase.master.MasterRpcServices.execMasterService(MasterRpcServices.java:666)
	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:51121)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2114)
{code}

and 

{code}
hbase(main):030:0> get_group 'nonexisting'
GROUP INFORMATION                                                                                                                                                                                                                                                  
Servers:                                                                                                                                                                                                                                                           

ERROR: undefined method `getServers' for nil:NilClass

Here is some help for this command:
Get a region server group's information.

Example:

  hbase> get_group 'default'
{code}

and 

{code}
hbase(main):077:0* move_group_tables 'nonexisting'

ERROR: undefined method `each' for nil:NilClass

Here is some help for this command:
Reassign tables from one group to another.

  hbase> move_group_tables 'dest',['table1','table2']
{code}

and 
{code}
hbase(main):173:0* move_group_servers 'nonexisting'

ERROR: undefined method `each' for nil:NilClass

Here is some help for this command:
Reassign a region server from one group to another.

  hbase> move_group_servers 'dest',['server1:port','server2:port']
{code}

3. Group names should be restricted to alphanumeric only. This one is pretty easy, but important. This following caused the master to abort, and the master cannot restart after this point (without manually removing the rsgroup entry from the table which you cannot do without master). I had to nuke the hdfs and zk to start over. 
{code}
hbase(main):033:0> add_group 'a-/:*'

ERROR: java.io.IOException: Failed to write to groupZNode
	at org.apache.hadoop.hbase.group.GroupInfoManagerImpl.flushConfig(GroupInfoManagerImpl.java:419)
	at org.apache.hadoop.hbase.group.GroupInfoManagerImpl.addGroup(GroupInfoManagerImpl.java:152)
	at org.apache.hadoop.hbase.group.GroupAdminServer.addGroup(GroupAdminServer.java:298)
	at org.apache.hadoop.hbase.group.GroupAdminEndpoint.addGroup(GroupAdminEndpoint.java:197)
	at org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos$GroupAdminService.callMethod(GroupAdminProtos.java:11146)
	at org.apache.hadoop.hbase.master.MasterRpcServices.execMasterService(MasterRpcServices.java:666)
	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:51121)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2114)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:101)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase-unsecure/groupInfo/a-/:*
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.createNonSequential(RecoverableZooKeeper.java:575)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.create(RecoverableZooKeeper.java:554)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndFailSilent(ZKUtil.java:1261)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndFailSilent(ZKUtil.java:1250)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndFailSilent(ZKUtil.java:1233)
	at org.apache.hadoop.hbase.group.GroupInfoManagerImpl.flushConfig(GroupInfoManagerImpl.java:408)
{code}
 
4. {{get_table_group}} and {{get_server_group}} shell commands do not work
{code}
hbase(main):019:0* get_table_group 'nonexisting'

ERROR: undefined local variable or method `s' for #<Hbase::GroupAdmin:0x64518270>

Here is some help for this command:
Get the group name the given table is a member of.

  hbase> get_table_group 'myTable'

 
hbase(main):022:0* get_server_group 'server'

ERROR: undefined local variable or method `s' for #<Hbase::GroupAdmin:0x64518270>

Here is some help for this command:
Get the group name the given region server is a member of.

  hbase> get_server_group 'server1:port1
{code}

5. {{move_group_servers}} and {{move_group_tables}} arguments are listed as 1, although should be 2: 
{code}
hbase(main):033:0* move_group_servers 

ERROR: wrong number of arguments (0 for 1)

Here is some help for this command:
Reassign a region server from one group to another.

  hbase> move_group_servers 'dest',['server1:port','server2:port']
{code}

6. Adding a server without port throws error, but no explanation (this one is a minor, not that important). 
{code}
hbase(main):070:0> move_group_servers 'group2', ['os-enis-hbase-oct27-a-3.novalocal']  

ERROR: 

Here is some help for this command:
Reassign a region server from one group to another.

  hbase> move_group_servers 'dest',['server1:port','server2:port']
{code}

7. From all the above, it is clear that we need a unit test over the new shell commands. 

Other than these, the feature is working as expected. Defining groups, moving servers and tables work. Regions get reassigned according to their groups. Restarting the cluster keeps assignments, etc. 

Some more findings: 
Test 1: 
Killed the last regionserver of a group, all 15 regions are in FAILED_OPEN state. 
 - restarted the master, regions still in FAILED_OPEN state (which is expected)
 - Added a new server to the group which had no remaining servers, regions still in FAILED_OPEN state (this is probably due to how assignment works, we give up after 10 retries and wait for manual assignment or master restart)
 - Started the region server that was killed before, still in FAILED_OPEN
 - Master restart reassigned these regions. 

Test 2: 
Tried to move all servers to a single group. Correctly handles last server in the default group by not allowing it to change. 

Test 3: 
Killed the last server in the default group, while all system tables are in the default group (and hence in that server). 
 -> hbase:meta was always in PENDING_OPEN in bogus server localhost,1,1. 
 -> Upon restarting the killed server, meta and other tables in the default group (including rsgroup table) got reassigned. 
 As a side note, having not enough servers in the group that has the meta or rsgroup table seems like a very good way of shoothing yourself in the foot. However, as discussed before this maybe needed for strong isolation. 


- Add non-existing server to the group. Is not allowed. 
- Checked JMX
- Adding group information for tables and regionserver to the master UI would be helpful. We can leave this to a follow up. 
- Obviously there should be a follow up to add at least some basic documentation on how to enable and configure and use RS groups in the book. 






Even more testing: 
Test 4: Create namespace with hbase.rsgroup.name pointing to a group, then create a table in that namespace. Works as expected, the new table created in correct group 

Run the integration test on the 7 node cluster: 
hbase org.apache.hadoop.hbase.IntegrationTestsDriver -regex IntegrationTestGroup 
Failed. Inspecting. 

8. I was thinking of testing to remove the default group, but did not do it yet. Reading of the code seems to allow it. We should fix that. 
9. We can add a description to the rsgroup table  (minor). The master UI displays the description. 
10. In IntegrationTestGroup.java, the wait condition should not wait for exactly NUM_SLAVES_BASE, there can be more servers that NUM_SLAVES_BASE in an actual cluster. 
11. Once balancer is turned off, there is no way to turn it on since GroupAdminEndpoint.preBalanceSwitch() always returns false. 




We may have a problem with IntegrationTestGroup.testMoveServers. 
12. Also the test leaves the balancer turned off, which should be be the case. 



Here is a master patch that is: 
 - rebased to latest master. 
 - Addresses (1), (10), (11), and (12) as above. 

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12769912/hbase-6721-v16.patch
  against master branch at commit 683f84e6a217dfd872e5f1be82c7aa4cdf232ec1.
  ATTACHMENT ID: 12769912

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 33 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 4 warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 1755 checkstyle errors (more than the master's current 1730 errors).

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupInfoOfServer(.hbase.pb.GetGroupInfoOfServerRequest) returns (.hbase.pb.GetGroupInfoOfServerResponse);</code>
+       * <code>rpc MoveServers(.hbase.pb.MoveServersRequest) returns (.hbase.pb.MoveServersResponse);</code>
+       * <code>rpc MoveTables(.hbase.pb.MoveTablesRequest) returns (.hbase.pb.MoveTablesResponse);</code>
+       * <code>rpc RemoveGroup(.hbase.pb.RemoveGroupRequest) returns (.hbase.pb.RemoveGroupResponse);</code>
+       * <code>rpc BalanceGroup(.hbase.pb.BalanceGroupRequest) returns (.hbase.pb.BalanceGroupResponse);</code>
+       * <code>rpc ListGroupInfos(.hbase.pb.ListGroupInfosRequest) returns (.hbase.pb.ListGroupInfosResponse);</code>
+     * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+     * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.client.TestAsyncProcess

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/16327//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/16327//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/16327//artifact/patchprocess/checkstyle-aggregate.html

                Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/16327//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/16327//console

This message is automatically generated.

Here is a v17 patch which has these changes on top of v16: 
 - Addresses item (4) above. Fixed get_server_group and get_table_group commands. 
 - Some ruby changes 
 - Changed the unit tests so that they will work the same on master code or branch-1 code (due to colocated master changes). 
 - Fixed a timing issue with the test causing the IT to fail. 

Now, I am able to run the IntegrationTestGroup in a 7 node cluster, and the unit test works. 

[~toffer] I think there are only a couple of remaining items left. Let me know, if you need help in addressing some of them. 

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12770514/hbase-6721-v17.patch
  against master branch at commit ceddec9141b658c790d2fb995c970982ab082028.
  ATTACHMENT ID: 12770514

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 36 new or modified tests.

    {color:red}-1 javac{color}.  The patch appears to cause mvn compile goal to fail with Hadoop version 2.4.0.

    Compilation errors resume:
    [ERROR] COMPILATION ERROR : 
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupInfoManagerImpl.java:[588,47] cannot find symbol
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.2:compile (default-compile) on project hbase-server: Compilation failure
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupInfoManagerImpl.java:[588,47] cannot find symbol
[ERROR] symbol:   method getHRegionInfo(org.apache.hadoop.hbase.client.Result)
[ERROR] location: class org.apache.hadoop.hbase.HRegionInfo
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hbase-server
    

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/16380//console

This message is automatically generated.

Thanks for the comprehensive testing and fixes [~enis]. Let me try and sum up the outstanding things that need to be done. Let me know if I missed anything:

1. Unit test for shell commands
2. Fix rsgroup table name description
3. "NPE in group shell commands with nonexisting groups"
4. "Group names should be restricted to alphanumeric only. "
5. "move_group_servers and move_group_tables arguments are listed as 1, although should be 2:"
6. File Jira for UI, do it as follow-up
7. File Jira for documentation, do it as follow-up. 

{quote}
8. I was thinking of testing to remove the default group, but did not do it yet. Reading of the code seems to allow it. We should fix that. 
{quote}
This is being checked in GroupInfomManagerImpl.removeGroup so this is covered.

With the next update, I can now buy the patch a beer (in some states). :-) 






Thanks [~toffer]. The list sounds about right. 

bq. With the next update, I can now buy the patch a beer (in some states).
We can all grab a (virtual) beer once this is committed. 

Added v18 patch which addresses metnioned comments. Also rebased to HEAD and fixed issues.

Also moved unassign of misplaced regions out of the balancer and added unit test.

Updated RB with both patches:

https://reviews.apache.org/r/38224/



{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12773431/hbase-6721-v18.patch
  against master branch at commit ea48ef86512addc3dc9bcde4b7433a3ac5881424.
  ATTACHMENT ID: 12773431

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 42 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 5 warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated 1752 checkstyle errors (more than the master's current 1727 errors).

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupInfoOfServer(.hbase.pb.GetGroupInfoOfServerRequest) returns (.hbase.pb.GetGroupInfoOfServerResponse);</code>
+       * <code>rpc MoveServers(.hbase.pb.MoveServersRequest) returns (.hbase.pb.MoveServersResponse);</code>
+       * <code>rpc MoveTables(.hbase.pb.MoveTablesRequest) returns (.hbase.pb.MoveTablesResponse);</code>
+       * <code>rpc RemoveGroup(.hbase.pb.RemoveGroupRequest) returns (.hbase.pb.RemoveGroupResponse);</code>
+       * <code>rpc BalanceGroup(.hbase.pb.BalanceGroupRequest) returns (.hbase.pb.BalanceGroupResponse);</code>
+       * <code>rpc ListGroupInfos(.hbase.pb.ListGroupInfosRequest) returns (.hbase.pb.ListGroupInfosResponse);</code>
+     * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+     * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>

  {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizer

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/16603//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/16603//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/16603//artifact/patchprocess/checkstyle-aggregate.html

                Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/16603//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/16603//console

This message is automatically generated.

Thanks [~toffer] for the updated patch. All of the review items have been addressed, and hence I'm +1 for committing this. 

[~apurtell], [~eclark] or others have any more concerns, feedback? Given that this is an optional coprocessor based thing, I think this can directly go to master. 

Running TestShell with latest patch, I got:
{code}
  1) Error:
test_Test_Basic_Group_Commands(Hbase::GroupShellTest):
NativeException: org.apache.hadoop.hbase.exceptions.UnknownProtocolException: org.apache.hadoop.hbase.exceptions.UnknownProtocolException: No registered master coprocessor     service found for name hbase.pb.GroupAdminService
  at org.apache.hadoop.hbase.master.MasterRpcServices.execMasterService(MasterRpcServices.java:682)
  at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:57964)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2184)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109)
  at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:133)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:108)
...
  2) Failure:
test_Test_bogus_arguments(Hbase::GroupShellTest)
    [./src/test/ruby/shell/group_shell_test.rb:85:in `test_Test_bogus_arguments'
     org/jruby/RubyProc.java:270:in `call'
     org/jruby/RubyKernel.java:2105:in `send'
     org/jruby/RubyArray.java:1620:in `each'
     org/jruby/RubyArray.java:1620:in `each']:
<ArgumentError> exception expected but was
Class: <NativeException>
Message: <"org.apache.hadoop.hbase.exceptions.UnknownProtocolException: org.apache.hadoop.hbase.exceptions.UnknownProtocolException: No registered master coprocessor service   found for name hbase.pb.GroupAdminService\n\tat org.apache.hadoop.hbase.master.MasterRpcServices.execMasterService(MasterRpcServices.java:682)\n\tat org.apache.hadoop.hbase.   protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:57964)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2184)\n\tat org.    apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:133)\n\tat org.apache.hadoop.hbase.ipc. RpcExecutor$1.run(RpcExecutor.java:108)\n\tat java.lang.Thread.run(Thread.java:745)\n">
{code}
Francis:
Do the tests pass for you ?

Test failure in TestSimpleRegionNormalizer is reproducible:
{code}
testNoNormalizationForMetaTable(org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizer)  Time elapsed: 0.337 sec  <<< ERROR!
java.lang.IllegalArgumentException: null
  at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76)
  at com.google.common.net.HostAndPort.fromParts(HostAndPort.java:130)
  at org.apache.hadoop.hbase.ServerName.<init>(ServerName.java:105)
  at org.apache.hadoop.hbase.ServerName.valueOf(ServerName.java:158)
  at org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizer.setupMocksForNormalizer(TestSimpleRegionNormalizer.java:223)
  at org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizer.testNoNormalizationForMetaTable(TestSimpleRegionNormalizer.java:71)

testNoNormalizationIfTooFewRegions(org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizer)  Time elapsed: 0.045 sec  <<< ERROR!
java.lang.IllegalArgumentException: null
  at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76)
  at com.google.common.net.HostAndPort.fromParts(HostAndPort.java:130)
  at org.apache.hadoop.hbase.ServerName.<init>(ServerName.java:105)
  at org.apache.hadoop.hbase.ServerName.valueOf(ServerName.java:158)
  at org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizer.setupMocksForNormalizer(TestSimpleRegionNormalizer.java:223)
  at org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizer.testNoNormalizationIfTooFewRegions(TestSimpleRegionNormalizer.java:90)
{code}

[~tedyu@apache.org] Looks like TestSimpleRegionNormalizer is using negative port numbers for ServerName. Should be a simple fix to update the test. How are you able to run TestShell? I only see an abstract class.

Patch fixes normalizer unit test failure.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12776745/hbase-6721-v19.patch
  against master branch at commit 9647fee3f0f196d064879afd41b9eff51d5aa036.
  ATTACHMENT ID: 12776745

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 45 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated new checkstyle errors. Check build console for list of new errors.

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupInfoOfServer(.hbase.pb.GetGroupInfoOfServerRequest) returns (.hbase.pb.GetGroupInfoOfServerResponse);</code>
+       * <code>rpc MoveServers(.hbase.pb.MoveServersRequest) returns (.hbase.pb.MoveServersResponse);</code>
+       * <code>rpc MoveTables(.hbase.pb.MoveTablesRequest) returns (.hbase.pb.MoveTablesResponse);</code>
+       * <code>rpc RemoveGroup(.hbase.pb.RemoveGroupRequest) returns (.hbase.pb.RemoveGroupResponse);</code>
+       * <code>rpc BalanceGroup(.hbase.pb.BalanceGroupRequest) returns (.hbase.pb.BalanceGroupResponse);</code>
+       * <code>rpc ListGroupInfos(.hbase.pb.ListGroupInfosRequest) returns (.hbase.pb.ListGroupInfosResponse);</code>
+     * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+     * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>

    {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

    {color:green}+1 zombies{color}. No zombie tests found running at the end of the build.

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/16827//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/16827//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/16827//artifact/patchprocess/checkstyle-aggregate.html

                Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/16827//console

This message is automatically generated.

[~tedyu] Looks like fixing the normalizer made the core tests pass, I'm not able to reproduce the shell failure you presented?

Looks like [~tedyu] is OOO.

[~enis] [~eclark] [~apurtell] Any other feedback/concerns? Anything else I should address? 

From https://builds.apache.org/job/PreCommit-HBASE-Build/16827/consoleFull :
{code}
testKillRS(org.apache.hadoop.hbase.group.TestGroups)  Time elapsed: 10.725 sec  <<< FAILURE!
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.hadoop.hbase.group.TestGroupsBase.testKillRS(TestGroupsBase.java:612)
{code}
I am running TestShell locally to see if it passes.

Francis:
You can copy the following file from branch-1.1 to master branch so that you can run TestShell:
hbase-shell/src/test/java/org/apache/hadoop/hbase/client/TestShell.java

I ran TestShell locally and still got:
{code}
test_Test_Basic_Group_Commands(Hbase::GroupShellTest):
NativeException: org.apache.hadoop.hbase.exceptions.UnknownProtocolException: org.apache.hadoop.hbase.exceptions.UnknownProtocolException: No registered master coprocessor    service found for name hbase.pb.GroupAdminService
  at org.apache.hadoop.hbase.master.MasterRpcServices.execMasterService(MasterRpcServices.java:682)
  at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:57964)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2213)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109)
  at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:133)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:108)
  at java.lang.Thread.run(Thread.java:745)

    sun/reflect/NativeConstructorAccessorImpl.java:-2:in `newInstance0'
    sun/reflect/NativeConstructorAccessorImpl.java:57:in `newInstance'
    sun/reflect/DelegatingConstructorAccessorImpl.java:45:in `newInstance'
    java/lang/reflect/Constructor.java:526:in `newInstance'
    org/apache/hadoop/ipc/RemoteException.java:106:in `instantiateException'
    org/apache/hadoop/ipc/RemoteException.java:95:in `unwrapRemoteException'
    org/apache/hadoop/hbase/ipc/AsyncCall.java:127:in `setFailed'
    org/apache/hadoop/hbase/ipc/AsyncServerResponseHandler.java:83:in `channelRead'
    io/netty/channel/AbstractChannelHandlerContext.java:308:in `invokeChannelRead'
    io/netty/channel/AbstractChannelHandlerContext.java:294:in `fireChannelRead'
    io/netty/handler/codec/ByteToMessageDecoder.java:244:in `channelRead'
    io/netty/channel/AbstractChannelHandlerContext.java:308:in `invokeChannelRead'
    io/netty/channel/AbstractChannelHandlerContext.java:294:in `fireChannelRead'
    io/netty/channel/DefaultChannelPipeline.java:846:in `fireChannelRead'
    io/netty/channel/nio/AbstractNioByteChannel.java:131:in `read'
    io/netty/channel/nio/NioEventLoop.java:511:in `processSelectedKey'
    io/netty/channel/nio/NioEventLoop.java:468:in `processSelectedKeysOptimized'
    io/netty/channel/nio/NioEventLoop.java:382:in `processSelectedKeys'
    io/netty/channel/nio/NioEventLoop.java:354:in `run'
    io/netty/util/concurrent/SingleThreadEventExecutor.java:110:in `run'
    java/lang/Thread.java:745:in `run'
    ./src/test/ruby/shell/group_shell_test.rb:43:in `test_Test_Basic_Group_Commands'
    org/jruby/RubyProc.java:270:in `call'
    org/jruby/RubyKernel.java:2105:in `send'
    org/jruby/RubyArray.java:1620:in `each'
    org/jruby/RubyArray.java:1620:in `each'

  2) Failure:
test_Test_bogus_arguments(Hbase::GroupShellTest)
    [./src/test/ruby/shell/group_shell_test.rb:85:in `test_Test_bogus_arguments'
     org/jruby/RubyProc.java:270:in `call'
     org/jruby/RubyKernel.java:2105:in `send'
     org/jruby/RubyArray.java:1620:in `each'
     org/jruby/RubyArray.java:1620:in `each']:
<ArgumentError> exception expected but was
Class: <NativeException>
Message: <"org.apache.hadoop.hbase.exceptions.UnknownProtocolException: org.apache.hadoop.hbase.exceptions.UnknownProtocolException: No registered master coprocessor service  found for name hbase.pb.GroupAdminService\n\tat org.apache.hadoop.hbase.master.MasterRpcServices.execMasterService(MasterRpcServices.java:682)\n\tat org.apache.hadoop.hbase.  protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:57964)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2213)\n\tat org.   apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:133)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:108)\n\tat java.lang.Thread.run(Thread.java:745)\n">
{code}

Is this really how we're supposed to run the other shell unit tests? What if there's something that needs to be changed/fixed in TestShell?

Took a quick look at GroupShellTest which misses master coprocessor setup similar to the following:
{code}
+    TEST_UTIL.getConfiguration().set(CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY,
+        GroupAdminEndpoint.class.getName());
{code}

{quote}
Took a quick look at GroupShellTest which misses master coprocessor setup similar to the following:
{quote}

Precisely, I created a separate ShellTest subclass as I needed to configure a different balancer and coprocessor which would not work with the existing shell unit tests. I think there's a bigger problem, that there is no proper way to run shell unit tests in trunk. I've filed HBASE-14981 to address that. I'm going to address the non-shell unit test failures for now.


addressed failing non-shell unit tests

Looks like TestShell has been temporarily removed as part of test stabilization experiment in HBASE-14678.

Given how it looks like when it was still there:

https://github.com/apache/hbase/commit/f670649f0e2cfba8a2112eb5c1d79b8b7f52c3b2

All I need to do is include the group rb file in the TestShell class. So we can either apply the TestShell change once it's been re-enabled. Or commit the group shell unit tests as a follow-up once TestShell has been re-enabled. Thoughts?

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12777930/hbase-6721-v20.patch
  against master branch at commit 60d33ce34191533bb858852584bd9bddfeb16a23.
  ATTACHMENT ID: 12777930

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 48 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 1 warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated new checkstyle errors. Check build console for list of new errors.

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupInfoOfServer(.hbase.pb.GetGroupInfoOfServerRequest) returns (.hbase.pb.GetGroupInfoOfServerResponse);</code>
+       * <code>rpc MoveServers(.hbase.pb.MoveServersRequest) returns (.hbase.pb.MoveServersResponse);</code>
+       * <code>rpc MoveTables(.hbase.pb.MoveTablesRequest) returns (.hbase.pb.MoveTablesResponse);</code>
+       * <code>rpc RemoveGroup(.hbase.pb.RemoveGroupRequest) returns (.hbase.pb.RemoveGroupResponse);</code>
+       * <code>rpc BalanceGroup(.hbase.pb.BalanceGroupRequest) returns (.hbase.pb.BalanceGroupResponse);</code>
+       * <code>rpc ListGroupInfos(.hbase.pb.ListGroupInfosRequest) returns (.hbase.pb.ListGroupInfosResponse);</code>
+     * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+     * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>

    {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

    {color:green}+1 zombies{color}. No zombie tests found running at the end of the build.

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/16873//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/16873//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/16873//artifact/patchprocess/checkstyle-aggregate.html

                Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/16873//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/16873//console

This message is automatically generated.

bq. include the group rb file in the TestShell class
TestShell works by excluding replication related shell tests:
{code}
  public void testRunShellTests() throws IOException {
    System.setProperty("shell.test.exclude", "replication_admin_test.rb");
{code}
Can you refer to hbase-shell//src/test/java/org/apache/hadoop/hbase/client/TestReplicationShell.java in branch-1.1 and create TestRegionServerGroupShell.java ?
This way, we don't need to wait for restoration of TestShell.

I pretty much did that with TestShellGroups class, I just used the 'shell.test' property to define what test to be run intead of shell.test.include/shell.test.exclude. The reason the TestShell was failing was because you copied the TestShell class and did not exclude group_shell_test.rb similar to how replication_admin_test.rb is set. So we will need to update TestShell once it's restored so it doesn't fail because it tries to run  group_shell_test.rb.

I see.
TestShellGroups passes locally.

Mind addressing checkstyle warnings ?

Patch on review board is v18.
Please upload v20 (or v21) there.

Thanks

I upload v20 on RB. Will address the checkstyle.

addressed checkstyle issues.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12778218/hbase-6721-v21.patch
  against master branch at commit d78eddfdc8bad5068600e28a039276cc55063ce2.
  ATTACHMENT ID: 12778218

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 48 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 1 warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated new checkstyle errors. Check build console for list of new errors.

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupInfoOfServer(.hbase.pb.GetGroupInfoOfServerRequest) returns (.hbase.pb.GetGroupInfoOfServerResponse);</code>
+       * <code>rpc MoveServers(.hbase.pb.MoveServersRequest) returns (.hbase.pb.MoveServersResponse);</code>
+       * <code>rpc MoveTables(.hbase.pb.MoveTablesRequest) returns (.hbase.pb.MoveTablesResponse);</code>
+       * <code>rpc RemoveGroup(.hbase.pb.RemoveGroupRequest) returns (.hbase.pb.RemoveGroupResponse);</code>
+       * <code>rpc BalanceGroup(.hbase.pb.BalanceGroupRequest) returns (.hbase.pb.BalanceGroupResponse);</code>
+       * <code>rpc ListGroupInfos(.hbase.pb.ListGroupInfosRequest) returns (.hbase.pb.ListGroupInfosResponse);</code>
+     * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+     * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>

    {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

    {color:green}+1 zombies{color}. No zombie tests found running at the end of the build.

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/16903//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/16903//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/16903//artifact/patchprocess/checkstyle-aggregate.html

                Javadoc warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/16903//artifact/patchprocess/patchJavadocWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/16903//console

This message is automatically generated.

There're some import order check warnings left:

./dev-support/checkstyle_report.py trunkCheckstyle.xml patchCheckstyle.xml
hbase-client/src/main/java/org/apache/hadoop/hbase/group/GroupAdminClient.java	ImportOrderCheck	0	1
hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java	RedundantImportCheck	0	4
hbase-server/target/generated-jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.java	ImportOrderCheck	12	13
hbase-server/target/generated-jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.java	UnusedImportsCheck	20	21
hbase-server/target/generated-jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmplImpl.java	IndentationCheck	61	62
hbase-server/target/generated-jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmplImpl.java	ImportOrderCheck	12	13

Sorry missed that. Fixed in updated patch.

Javadoc is not related. I think there is an issue tracking that already. 

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12778306/hbase-6721-v22.patch
  against master branch at commit 80fc18d2e1f72d312a4ab90a32d6e44c9b560493.
  ATTACHMENT ID: 12778306

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 48 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated new checkstyle errors. Check build console for list of new errors.

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupInfoOfServer(.hbase.pb.GetGroupInfoOfServerRequest) returns (.hbase.pb.GetGroupInfoOfServerResponse);</code>
+       * <code>rpc MoveServers(.hbase.pb.MoveServersRequest) returns (.hbase.pb.MoveServersResponse);</code>
+       * <code>rpc MoveTables(.hbase.pb.MoveTablesRequest) returns (.hbase.pb.MoveTablesResponse);</code>
+       * <code>rpc RemoveGroup(.hbase.pb.RemoveGroupRequest) returns (.hbase.pb.RemoveGroupResponse);</code>
+       * <code>rpc BalanceGroup(.hbase.pb.BalanceGroupRequest) returns (.hbase.pb.BalanceGroupResponse);</code>
+       * <code>rpc ListGroupInfos(.hbase.pb.ListGroupInfosRequest) returns (.hbase.pb.ListGroupInfosResponse);</code>
+     * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+     * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>

    {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

    {color:green}+1 zombies{color}. No zombie tests found running at the end of the build.

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/16912//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/16912//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/16912//artifact/patchprocess/checkstyle-aggregate.html

                Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/16912//console

This message is automatically generated.

The remaining checkstyle failures are mainly because of the auto-generated MasterStatus java files. Do we need to fix this?

{code}
hbase-server/target/generated-jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmplImpl.java	IndentationCheck	61	62
hbase-server/target/generated-jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmplImpl.java	ImportOrderCheck	12	13
hbase-server/target/generated-jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.java	ImportOrderCheck	12	13
hbase-server/target/generated-jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.java	UnusedImportsCheck	20	21
{code}



Looks like I can only influence the import order via the jamon file (which was a mess to begin with). The rest cannot be fixed it seems.

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12778573/hbase-6721-v23.patch
  against master branch at commit eb59d4d7690bb4d8d0afa460202c68885ef6a271.
  ATTACHMENT ID: 12778573

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 48 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/16931//console

This message is automatically generated.

rebased patch

{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12778630/hbase-6721-v25.patch
  against master branch at commit cdca22a36d0e2ec87cea732e0769c4fa4fa37e57.
  ATTACHMENT ID: 12778630

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 48 new or modified tests.

    {color:green}+1 hadoop versions{color}. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 protoc{color}.  The applied patch does not increase the total number of protoc compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

                {color:red}-1 checkstyle{color}.  The applied patch generated new checkstyle errors. Check build console for list of new errors.

    {color:green}+1 findbugs{color}.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:
    +       * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+       * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>
+       * <code>rpc GetGroupInfoOfServer(.hbase.pb.GetGroupInfoOfServerRequest) returns (.hbase.pb.GetGroupInfoOfServerResponse);</code>
+       * <code>rpc MoveServers(.hbase.pb.MoveServersRequest) returns (.hbase.pb.MoveServersResponse);</code>
+       * <code>rpc MoveTables(.hbase.pb.MoveTablesRequest) returns (.hbase.pb.MoveTablesResponse);</code>
+       * <code>rpc RemoveGroup(.hbase.pb.RemoveGroupRequest) returns (.hbase.pb.RemoveGroupResponse);</code>
+       * <code>rpc BalanceGroup(.hbase.pb.BalanceGroupRequest) returns (.hbase.pb.BalanceGroupResponse);</code>
+       * <code>rpc ListGroupInfos(.hbase.pb.ListGroupInfosRequest) returns (.hbase.pb.ListGroupInfosResponse);</code>
+     * <code>rpc GetGroupInfo(.hbase.pb.GetGroupInfoRequest) returns (.hbase.pb.GetGroupInfoResponse);</code>
+     * <code>rpc GetGroupInfoOfTable(.hbase.pb.GetGroupInfoOfTableRequest) returns (.hbase.pb.GetGroupInfoOfTableResponse);</code>

    {color:green}+1 site{color}.  The mvn post-site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

    {color:green}+1 zombies{color}. No zombie tests found running at the end of the build.

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/16934//testReport/
Release Findbugs (version 2.0.3) 	warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/16934//artifact/patchprocess/newFindbugsWarnings.html
Checkstyle Errors: https://builds.apache.org/job/PreCommit-HBASE-Build/16934//artifact/patchprocess/checkstyle-aggregate.html

                Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/16934//console

This message is automatically generated.

{code}
hbase-server/target/generated-jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.java	UnusedImportsCheck	20	21
hbase-server/target/generated-jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmplImpl.java	IndentationCheck	61	62
{code}
To my knowledge, HBASE-15015 (committed after the QA run) should have disabled the above check.


+1

Francis:
Please update Release Note.

Any more review comments ?

From what I can see most of my comments still stand. It's still built into the default client. It's still in the main module. Though I do appreciate that it's a co-processor.

If we can move the co-processor to a different module and move the methods off the main admin classes I would be fine with it.

With this patch, the RS groups is completely optional and non-core. I don't think having the code in a different module helps in any way.  
The patch corresponds to the plan that Francis put up here: https://issues.apache.org/jira/browse/HBASE-6721?focusedCommentId=14728516&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14728516. I thought there was an agreement already for this. We did not talk about having a different module in that. 

[~eclark] ping. Please respond when you have a moment.

There is a +1 outstanding for this patch. To reiterate the above comment, I think the current patch covers the already-agreed-upon requirement that the feature should be non-core and optional. We have not talked about needing a different module previously, and I am not sure why it is necessary. Unless, I hear otherwise by Friday, I'll go ahead with committing this. 

bq.I think the current patch covers the already-agreed-upon requirement that the feature should be non-core and optional.
The exact same was requested for memcached block cache, and it was reasonable then. I'm simply asking for this feature to get the same treatment that optional off by default removable features should get. This feature should never be used by anyone other than yahoo and we have a duty to our users to make sure that they understand that.

On top of that the naming on this is awful. Groups are something that users are a part of. Its a term that been around unix for a very long time. The term is used all over the Hadoop and HBase code. We shouldn't overload the term. rsgroup as used in the table name is better.
Why all the changes to comma for server name rather than colon ? Isn't that a breaking change? If it's not required then we should add tests, not change them.
Going around the current metrics system is a code smell.
Start code is optional on ServerName so no need to create a new protobuf.

I have not withdrawn my explicit -1 as per the voting rules of Apache.

bq. The exact same was requested for memcached block cache, and it was reasonable then. I'm simply asking for this feature to get the same treatment that optional off by default removable features should get
Yep, everybody agrees that it should be completely optional and non-core for this. What I am saying is that there is no need to fork out a different module for this. I just checked, nobody asked the memcache thing to be a different module above. 
bq. This feature should never be used by anyone other than yahoo and we have a duty to our users to make sure that they understand that.
That is up for the users to decide. Sophisticated users can make their own decisions. 
bq.  rsgroup as used in the table name is better.
Agreed. I've brought this up in the review already. I thought we have addressed the cases, but if we haven't, we should stick with rsgroups rather than groups.  Having consistent naming is something we are lacking in hbase (alter table in shell vs modify table in java, etc).



I would like to register that I am not at Yahoo! but as an operator of a large number of clusters will certainly take advantage of this feature. In fact, Guorui's work was motivated by the need for this feature here.

What we going to do here?

One of our biggest users has been working on getting this patch landed for years now. They put up a design and tried to address commentary as it came in. The patch has gone through many iterations. It even got deployed by another committer on a cluster and exercised. Implementation could be better but our internals are a little ahem, messy, so hard to overlay a new facility. A case has been made that this feature makes hbase palatable at scale, at least for how it is being used at Y!

On the other hand, [~eclark] objections are wholesome/valid. By default, this new code should not run. Having it out in a module doesn't sound like a bad idea if optional on and a bunch of new code. Yes, overloading 'group' will confuse as we progress. You are probably busy [~eclark] but any chance of a list of what it would take to get you to drop your -1.

Thanks

just gave a look at the patch. most of the code seems isolated. 
the client is separate from Admin with its own GroupAdmin
the rpc is not in the master but in its own GroupAdminEndpoint
the "group assignment" logic seems isolated in GroupBasedLoadBalancer
and there are not many changes to the core. 

The only "big" thing that should be fixed are that too generic Group name and maybe doing something around that processBogusAssignments()/BOGUS_SERVER_NAME

moving in its own module seems easy since most of the code is already isolated, but there are few things that we never done, like having the shell commands in a module different than hbase-shell

[~eclark] if was in a separate module and feature name was changed to not be 'group', would you  your -1? Seems like it well isolated currently going by Matteo review above. Could hack on shell so it picked up commands at runtime so we could have downstream modules add commands....

bq.That is up for the users to decide. 
And up the the PMC to steer people in the correct direction based upon having deep knowledge of running HBase in production. So I'm doing that.

bq.Elliott Clark if was in a separate module and feature name was changed to not be 'group', would you your -1?
Yeah, address the groups naming thing.
Address the ServerName questions, breaking compat is not an option.
Don't go around the Metrics to get to jmx
Don't create a new protobuf class when not needed
Move it into a different module with a profile that doesn't include it.

If/when those are addressed then I would remove my -1. I would be a -0.9 but I wouldn't block it.

Thanks [~eclark]

How about we commit what we have here to a branch and then work on the list above to get it committed? I'm game for helping out on fixing the ServerName stuff, the protobuf, and moving the thing to its own module (shell-work, etc.).

Thanks for the list [~eclark]. Some comments.

> Yeah, address the groups naming thing.

Sounds good. 

> moving in its own module seems easy since most of the code is already isolated, but there are few things that we never done, like having the shell commands in a module different than hbase-shell

The previous agreement was to keep the gorup commands in hbase-shell the same as security before. Would guys still be ok with this? [~eclark] [~mbertozzi]

> Address the ServerName questions, breaking compat is not an option.

I think you misread the change. The code change was to actually fix a broken unit test as it was using colon as a delimiter to serverName. I just fixed it by changing the colon to ','. If this is not the case please feel free to point it out in RB so I can address it properly.

> Don't go around the Metrics to get to jmx

The reason I went around metrics was because the information being published is not really what is consumed as metrics, it's basically just group information (name, tables, servers, etc) . It'd be ugly but probably possible to shoehorn this into the metrics subsystem. Is this really how we do this even for non-metrics related information to be published? There's alot less flexibility in how we can structure the metadata. 

> Don't create a new protobuf class when not needed

This was actually a review comment made by [~jmhsieh]. I'll take a look if ServerName can suffice as a substitute. 

> Move it into a different module with a profile that doesn't include it.

Will do.

> If/when those are addressed then I would remove my -1. I would be a -0.9 but I wouldn't block it.

Thanks. 


Took a quick look at how the NN publishes metadata via JMX. And for the HA information at least it does it the same way this patch is doing in for HBase:

https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java#L1727

Just in case it wasn't common knowledge. I'll publish the information in whatever form is acceptable.

bq.Took a quick look at how the NN publishes metadata via JMX.
Yeah hadoop can do whatever they want. I would prefer that HBase is consistent whenever interacting with jmx so when we make changes in the future it's all moving from the same base.

bq.I think you misread the change. 
Both formats have been able to be parsed for a long time. So if the test needs to be changed to pass then that's breaking change. If not then we shouldn't have changes that aren't needed in a patch this large.

{quote}
Yeah hadoop can do whatever they want. I would prefer that HBase is consistent whenever interacting with jmx so when we make changes in the future it's all moving from the same base.
{quote}
Took a look at this further it may be possible to reuse tags to publish metadata in some form but I would need to add APIs to the base (BaseSource?). Tho IMHO we could probably do better by providing a common base on which to publish metadata via jmx instead of retrofitting it into hadoop-metrics which looking at the direction hadoop went is not how they intend metrics subsystem to be used. So instead of addressing this in this patch I propose we create two new jiras: 1. create apis to publish metadata via jmx, 2. create jira to leverage #1 and publish region server groups metadata. Thoughts?

{quote}
Both formats have been able to be parsed for a long time. So if the test needs to be changed to pass then that's breaking change. If not then we shouldn't have changes that aren't needed in a patch this large.
{quote}
I'm not sure how the format used by TestMasterStatusServlet: rootserver:123,12345 is actually a valid format. It seems like a typo/bug to me. For a few reasons:

1. The format is not documented in the javadoc
2. If you look at how the format mentioned is parsed the components would come out as host=rootserver:123/port=12345/startcode=12345. Which doesn't make much sense? Also ':' is not a valid character for a hostname according to RFC. 

What uses this format? Let me know what I'm missing. On a related note if we support this format we really can't use HostAndPort as it assumes ':' will delimit a port and will throw an exception if it finds it in the hostname.

On a related note another behavior change since the introduction of HostAndPort is that '-1' can no longer be used as a port number (or any negative number). Would this be a breaking change? Let me know how you'd like to go about this change. 



Attached WIP rebased patch which addresses the following:

{quote}
Yeah, address the groups naming thing. 
{quote}
Partially done. I renamed the main proto classes and package name to Group -> RSGroup. Should I just blindly prepend all entities with Group in it to RSGroup?

{quote}
Address the ServerName questions, breaking compat is not an option.
{quote}
Waiting on response from [~eclark]

{quote}
Don't go around the Metrics to get to jmx
{quote}
Waiting on response from [~eclark]

{quote}
Don't create a new protobuf class when not needed
{quote}
Done

{quote}
Move it into a different module with a profile that doesn't include it.
{quote}
Partially done. Most of the classes are in a different module. Need to address circular dependecy between hbase-rsgroup and hbase-server. Since the MasterObserver CP has hooks into the GroupAdminEndpoint. Pom file needs a bit of streamlining as well.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green} 0m 0s {color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 15 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 4m 5s {color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 5m 35s {color} | {color:green} master passed with JDK v1.8.0_72 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 5m 5s {color} | {color:green} master passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 25m 35s {color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 3m 4s {color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} hbaseprotoc {color} | {color:green} 4m 0s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 10m 25s {color} | {color:red} branch/. no findbugs output file (./target/findbugsXml.xml) {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 3m 48s {color} | {color:green} master passed with JDK v1.8.0_72 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 4m 46s {color} | {color:green} master passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 5m 18s {color} | {color:green} the patch passed with JDK v1.8.0_72 {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green} 5m 18s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 5m 18s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 5m 32s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green} 5m 32s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 5m 32s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 13m 3s {color} | {color:red} Patch generated 17 new checkstyle issues in root (total was 637, now 650). {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 1m 38s {color} | {color:red} Patch generated 3 new checkstyle issues in hbase-client (total was 188, now 189). {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 12s {color} | {color:red} Patch generated 4 new checkstyle issues in hbase-rsgroup (total was 0, now 4). {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 3m 55s {color} | {color:red} Patch generated 10 new checkstyle issues in hbase-server (total was 415, now 424). {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 3m 16s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} rubocop {color} | {color:red} 0m 28s {color} | {color:red} The applied patch generated 137 new rubocop issues (total was 263, now 399). {color} |
| {color:red}-1{color} | {color:red} ruby-lint {color} | {color:red} 0m 11s {color} | {color:red} The applied patch generated 68 new ruby-lint issues (total was 137, now 205). {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 1s {color} | {color:red} The patch has 94 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 2s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 29m 22s {color} | {color:green} Patch does not cause any errors with Hadoop 2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.1 2.6.2 2.6.3 2.7.1. {color} |
| {color:green}+1{color} | {color:green} hbaseprotoc {color} | {color:green} 3m 39s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 10m 49s {color} | {color:red} patch/. no findbugs output file (./target/findbugsXml.xml) {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 1m 8s {color} | {color:red} hbase-common introduced 3 new FindBugs issues. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 4m 0s {color} | {color:green} the patch passed with JDK v1.8.0_72 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 4m 55s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 26m 7s {color} | {color:red} root in the patch failed with JDK v1.8.0_72. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 1m 3s {color} | {color:green} hbase-client in the patch passed with JDK v1.8.0_72. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 1s {color} | {color:green} hbase-common in the patch passed with JDK v1.8.0_72. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 19s {color} | {color:green} hbase-it in the patch passed with JDK v1.8.0_72. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 29s {color} | {color:green} hbase-protocol in the patch passed with JDK v1.8.0_72. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 23s {color} | {color:green} hbase-rsgroup in the patch passed with JDK v1.8.0_72. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 18m 19s {color} | {color:red} hbase-server in the patch failed with JDK v1.8.0_72. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 7m 47s {color} | {color:red} hbase-shell in the patch failed with JDK v1.8.0_72. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 27m 30s {color} | {color:red} root in the patch failed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 1m 3s {color} | {color:green} hbase-client in the patch passed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 1m 56s {color} | {color:green} hbase-common in the patch passed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 20s {color} | {color:green} hbase-it in the patch passed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 26s {color} | {color:green} hbase-protocol in the patch passed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 23s {color} | {color:green} hbase-rsgroup in the patch passed with JDK v1.7.0_95. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 18m 58s {color} | {color:red} hbase-server in the patch failed with JDK v1.7.0_95. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 7m 16s {color} | {color:red} hbase-shell in the patch failed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 1m 26s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 297m 3s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hbase-common |
|  |  Class org.apache.hadoop.hbase.ServerName defines non-transient non-serializable instance field hostAndPort  In ServerName.java:instance field hostAndPort  In ServerName.java |
|  |  Class org.apache.hadoop.hbase.rsgroup.GroupInfo defines non-transient non-serializable instance field servers  In GroupInfo.java:instance field servers  In GroupInfo.java |
|  |  Class org.apache.hadoop.hbase.rsgroup.GroupInfo defines non-transient non-serializable instance field tables  In GroupInfo.java:instance field tables  In GroupInfo.java |
| JDK v1.8.0_72 Failed junit tests | hadoop.hbase.ipc.TestRpcMetrics |
|   | hadoop.hbase.regionserver.TestMetricsRegion |
|   | hadoop.hbase.regionserver.TestMetricsRegionServer |
|   | hadoop.hbase.client.TestShell |
|   | hadoop.hbase.client.TestShellGroups |
|   | hadoop.hbase.ipc.TestRpcMetrics |
|   | hadoop.hbase.regionserver.TestMetricsRegion |
|   | hadoop.hbase.regionserver.TestMetricsRegionServer |
|   | hadoop.hbase.client.TestShell |
|   | hadoop.hbase.client.TestShellGroups |
| JDK v1.7.0_95 Failed junit tests | hadoop.hbase.ipc.TestRpcMetrics |
|   | hadoop.hbase.regionserver.TestMetricsRegion |
|   | hadoop.hbase.regionserver.TestMetricsRegionServer |
|   | hadoop.hbase.client.TestShell |
|   | hadoop.hbase.client.TestShellGroups |
|   | hadoop.hbase.ipc.TestRpcMetrics |
|   | hadoop.hbase.regionserver.TestMetricsRegion |
|   | hadoop.hbase.regionserver.TestMetricsRegionServer |
|   | hadoop.hbase.client.TestShell |
|   | hadoop.hbase.client.TestShellGroups |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=1.9.1 Server=1.9.1 Image:yetus/hbase:date2016-02-25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12789833/hbase-6721-v26_draft1.patch |
| JIRA Issue | HBASE-6721 |
| Optional Tests |  asflicense  javac  javadoc  unit  findbugs  hadoopcheck  hbaseanti  checkstyle  compile  xml  cc  hbaseprotoc  rubocop  ruby_lint  |
| uname | Linux ae4c5edeceba 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/component/dev-support/hbase-personality.sh |
| git revision | master / 6e9d355 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/diff-checkstyle-root.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/diff-checkstyle-hbase-client.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/diff-checkstyle-hbase-rsgroup.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/diff-checkstyle-hbase-server.txt |
| rubocop | v0.37.2 |
| rubocop | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/diff-patch-rubocop.txt |
| ruby-lint | v2.1.0 |
| ruby-lint | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/diff-patch-ruby-lint.txt |
| whitespace | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/whitespace-eol.txt |
| findbugs | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/new-findbugs-hbase-common.html |
| unit | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-root-jdk1.8.0_72.txt |
| unit | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-hbase-server-jdk1.8.0_72.txt |
| unit | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-hbase-shell-jdk1.8.0_72.txt |
| unit | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-root-jdk1.7.0_95.txt |
| unit | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-hbase-server-jdk1.7.0_95.txt |
| unit | https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-hbase-shell-jdk1.7.0_95.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-root-jdk1.8.0_72.txt https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-hbase-server-jdk1.8.0_72.txt https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-hbase-shell-jdk1.8.0_72.txt https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-root-jdk1.7.0_95.txt https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-hbase-server-jdk1.7.0_95.txt https://builds.apache.org/job/PreCommit-HBASE-Build/688/artifact/patchprocess/patch-unit-hbase-shell-jdk1.7.0_95.txt |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/688/testReport/ |
| modules | C: . hbase-client hbase-common hbase-it hbase-protocol hbase-rsgroup hbase-server hbase-shell U: . |
| Max memory used | 195MB |
| Powered by | Apache Yetus 0.1.0   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/688/console |


This message was automatically generated.



bq. Need to address circular dependecy between hbase-rsgroup and hbase-server. Since the MasterObserver CP has hooks into the GroupAdminEndpoint.
If it is a strict "no" on the RS Group coprocessor reaching out to the internals of AccessController (as per above comment from [~apurtell]), then the only option is to add the rs group specific methods to the base MasterObserver. I think it is acceptable since they would be empty method declarations if the module is not used. 

For the ServerName change, this parts seems relevant (from ServerName.parseFrom()): 
{code}
    // The str returned could be old style -- pre hbase-1502 -- which was
    // hostname and port seperated by a colon rather than hostname, port and
    // startcode delimited by a ','.​
{code} 
HBASE-1502 is committed around 0.92 timeframe, so I am not sure whether we will have any such server name strings around anymore. 

{quote}
HBASE-1502 is committed around 0.92 timeframe, so I am not sure whether we will have any such server name strings around anymore.
{quote}
Thanks for pointing that out [~enis]. This is a clear indication that the ServerNameform in TestMasterStatusServlet  is a typo 'rootserver:123,12345' as it is neither the old form or the new form. Then that leaves wether negative port numbers are a breaking change or not? As HostAndPort doesn't allow it.

The negative port numbers are unlikely to cause any errors so I would say we can ignore that. So then the last part is getting better naming for things.

Attached patch which is pretty much done except for the negative port concern.

{quote}
Yeah, address the groups naming thing.
{quote}
Done. Change classes, packages, etc to RSGroups.

{quote}
Address the ServerName questions, breaking compat is not an option.
{quote}
It seems clear now the serverName form I fixed in TestMasterStatusServlet is indeed a typo. So the current patch can still support the old form. Tho it cannot support negative port numbers due to port restrictions. Let me know if this is not acceptable. 

{quote}
Don't go around the Metrics to get to jmx
{quote}
Removed JMX this code from the base patch and filed HBASE-15382

{quote}
Don't create a new protobuf class when not needed
{quote}
Done

{quote}
Move it into a different module with a profile that doesn't include it.
{quote}
Done. Tho RSGroup protos are still in hbase-protocol and there's one POJO (RSGroupInfo) that had to stay in hbase-common as it is needed by the RSGroup related MasterObserver CP methods. RSGroup shell is also still in hbase-shell.


{quote}
So then the last part is getting better naming 
{quote}
Are you referring to the rename from Group -> RSGroup? Then the just uploaded patch should've addressed it. Let me know if I missed anything. If not let me know what I'm missing?

Oh I was looking at the draft rather than the last version. I see that lots of stuff has been fixed.  Thanks.

Ok i'm officially -0.9999 this patch. Not minus one.

I skimmed the patch looking around the changes, looks good. One minor nit is that instead of {{mvn install -Dskip-rsgroup=false}} which is double negation, we can do {{mvn install -Drsgroup=true}}. 

Let's get a hadoopqa run. After we address the reported issues, I'll do the commit. 



Thanks [~eclark]!

[~enis] doesn't the activation property actually only tests for the presence of a propery and ignores the value:

http://maven.apache.org/guides/introduction/introduction-to-profiles.html

So in your case  'mvn install -Drsgroup=false' would still enable the rsgroup profile?

I forgot to rebase when I did rebase had a minor conflict. So I uploaded a rebased patch.

bq. So in your case 'mvn install -Drsgroup=false' would still enable the rsgroup profile?
If that is the case, it is still fine. If you don't want to activate, don't pass the property at all. Then how we can activate this with {{!skip-rsgroup}} ? I mean how do you activate the profile with the current patch without specifying {{-P}}? 

{quote}
If that is the case, it is still fine. If you don't want to activate, don't pass the property at all. Then how we can activate this with !skip-rsgroup ? I mean how do you activate the profile with the current patch without specifying -P?
{quote}

It is active by default. If you don't specify the skip-rsgroup property the rsgroup module will get built. 

ie

# builds hbase-rsgoup
mvn install 

#does not build hbase-rsgroup
mvn install -Dskip-rsgoup 

It's basically the same idea as -DskipITs and -DskipTests.

Of course you can explicitly specify it via the -P switch as well:

# builds hbase-rsgoup
mvn install -P rsgroup

#does not build hbase-rsgroup
mvn install -P !rsgroup




bq. It is active by default. If you don't specify the skip-rsgroup property the rsgroup module will get built.
Ok, perfect. I did not see the {{<activeByDefault>true</activeByDefault>}} tag, so I was assuming the other way around. 

Not sure why the hadoopqa is not running for the latest patches.  

Looks like the build aborted because it timed out. 

https://builds.apache.org/job/PreCommit-HBASE-Build/816/console

Can we try and rerun the build? I'm going to try and run the tests on my local machine as well.


Reattaching for hadoopqa. 

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} patch {color} | {color:blue} 0m 2s {color} | {color:blue} The patch file was not named according to hbase's naming conventions. Please see https://yetus.apache.org/documentation/latest/precommit-patchnames for instructions. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red} 0m 6s {color} | {color:red} HBASE-6721 does not apply to master. Rebase required? Wrong Branch? See https://yetus.apache.org/documentation/latest/precommit-patchnames for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12791376/hbase-6721-v27.patch.txt |
| JIRA Issue | HBASE-6721 |
| Powered by | Apache Yetus 0.1.0   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/826/console |


This message was automatically generated.



| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | {color:red} patch {color} | {color:red} 0m 4s {color} | {color:red} HBASE-6721 does not apply to master. Rebase required? Wrong Branch? See https://yetus.apache.org/documentation/latest/precommit-patchnames for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12791388/hbase-6721-v27.patch |
| JIRA Issue | HBASE-6721 |
| Powered by | Apache Yetus 0.1.0   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/828/console |


This message was automatically generated.



rebase. 

rebased patch.

[~enis] Looks like the build aborted again due to 500mins timeout:

https://builds.apache.org/job/PreCommit-HBASE-Build/843/console

I'm wondering if it's really expected to finish in 500 mins? 

- Pre-unit tests (~160mins total elapsed)
- jdk7 unit tests (410mins total elapsed)
-jdk8....

The longest component unit test run by a large margin is hbase-server and hbase-component. I compared the run times to other successful pre-commit builds and hbase-server runtime seems to be pretty similar. Tho I'm wondering why we are running hbase-server again when hbase-component has already been run? The redundancy seems to be what's making the build go over the time limit?



bq. The longest component unit test run by a large margin is hbase-server and hbase-component
I am not sure what is /component. Maybe [~busbey] can comment on it. 

At the worst we can run the tests locally (I can do that sometime this week). I saw a couple of checkstyle / findbugs issues. Did you have a chance to look into those? 

I don't see a test result for "hbase-component" in the yetus output. the hbase-server module is ridiculously slow and I see that we're running both at "root" (the top of the project) and other individual modules. That looks like a bug.

Can someone rebase the patch so I can retry now that we've update to Yetus 0.2.0?

There's an open discussion on the "Testing and CI" dev@hbase thread about switching to only doing unit tests with jdk7, which would also help keep us under the 500 minute limit by avoiding 2x hbase-server runs.

bq. Can someone rebase the patch so I can retry now that we've update to Yetus 0.2.0?

And now I see that it is already doing this. Ignore this part :)

I started [build #894|https://builds.apache.org/job/PreCommit-HBASE-Build/894] with the flags to tell yetus not to re-run unit tests per jdk.

{quote}
I don't see a test result for "hbase-component" in the yetus output. the hbase-server module is ridiculously slow and I see that we're running both at "root" (the top of the project) and other individual modules. That looks like a bug.
{quote}
Thanks for taking a look at this [~busbey].  Sorry I meant "root", I mentioned component because the directory is called "component" which I realize now just makes things confusing. If this is a bug that'd be in Yetus?



Addressed findbugs and checkstyle comments.

Findbugs warned about ServerName being serializable while HostAndPort is not. So I made HostAndPort transient and resolved lazily. Let me know if I should be addressing this in another way.

bq. Thanks for taking a look at this Sean Busbey. Sorry I meant "root", I mentioned component because the directory is called "component" which I realize now just makes things confusing. If this is a bug that'd be in Yetus?

Ah. the directory is named component because of how I set up the jenkins job. That's just to distinguish it from "test_framework" in a way that I can copy/paste as I set up other projects.

We're definitely running individual modules even after we've determined we have to run at the top level of the project. That's *probably* a bug in yetus, but I haven't confirmed yet that we don't do module reordering in [our test personality|https://github.com/apache/hbase/blob/master/dev-support/hbase-personality.sh]. If we do, then the bug could be in our personality rather than yetus proper.

[~busbey] looks like the build timed out again but this time because each module was run twice? 

thanks for finding an edge case for yetus. ;)

I'll spin up a clean build environment and see what yetus does there.

Thanks [~busbey], please share a link to the build once it's running.

[~enis] Even tho the last run timed out, there are no longer any new checkstyle or findbugs issues.

Given the recent problems with hadoopqa, I've checked the https://builds.apache.org/job/PreCommit-HBASE-Build/896 manually which is a timed-out build. The findbugs and checkstyle issues are addressed. I've also run the unit tests on a node, and it passed except for 1 flaky test. 

I'll commit this shortly. 

Committed this to master. Thanks [~toffer] and others. 



Added a release note as well. Hopefully it is clear enough. 

FAILURE: Integrated in HBase-Trunk_matrix #777 (See [https://builds.apache.org/job/HBase-Trunk_matrix/777/])
HBASE-6721 RegionServer Group based Assignment (Francis Liu) (enis: rev ca816f0780f6e5a117b85810cf35f3b29c964ddc)
* hbase-shell/src/main/ruby/shell/commands/get_table_rsgroup.rb
* hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRSGroupBasedLoadBalancer.java
* hbase-protocol/pom.xml
* hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterAndRegionObserver.java
* hbase-shell/src/main/ruby/hbase/rsgroup_admin.rb
* hbase-it/pom.xml
* hbase-rsgroup/pom.xml
* hbase-shell/src/main/ruby/shell/commands/balance_rsgroup.rb
* hbase-shell/src/main/ruby/shell/commands/move_rsgroup_servers.rb
* hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManager.java
* hbase-shell/src/main/ruby/shell/commands/get_server_rsgroup.rb
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
* hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
* hbase-it/src/test/java/org/apache/hadoop/hbase/rsgroup/IntegrationTestRSGroup.java
* hbase-shell/src/main/ruby/shell/commands/list_rsgroups.rb
* hbase-shell/src/main/ruby/shell/commands/get_rsgroup.rb
* hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/VerifyingRSGroupAdminClient.java
* hbase-shell/src/test/rsgroup/org/apache/hadoop/hbase/client/rsgroup/TestShellRSGroups.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
* hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java
* hbase-shell/src/test/java/org/apache/hadoop/hbase/client/TestShell.java
* hbase-shell/src/main/ruby/shell/commands/add_rsgroup.rb
* hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupableBalancer.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestSimpleRegionNormalizer.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
* hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroups.java
* hbase-protocol/src/main/protobuf/RSGroupAdmin.proto
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
* hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminEndpoint.java
* hbase-common/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java
* hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
* hbase-protocol/src/main/protobuf/RSGroup.proto
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
* hbase-shell/src/main/ruby/hbase.rb
* hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/RSGroupAdminProtos.java
* hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java
* hbase-shell/src/test/ruby/test_helper.rb
* hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/RSGroupProtos.java
* hbase-shell/pom.xml
* hbase-shell/src/test/ruby/shell/rsgroup_shell_test.rb
* hbase-shell/src/main/ruby/shell/commands/move_rsgroup_tables.rb
* hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java
* hbase-shell/src/main/ruby/hbase/hbase.rb
* hbase-shell/src/main/ruby/shell/commands/remove_rsgroup.rb
* hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
* hbase-shell/src/main/ruby/shell.rb
* hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterObserver.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java
* hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupSerDe.java
* hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
* pom.xml
* hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsBase.java
* hbase-shell/src/main/ruby/shell/commands.rb
* hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java
* hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java
* hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon
* hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdmin.java
* hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsOfflineMode.java
* hbase-common/src/main/java/org/apache/hadoop/hbase/ServerName.java


Awesome! Thanks a lot [~enis]! Thanks a lot to all the reviewers and commenters!

BTW given this is pluggable any chance we can backport this to 1.x and 0.98 as an experimental feature? I'd like to start using this version internally.


please move remaining open sub tasks to top-level tasks.

If this is going to go back earlier than 2.0, I would like a [DISCUSS] thread on what folks want to use as a gating threshold.

{quote}
please move remaining open sub tasks to top-level tasks.
{quote}
Will do. 

{quote}
If this is going to go back earlier than 2.0, I would like a [DISCUSS] thread on what folks want to use as a gating threshold.
{quote}
I can start the thread. Is the discussion about region server groups specifically or about a more general gating threshold?

{quote}
bq. If this is going to go back earlier than 2.0, I would like a \[DISCUSS\] thread on what folks want to use as a gating threshold.
I can start the thread. Is the discussion about region server groups specifically or about a more general gating threshold?
{quote}

In the past we've only done such threads for specific issues. I'd love to have a generally agreed upon set of gates for new features, but I'm not sure we'll reach a conclusion in a timely manner. ;)

I'd say either of the two you'd prefer to start is good.

As where I was on the MOB backport, I think it would be good to have evidence from functional, stability, and performance tests (the basics: PE, LTT, YCSB, ITs) that the feature does not impact folks who don't want it. The way group based assignment is implemented suggests we will get a good result. 

BTW, if there were a backport to branch-1 we'd probably try it. (Or if there were not a backport at the time, I might do it.)

{quote}
I'd say either of the two you'd prefer to start is good.
{quote}
Thanks [~busbey], I started the discuss thread. Also, I've moved the jiras. 

{quote}
BTW, if there were a backport to branch-1 we'd probably try it. (Or if there were not a backport at the time, I might do it.)
{quote}
Thanks! BTW I think [~enis] might already have a backport as he created one before. 



I understand the desire to keep this feature away from casual use by avoiding the ref guide, but I remain uncomfortable with the release note on a JIRA being the only docs on how to enable it, what it does, and what to worry about. Especially because that makes us less likely, as a community, to put in the needed hardening over time to open up for general use.

I know I've joked before about writing a book on "The Undocumented HBase" to mixed responses, but what do folks think about starting docs outside of the ref guide for general "you might cut yourself" / "we aren't sure this is a good idea or staying" features so that we at least have a common starting point for invested in hbase looking to push what it can reasonably do with an experienced ops team?

...maybe I should post this to dev@? after the holidays?

FAILURE: Integrated in Jenkins build HBase-Trunk_matrix #2505 (See [https://builds.apache.org/job/HBase-Trunk_matrix/2505/])
HBASE-17624 Address late review of HBASE-6721, rsgroups feature (stack: rev e019961150624236615fe7bd4a9017622d9f78d0)
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
* (edit) hbase-common/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
* (edit) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroups.java
* (edit) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRSGroupBasedLoadBalancer.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupSerDe.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java
* (delete) hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestAddressing.java
* (edit) hbase-shell/src/main/ruby/shell/commands/get_table_rsgroup.rb
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java
* (edit) hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java
* (edit) hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
* (edit) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/VerifyingRSGroupAdminClient.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManager.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestDurability.java
* (edit) hbase-it/src/test/java/org/apache/hadoop/hbase/rsgroup/IntegrationTestRSGroup.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdmin.java
* (edit) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsOfflineMode.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
* (add) hbase-common/src/main/java/org/apache/hadoop/hbase/util/Address.java
* (edit) hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestSimpleRequestController.java
* (edit) hbase-shell/src/main/ruby/shell/commands/balance_rsgroup.rb
* (edit) hbase-common/src/main/java/org/apache/hadoop/hbase/util/Addressing.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
* (edit) hbase-shell/src/main/ruby/shell/commands/list_rsgroups.rb
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/TestServerName.java
* (edit) hbase-shell/src/main/ruby/shell/commands/remove_rsgroup.rb
* (edit) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsBase.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterAndRegionObserver.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
* (edit) hbase-rsgroup/pom.xml
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java
* (edit) hbase-shell/src/main/ruby/shell/commands/move_tables_rsgroup.rb
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java
* (edit) hbase-shell/src/main/ruby/shell/commands/get_server_rsgroup.rb
* (edit) hbase-shell/src/main/ruby/shell/commands/get_rsgroup.rb
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterObserver.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java
* (edit) hbase-shell/src/main/ruby/hbase/rsgroup_admin.rb
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java
* (edit) hbase-shell/src/test/ruby/shell/rsgroup_shell_test.rb
* (edit) hbase-shell/src/main/ruby/shell/commands/move_servers_rsgroup.rb
* (edit) hbase-common/src/main/java/org/apache/hadoop/hbase/ServerName.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupableBalancer.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALReaderOnSecureWAL.java
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/Utility.java
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminEndpoint.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestWALReplay.java
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java


{code:title=RSGroupBasedLoadBalancer.roundRobinAssignment|borderStyle=solid}
  ...
  for(String groupKey : regionMap.keySet()) {
    if (regionMap.get(groupKey).size() > 0) {
      Map<ServerName, List<HRegionInfo>> result =
      this.internalBalancer.roundRobinAssignment(
        regionMap.get(groupKey),
        serverMap.get(groupKey));
      if(result != null) {
        assignments.putAll(result);
      }
    }
  }
  ...
{code}
if two group both has BOGUS_SERVER_NAME,  assignments.putAll may be not right

{code:title=RSGroupBasedLoadBalancer.randomAssignment|borderStyle=solid}
public ServerName randomAssignment(HRegionInfo region,
  List<ServerName> servers) throws HBaseIOException {
  ListMultimap<String,HRegionInfo> regionMap = LinkedListMultimap.create();
  ListMultimap<String,ServerName> serverMap = LinkedListMultimap.create();
  generateGroupMaps(Lists.newArrayList(region), servers, regionMap, serverMap);
  List<ServerName> filteredServers = serverMap.get(regionMap.keySet().iterator().next());
  return this.internalBalancer.randomAssignment(region, filteredServers);
}
{code}
if internalBalancer.randomAssignment return BOGUS_SERVER_NAME, how does AM work?
I think we shoud return null instead of BOGUS_SERVER_NAME

This issue is resolved [~javaman_chen] Open a new one or ask up on the dev mailing list? Thanks.

FAILURE: Integrated in Jenkins build HBase-1.5 #110 (See [https://builds.apache.org/job/HBase-1.5/110/])
HBASE-15631 Backport Regionserver Groups (HBASE-6721) to branch-1 (apurtell: rev 64328caef0bb712bb69d0241b4b8b3474a82702c)
* (edit) pom.xml
* (edit) hbase-protocol/pom.xml
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupProtobufUtil.java
* (add) hbase-it/src/test/rsgroup/org/apache/hadoop/hbase/rsgroup/IntegrationTestRSGroup.java
* (add) hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/RSGroupProtos.java
* (edit) hbase-shell/src/main/ruby/shell/commands.rb
* (add) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRSGroupBasedLoadBalancer.java
* (edit) hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
* (add) hbase-shell/src/main/ruby/shell/commands/balance_rsgroup.rb
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupSerDe.java
* (add) hbase-shell/src/main/ruby/shell/commands/move_tables_rsgroup.rb
* (edit) hbase-shell/src/test/java/org/apache/hadoop/hbase/client/TestShell.java
* (add) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsOfflineMode.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java
* (edit) hbase-shell/src/main/ruby/hbase.rb
* (edit) hbase-shell/src/main/ruby/hbase/hbase.rb
* (add) hbase-protocol/src/main/protobuf/RSGroup.proto
* (add) hbase-shell/src/test/ruby/shell/rsgroup_shell_test.rb
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupableBalancer.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
* (add) hbase-shell/src/main/ruby/shell/commands/get_server_rsgroup.rb
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestSimpleRegionNormalizer.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdmin.java
* (add) hbase-shell/src/main/ruby/shell/commands/list_rsgroups.rb
* (add) hbase-shell/src/main/ruby/shell/commands/move_servers_tables_rsgroup.rb
* (edit) hbase-shell/pom.xml
* (add) hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/RSGroupAdminProtos.java
* (add) hbase-shell/src/main/ruby/shell/commands/get_table_rsgroup.rb
* (add) hbase-shell/src/main/ruby/hbase/rsgroup_admin.rb
* (add) hbase-shell/src/test/rsgroup/org/apache/hadoop/hbase/client/rsgroup/TestShellRSGroups.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
* (edit) hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon
* (edit) hbase-it/pom.xml
* (add) hbase-common/src/main/java/org/apache/hadoop/hbase/net/Address.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterObserver.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
* (add) hbase-shell/src/main/ruby/shell/commands/remove_rsgroup.rb
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java
* (edit) hbase-shell/src/main/ruby/shell.rb
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java
* (add) hbase-shell/src/main/ruby/shell/commands/move_servers_rsgroup.rb
* (add) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/VerifyingRSGroupAdminClient.java
* (add) hbase-protocol/src/main/protobuf/RSGroupAdmin.proto
* (edit) hbase-client/src/main/java/org/apache/hadoop/hbase/ServerName.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminEndpoint.java
* (add) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroups.java
* (add) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsBase.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterAndRegionObserver.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManager.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
* (add) hbase-shell/src/main/ruby/shell/commands/get_rsgroup.rb
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
* (add) hbase-shell/src/main/ruby/shell/commands/add_rsgroup.rb
* (edit) hbase-shell/src/test/ruby/test_helper.rb
* (add) hbase-rsgroup/pom.xml
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java


SUCCESS: Integrated in Jenkins build HBase-1.4 #967 (See [https://builds.apache.org/job/HBase-1.4/967/])
HBASE-15631 Backport Regionserver Groups (HBASE-6721) to branch-1 (apurtell: rev c320007638fe57f858aeac974cdd6bbd6b9dd5eb)
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterObserver.java
* (add) hbase-protocol/src/main/protobuf/RSGroupAdmin.proto
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
* (add) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/VerifyingRSGroupAdminClient.java
* (add) hbase-shell/src/main/ruby/shell/commands/move_tables_rsgroup.rb
* (add) hbase-shell/src/main/ruby/shell/commands/get_table_rsgroup.rb
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManager.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java
* (add) hbase-shell/src/main/ruby/shell/commands/add_rsgroup.rb
* (add) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsOfflineMode.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
* (edit) hbase-shell/src/test/ruby/test_helper.rb
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestSimpleRegionNormalizer.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterAndRegionObserver.java
* (edit) hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon
* (add) hbase-shell/src/main/ruby/shell/commands/move_servers_tables_rsgroup.rb
* (add) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroups.java
* (edit) hbase-shell/src/test/java/org/apache/hadoop/hbase/client/TestShell.java
* (edit) pom.xml
* (edit) hbase-shell/src/main/ruby/hbase/hbase.rb
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
* (add) hbase-shell/src/main/ruby/shell/commands/move_servers_rsgroup.rb
* (add) hbase-it/src/test/rsgroup/org/apache/hadoop/hbase/rsgroup/IntegrationTestRSGroup.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupSerDe.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
* (edit) hbase-shell/src/main/ruby/hbase.rb
* (add) hbase-shell/src/main/ruby/shell/commands/get_server_rsgroup.rb
* (edit) hbase-it/pom.xml
* (add) hbase-protocol/src/main/protobuf/RSGroup.proto
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
* (add) hbase-shell/src/main/ruby/hbase/rsgroup_admin.rb
* (add) hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/RSGroupProtos.java
* (edit) hbase-shell/src/main/ruby/shell.rb
* (add) hbase-shell/src/main/ruby/shell/commands/remove_rsgroup.rb
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminEndpoint.java
* (add) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsBase.java
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupProtobufUtil.java
* (add) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRSGroupBasedLoadBalancer.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
* (edit) hbase-protocol/pom.xml
* (edit) hbase-shell/pom.xml
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java
* (add) hbase-common/src/main/java/org/apache/hadoop/hbase/net/Address.java
* (add) hbase-shell/src/test/rsgroup/org/apache/hadoop/hbase/client/rsgroup/TestShellRSGroups.java
* (edit) hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
* (edit) hbase-shell/src/main/ruby/shell/commands.rb
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdmin.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java
* (add) hbase-shell/src/main/ruby/shell/commands/get_rsgroup.rb
* (add) hbase-rsgroup/pom.xml
* (edit) hbase-client/src/main/java/org/apache/hadoop/hbase/ServerName.java
* (add) hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/RSGroupAdminProtos.java
* (add) hbase-shell/src/main/ruby/shell/commands/balance_rsgroup.rb
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupableBalancer.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
* (add) hbase-shell/src/main/ruby/shell/commands/list_rsgroups.rb
* (add) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java
* (add) hbase-shell/src/test/ruby/shell/rsgroup_shell_test.rb
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
* (edit) hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java


FAILURE: Integrated in Jenkins build HBase-1.5 #111 (See [https://builds.apache.org/job/HBase-1.5/111/])
Amend HBASE-15631 Backport Regionserver Groups (HBASE-6721) to branch-1 (apurtell: rev d0629d0f121e66a7d52caceb4ac290e879ba6eb1)
* (edit) hbase-rsgroup/pom.xml


FAILURE: Integrated in Jenkins build HBase-1.4 #972 (See [https://builds.apache.org/job/HBase-1.4/972/])
Amend HBASE-15631 Backport Regionserver Groups (HBASE-6721) to branch-1 (apurtell: rev 11b7e1bc8cafca686d020b67239248f1347ad775)
* (edit) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRSGroupBasedLoadBalancer.java
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java


FAILURE: Integrated in Jenkins build HBase-1.5 #115 (See [https://builds.apache.org/job/HBase-1.5/115/])
Amend HBASE-15631 Backport Regionserver Groups (HBASE-6721) to branch-1 (apurtell: rev 089acc74dad0bd6368e4bb584fde2a84b8c8ab56)
* (edit) hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java
* (edit) hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRSGroupBasedLoadBalancer.java


Revisiting this as I am wondering if we could evaluate and commit the master WebUI patch {{6721-master-webUI.patch}}, which was posted by [~zjushch] but not committed yet. Thoughts?

Sounds good [~daisuke.kobayashi]. Open subtask or new issue? Have you tried it sir?

bq. Open subtask or new issue?

Ok, I will open a new subtask of this.

bq. Have you tried it sir?

Due to a build issue on my local instance, I have not yet completed it. 

Thanks [~stack]!

bq. Due to a build issue on my local instance, I have not yet completed it.

I can run any patch you might have [~daisuke.kobayashi] ... Just say sir.

While studying the code, I happen to find HBASE-19799 where it added UI for RSGroups. Hence I may close this request then.
Thank you for having a look, sir [~stack]!

