This was working correctly in the initial implementation of CASSANDRA-16843, but ended up getting removed when I simplified the patch in the final review pass.

I added a few regression tests [on this commit|https://github.com/pauloricardomg/cassandra/commit/dd10697ecdd234ddbbbbe032fc076f36787c9197] reproducing the issue with both auto_snapshots and manual snapshots.

The fix is straightforward: just use the new {{SnapshotLoader}} implementation when doing the initial snapshot load on {{{}SnapshotManager{}}}, which loads snapshots of dropped tables correctly ([commit|https://github.com/pauloricardomg/cassandra/commit/6731ee21b79b2345274307e2ba8800f50eb36a98]).

|[PR|https://github.com/apache/cassandra/pull/1615]|[4.1 CI|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1701/]|[trunk CI|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1702/]|

[~smiklosovic] mind taking a look?

Thanks Paulo, I did the review. Waiting for your feedback.

Addressed review comments on github [PR|https://github.com/apache/cassandra/pull/1615] and fixed two extra nits found during testing:
 * [fix log|https://github.com/apache/cassandra/pull/1615/commits/333f813f6872b87f7e0f79f71a4f9688a9cc3193]
 * [Add verbose log to trace|https://github.com/apache/cassandra/pull/1615/commits/c2d7c679c3492b66d41f8c60ce29d38efdf1c88e]

This is what the output of nodetool listsnapshots looks like after doing the following operations:
 * global snapshot of all keyspaces
 * drop keyspace ks2 with auto_snapshot_ttl unset
 * drop keyspace ks1 with auto_snapshot_ttl=1 minute
 * nodetool snapshots

{noformat}
grep -v "all_keyspaces_snapshot" snapshot.txt
Snapshot Details:
Snapshot name                   Keyspace name      Column family name             True size Size on disk Creation time            Expiration time
dropped-1652386867075-my_table  ks2                my_table                       5.81 KiB  5.81 KiB     2022-05-12T20:21:07.075Z
dropped-1652388565491-my_table  ks                 my_table                       5.83 KiB  5.83 KiB     2022-05-12T20:49:25.491Z 2022-05-12T20:50:25.491Z
dropped-1652388565604-my_table2 ks                 my_table2                      5.83 KiB  5.83 KiB     2022-05-12T20:49:25.604Z 2022-05-12T20:50:25.604Z
dropped-1652386867016-my_table2 ks2                my_table2                      5.81 KiB  5.81 KiB     2022-05-12T20:21:07.016Z

Total TrueDiskSpaceUsed: 0 byte
pmottagomes@C02FD3GTMD6R Documents %
{noformat}
After this I restarted the node and verified the snapshot from ks1 was removed after 1 minute:
{noformat}
Snapshot Details:
Snapshot name                   Keyspace name      Column family name             True size Size on disk Creation time            Expiration time
dropped-1652386867075-my_table  ks2                my_table                       5.81 KiB  5.81 KiB     2022-05-12T20:21:07.075Z
dropped-1652386867016-my_table2 ks2                my_table2                      5.81 KiB  5.81 KiB     2022-05-12T20:21:07.016Z

Total TrueDiskSpaceUsed: 0 bytes
{noformat}
This is the debug logs of the manual test above:
{noformat}
DEBUG [main] 2022-05-12 17:51:07,265 SnapshotManager.java:119 - Adding snapshots: system:peers:37f71aca-7dc2-383b-a706-72528af04d4f:all_keyspaces_snapshot, system:transferred_ranges:6cad20f7-d4f5-3af2-b6e2-0da33c6c1f83:all_keyspaces_snapshot, system_traces:sessions:c5e99f16-8677-3914-b17e-960613512345:all_keyspaces_snapshot, system_schema:dropped_columns:5e7583b5-f3f4-3af1-9a39-b7e1d6f5f11f:all_keyspaces_snapshot, system_auth:role_members:0ecdaa87-f8fb-3e60-88d1-74fb36fe5c0d:all_keyspaces_snapshot, ks:my_table2:74a02c00-d230-11ec-b12d-d9d82874084f:dropped-1652388565604-my_table2, system:repairs:a3d277d1-cfaf-36f5-a2a7-38d5eea9ad6a:all_keyspaces_snapshot, ks2:my_table2:846d1fd0-d230-11ec-b12d-d9d82874084f:all_keyspaces_snapshot, system:IndexInfo:9f5c6374-d485-3229-9a0a-5094af9ad1e3:all_keyspaces_snapshot, system_schema:functions:96489b79-80be-3e14-a701-66a0b9159450:all_keyspaces_snapshot, system:top_partitions:7e5a361c-317c-351f-b15f-ffd8afd3dd4b:all_keyspaces_snapshot, system:peers_v2:c4325fbb-8e5e-3baf-bd07-0f9250ed818e:all_keyspaces_snapshot, system_distributed:partition_denylist:d6123acc-8649-3496-9d4e-f3fe39a6018b:all_keyspaces_snapshot, system_schema:views:9786ac1c-dd58-3201-a7cd-ad556410c985:all_keyspaces_snapshot, system_auth:resource_role_permissons_index:5f2fbdad-91f1-3946-bd25-d5da3a5c35ec:all_keyspaces_snapshot, system_schema:columns:24101c25-a2ae-3af7-87c1-b40ee1aca33f:all_keyspaces_snapshot, system:transferred_ranges_v2:1ff78f1a-7df1-3a2a-a998-6f4932270af5:all_keyspaces_snapshot, system:prepared_statements:18a9c257-6a0c-3841-ba71-8cd529849fef:all_keyspaces_snapshot, system:local:7ad54392-bcdd-35a6-8417-4e047860b377:all_keyspaces_snapshot, system_schema:types:5a8b1ca8-6602-3f77-a045-9273d308917a:all_keyspaces_snapshot, ks2:my_table:8eada2d0-d230-11ec-b12d-d9d82874084f:dropped-1652386867075-my_table, system:size_estimates:618f817b-005f-3678-b8a4-53f3930b8e86:all_keyspaces_snapshot, ks2:my_table:8eada2d0-d230-11ec-b12d-d9d82874084f:all_keyspaces_snapshot, system:peer_events:59dfeaea-8db2-3341-91ef-109974d81484:all_keyspaces_snapshot, system_auth:roles:5bc52802-de25-35ed-aeab-188eecebb090:all_keyspaces_snapshot, system_schema:indexes:0feb57ac-311f-382f-ba6d-9024d305702f:all_keyspaces_snapshot, system_schema:keyspaces:abac5682-dea6-31c5-b535-b3d6cffd0fb6:all_keyspaces_snapshot, system:table_estimates:176c39cd-b93d-33a5-a218-8eb06a56f66e:all_keyspaces_snapshot, system_auth:network_permissions:d46780c2-2f1c-3db9-b4c1-b8d9fbc0cc23:all_keyspaces_snapshot, ks:my_table:704a1850-d230-11ec-b12d-d9d82874084f:dropped-1652388565491-my_table, system:available_ranges:c539fcab-d65a-31d1-8133-d25605643ee3:all_keyspaces_snapshot, ks:my_table:704a1850-d230-11ec-b12d-d9d82874084f:all_keyspaces_snapshot, system_schema:aggregates:924c5587-2e3a-345b-b10c-12f37c1ba895:all_keyspaces_snapshot, system:view_builds_in_progress:6c22df66-c3bd-3df6-b74d-21179c6a9fe9:all_keyspaces_snapshot, system:batches:919a4bc5-7a33-3573-b03e-13fc3f68b465:all_keyspaces_snapshot, system:available_ranges_v2:4224a088-2ac9-3d0c-889d-fbb5f0facda0:all_keyspaces_snapshot, system_distributed:view_build_status:5582b59f-8e4e-35e1-b913-3acada51eb04:all_keyspaces_snapshot, system:peer_events_v2:0e65065f-e401-38ed-9507-b9213fae8d11:all_keyspaces_snapshot, system:paxos:b7b7f0c2-fd0a-3410-8c05-3ef614bb7c2d:all_keyspaces_snapshot, system:paxos_repair_history:ecb86667-40b2-3316-bb91-e612c8047457:all_keyspaces_snapshot, system:built_views:4b3c50a9-ea87-3d76-9101-6dbc9c38494a:all_keyspaces_snapshot, system_traces:events:8826e8e9-e16a-3728-8753-3bc1fc713c25:all_keyspaces_snapshot, system:sstable_activity:5a1ff267-ace0-3f12-8563-cfae6103c65e:all_keyspaces_snapshot, ks2:my_table2:846d1fd0-d230-11ec-b12d-d9d82874084f:dropped-1652386867016-my_table2, system_schema:triggers:4df70b66-6b05-3251-95a1-32b54005fd48:all_keyspaces_snapshot, system_distributed:repair_history:759fffad-624b-3181-80ee-fa9a52d1f627:all_keyspaces_snapshot, system_distributed:parent_repair_history:deabd734-b99d-3b9c-92e5-fd92eb5abf14:all_keyspaces_snapshot, system_schema:tables:afddfb9d-bc1e-3068-8056-eed6c302ba09:all_keyspaces_snapshot, system:compaction_history:b4dbb7b4-dc49-3fb5-b3bf-ce6e434832ca:all_keyspaces_snapshot, system:sstable_activity_v2:62efe31f-3be8-310c-8d29-8963439c1288:all_keyspaces_snapshot, system_auth:role_permissions:3afbe79f-2194-31a7-add7-f5ab90d8ec9c:all_keyspaces_snapshot, ks:my_table2:74a02c00-d230-11ec-b12d-d9d82874084f:all_keyspaces_snapshot.
DEBUG [main] 2022-05-12 17:51:07,266 SnapshotManager.java:104 - Adding expiring snapshot TableSnapshot{keyspaceName='ks', tableName='my_table2', tableId=74a02c00-d230-11ec-b12d-d9d82874084f, tag='dropped-1652388565604-my_table2', createdAt=2022-05-12T20:49:25.604Z, expiresAt=2022-05-12T20:50:25.604Z, snapshotDirs=[/Users/pmottagomes/.ccm/test/node1/data0/ks/my_table2-74a02c00d23011ecb12dd9d82874084f/snapshots/dropped-1652388565604-my_table2]}
DEBUG [main] 2022-05-12 17:51:07,266 SnapshotManager.java:104 - Adding expiring snapshot TableSnapshot{keyspaceName='ks', tableName='my_table', tableId=704a1850-d230-11ec-b12d-d9d82874084f, tag='dropped-1652388565491-my_table', createdAt=2022-05-12T20:49:25.491Z, expiresAt=2022-05-12T20:50:25.491Z, snapshotDirs=[/Users/pmottagomes/.ccm/test/node1/data0/ks/my_table-704a1850d23011ecb12dd9d82874084f/snapshots/dropped-1652388565491-my_table]}
INFO  [main] 2022-05-12 17:51:07,266 SnapshotManager.java:128 - Scheduling expired snapshot cleanup with initialDelaySeconds={} and cleanupPeriodSeconds={}
DEBUG [SnapshotCleanup:1] 2022-05-12 17:51:12,269 SnapshotManager.java:141 - Removing expired snapshot TableSnapshot{keyspaceName='ks', tableName='my_table', tableId=704a1850-d230-11ec-b12d-d9d82874084f, tag='dropped-1652388565491-my_table', createdAt=2022-05-12T20:49:25.491Z, expiresAt=2022-05-12T20:50:25.491Z, snapshotDirs=[/Users/pmottagomes/.ccm/test/node1/data0/ks/my_table-704a1850d23011ecb12dd9d82874084f/snapshots/dropped-1652388565491-my_table]}.
DEBUG [SnapshotCleanup:1] 2022-05-12 17:51:12,274 SnapshotManager.java:141 - Removing expired snapshot TableSnapshot{keyspaceName='ks', tableName='my_table2', tableId=74a02c00-d230-11ec-b12d-d9d82874084f, tag='dropped-1652388565604-my_table2', createdAt=2022-05-12T20:49:25.604Z, expiresAt=2022-05-12T20:50:25.604Z, snapshotDirs=[/Users/pmottagomes/.ccm/test/node1/data0/ks/my_table2-74a02c00d23011ecb12dd9d82874084f/snapshots/dropped-1652388565604-my_table2]}.
{noformat}
I will create another ticket to fix the order items are displayed in nodetool listsnapshots. Also we should probably expose nodetool listsnapshots output via a virtual table.

Previous CI did not complete due to OOM.

Resubmitted new CI on rebased patch:
|[CASSANDRA-17619-4.1-rebased|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-17619-4.1-rebased]|[4.1 CI|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1704/]|



-I reviewed again, I have one minor issue / question otherwise when addressed / resolved I am +1. Thanks a lot!-

+1. I overlooked the last component of the logs where snapshot tag is.

Hi[~smiklosovic] , do you mind committing this? I don’t have access to a computer until next week.

The latest posted branch is squashed and prepared for commit.

Thanks

 

==



Sent from Jira Mobile

Yes, I will commit it. No worries.

[~paulo] I ve run one more circle job for 4.1 with rebased stuff and this is what I got. Check the first failed test there. It can not load snapshots as there is "no file found exception". I am not completely sure about the anatomy of that test yet but this is quite weird that we are getting this exception.

I think it is just flaky as locally it just went fine.

It seems like we were about to load the snapshots and it was walking the tree but all of sudden these files were deleted under its hands.

Maybe it is caused by the node starting and compacting some sstables as part of the start (yeah that might happen) and it just clashes with snapshot loader doing its job.

[https://app.circleci.com/pipelines/github/instaclustr/cassandra/976/workflows/1fd26858-c5ad-44f3-bab3-2318da3bd729/jobs/4226/tests]

[~paulo]  I tried to fix it here

[https://github.com/instaclustr/cassandra/commit/d04390a64b4d4d986804282fb82d60578919cad4]

I run repeated test here: [https://app.circleci.com/pipelines/github/instaclustr/cassandra/977/workflows/b6399095-7ce6-435c-aeaa-3dac55f287a7/jobs/4231/parallel-runs/0?filterBy=ALL]

Nice catch - it seems this also failed on ASF CI ([PreviewRepairCoordinatorNeighbourDownTest.validationParticipentCrashesAndComesBack|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1704/testReport/junit/org.apache.cassandra.distributed.test/PreviewRepairCoordinatorNeighbourDownTest/validationParticipentCrashesAndComesBack_SEQUENTIAL_true__2/]). 

The fix looks good to me.

Submitted one full run to check if failures are fixed:
* https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1729/

Seems to be fixed. I will merge it when 4.1-alpha1 is released.

bq.  I will merge it when 4.1-alpha1 is released.

Just curious, why does the release gate committing here?

Well ... because it is not fully out yet and Mick just tagged it by -tentative, if he still has some work to do related to releasing, I just do not want to mess with my stuff in the middle. He just tagged it, 4.1-alpha1 is not a branch, right?

You shouldn't worry about any of that, unless you're trying to get something _into_ alpha1.

[~paulo] there is still this (1). There is one more case of this faulty comparision later in that method. Do not you have your IDEA set up in such a way that it will automatically hightlight this?

{code}
        return subdir.getFileName().equals(Directories.BACKUPS_SUBDIR)
               ? FileVisitResult.SKIP_SUBTREE
               : FileVisitResult.CONTINUE;
{code}

I will fix it as part of this patch. It is very minor bug which will just scan backup dirs but they should not be.

(1) https://github.com/apache/cassandra/pull/1595#discussion_r861730641

bq. I will fix it as part of this patch. It is very minor bug which will just scan backup dirs but they should not be.

Sounds good, nice catch!

I ve changed some tests as well, made it more solid, they were flaky a bit, i saw a circle which failed, I will wrap it following days, just busy.

bq. I ve changed some tests as well, made it more solid, they were flaky a bit, i saw a circle which failed, I will wrap it following days, just busy.

Thanks for the heads up. Can you give more details on what tests were flaky and why?

It may be worth to multiplex the snapshot test suites to verify they're not flaky before committing.

I run the tests like 50 times, the same one
https://github.com/instaclustr/cassandra/commit/7017450d092f7a111239020fca5d7b76529239ee

Basically, some snapshot operation was done via nodetool and then it was asserting some state but it was not done yet / it was / was not in the output and it was throwing exceptions. I had to rewrite it like we are waiting for some state to happen.

