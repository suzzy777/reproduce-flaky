Just start use libcurl and start buffering all received data instead of read it from network again.

I'm raising the priority of this issue because there are 39 disabled cross-language tests with "cpp" and "http" in the test name, which indicates that this is fundamentally broken.  From a cursory analysis it looks like tests work well up to the "oneway" test.  Once the server receives "oneway" the client sends an i32(-1) and the client and server hang (this would be cpp-cpp http-ip or http-domain).  This represents over 16% of the known test failures in the cross test suite.

I stumbled across this problem myself, and then found this JIRA ticket.  Thought I'd update it with my latest findings, in the possibility that if somebody here knows the THttpTransport (C++) code better than me  (completely new to it), they'll chime in.

Diagnosis: if a client sends a oneway RPC followed by a normal RPC (over HTTP) both RPCs can end up in the server-side socket, and hence both get read at once.  The server-side demarshalling logic pulls off the first message (for the oneway RPC), processes as normal, and then proceeds to (erroneously) read from the socket, instead of the observe that there is more data in its "httpBuf_" to be read.

I'm in the process of figuring out the logic of this module, and should hopefully have a fix soon.  But if somebody reads this and says "oh, I know how that goes!" I'd love it.


[being tentative b/c I sure can't claim to fully-understand this code yet]

Yeah, it looks like the problem was with the logic of HTTP read-buffering in "httpBuf_" and the various http{BufLen,BufSize,Pos} indexes into it.  This patch seems to make the problem disappear, but geez, I sure would like a unit-test that demonstrates the problem is gone.  I'll work on that ....
{code:java}
@@ -84,8 +93,10 @@ uint32_t THttpTransport::readEnd() {
 uint32_t THttpTransport::readMoreData() {
   uint32_t size;
 
-  // Get more data!
-  refill();
+  if (httpPos_ == httpBufLen_) {
+    // Get more data!
+    refill();
+  }
 
   if (readHeaders_) {
     readHeaders();
{code}


This PR should resolve the problem.  Structured in *two* commits (I realize the guidelines say "one commit", but the first adds a testcase, then the second adds the fix.  So the reviewer can apply the first to elicit the bug, then apply the second to fix it.

https://github.com/apache/thrift/pull/1418


It'll be interesting to see how many disabled cross tests this fixes...

I'm new to contributing to OSS, but very, very old in the business of finding/shooting bugs.  Is there some way to find out the list of highest-priority bugs in the C++ codebase?  If there are really glaring ones that need to be debugged, I'd be happy to dig in.

{noformat}
===============================================================================
Following 30 tests were known to fail but succeeded (maybe flaky):
===============================================================================
server-client:          protocol:         transport:               result:
cpp-cpp                 multi-binary      http-ip                  success
cpp-cpp                 multi-binary      http-domain              success
cpp-cpp                 compact           http-domain              success
cpp-cpp                 compact           http-ip                  success
cpp-cpp                 multih            http-domain              success
cpp-cpp                 multih            http-ip                  success
cpp-cpp                 multih-header     http-domain              success
cpp-cpp                 multih-header     http-ip                  success
cpp-cpp                 multij-json       http-domain              success
cpp-cpp                 multij-json       http-ip                  success
cpp-cpp                 json              http-domain              success
cpp-cpp                 json              http-ip                  success
cpp-cpp                 multic            http-domain              success
cpp-cpp                 multic            http-ip                  success
cpp-cpp                 multic-compact    http-domain              success
cpp-cpp                 multic-compact    http-ip                  success
cpp-cpp                 multij-json       http-domain              success
cpp-cpp                 multij-json       http-ip                  success
cpp-cpp                 multij            http-ip                  success
cpp-cpp                 multij            http-domain              success
cpp-cpp                 header            http-domain              success
cpp-cpp                 multic-compact    http-domain              success
cpp-cpp                 multi-binary      http-domain              success
cpp-cpp                 multic-compact    http-ip                  success
cpp-cpp                 multi-binary      http-ip                  success
cpp-cpp                 multi             http-domain              success
cpp-cpp                 multih-header     http-domain              success
cpp-cpp                 multi             http-ip                  success
cpp-cpp                 multih-header     http-ip                  success
cpp-cpp                 binary            http-domain              success
{noformat}

There are still a lot of cpp-other failures in the suite however.  This improved cpp to cpp, but not cpp to other languages.  There are still issues that need resolving, but this is an improvement.

For example, All go server, cpp client cross tests fail:
{noformat}
root@14997434bcab:/thrift/src# test/test.py --server go --client cpp
Apache Thrift - Integration Test Suite
Wed Nov 29 18:30:27 2017
===============================================================================
server-client:          protocol:         transport:               result:
go-cpp                  compact           http-ip                  failure(-6)
go-cpp                  compact           http-ip-ssl              failure(-6)
go-cpp                  binary            http-ip-ssl              failure(-6)
go-cpp                  binary            http-ip                  failure(-6)
go-cpp                  compact           buffered-ip              success
go-cpp                  compact           framed-ip                success
go-cpp                  compact           buffered-ip-ssl          success
go-cpp                  compact           framed-ip-ssl            success
go-cpp                  json              http-ip                  failure(-6)
go-cpp                  json              http-ip-ssl              failure(-6)
go-cpp                  binary            buffered-ip              success
go-cpp                  binary            buffered-ip-ssl          success
go-cpp                  binary            framed-ip                success
go-cpp                  binary            framed-ip-ssl            success
go-cpp                  json              buffered-ip              success
go-cpp                  json              buffered-ip-ssl          success
go-cpp                  json              framed-ip                success
go-cpp                  json              framed-ip-ssl            success
===============================================================================
No unexpected failures.
Known failures are logged to test/log/known_failures.log
{noformat}

If you run all of the binary_http-ip tests, cpp server works against haskell, cpp, and lua http clients.  It fails against go, nodejs, java, dart clients.  nodejs server fails against lua, java, and dart clients.  So there are issues with http all around (or there are some badly written tests).

To move THRIFT-3877 further we will need someone to add "http" support to the test/csharp TestServer and TestClient so that we can cross test against other languages since that's the original bug report.

I can't help with C# (unless we can find somebody who knows C# whom I could work with), but for golang, I sure can look into it.  And probably other languages with which I have some "passing familiarity".

Let me first address your comments for the PR, and then get onto further debugging.

[It's OK if the answer is no, but just in case ....] can we address what's needed to check in this (agreed, only partial) fix and then move forward with debugging the other instances of this problem?

I prefer to have small PRs with limited scope, is all.  Makes updating them as master changes ... so much easier.  And since they're open for less time, fewer updates.


Any pull requests you can submit that fix currently broken cross tests will be quite welcome. :)

James, if you could give me *any* advice to help me get to where I can run the docker-based testsuite (so for instance, I can run the cross-language tests in the same way that Travis CI does), I'd really appreciate it.

Both to push forward the C++ HTTP code fix, and also so I can move forward with debugging the problem in other languages.

I'm really stuck there, b/c have never used docker before.


I dug into the go-cpp (go server, cpp client) problem.

TL;DR this is a basic limitation of the Golang HTTP stack, which is .... designed with no flexibility in mind.  None whatsoever.  To allow what Thrift "oneway" needs, we need to write our own HTTP stack.

(0) reminder: the semantics of "oneway" is that literally no response is sent.  Not a single byte.  This is  .... well, an interesting perversion of the HTTP protocol, and we might discuss whether we want to undo it.  But let's assume for this discussion that we want to keep things this way .....

(1) in golang, the http stack "comes with" the language.  And for instance in golang 1.8, in go/src/net/http/server.go, we learn that the HTTP server stack is configured with a set of "handler functions", for various paths.  A handler function is invoked this:

type HandlerFunc func(ResponseWriter, *Request)

So y'know, it's got a place to which it can write its response.

If it doesn't write a response, that is the same as a zero-byte response.  So upon return from the method (e.g. "TestOneway"), the HTTP server stack will put out its response, with a zero-byte body.  But it puts out a response.

(2) There are two solutions:

(a) write our own HTTP stack.  This isn't so hard, eh?

(b) But I think that the right approach, is to change the semantics of "oneway HTTP" so that a response is always returned.  Then via HTTP request pipelining, we get the same client-visible behaviour as today.  Though, the spec says that clients SHOULD NOT pipeline non-idempotent requests (and all POSTs are assumed non-idempotent).  But this is a quibble, and in any case, it's a "SHOULD NOT".

Why do I think that the right thing is for the HTTP transports to (all) switch to (silently) returning a response for a oneway method?  B/c otherwise, the HTTP transport isn't very useful.  Anyplace where there are HTTP proxies (and for instance, there are transparent network proxies in the wild, right?) you'll break them.  Also, if Thrift-HTTP conformed to the HTTP spec, it might be worth looking into building a Thrift plugin for the Apache webserver.  It would widen the utility of Thrift, I think.

And the work isn't all that significant.

(3) I feel like it's worth noting that Thrift's use of HTTP (for oneway) really isn't "conforming".  In the spec, the assumption that every request will receive a response is .... a pervasive unstated assumption.  I doubt any HTTP standards-person would say it's even remotely conforming.

Unsurprisingly the cpp-go (cpp server, go client) HTTP tests all fail also.  B/c it's the converse problem:

the C++ server doesn't send a response for the onway method

but the Golang HTTP stack expects that response.

So it hangs.  Again, looking at the Golang http client, we can see that deeply baked-into the idea of the client, is a "RoundTripper", which is the "transport".  It is effectively a function,

RoundTrip(*Request) (*Response, error)

And guess what?  a Reponse is a struct that starts ...
```
type Response struct {
        Status     string // e.g. "200 OK"
        StatusCode int    // e.g. 200
        Proto      string // e.g. "HTTP/1.0"
        ProtoMajor int    // e.g. 1
        ProtoMinor int    // e.g. 0
```
Baked deeply into this code, is the assumption that all requests receive responses, and if they do not, then it's an error.

But even *worse*, it does not appear that the Go http client is written to support pipelineing.  At.  All.

My suggestion: either write or find an OSS HTTP client that supports pipelineing properly.  That, combined with the Golang HTTP server behaviour, would yield the client-visible behaviour of "oneway".

I'll continue investigating other language-combinations that fail; let's see if they have similar issues.

Interesting analysis - so the transport protocol (http) requires request/response, however the application protocol (thrift) requires only a request.  If this is the case (and that makes sense to me), then the client library needs to consume and drop that response somehow, as the application doesn't care.  I suspect oneway existed before http protocol.  This is something I'd love to get more committer feedback on to make sure we understand it fully.  I will draw some attention to it.

I see FB has OSSed an HTTP framework, Proxygen.  Has there been any thought to using that (for C++ HTTP server)?

James,

I'm at an impasse on this PR.  I have neither knowledge of nor access to Windows.  There's a failure in on AppVeyor Windows test, and it seems paradoxical, b/c the equivalent test seems to work on Linux (cmake-related).  So ... uh .... like I said, I'm at an impasse.

If there's somebody with some Windows expertise who could lend a hand and explain what's going wrong, that might be useful.


[~chetmurthy]: The fact that HTTP always needs a response does not imply that 
a) there must be any data in it (it may consist only of HTTP headers) and 
b) the thrift client needs to even look at the data read from the wire.

So yes, the HTTP response should probably be read, but the Thrift {{oneway}} semantics are not to be affected w/regard to Thrift. What happens inside the HTTP transport does mot matter and should be kept there. If that means that HTTP is not really "fire and forget" because we have to wait for the impact confirmation (to stick with the picture), well so be it. we dont want to break HTTP ...

... but we also don't want to break {{oneway}}. And the latter implies that we really should ignore whatever response is coming back, because {{oneway}} means that there is no response, and this also includes no errors, exceptions, panics or whatever: The server did return a HTTP 500? Who cares!


Jens,

I agree 100%.  What we're looking for is a response of any kind: *any* kind.

One (incomplete) solution: keep a counter of how many "must-discard" responses we expect from the server.  When we send a roundtrip RPC, and are reading for *its* response, first read-and-discard the must-discard ones.

The problem with this is that we might deadlock ourselves, if a client sends a *ton* of oneway RPCs down a transport.  So we'd need to (perhaps in the writer flush() operation) check to see if there's data to read, and if so, read-and-discard responses.  As long as the HTTP transport read-side is buffered, this works.  So we'd need to always have a TBufferedTransport between the HTTPClient and whatever protocol is being used.

At least, that's what seems to be needed.  What do you think?

That seems reasonable; we need to maintain transport integrity.  As long as the transport response for a ONEWAY type message is handled immediately by the server's HTTP transport (so as soon as we know we're reading a ONEWAY in the HTTP Transport, we should issue a HTTP OK response, and the client should consume that before returning (but since it's oneway, throw any status away).

Another possibility is the new Boost.Beast library, although that would force users to Boost 1.66.0 or later...

James, wait, I'm not sure you want that.

(1) first, to echo back: you're proposing that a "oneway" be a roundtrip from client to server's HTTP stack, but not all the way into the server's handler.  I think this is not what you'd want, and certainly not consistent with the semantics that a oneway over the TCP transport provides.

EDIT: After all, the TCP transport doesn't even really guarantee that the message left the client's kernel buffers, when it returns to the client application, right?

EDIT2: Nor with the semantics of oneway over http in the cpp library, either.

(2) What I proposed, was that the client HTTP stack, would "somewhat lazily and silently" consume the (as Jens notes) "possibly highly abbreviated responses" from the server.  If there's a buffer between the HTTP Transport and the Protocol, this isn't hard to implement.

(3) I'd be a little leery of making a big change like switching to Boost.Beast, without at least evaluating what Facebook's doing.  Not because I think they're doing the right thing, but b/c they at least have large-scale experience to guide them.


Github user jeking3 commented on the issue:

    https://github.com/apache/thrift/pull/1418
  
    I'm not certain this resolves THRIFT-3877 entirely - your take?


James, I hope you weren't waiting on me to proceed on this issue?  I was pretty much stopped, since it wasn't obvious how to proceed, given deep-seated mismatch between HTTP's baked-in round-trip-ness and Thrift's oneway semantics.

Hi, no, I set it back "to-do" because it's a fairly large problem and I haven't been working on it.  It looks like we need to make some fairly big changes and someone will need to step up and do that.

 

As part of some work on support message bus, I need to add some context at the transport level to understand a complete round trip request/response cycle.  Right now I am leveraging the "readEnd" and "writeEnd" methods (I changed their signature, so it's breaking, which is not ideal, but it's also prototype for now).  In those calls it will be possible to track the request/response with a unique discriminator, so for example readEnd can tuck something away like a response queue name, and writeEnd allows the response to be routed back to the request queue (in a message bus).  This will also carry whether the read or write is for a request or a response, and an indicator as to whether the request is oneway.

So I think that if the code generator tells the transport through readEnd() (which is sent at the end of reading a complete message) that what was just read is oneway, the server's THttpTransport can send the required transport response (200 OK) even when receiving a oneway request.


Apparently this problem is evident elsewhere, for example all of the py3-java http cross tests are failing when they get to the ONEWAY call.  I dug into it some more and I found that the python HTTP server is responding with a "200 OK" (see THttpServer.py - it always responds even if there is no data to post).  However on the java client side, the base class TServiceClient.java does not give the transport an opportunity to pull the HTTP response off the wire.  While there is no expected protocol response to this oneway request, there is a transport response, and it has to be consumed...

After further examination it looks like the java client does always grab the response.  I think it is getting confused because the length of the response is empty.  I think I will update the java client to check the content length before attempting to read anything.

Dug into this tonight and it looks like the issue here is the java client test is sending a request for the server to sleep 3 seconds, and then raising an error if it took more than 200ms to proceed.  This would indicate that the server HTTP implementation is not pushing a response immediately but instead waiting for the RPC to complete before responding.

It turns out the cpp, d, go, nodejs, py and py3 servers all have been disabled with the java client for http for this very reason.  The http protocol requires a transport level response, and none of the others do.  For a oneway request, servers probably have to change here to identify it is a oneway request, then respond on the transport, then run the request.

Could issue THRIFT-5180 be related?

