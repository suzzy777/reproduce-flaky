The patch looks OK so far, I've pushed some minor changes in a separate commit on [this branch | https://github.com/stef1927/cassandra/commits/9591-2.0]. It's pretty straightforward stuff but do let me know if you have any concerns.

I have also started the integration to 2.1 on [this branch | https://github.com/stef1927/cassandra/commits/9591-2.1]. Unfortunately the code in SSTableReader is a bit divergent so it's probably not working in 2.1 right now.

These two branches will be picked up by our continuous integration server, the results will (eventually) be available [here|http://cassci.datastax.com/view/Dev/view/stef1927]. I will check tomorrow for any broken tests.

I've added a very basic unit test. However we need to add more tests, probably in _scrub_test.py_ of [dtests|https://github.com/riptano/cassandra-dtest]. Here we should test both standalone and nodetool scrub, albeit with an index in this latter case. I don't mind writing some more tests as this area is a bit lacking (we do have some tests for scrubbing secondary indexes but they are only active on >= 2.2). However do let me know if you want to write them yourself.

Once all the tests are clear, new and existing, I will try to find a committer, thanks for submitting the patch!


I've created some dtests, branch attached, the pull request is [here|https://github.com/riptano/cassandra-dtest/pull/331].

I integrated the original patch and my suggestions to 2.1 and 2.2. You find attached all 3 GitHub branches.

Pending CI, the unit and dtests for scrub pass on my box for 2.0 and 2.1 but not for 2.2. Here we have a problem, it seems we need the first and last keys created in buildSummary() because of the new lifecycle code:

{code}
nosetests -s scrub_test.py:TestScrub.test_standalone_scrub_data_file_only

dtest: DEBUG: Pre-scrub sstables snapshotted into snapshot pre-scrub-1434513958878
WARNING: Missing component: /tmp/dtest-UPhj9E/test/node1/data/ks/users-254da3c014a611e59b004b06169f4ffa/la-1-big-Index.db
Scrubbing BigTableReader(path='/tmp/dtest-UPhj9E/test/node1/data/ks/users-254da3c014a611e59b004b06169f4ffa/la-1-big-Data.db') (863 bytes)
null

dtest: DEBUG: Error scrubbing BigTableReader(path='/tmp/dtest-UPhj9E/test/node1/data/ks/users-254da3c014a611e59b004b06169f4ffa/la-1-big-Data.db'): null
java.lang.NullPointerException
        at java.util.ComparableTimSort.countRunAndMakeAscending(ComparableTimSort.java:321)
        at java.util.ComparableTimSort.sort(ComparableTimSort.java:184)
        at java.util.Arrays.sort(Arrays.java:1312)
        at java.util.Arrays.sort(Arrays.java:1506)
        at java.util.ArrayList.sort(ArrayList.java:1454)
        at java.util.Collections.sort(Collections.java:141)
        at org.apache.cassandra.utils.IntervalTree$IntervalNode.<init>(IntervalTree.java:187)
        at org.apache.cassandra.utils.IntervalTree.<init>(IntervalTree.java:50)
        at org.apache.cassandra.db.lifecycle.SSTableIntervalTree.<init>(SSTableIntervalTree.java:20)
        at org.apache.cassandra.db.lifecycle.SSTableIntervalTree.build(SSTableIntervalTree.java:30)
        at org.apache.cassandra.db.lifecycle.View$4.apply(View.java:183)
        at org.apache.cassandra.db.lifecycle.View$4.apply(View.java:178)
        at com.google.common.base.Functions$FunctionComposition.apply(Functions.java:211)
        at org.apache.cassandra.db.lifecycle.Tracker.apply(Tracker.java:126)
        at org.apache.cassandra.db.lifecycle.Tracker.apply(Tracker.java:99)
        at org.apache.cassandra.db.lifecycle.LifecycleTransaction.checkpoint(LifecycleTransaction.java:233)
        at org.apache.cassandra.db.lifecycle.LifecycleTransaction.checkpoint(LifecycleTransaction.java:214)
        at org.apache.cassandra.io.sstable.SSTableRewriter.switchWriter(SSTableRewriter.java:285)
        at org.apache.cassandra.io.sstable.SSTableRewriter.doPrepare(SSTableRewriter.java:330)
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:169)
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish(Transactional.java:179)
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewriter.java:317)
        at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:299)
        at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:124)

--------------------- >> end captured logging << ---------------------

----------------------------------------------------------------------
Ran 1 test in 10.984s

FAILED (failures=1)
{code}

I think we either need to set first and last in SSTableReader also when the index is not available or we need to see if we can avoid updating the live set when we are offline. cc [~benedict] for suggestions re lifecycle code.

[~mck], could you review my suggestions and if you are happy prepare a final patch for 2.0 and 2.1 that can be committed in your name? You can just attach the GitHub branch if easier. However if you do change things slightly let us know so we can rerun in CI (continuous integration).

Then could you work with Benedict's suggestion to fix the 2.2. problem? We can help you with this if required. 




If it's impossible to have those values wired up, we can pass a flag into {{updateLiveSet()}} asking it not to build an interval tree in the case we're offline (which is detected by {{cfstore}} being {{null}} in the {{Tracker}}).


{quote}The patch looks OK so far, I've pushed some minor changes in a separate commit on this branch . It's pretty straightforward stuff but do let me know if you have any concerns.{quote}

Nice. Added two comments to the "suggestions" comment: one out of curiosity, the other feedback i believe.

{quote}I have also started the integration to 2.1 on this branch . Unfortunately the code in SSTableReader is a bit divergent so it's probably not working in 2.1 right now.{quote}
There's not much i can offer on 2.1 moderation at this point, simply not being familiar with it.

{quote}mck, could you review my suggestions and if you are happy prepare a final patch for 2.0 and 2.1 that can be committed in your name? You can just attach the GitHub branch if easier. However if you do change things slightly let us know so we can rerun in CI (continuous integration).{quote}

working on it… (and will see if i can incorporate Benedict's idea)

new patch against 2.0 w/ Stefania's suggestions. (also readable in [github|https://github.com/michaelsembwever/cassandra/commit/c10b07a1b207fec58b7d9834296d88fc06e5b4e5])

patch against 2.1 uploaded (also readable in [github|https://github.com/michaelsembwever/cassandra/commit/34bc583614c74582e095ef0f5ee421d88ccddccb])

bq. If it's impossible to have those values wired up…

It's possible to "wire" these values up doing a poorman's approach of doing a complete pass through the data file. That's pretty wasteful, but we're only talking about the edge-case of the StandaloneScrubber here.

eg
{code}
private void findFirstAndLast() throws IOException
{
    // we have no primary index. take a pass through the data file to assign first and last. costly, but only for StandaloneScrubber
    try (RandomAccessReader dataFile = openDataReader())
    {
        while (!dataFile.isEOF())
        {
            DecoratedKey decoratedKey = partitioner.decorateKey(ByteBufferUtil.readWithShortLength(dataFile));
            if (first == null)
                 first = decoratedKey;
            last = decoratedKey;

            SSTableIdentityIterator atoms = new SSTableIdentityIterator(this, dataFile, decoratedKey, false);
            while (atoms.hasNext())
                atoms.next();
        }
    }

    first = getMinimalKey(first);
    last = getMinimalKey(last);
}
{code}

Would you rather see the flag into {{updateLiveSet()}}?

Patches for 2.0 and 2.1 are +1, thanks.

For 2.2 there is a very simple way around it actually, which is to obsolete the originals early, this way they are never used to build the interval tree.

[~benedict] could you review [this commit|https://github.com/stef1927/cassandra/commit/2bc8e7f10a5129686f5c2e997f84e1578457bd16] please?
 

Perhaps we should just {{obsoleteOriginals}} up-front, if offline? We could even do it in the {{StandaloneScrubber}}, before calling {{scrubber.scrub()}}, to avoid polluting the general purpose {{Scrubber}}.

No strong feelings though - the patch looks like it works to me.

[~stefania]: could you rebase your branches and once CI passes I'll commit.

[~Benedict], I've rebased and squashed all 3 patched and created a new one for trunk, given that 2.2 did not automatically apply.

I agree on obsoleting the originals in the standalone scrubber, it's neater. 

On trunk we must retain the statistcs file as well, the data file alone is not sufficient. I've changed the dtest to reflect this, pull request is [here|https://github.com/riptano/cassandra-dtest/pull/364].

On 2.2 I encountered a new NPE after rebasing, it was caused by the key cache being missing. I changed the txn code to set up the key cache for the originals if they don't already have one, which could be true when offline. You may want to double check this.

I also updated CHANGES.txt.

The CI results are here:
http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-9591-2.0-testall/lastCompletedBuild/testReport/
http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-9591-2.0-dtest/lastCompletedBuild/testReport/
http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-9591-2.1-testall/lastCompletedBuild/testReport/
http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-9591-2.1-dtest/lastCompletedBuild/testReport/
http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-9591-2.2-testall/lastCompletedBuild/testReport/
http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-9591-2.2-dtest/lastCompletedBuild/testReport/
http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-9591-testall/lastCompletedBuild/testReport/
http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-9591-dtest/lastCompletedBuild/testReport/

There are several failures but the same is true for the unpatched branches. For a few tests you need to go back several builds to find the same test failing on the unpatched branch or you can just run it locally and it should pass. I've also launched the builds at least twice to shake off some flakiness. 

If there are any failing tests that concern you do let me know.

[~Stefania] could I ask you to rebase once more? The problems you mention were introduced by me in CASSANDRA-9656, and have now been fixed in mainline(s)

Rebased all except for 2.0, which did not need to. 

CI is currently running.

Thanks. Committed.

Could I ask that in future when merging up the branches, you leave the full merge commits in place? i.e. when constructing your branches, start with 2.0 (or whatever the lowest affected branch is), merge that into 2.1 (without --no-commit or --squash), and continue this all the way up to trunk. This is how patches must be committed into mainline, and helps committers with conflict resolutions when applying your patches.

AFAICT you have created completely distinct branches all sourced from their mainline branches, and this means it is a lot more labour intensive for me to merge these upwards myself when (inevitably) one of the source branches diverges, as I have to construct my own versions of each, rebase them, perform diffs and patch applications. If the merge commits were in place, I could (in theory) use git rerere to resolve conflicts without any manual intervention, ensuring we do not break upstream due to my misapplication of your changes.

Thanks for committing.

I actually created the new branches and used rebase --onto. The mistake was also to squash all branches afterwards, thinking it would help to keep things tidier. My bad. I'll use merge in future and keep commits in place.



