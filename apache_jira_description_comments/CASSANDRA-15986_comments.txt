[~e.dimitrova] reported she can repro locally and the test will still fail but differently. Unassigning so sbdy that can repro can pick it up.

Just adding two more failure reports for reference:

https://app.circleci.com/pipelines/github/adelapena/cassandra/79/workflows/a841da8f-d565-4608-85c6-1432733faeb3/jobs/606

https://app.circleci.com/pipelines/github/dineshjoshi/cassandra/47/workflows/de5f7cdb-06b6-4869-9d19-81a145e79f3f/jobs/2531

If you check ci-cassandra you'll see it fails almost daily. But I can't repro so... so sbdy else that can repro can be the RepairTest hero :)

Now it is not failing on the Mac... :-( I will spin up a slow Linux VM and try again later

Yeah that's about the only thing left I can try. I need to finish 15583 first, then try that and I also want to try jenkins to keep the node's logs if there is a failure. So at least I can see what was going on.

[~maedhroz] shared links from CircleCI and I see it is again time out so we can see the logs under artifacts. Thanks!
[~Bereng] Looking into them I am kind of tempted to say your patch will solve the issue but let's give it a try on a slow linux machine.

[~e.dimitrova] didn't you mention above my patch was failing in a different way?

On a very slow Ubuntu VM it was failing consistently. With the patch, it timed out again with the same error on the 4th run. :( 

I'd be interested to know how to build that slow Ubuntu VM (or can I download it somwhere/somehow?) so I don't have to reinvent the wheel trying which config is slow enough...

Sure! I just created a VM in VirtualBox with Ubuntu guest OS - [ubuntu-18.04.4-desktop-amd64.iso |https://releases.ubuntu.com/18.04/]. I set RAM to 4096MB and disk space to 25GB instead of 10GB, everything else is just default config. 

Nice that's what I needed, the specs thx :)

So I have tried with a VM on diff memory settings down to 3GB and even capping CPU to 10% only. That thing is solid as a rock. I can't repro. On too low specs it starts to fail on legit timeouts on other areas. So no way to repro this one for me.

On the other hand I tried to repro the Tracing test failures and I did manage to. So I will try that once I am back from my 1w break #justfyi

Thank you for the update [~Bereng], I will try to look at this next week. Have a nice vacation.

 

I am adding also this failure of test_simple_parallel_repair to be checked as I feel those might be related.

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/296/workflows/953f79e3-7416-4375-82cf-3dcfeda61ab3/jobs/1816

I am puzzled what is going on, rebased yesterday and started the tests on the same VM I was using before - there is no single failure! I left it looping overnight...  (without the suggested patch to make the issue more obvious). I don't see any related patch committed recently?
At the same time, I still see it failing in Jenkins and CircleCI these days with the same error.
I would suggest to commit the suggested patch and continue monitoring, not close the ticket in the next 2 weeks for example.WDYT [~maedhroz], [~Bereng]?
I wish there was a way to multiplex a test In Jenkins. I will ping Mick to check how hard this would be to be implemented. It will help a lot in such cases. 
I can try to go through the logs further without being able to reproduce the failure but not sure how reasonable it will be to spend that time now. As I said in the beginning, the suggestion of [~Bereng] sounds reasonable to try to stabilize them, I might be wrong,  looking for a second opinion. :-) 

bq. I would suggest to commit the suggested patch and continue monitoring, not close the ticket in the next 2 weeks for example.

WFM

Thanks [~maedhroz]. [~brandon.williams], can you, please, commit [~Bereng]'s patch?

Committed. https://github.com/apache/cassandra-dtest/commit/79c0120fbd659a7a5169ed945279dfed688705d8

Thank you [~brandon.williams], let's keep the ticket open in the next two weeks. I will check the CI report and follow up. 

I see repair failures still. But with CASSANDRA-16070 keeping logs now + a 'generic timeout' fix I am working on for another flaky in CASSANDRA-16073 yes, let's let it roll a few days +1

Repair tests seem to still be failing. Less often but still. So let's see if recent 'disk full' failures in ci-cass get fixed and we can look into this again...

No repair failures recorded in Jenkins lately. I plan to close this ticket next week in case nothing changes

 

That's bc we lost history. I remember we had some Repair failure. I want to keep an eye longer I am not comfortable with this yet. Happy to take it back off your plate.

Looks like we are on the same page :) Let's keep it for a while, yes :) There is history but only like 10 runs or something for now :) But at least those 10 look successful for now so I am positive. As far as I remember there was some repair failure almost with every run in Jenkins before. Let's see

That's the thing. I look on a daily basis and I was seeing repair failures. Since jenkins was rebuilt we lost that history and failures have gone down from <10-ish to <5-ish. So I want to keep an eye a bit longer. Maybe jenkins rebuilt fixed sthg, maybe sthg else got merged and got this fixed as a side effect,...

Repair tests are gone. Let's close and we can always reopen if needed.

Edit: [~brandon.williams] I can't seem to move past commit status, can you close this for me please? Thx in advance.

