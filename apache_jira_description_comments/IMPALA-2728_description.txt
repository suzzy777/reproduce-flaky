impala-master-repeated-runs-cdh5 is broken due to test_low_mem_limit_q21 exceeding the mem limit of 700M. FWIW, the minimum required mem_limit specified for tpch-q21 is 621M so it shouldn't fail with 700M.

The failure has been seen once so far. Waiting for a second run to see if it reproduces again.

http://sandbox.jenkins.cloudera.com/job/impala-master-repeated-runs-cdh5/377/testReport/junit/query_test.test_mem_usage_scaling/TestTpchMemLimitError/test_low_mem_limit_q21_mem_limit__700___exec_option____disable_codegen___False___abort_on_error___1___exec_single_node_rows_threshold___0___batch_size___0___num_nodes___0____table_format__parquet_none_/

{code}
query_test/test_mem_usage_scaling.py:187: in test_low_mem_limit_q21
    self.low_memory_limit_test(vector, 'tpch-q21', self.MIN_MEM_FOR_TPCH['Q21']);
query_test/test_mem_usage_scaling.py:93: in low_memory_limit_test
    self.run_test_case(tpch_query, new_vector)
common/impala_test_suite.py:210: in run_test_case
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:400: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:158: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:161: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:327: in __execute_query
    self.wait_for_completion(handle)
beeswax/impala_beeswax.py:347: in wait_for_completion
    raise ImpalaBeeswaxException("Query aborted:" + error_log, None)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    Query aborted:
E   Memory limit exceeded
E   Query did not have enough memory to get the minimum required buffers in the block manager.
E   
E   
E   
E   Memory Limit Exceeded
E   Query(cc44d1b4dd89d8a3:a02b51c53ba8b0) Limit: Limit=700.00 MB Consumption=617.95 MB
E     Fragment cc44d1b4dd89d8a3:a02b51c53ba8b4: Consumption=2.27 MB
E       SORT_NODE (id=12): Consumption=4.00 KB
E       AGGREGATION_NODE (id=20): Consumption=2.25 MB
E       EXCHANGE_NODE (id=19): Consumption=0
E       DataStreamRecvr: Consumption=0
E       DataStreamSender: Consumption=4.00 KB
E     Block Manager: Limit=560.00 MB Consumption=558.75 MB
E     Fragment cc44d1b4dd89d8a3:a02b51c53ba8b7: Consumption=440.86 MB
E       AGGREGATION_NODE (id=11): Consumption=2.25 MB
E       HASH_JOIN_NODE (id=10): Consumption=138.06 MB
E       HASH_JOIN_NODE (id=9): Consumption=300.53 MB
E       EXCHANGE_NODE (id=16): Consumption=0
E       DataStreamRecvr: Consumption=0
E       EXCHANGE_NODE (id=17): Consumption=0
E       DataStreamRecvr: Consumption=0
E       EXCHANGE_NODE (id=18): Consumption=0
E       DataStreamRecvr: Consumption=2.28 KB
E       DataStreamSender: Consumption=12.00 KB
E     Fragment cc44d1b4dd89d8a3:a02b51c53ba8ba: Consumption=17.42 MB
E       HDFS_SCAN_NODE (id=5): Consumption=17.35 MB
E       DataStreamSender: Consumption=58.84 KB
E     Fragment cc44d1b4dd89d8a3:a02b51c53ba8c0: Consumption=157.40 MB
E       HASH_JOIN_NODE (id=8): Consumption=88.02 KB
E       HASH_JOIN_NODE (id=7): Consumption=1.27 MB
E       HASH_JOIN_NODE (id=6): Consumption=156.01 MB
E       HDFS_SCAN_NODE (id=1): Consumption=0
E       EXCHANGE_NODE (id=13): Consumption=0
E       EXCHANGE_NODE (id=14): Consumption=0
E       DataStreamRecvr: Consumption=0
E       EXCHANGE_NODE (id=15): Consumption=0
E       DataStreamRecvr: Consumption=0
E       DataStreamSender: Consumption=10.59 KB
{code}