I'm interested in taking on this task. I'm new to the project and would appreciate any guidance.

Thanks.

That's great to hear, [~alourie]! 

There's a quick intro for [how to contribute|http://cassandra.apache.org/doc/latest/development/patches.html] on the site. I'm not sure how much guidance you need, but I'll try to give you a  REALLY quick intro - feel free to ask for more (or less) as appropriate:

{{nodetool}} is the general administrative tool that most cassandra operators use. It connects to cassandra primarily using JMX. 
{{nodetool compactionstats}} uses the {{CompactionManager}} MBean to call [getCompactions|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/nodetool/CompactionStats.java#L70]  , which basically [returns a list of maps|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1857] (where the map comes from a [CompactionInfo$Holder|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionInfo.java#L135] ) - you'll need to include the directory as an element in that map so that the {{nodetool}} command can print it out nicely. 

Thanks [~jjirsa]!

I've started by adding a targetDirectory to the *CompactionInfo* class. Then I'm working backwards handling new instantiations with appropriate path parameters.

The problem I got at the moment is finding the correct *path* parameter for every such new call.  For instance, in https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/index/internal/CollatedViewIndexBuilder.java#L54; I can't find a proper argument to add as a target directory.

Am I going the right direction? I've tried running the test with building ccm cluster and causing it to run compactions, but the target dir shown for a running compaction is empty, so I probably do something wrong somewhere.

Thanks.

[~jjirsa] I've created a branch with my work at https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010, please provide feedback and whether I'm on the right path.

Thanks a lot!

[~rustyrazorblade] says he's happy to review for you.



[~jjirsa] Thanks!

[~rustyrazorblade] please be gentle :-)

Hi [~alourie]!   Thanks for the patch.  I'm taking a look today.  Schedule is a bit busy since I'm on the road but I'll try to get you some feedback tonight.

Anything I should look out for?  Any questions you have?


Hi [~rustyrazorblade]

In essence, I'd like to know if I'm on the right path. There are a lot of things in the codebase that I don't understand, but I hope that I'm at least going in right direction.

Thanks.

I set up 6 data directories and dropped my memtable_heap_space_in_mb to 32 MB to force a ton of flushes and lots of compactions.  I do see directories in the compactionstats, indicating that it's working, but i haven't verified the right directories are listed.  

The output is a little hard to read, since the directories make the output look like this:

{code}
jhaddad@rustyrazorblade ~/dev/cassandra$ bin/nodetool compactionstats
pending tasks: 6
- keyspace1.standard1: 6

id                                   compaction type keyspace  table     target directory                                                                            completed total    unit  progress
7f3e7a40-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 /Users/jhaddad/var/lib/cassandra/data1/keyspace1/standard1-f867e470fedb11e6a2c121962153156a 17696623  38340460 bytes 46.16%
83c14340-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 /Users/jhaddad/var/lib/cassandra/data4/keyspace1/standard1-f867e470fedb11e6a2c121962153156a 3651789   33841385 bytes 10.79%
7a1beed0-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 /Users/jhaddad/var/lib/cassandra/data3/keyspace1/standard1-f867e470fedb11e6a2c121962153156a 34012548  37166390 bytes 91.51%
7a2e6560-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 /Users/jhaddad/var/lib/cassandra/data6/keyspace1/standard1-f867e470fedb11e6a2c121962153156a 33334698  36996530 bytes 90.10%
7dabcc50-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 /Users/jhaddad/var/lib/cassandra/data2/keyspace1/standard1-f867e470fedb11e6a2c121962153156a 22993893  37875910 bytes 60.71%
Active compaction remaining time :   0h00m04s
{code}

From a user interface perspective, It would be great if the tasks could be separated by directory, rather than inlining the directory in the table.  So in my example something more like:

{code}
jhaddad@rustyrazorblade ~/dev/cassandra$ bin/nodetool compactionstats  
pending tasks: 6
- keyspace1.standard1: 6

id                                   compaction type keyspace  table  completed total    unit  progress

/Users/jhaddad/var/lib/cassandra/data1/keyspace1/standard1-f867e470fedb11e6a2c121962153156a
7f3e7a40-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  17696623  38340460 bytes 46.16%

/Users/jhaddad/var/lib/cassandra/data4/keyspace1/standard1-f867e470fedb11e6a2c121962153156a
83c14340-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 3651789   33841385 bytes 10.79%

/Users/jhaddad/var/lib/cassandra/data3/keyspace1/standard1-f867e470fedb11e6a2c121962153156a
7a1beed0-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  34012548  37166390 bytes 91.51%

/Users/jhaddad/var/lib/cassandra/data6/keyspace1/standard1-f867e470fedb11e6a2c121962153156a
7a2e6560-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  33334698  36996530 bytes 90.10%

/Users/jhaddad/var/lib/cassandra/data2/keyspace1/standard1-f867e470fedb11e6a2c121962153156a
7dabcc50-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  22993893  37875910 bytes 60.71%
Active compaction remaining time :   0h00m04s
{code}

If there are multiple compactions in a single directory, you would see this:

{code}
/Users/jhaddad/var/lib/cassandra/data3/keyspace1/standard1-f867e470fedb11e6a2c121962153156a
7a1beed0-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  34012548  37166390 bytes 91.51%
7a2e6560-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  33334698  36996530 bytes 90.10%
7dabcc50-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  22993893  37875910 bytes 60.71%
{code}

I'm also not sure if there's value in including the keyspace & table directory, since that information is duplicated.  If we limit the result to the data directory we would end up with at most N sections, 1 per data directory specified in the yaml.  I think that would be the *most* useful form of this output (note the line between the directories):

{code}
/Users/jhaddad/var/lib/cassandra/data4
83c14340-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 3651789   33841385 bytes 10.79%
03c14340-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard2 3651789   13841385 bytes 10.79%

/Users/jhaddad/var/lib/cassandra/data3
7a1beed0-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard3  34012548  57166390 bytes 91.51%
83c14340-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard5 3651789   43841385 bytes 10.79%
{code}

What do you think?

[~rustyrazorblade] I've updated the patch with the suggested UI changes. It will now group compactions by a directory.

Please let me know how it looks, and also I'd love a feedback for the actual implementation of the feature.

Thanks!

On the nit side, I've already noticed a handful of cases that violate the style guide.  { and } should always be on their own lines, even for single line functions.  

http://cassandra.apache.org/doc/latest/development/code_style.html#general-code-conventions

For instance:
CompactionInfo.java: targetDirectory() 
CompactionIterator.java: setTargetDirectory

There may be others, but I won't bother listing them (for now).  If you can take a pass & ensure the patch conforms to the style guide, I'll focus on the content of the patch.

[~rustyrazorblade] totally fair. I'll ensure the conformity to the style guide.

Thanks.

I don't believe this line does what it's supposed to:

{code}
ci.setTargetDirectory(cfs.getDirectories().getDirectoryForNewSSTables().getPath());
{code}

First off, getDirectoryForNewSSTables returns *any* directory that's available to be written to. The call {{getDirectoryForNewSSTables}} has this comment:

{code}
    /**
     * Basically the same as calling {@link #getWriteableLocationAsFile(long)} with an unknown size ({@code -1L}),
     * which may return any non-blacklisted directory - even a data directory that has no usable space.
     * Do not use this method in production code.
     *
     * @throws FSWriteError if all directories are blacklisted.
     */
    public File getDirectoryForNewSSTables()
    {
        return getWriteableLocationAsFile(-1L);
    }
{code}

I believe a better way to approach this would be to use {{SSTableWriter.descriptor}} to pull out the target directory.

[~rustyrazorblade] I've updated the code with a couple of code style fixes and a couple of code updates as well.

At the moment I'm having a trouble with
https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010#diff-2106480b863a1ea6485772847314cb06,
https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010#diff-c0089857a31093b8546a1ec95541a529 and
https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010#diff-d4e3b82e9bebfd2cb466b4a30af07fa4

These are instances where I can't figure out an SSTableWriter not any other writer or reader object, so I'd need some help on figuring this out.

I hope that in other instances I got them right though.

[~rustyrazorblade] I've got back to working on this ticket. I think I've covered all possible operations and the patch is now in a good shape.

I've tested it with compactions(including split and user-defined), repair, scrub, upgradesstables and cleanup operations; I also tested with multiple data directories. It looks ok for all of them, here are a couple of screenshots:

[^cleanup.png]
[^multiple operations.png]

I think that the patch is ready for review at github (https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010) or as a patch [^13010.patch]

Would appreciate any feedback.
Thanks.

[~rustyrazorblade] Would you be able to have a look at the patch? Thanks!

Hey [~alourie], thanks for the patch.  I've got a lot on my plate atm but I'll try to get to it tomorrow.

Looking at the result here, and doing some work / evaluation on my own, I wonder if this would be better suited for a {{nodetool compactioninfo ID}} command, where we could really show what's happening.  I've needed to know what sstables are being compacted, where it's being written, what levels are involved, etc several times now and it might be better to keep the compaction stats report simple and if you want the extra information the details can be listed out in a more logical fashion. 

What do you think?

[~rustyrazorblade] Thinking about your suggestion, and with some discussion with others, I think that adding an additional option flag to *compactionstats* would be more beneficial. I really like that I can see _all_ the information about all running compactions at the same time, including the directories. Hence, we could keep the current presentation intact and just add a flag to show any additional info. Also, if we wanted to, we could then expand *compactionstats* to accept an ID and only show info for that specific compaction.

What do you think?

I like the idea of hiding the extra stuff behind a flag, I think that would be great.  

Awesome. I just pushed an update of the patch to the github at [https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010#files_bucket.]

The option for more info is --
{noformat}-v/--with-more-info{noformat}

Please let me know what you think.

Hey [~alourie], sorry for the delay.  The patch no longer applies cleanly.  Would you mind taking care of the conflicts?  I'll review it immediately.

[~rustyrazorblade] Fixed. If any more updates needed, just let me know, I'm interested in pulling this in, so I'll be online for quite some time :)

Thanks for the review!

I'm done with my day over here, can't read any more code.  I'll get it reviewed tomorrow, thanks for the quick turnaround!

No worries [~rustyrazorblade] , there's no rush. Thank you for reviewing!

Hey [~alourie] I just took a look at the patch.  There's still a pretty big (200 LOC) conflict between trunk and what's in your branch.  Would you mind rebasing your branch off trunk so it applies cleanly?  According to the git history you merged in some changes from trunk, which are now making a lot harder to do the review, as I have to look through the history to determine what's actually been deleted and what's a change you made.  For instance it looks like you deleted {{doValidationCompaction}}, which you didn't, Blake Eggleston did in {{ c5a7fcaa8e000}}.

A few other notes while I'm in here to avoid lots of iterations:

# there's almost no comments added despite it touching almost 20 files.  I realize it's not the best commented codebase, but I'd like to see comments on any new variables like {{targetDirectory}}.  Specifically, consider why something is there {{// needed for nodetool compactioninfo output}} is better than {{// holds directory name}}.  Please add comments conveying intent for each class method and variable added.
# {{import org.apache.cassandra.cql3.Operation}} was added as an import to {{src/java/org/apache/cassandra/index/internal/CollatedViewIndexBuilder.java}} but not used

Outside that, I think it's looking pretty good, I'll be pretty happy to get this merged in soon!

[~rustyrazorblade], I don't have a clue what happened during that previous "fix". I've rebased my local stuff again, so hopefully, that is now truly fixed.

Additionally, I did a bit of cleanup and commenting, so now more stuff is commented on and the changeset got smaller by 2 files :)

So I hope it's looking better now and is much easier to read and review.

Thanks!

It doesn't look like you updated your branch.  This page still lists a bunch of merges in there and there's still conflicts when I try to merge it in.

It'll be a lot easier if you rebase off trunk and force push it up to your branch.

[~rustyrazorblade] I have no idea what's going on. I'll check.

[~rustyrazorblade] ok, sorry for the mess. I have still no idea how did that all got broken.

But, I did fix it now, so github should work, but just in case I'm attaching the patch files as well.

Please let me know if there are any more issues.

 

Thanks.

Hi [~rustyrazorblade]

I've just rebased this work on top of the latest trunk. Would you be able to have a look? The code is at https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010?expand=1 or patch at https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010.patch

Thanks!

Hey Alex, I'll try to get to this tomorrow.

Hey [~alourie], I took a look at the patch. I'm a little confused by the output though. Is this what you're expecting?:
{noformat}
$ bin/nodetool compactionstats -v   
pending tasks: 1
- tlp_stress.sensor_data: 1

id                                            compaction type keyspace   table       completed total     unit  progress
/home/jhaddad/dev/cassandra/bin/../data/data/                                                                          
8d542480-8f69-11e8-b6a9-4905c6229fa6          Compaction      tlp_stress sensor_data 397746242 691456147 bytes 57.52%  
Active compaction remaining time :   0h00m17s
{noformat}

I'm going to remove myself as the reviewer, I'm not sure if I'll have time to get another review in anytime soon.

I believe that the output matches the request, and was reviewed more than once. Nevertheless, I'm ready to change it if required.

[~alourie], this patch hasn't really been reviewed.  There were multiple iterations where you submitted patches and branches incorrectly, and figuring that out has been the bulk of the time spent.  

Regarind the output, does {{/home/jhaddad/dev/cassandra/bin/../data/data/}} really look right to you?  As a user I don't expect to see relative paths within absolute paths.

We can't even merge this into trunk at the moment since we're in a feature freeze, we're only merging bug fixes for now.  This will have to wait.

[~rustyrazorblade] by "reviewed" I meant the output format, not the patch. I apologise, I thought I was clear in this regard. Also, it was not clear that the problem was this mixture of relative/absolute paths and not the format. I'll check the code again to make it include absolute paths only.

Thanks!

[~rustyrazorblade] - this seems to be a usability bug. It will not affect the core storage / networking layer in the database. This is probably ok to merge during the freeze. WDYT?

I don't have any objections.  Would you mind bringing it up on the dev ML to see if anyone does?

hi [~alourie] , do you plan to work on this? Otherwise I would be interested in taking your work and make it happen.

[~smiklosovic] Yea, go ahead, I don't plan to work on it.

Note that we should add a flag for this to maintain output compatibility, but we can add it to vtables freely.

Yeah I think Alex already provided a flag for this if I am not mistaken.

PR: [https://github.com/apache/cassandra/pull/1791]
build: [https://app.circleci.com/pipelines/github/instaclustr/cassandra/1207/workflows/19676308-1a92-4b2d-a16a-ea8dad9054d5]

Below is the output from the test case to see this feature in action with some real-world load.

I renamed the flag to be "-v" or "--verbose". I am not completely sure if that is what we want but it seem to me it is better that "more info" flag before.

I am looking for a reviewer, [~rustyrazorblade] [~brandon.williams] would you please take a look if you dont mind and give me the feedback?

EDIT: pinging [~djoshi] and [~jjirsa] who are among the watchers as well, maybe they would be willing to spare 10 minutes on this? :)

{code:java}
pending tasks: 37
- keyspace_24.table_25: 1
- keyspace_24.table_26: 2
- keyspace_24.table_27: 1
- keyspace_24.table_28: 2
- keyspace_24.table_29: 1
- keyspace_00.table_01: 1
- keyspace_00.table_02: 2
- keyspace_00.table_03: 1
- keyspace_00.table_04: 2
- keyspace_00.table_05: 1
- keyspace_12.table_13: 1
- keyspace_12.table_14: 2
- keyspace_12.table_15: 1
- keyspace_12.table_16: 2
- keyspace_12.table_17: 1
- keyspace_06.table_10: 1
- keyspace_06.table_11: 2
- keyspace_06.table_07: 2
- keyspace_06.table_08: 1
- keyspace_06.table_09: 2
- keyspace_18.table_21: 2
- keyspace_18.table_22: 1
- keyspace_18.table_23: 2
- keyspace_18.table_19: 2
- keyspace_18.table_20: 1

id                                   compaction type                   keyspace    table    completed total  unit  progress
/my/data/dir0                                                                                                              
7e40e800-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_00 table_01 123       123456 bytes 0.10%   
7e70d1a0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_00 table_04 123       123456 bytes 0.10%   
7e529b40-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_00 table_02 123       123456 bytes 0.10%   
7e638b30-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_00 table_03 123       123456 bytes 0.10%   
7e7edb60-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_00 table_05 123       123456 bytes 0.10%   
                                                                                                                           
/my/data/dir1                                                                                                              
7e9e9860-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_06 table_08 123       123456 bytes 0.10%   
7ecea910-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_06 table_11 123       123456 bytes 0.10%   
7ebf3fc0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_06 table_10 123       123456 bytes 0.10%   
7eaddaa0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_06 table_09 123       123456 bytes 0.10%   
7e8fcb50-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_06 table_07 123       123456 bytes 0.10%   
                                                                                                                           
/my/data/dir2                                                                                                              
7ee194d0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_12 table_13 123       123456 bytes 0.10%   
7f1771e0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_12 table_17 123       123456 bytes 0.10%   
7f0b3ce0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_12 table_16 123       123456 bytes 0.10%   
7eef9e90-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_12 table_14 123       123456 bytes 0.10%   
7eff5600-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_12 table_15 123       123456 bytes 0.10%   
                                                                                                                           
/my/data/dir3                                                                                                              
7f257ba0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_18 table_19 123       123456 bytes 0.10%   
7f305110-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_18 table_20 123       123456 bytes 0.10%   
7f3b2680-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_18 table_21 123       123456 bytes 0.10%   
7f4845e0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_18 table_22 123       123456 bytes 0.10%   
7f560180-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_18 table_23 123       123456 bytes 0.10%   
                                                                                                                           
/my/data/dir4                                                                                                              
7f8575f0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_24 table_28 123       123456 bytes 0.10%   
7f91f910-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_24 table_29 123       123456 bytes 0.10%   
7f645960-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_24 table_25 123       123456 bytes 0.10%   
7f70dc80-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_24 table_26 123       123456 bytes 0.10%   
7f7b8ae0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_24 table_27 123       123456 bytes 0.10%   
                                                                                                                           
-------- Other operations --------                                                                                         
7f257ba0-1d9c-11ed-8f7d-0df44c673538 View build                        keyspace_18 table_19 123       123456 bytes 0.10%   
7e8fcb50-1d9c-11ed-8f7d-0df44c673538 Scrub                             keyspace_06 table_07 123       123456 bytes 0.10%   
7eaddaa0-1d9c-11ed-8f7d-0df44c673538 Secondary index build             keyspace_06 table_09 123       123456 bytes 0.10%   
7f560180-1d9c-11ed-8f7d-0df44c673538 Validation                        keyspace_18 table_23 123       123456 bytes 0.10%   
7eef9e90-1d9c-11ed-8f7d-0df44c673538 Verify                            keyspace_12 table_14 123       123456 bytes 0.10%   
7f70dc80-1d9c-11ed-8f7d-0df44c673538 Row cache save                    keyspace_24 table_26 123       123456 bytes 0.10%   
7f3b2680-1d9c-11ed-8f7d-0df44c673538 Relocate sstables to correct disk keyspace_18 table_21 123       123456 bytes 0.10%   
7f0b3ce0-1d9c-11ed-8f7d-0df44c673538 Stream                            keyspace_12 table_16 123       123456 bytes 0.10%   
7e529b40-1d9c-11ed-8f7d-0df44c673538 Key cache save                    keyspace_00 table_02 123       123456 bytes 0.10%   
7ecea910-1d9c-11ed-8f7d-0df44c673538 Unknown compaction type           keyspace_06 table_11 123       123456 bytes 0.10%   
7f8575f0-1d9c-11ed-8f7d-0df44c673538 Cleanup                           keyspace_24 table_28 123       123456 bytes 0.10%   
7e70d1a0-1d9c-11ed-8f7d-0df44c673538 Counter cache save                keyspace_00 table_04 123       123456 bytes 0.10%   
Active compaction remaining time :        n/a
{code}



I think this looks good but shouldn't this be available via vtable too?

Hmmm interesting, let me think about how that would actually look like ... Do you want to expose additional column per entry which would contain the disk path?

Yeah, that makes sense.

for vtable in nodetool it looks like this:

{code}
pending tasks: 2
- cql_test_keyspace.table_00: 2

keyspace          table    task id                              completion ratio kind       progress sstables total  unit  target directory                                                         
cql_test_keyspace table_00 6f155250-1e62-11ed-81ed-f13ee02c0a6e 0.10%            Cleanup    123      1        123456 bytes                                                                          
cql_test_keyspace table_00 6d465eb0-1e62-11ed-81ed-f13ee02c0a6e 0.10%            Compaction 123      10       123456 bytes /some/dir/cql_test_keyspace/table_00-6b470c40-1e62-11ed-81ed-f13ee02c0a6e
Active compaction remaining time :        n/a
{code}

I fixed one issue - it is not possible to mix "verbose" with "vtable" output in nodetool. They are mutually exclusive. One can do either verbose or vtable.

CQL vtable output contains that dir as well (new column).

It is same PR / branch. I am running the build as we speak.

This looks good. 

bq.  it is not possible to mix "verbose" with "vtable" output in nodetoo

I think it makes sense to keep building on vtables, and thus the vtable output in compactionstats when someone wants more information than what's in the backward-compatible view.  So I think keeping this behind the vtable flag is fine, and helps cuts down on the number of flags.

the build with the latest changes is here: https://app.circleci.com/pipelines/github/instaclustr/cassandra/1213/workflows/b8445da5-3f64-46c5-8331-f72f90523835
PR is same: https://github.com/apache/cassandra/pull/1791

We also need to run the j11 precommit pipeline.

j11 precommit build https://app.circleci.com/pipelines/github/instaclustr/cassandra/1213/workflows/2ec59cb3-59fe-470f-bc33-3e7d09360501

bq. it is not possible to mix "verbose" with "vtable" output in nodetool

To be clear, I think we should drop the 'verbose' flag and leave this accessed via the vtable flag.  There's really no utility in having it available via two flags, and it's good to nudge people toward vtables when we can.

I am confused. What I was trying to do all the time was to follow the idea of Jon to write that output in such a way that it would group all compactions under the same disk. Now if we do want to have this only for vtable, do I understand it right that we will not have such output anymore? Each vtable entry for compaction will contain just a respective directory, that is all. Is this ok for people? I think the way of doing things per Jon makes sense to me.

The scenario I'd like to avoid is adding a flag each time we want to vary the output, and appending the vtable output is a good way of doing that.  The next time we want to add something here, are you suggesting we would also lump it under the verbose flag?

If there is ever some other field added into CompactionInfo and we want to show it to a user, we would add it everywhere - to whatever output. So to answer your question, yes, we would add it under "verbose" as well.

Having said that, I have to admit that "verbose" is rather unfortunate name for that flag. What I am trying to capture is to have the output grouped per disk as Jon suggested. Maybe "--group-by-target-dir" would be more appropriate?

If we go with vtable only, to achieve this "grouping by disk", we could at least order the output somehow but that is tricky, we would probably order the cql output too and that is just ... strange.

bq. Having said that, I have to admit that "verbose" is rather unfortunate name for that flag.

I thought this too, until you confirmed the future use of "verbose" and then it made sense .

bq.  Maybe "--group-by-target-dir" would be more appropriate?

This is exactly what I'm trying to avoid, because if we do that, then every time we want to change the output we'll have a new flag to append, and it will become a mess to get actual verbose output from compactionstats.

bq. If we go with vtable only, to achieve this "grouping by disk"

The nice thing about vtables is if you want different output, switch to cql and select it however you like.

But I see the problem with the backward compatibility - like we want to have the non-vtable output same as it was. Look, lets go with your idea. I already rewrote the patch (actually simplified) and I am running the build.

One point to add is that I do not think that besides this "group by dir" there will ever be any other flag like that. What flag that might be? 

Very recently we added CASSANDRA-16844 which started the whole compatibility flag situation we are in now, so I'm sure there will be others.

8 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1216/workflows/19d0beb7-310b-4170-8863-608bff7adb40
11 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1216/workflows/5ed6064f-f72d-4b44-bd0f-a47885e10de1

simplified solution we just talked about is here: https://github.com/apache/cassandra/pull/1801

one test is repeatedly failing (not related to this PR)

+1

{quote}one test is repeatedly failing (not related to this PR)
{quote}
I guess that's {{{}org.apache.cassandra.tools.StandaloneUpgraderOnSStablesTest.testUpgradeSnapshot{}}}. It seems caused by this patch, since [repeated runs on CI|https://app.circleci.com/pipelines/github/adelapena/cassandra/2009/workflows/e32acbf3-05e2-4cdb-a60d-154aa4326f78] for [the immediately previous commit|https://github.com/adelapena/cassandra/commit/c4b1c0614e42b4ea2064822d31c28aa5d4f1450a] don't hit it.

I have opened CASSANDRA-17849 for it.

bq. one test is repeatedly failing (not related to this PR)

I unfortunately confused it with CASSANDRA-17804.

bq. I have opened CASSANDRA-17849 for it.

Thanks, I'll take a look.

Interesting, I was thinking it is some flake as it was totally unrelated and was failing on timeout. I did a repeated run on this ticket though.

I take a look too.


Sent from ProtonMail mobile



\

