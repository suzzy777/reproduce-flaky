I generated the patch from my clone of the git mirror.  If this patch is unacceptable in anyway I'm willing to redo.  Just thought I'd take the easy route first.

Interested user here, not anyone linked to Solr or Lucene.

Would the 100ms latency you have described here show up in QTime values or be purely client-side?  I ask because the median (not average) QTime value I see in disributed searches (seven shards on two servers) on Solr 3.5.0 is about 10 milliseconds, and even faster on 4.2-SNAPSHOT ... and that involves the stale check both on the client side and the seven shards.  The client is SolrJ 4.1, with max retries (1) and connection timeout (5000ms) being the only low-level parameters that are set.  SolrCloud is not involved here.

The idea of a sweeper thread makes me nervous, given the project's general reaction to the background threads created by an intermediate SOLR-1972 patch.  Also, with 5000 milliseconds between executions of the sweeper thread and an assumption of a server timeout of 50 milliseconds, will it be able to effectively avoid problems in all environments?  If these are not actual worries, then I can be quiet.


> Would the 100ms latency you have described here show up in QTime values or be purely client-side?

So my latency measurements include end-to-end from client->yokozuna->solr coordinator->shards->solr coordinator->yokozuna->client.  I didn't track the QTimes, but in benchmarks I ran months ago I saw the same results whether I went through Yokozuna or hit Solr directly with the distributed search.  I could probably re-run to report QTime changes.

> I ask because the median (not average)...

I'm also reporting median latency, not mean.  In my first link you can also find graphs with 95th, 99th, 99.9 percentiles.

> ...an assumption of a server timeout of 50 milliseconds

The assumption is 50s, but yes, it is an assumption.  A better patch would make the idle timeout configurable as well as the period.

> If these are not actual worries, then I can be quiet.

No, these are great points.  This patch, as it sits now, is for a specific context.  I did what I needed to do to improve my specific situation.  Honestly, I would like to see another person run this patch and verify at least some amount of speed-up.  Otherwise, it might best stay downstream.

Ryan, this is very interesting indeed! I've read your patch and it looks good, i'll be testing this in the next few days.

I'm not very sure enabling TCP_NODELAY (e.g. disable Nagle's algorithm) will help a lot. In theory it could but we already did performance tests a few months ago on two clusters where one had it enabled and the other didn't. I could not measure a significant difference, my tests may have been bad of course :)



> ...i'll be testing this in the next few days.

Great, I would love to see these results reproduced by someone else.

> I'm not very sure enabling TCP_NODELAY (e.g. disable Nagle's algorithm) will help a lot

I agree, it probably doesn't matter but I disabled it anyways given that nagle is really meant for things like telnet and such.  I figured it couldn't hurt to disable.



Hi Ryan,

Until now i've not seen real significant latency improvements when performing concurrent stress tests. The response time still hovers between 70ms and 100ms with and without the patch using today's trunk. I did see some disturbing exceptions, it looks like sometimes connections are reused that are already dead, returning a connection refused exception. Sometimes shards are also reported dead yielding the dreaded no servers hosting shard error.

Here are some exceptions:

{code}
Caused by: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:[...hosts here...]
        at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:333)
        at org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:171)
        at org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:135)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: org.apache.solr.client.solrj.SolrServerException: IOException occured when talking to server at: http://host/solr/shard_d
        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:416)
        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:181)
        at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:264)
        ... 10 more
Caused by: org.apache.http.NoHttpResponseException: The target server failed to respond
        at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:95)
        at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:62)
        at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:254)
        at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:289)
        at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:252)
        at org.apache.http.impl.conn.ManagedClientConnectionImpl.receiveResponseHeader(ManagedClientConnectionImpl.java:191)
        at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:300)
        at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:127)
        at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:717)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:522)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:784)
        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:353)
        ... 12 more
{code}

{code}
2013-02-28 13:00:15,666 WARN [solr.cloud.RecoveryStrategy] - [main-EventThread] - : Stopping recovery for zkNodeName=178.21.118.192:8080_solr_openindex_dcore=openindex_d
2013-02-28 13:00:17,199 WARN [solr.update.PeerSync] - [main-EventThread] - : PeerSync: core=shard_d url=http://host/solr  couldn't connect to http://host/shard/, counting as success
2013-02-28 13:00:17,201 ERROR [solr.cloud.SyncStrategy] - [main-EventThread] - : Sync request error: org.apache.solr.client.solrj.SolrServerException: Server refused connection at: http://host/shard
2013-02-28 13:00:17,205 ERROR [solr.cloud.SyncStrategy] - [recoveryCmdExecutor-17-thread-1] - : http://host/shard/: Could not tell a replica to recover:org.apache.solr.
client.solrj.SolrServerException: Server refused connection at: http://host
        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:409)
        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:181)
        at org.apache.solr.cloud.SyncStrategy$1.run(SyncStrategy.java:298)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.http.conn.HttpHostConnectException: Connection to http://host refused
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:190)
        at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:294)
        at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:645)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:480)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:784)
        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:353)
        ... 5 more
Caused by: java.net.ConnectException: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:127)
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:180)
        ... 12 more
{code}

I'm not sure yet why these occur, maybe it's my Tomcat settings that do not match Jetty's defaults.

Markus,

> Until now i've not seen real significant latency improvements when performing concurrent stress tests. The response time still hovers between 70ms and 100ms with and without the patch using today's trunk.

Fair enough, it could be that my particular setup had something to do with it.  I should verify on a 2nd set of hardware.

What is this "concurrent stress test" you are running?  Is it something public I could run as well?  How many shards is each query hitting and is each shard on its own physical machine?

Also, I'm going to attach a BTrace script.  If you could run this script it would be really helpful as it would tell us how long the stale check is taking in your environment.  I included instructions on running the script in the comments of the source.

> I'm not sure yet why these occur, maybe it's my Tomcat settings that do not match Jetty's defaults.

My patch currently assumes a 50s idle timeout (or larger) on the server.  Any smaller and you might see socket reset errors.

I'm not sure why you are seeing the errors you mentioned.  Your first trace doesn't match up with the 4.1.3 HTTP Client code.  Has the client version been updated for the latest?




BTrace script to dynamically time the stale check

I have some new results from a different cluster.  The short story is
that I still see improvement from removing the stale check, just not
as dramatic as on my SmartOS cluster.  Throughput improved by 108-120%
and there was a 0-5ms delta in latency.

What I take from this is that the benefits of removing the stale check
will vary depending on # of nodes, hardware, query and load.  In
theory removing the stale check should never hurt as removing blocking
syscalls should only help.  But I totally understand if about being
cautious with a change like this.  Personally I'd like to see at least
one other person confirm a non-negligible difference before I bother to
make this patch more acceptable.  Best to let this ticket stir a while
I suppose.

## Cluster Specs

Add nodes are running on baremetalcloud so this time they are truly
different physical machines with no virtualization involved.

* 8 nodes/shards
* 1 x 2.66GHz Woodcrest E5150 (2 cores)
* 2GB DDR2-667
* 73GB SAS 10k RPM
* Ubuntu 12.04
* Oracle JDK: Java HotSpot(TM) 64-Bit Server VM (build 23.7-b01, mixed mode)
* 512MB max heap
* Example schema

## Bench Runner

* 1 node
* 2 x 2.66GHz Woodcrest E5150
* 8GB DDR2-667
* Using Basho Bench as load gen

## Queries


All queries hit all shards.  All queries were single term queries
except for alpha which is conjunction.  The numbers listed are the
number of documents matching each term query.

* alpha: 100K, 100K, 0
* lima: 1
* mike: 10
* november: 100
* oscar: 1K
* papa: 10K
* quebec: 100K

Attached is the aggregate data (.dat) and corresponding plots (.svg)
of that data.  The data was aggregated from raw data collected by
Basho Bench (and this raw data is actually the aggregate of all events
at 10s intervals).  E.g. the median latency is actually the mean of
the median latencies calculated against all events in a given 10s
period.  What I'm saying is, it's rollup or a rollup so while there
are 2 decimals of precision those numbers are not actually that
precise.  But this should be good for ballpark figures (if you're a
stats geek please let me know if I'm committing a sin here).

There is a big delta in latency for the mike benchmark but I'm
chalking that up to an anomaly for the time being.


Markus,
The tomcat default work threads are only 150, connection refuse is cased by Socket listener queue full, you can expand the work threads in the tomcat configuration.

I recently updated Yokozuna (1) to use Solr 4.4.0.  After running a
query benchmark I noticed that throughput had dropped to 44% of the
baseline.  After some head scratching I realized that my distributed
search patch had not applied successfully.  Sure enough, after I
updated the patch for 4.4.0 throughput returned to 100%+ of baseline.
Below is a table showing results of query benchmark for 4.3.0, 4.4.0
and 4.4.0 without this patch.  The throughput drops to less than half
of Solr 4.4.0 with the patch and the latency more than doubles.

|Measurement     |Solr 4.3.0       |Solr 4.4.0       |Solr 4.4.0 w/o Patch  |
|----------------|-----------------|-----------------|----------------------|
|Mean Throughput |1512 ops/s       |1525 ops/s       |670 ops/s (44%)       |
|Median Latency  |22.0ms           |21.6ms           |46.2ms (2.1x)         |
|95th Latency    |29.8ms           |29.4ms           |76.8ms (2.6x)         |
|99th Latency    |35.3ms           |34.6ms           |86.2ms (2.5x)         |

These results are against a 4-node cluster all hosted on 1 physical
machine.  Manual distributed search is used for querying, there is no
use of SolrCloud.  There are only 1 million small text documents
stored.  The query matches only 1 of these documents.  The query
results and filter caches are enabled and should have a high hit
ratio.  The point is to make the queries inexpensive as possible to
see what other overhead might occur.  There may very well be scenarios
where this patch makes little to no difference.  But in this case it
seems to make a big one.

This update is not to prove that my patch makes a significant
difference in all cases.  Rather, I accidentally ran this benchmark
and was surprised at the difference I saw.  I wanted to ping this
issue in hopes that others might try the patch to see if it helps.

Here is the corresponding ticket on the Yokozuna repo: https://github.com/basho/yokozuna/pull/197

1: Yokozuna is a project with integrates Solr with the Riak database.  https://github.com/basho/yokozuna

Version of the patch that applies to 4.4.0.

My biggest hesitation on this patch is:

{code}
+    // NOTE: The sweeper task is assuming hard-coded Jetty max-idle of 50s.
+    final Runnable sweeper = new Runnable() {
+            public void run() {
+                mgr.closeIdleConnections(40, TimeUnit.SECONDS);
+            }
+        };
+    final ScheduledExecutorService stp = Executors.newScheduledThreadPool(1);
+    stp.scheduleWithFixedDelay(sweeper, 5, 5, TimeUnit.SECONDS);
{code}

Great write up Ryan! I looked this over once or twice when you first filed it, but I just never remembered it when I had the time to dig into it. Sorry this didn't get more attention sooner, but sometimes a good issue just can't find a committer for some time for one reason or another. [~hgadre] has also been looking into connection reset errors during distributed updates (an occasionally reported issue) and tracked it down to this stale check being imperfect. It really seems wise to remove it at this point.

I'll put up a draft patch for comment in a bit that builds on this one.

FYI: Some info around the stale check mentioning that it's not 100% reliable and suggestion of this workaround http://hc.apache.org/httpcomponents-client-ga/tutorial/html/connmgmt.html : 2.5. Connection eviction policy

[~rzezeski], what drove your decision to make the sweeper thread interval 5 seconds (considering what the default should be)?

This patch removes some nocommits around config and cleans a few things up a bit.

I barely remember making my earlier comment on this issue.  A year and a half ago I was still running 3.5, apparently!  I'm pretty sure that my distributed search still doesn't have 100 milliseconds of latency to lose, but hopefully there will be some improvement when this change is made.


My main concern is the flakey stale check in httpclient. I'll happily accept any perf improvements. HttpClient docs also suggest a 10-30ms per req cost if I remember right. The. It seems the blocking call can prob make it even worse under concurrency. 

New patch. Some more cleanup. Also added closeExpiredConnections to the sweeper thread as the documentation recommends.

I've added the latest patch to review board: https://reviews.apache.org/r/28393/

bq. Hrishikesh Gadre has also been looking into connection reset errors during distributed updates (an occasionally reported issue) and tracked it down to this stale check being imperfect. It really seems wise to remove it at this point.

Can you or [~hgadre] explain this more?  From my reading of it, there's nothing you can do to be 100% correct, i.e. the server closes the connection for whatever reason, you make a request on the client before anything is returned, and you get back a RST.  I could be wrong about that -- the few tcp state machine diagrams I looked up don't cover that case.  I guess I'm trying to understand the purpose of this patch -- is it to reduce latency?  to reduce the number of connection resets?  both?

It reduces latency and removes the stale check that can easily race with the server. Did you read the linked original write up and http client docs around stale check?

To sum up the very high level: currently, each connection thread does a stale check, and of course, even it if it passes the server could close the connection a millisecond later. It's better to remove this stale check that every thread has to deal with and have a dedicated thread that handles stale connections itself, and in a way that they are closed on the client before they are closed on the server. We want the client to handle connection lifecycle. Nothing promises you that you will never get a connection reset - but rather, it should go from fairly easy to randomly happen for apparently no good reason to almost never happening (and perhaps it is a better reason than bad luck timing if it does - a reason you might be able to address).

Would be good to remove that stale check also in solrj.

bq. Did you read the linked original write up and http client docs around stale check?

Yes, I was responding specifically to the "tracked it down to this stale check being imperfect" -- given that any check is imperfect, I wanted to make sure I understood the motivation.  Your latest comment addresses that.

> Ryan Zezeski, what drove your decision to make the sweeper thread interval 5 seconds (considering what the default should be)?

It had to be shorter than the server connection timeout; 5 seconds was
less than the 60 second server timeout I had.

It seems you are running with this patch nicely. Feel free to modify
it however you see fit.

As for the latency differences that Shawn mentioned, keep in mind I
was running microbenchmarks. It was a tight loop of small queries
running at max throughput and everything was a cache hit. IIRC, faster
CPUs showed less of a gain. Part of this is explained because it
removes at least one syscall for every request. Faster, more modern,
CPUs should context switch more efficiently. If I was doing this
benchmark over again today I would try other variations (low-rate to
avoid any CPU run-queue buildup and focus on latency only) and examine
more performance metrics.

That aside, this patch should avoid the case where a stale conn is
chosen and a new one has to be created as part of the request which
will result in latency outliers. A win in my book.


I've got a patch coming that tries to extend this to all our http client usage - that ended up being a fairly long thread to pull on. Ill post a progress patch soon but there are still some things to address. 

bq. given that any check is imperfect

Yeah, I was more referring to the fact that the whole thing kind of seems like a bad bug to me - except that in pure http, a retry is often fine and I guess keeps it from being a flat out buggy situation. If you turn off retries, it does seem like a buggy implementation, though a tough one to solve generally for HttpClient I guess. Even with Solr, I'm not super happy you have to line up the server and client idle setting reasonably, but it appears the best we can do.  

I guess in my mind, a non imperfect implementation would not have this built in 'random race condition fail'.

Here is the latest in progress patch. It moves configuration to system properties and away from solr.xml so that we can try and use the new stale connection approach with more of our HttpClient usage.

ReviewBoard doesn't seem to like a move that I did of IOUtils in the patch, so moving back to the JIRA issue.

I thought I had killed stale connection checks long ago.  Maybe it was long enough ago that it was back in the CNET days or something...

Can we tell when this happens (and that we are certain that the server did not receive the request) so that we can do a retry?



bq. Can we tell when this happens (and that we are certain that the server did not receive the request) so that we can do a retry?

From what I can tell, the default retry handler will retry in cases we don't want. We may be able to implement one that works, but it seems a little tricky to be 100% sure of that work, and it still kind of sucks that every thread has to deal with this stale connection check (docs claim 10-30ms hit, Ryan ran into worse troubles under concurrency) rather than pulling out that work to a background thread.

bq. rather than pulling out that work to a background thread.

Yeah, I was talking about even after this patch is applied. This should drop the errors by an order of magnitude or so... but it would be nice to drop to zero.

Ah, okay - probably worth following up with another issue.

I was about to try upgrading httpclient to 4.3.6 on branch_5x to see if anything breaks, and went looking at release notes to see if there's any compelling reason to commit the upgrade.

After checking out the latest 4.3 release notes, I looked at the 4.4 beta release notes.  Looks like stale connections are getting attention there too.

http://www.apache.org/dist/httpcomponents/httpclient/RELEASE_NOTES-4.4.x.txt

I don't know what the timeframe for the new HC release is, but perhaps we can let it be solved upstream?


Thanks for the info Shawn - certainly worth investigating. On a high level reading, my worry is that this works to minimize the performance penalty but I don't know that it tries to solve the connection reset issue that I'm more concerned about.

bq. On a high level reading, my worry is that this works to minimize the performance penalty but I don't know that it tries to solve the connection reset issue that I'm more concerned about.

This seems to be the case. It uses the same stale check, it just doesn't do it on every request anymore. https://issues.apache.org/jira/browse/HTTPCLIENT-411

One other thing to consider, and I'm not sure if this applies to that release note, is that there are major API changes that in HttpClient 4.4 that you need to use to get the new features.  In general everything is done via builders, so you can't change many configuration settings after creating the httpclient.  There are APIs in HttpSolrServer and elsewhere that let you change the configuration that would have to be reworked.

We have SOLR-5604 to address the deprecated HttpClient methods that we currently use.  I already attempted it once ... it's not going to be a trivial change, and my knowledge of HttpClient is too limited to be useful.  Thankfully those methods will stick around until HC 5.0 comes out, so there's not currently a pressing need.

If the method of dealing with stale checks that is being developed here is superior to HC 4.4, then we might want to ask that [~olegk] consider it for HC itself.  I haven't looked at either solution, and I doubt that I would understand it even if I did look.


The idea behind connection check optimizations in HC 4.4 is rather straight-forward: given the 'stale' check cannot be 100% reliable (the connection can turn 'stale' immediately after passing the check but before a request is issued) there is no point checking the connection indiscriminately before each and every request. It generally should be sufficient to check those connections that are more likely to get 'stale'. Usually those are the connections that have been sitting idle in the connection pool for a considerable period of time.

HC 4.4 is most likely going to get delayed by a month or two. So, there should enough time to contribute an alternative solution if it turns out to be better than what we currently have.

Oleg

I've been pushing this along off and on for a while now. This had a rather large impact on a variety of tests - especially with SSL. I think I'm pretty close though - one more flaky test to investigate I believe.

There is a rather interesting affect from this change - the 'close idle' thread will trump the socket timeout. So we are looking at lower socket timeouts or longer idle timeouts or some compromise in between.

My latest work attached. I think this is almost ready.

We do need to do a retry, but it's only once, and it's much more limited than the default retry impl. Without that, simple things like stopping and starting a server can lead to client side connection reset errors.

bq. stopping and starting a

Well, depending on how graceful.

After this patch, tests have gone from about 16-17 minutes to run for me to 10-11 minutes. I've tweaked Jetty shutdown to be faster, but most of this is probably related to SSL tests and SOLR-5776 and SOLR-6293.

Because this change effectively lowered client idle timeouts, didn't have any retries for a while, and perhaps my system random entropy has been low, I really had to clamp down on how much SSL is done in a test run to be able to consistently get clean runs with timeouts from SSL lack of random entropy blocking. As I'm getting speed much closer to what I used to get pre SOLR-6293, this looks like a prime suspect for that major loss.

bq. There is a rather interesting affect from this change - the 'close idle' thread will trump the socket timeout.

I gathered this because when I run the close idle connection thread at a lower interval than the socket timeout, slow SSL tests where hitting a socket error (no response from server) rather than the socket timeout they would hit if I raised the close idle connection thread interval.

Looking at the code though, I think this idle connection close should only apply to unused connections in the pool. I have to look closer at what is happening.

Okay, this will not affect socket timeout settings. I was seeing something else with the difference in exceptions from SSL slowness it seems.

I have something pretty solid now. I still have not figured out why the SolrCloud tests that use the proxy stuff to simulate network partitions don't work yet, but other than that things are coming together nicely I think.

So one advantage that the stale check had was that it could handle (at least most of the time) a NoHttpResponseException. We can't currently retry on that if we have sent the whole request, but stale check can detect cases of it before sending. We can retry on pertinent connection reset and broken pipe socket exceptions though.

I skimmed through the patch. Looks good, thank you for taking this up!

>>So one advantage that the stale check had was that it could handle (at least most of the time) a NoHttpResponseException

Could you please elaborate? I don't quite get it.

Here is a dump of my latest state. More to come.

One exception that pops up in some places it did not before is NoHttpResponseException - after we have sent the full request and so we cannot retry. It would appear that the stale check could usually detect this broken connection before even sending the request and then get a new valid connection before making the request. I've seen this exception even with the stale check, so I think there is still a race, just like connection reset, but often, it seems it can handle this.

I brought this up to date with trunk and spent some more time hardening and investigating test issues. Getting close to a resolution.

The final 1% of this patch has gotten a bit complicated. This patch has also grown to the point where it is difficult to maintain.

I'm going to spin off and commit a few sub patches. First, adding a limited retry and second, closing all httpclient instances properly. That and a few other fixes / changes I've made along the way will significantly reduce the size and overhead of this patch.

SOLR-6931 We should do a limited retry when using HttpClient.

SOLR-6932 All HttpClient ConnectionManagers and SolrJ clients should always be shutdown in tests and regular code.

I'm seeing lots of test fails on the mailing list due to NoHttpResponseExceptions, which this StackOverflow answer suggests is due to stale connections: http://stackoverflow.com/questions/10558791/apache-httpclient-interim-error-nohttpresponseexception.

Following the links to http://hc.apache.org/httpcomponents-client-ga/tutorial/html/connmgmt.html, the section on connection evictions policy says that you should have a separate thread that periodically closes idle or dead connections.  Is this something we should look into for HttpSolrClient?

You are probably seeing SOLR-6944.

Here is a first pass at shrinking this patch to trunk now that all the issues I spun off are committed.

It is not complete anymore because of the change to Jetty 9.

For the record (if anyone is interested) the patch on Yokozuna has moved since Ryan originally commented on this ticket. The repository moved, and the original patch can now be found at https://github.com/basho/yokozuna/pull/26


I'd like to come back to this. Previously, it was causing spurious connection resets in tests. I'm hoping we have addressed that with other fixes.

SOLR-6625 seems to have given users the ability to use any httpclient impl they want - that kind of sucks in this case. To do advanced stuff like this, we pretty much need to own the impl.

I see nothing that calls setHttpClientImpl and there are no tests or documentation though. I think we should find an alternative to whatever was done here (no one even doc'd it?). We don't want the user to be able to set HttpClientImpls via a static method...not documented, not tested, not a thread safe way to configure. 

I've just about got a new patch ready to show here.

I still don't have a decent way to hook into httpclient close anymore, so that's an issue that needs to be worked around outside of just tests (as I've done).

Some things to consider:

Supposedly the stale connection check is not as bad a performance killer as it used to be as it's not done every request any longer?
Without the stale check, when a server drops, even if it comes back up, the client might try to use a bad connection - in the past the stale connection check could catch that.
However, the stale connection check is still not 100% reliable and I assume the perf optimization that did has a similar issue as above in the right circumstance.
It still would be nice to try and control connection lifecycle from the client as much as possible.

Actually, the work around for the stale check they have done is the answer to the bad connection after quick restart issue.

We need to get off deprecated classes though. The work in SOLR-6625 interferes with that, but we have always needed to consider this stuff internal. We cant expose such low level impl details as supported user surface area. Bug fixes and advances are not going into these deprecated classes like the new ones, and they will be gone eventually.

Nice, if we move to the new API's we also get most of the work I've been doing here built in, including the idle connection sweeper.


I've got this mostly working - new httpclient impls are much better. Security and SSL have really nestled into this late config binding that the httpclientconfigurer stuff allowed and does not anymore (builder pattern) though, so I'll probably bail on it.

I hate that we leaked httpclient apis into our user apis :(  I think it was just the quick and easy way to deal with SSL and then security glommed on later as well. Too late for 6 now though, and even 7 would need a bunch of work to make that all work with preconfiguration and no impl level to user leakage.

Actually this snuck in even before SSL, it was added in 4.0.  It was quietly added, but internal features did not count on it or use it. SSL only used it for test purposes later on. Security has bear hugged it though - it's how you configure security now and is part of a user plugin api. Bummer given the old deprecated HttpClient classes involved and the extra pain to move to preconfiguration. We should try and minimize exposing internal client API's as part of our API's to users. It really locks us in.

One can override SSL configuration on a per request basis by setting a custom connection socket registry in the HttpContext of the request. 

http://hc.apache.org/httpcomponents-client-4.5.x/httpclient/xref/org/apache/http/impl/conn/DefaultHttpClientConnectionOperator.html#66

This however can potentially lead to SSL connections being re-used by another thread with a different user identity / security context, that is why we do not really advertise this feature.

Oleg

6.0 isn't out the door yet... can we somehow change the status of the current stuff that relies/exposes those deprecated APIs to allow for change/improvement (and upgrading HttpClient) in 6.x?

Linking to SOLR-5604, where this was explored. Given the non-trivial nature of such a refactoring (as it appears to me from the linked issue, but I could be wrong as I haven't understood all aspects of that refactoring yet), I think we should target this for later (possibly 7.0).

Still a bunch of work to do to finish beating this into shape, but it compiles and handles most of the API changes and builder patterns we need to move to.

bq.  I think we should target this for later (possibly 7.0).

Luckily, the plugin API itself is marked as experimental even, so I see no need to wait to fix our connection management for a full major release.

Might be able to fix those APIs now, but we will see. This issue is a b$@& as is. 

Here is a much more functionally complete patch. Tests should be passing, but a few are ignored. Much closer to done, but still some things to do.

bq. Might be able to fix those APIs now, but we will see.

Actually, it's probably not very useful given we cannot put the new HttpClient APIs in the plugin API, and we basically forced to expose that.

Had not cleared the solrj tests. Still a mess, but some light at the end of the tunnel. Now all tests, including solrj, should be passing. A few remain ignored.

New patch. Getting pretty damn close.

Okay, removed most of those nocommits. One final nocommit left around using the SolrPortAwareCookieSpecFactory for kerberos.

Okay, here is pretty reasonable patch I think. I will start reviewing all these changes.

Patch after first pass of review and cleanup.

I think we should make these interfaces instead of abstract classes. Before Java 8 we could not add methods to interfaces w/o breaking compatibility. So, we always created abstract base classes . Now that we have moved to java 8 we can add methods at will 
{code}
public static abstract class HttpRequestInterceptorProvider {
    public abstract HttpRequestInterceptor getHttpRequestInterceptor();
  }
  
  public static abstract class CredentialsProviderProvider {
    public abstract CredentialsProvider getCredentialsProvider();
  }
  
  public static abstract class AuthSchemeRegistryProvider {
    public abstract Lookup<AuthSchemeProvider> getAuthSchemeRegistry();
  }
  
  public static abstract class CookieSpecRegistryProvider {
    public abstract Lookup<CookieSpecProvider> getCookieSpecRegistry();
  }
{code}

Sure, we can make the interfaces.

I'll commit this to master shortly so we can see how Jenkins takes to it and iterate from there. Rather large set of changes to keep up to date.

Commit ce172acb8fec6c3bbb18837a4d640da6c5aad649 in lucene-solr's branch refs/heads/master from markrmiller
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ce172ac ]

SOLR-4509: Move to non deprecated HttpClient impl classes to remove stale connection check on every request and move connection lifecycle management towards the client.


Commit d0156b1126f094e4e469172d55842ed77cb82943 in lucene-solr's branch refs/heads/master from [~thetaphi]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d0156b1 ]

SOLR-4509: Fix test failures with Java 9 module system by doing a correct cleanup


Hi [~markrmiller@gmail.com],
I committed a fix for 3 tests that now leaked SolrClients in static fields, which made the test leak detector angry on Java 9's module system (it tried to measure private, internal classes). If this is backported, please also backport my fix.


GIT commit ce172acb appears to have broken SolrCLI so that {{bin/solr}} client actions (ie: {{bin/solr status}} {{bin/solr create ...}}, etc..) no longer work when solr.in.sh is configured to use SSL.

Tracking this in SOLR-9040

Commit 9ab76a1e41d7019fd07b16a79a587653cf6d76a4 in lucene-solr's branch refs/heads/master from [~hossman_lucene@fucit.org]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9ab76a1 ]

SOLR-9040 / SOLR-4509: Fix default SchemaRegistryProvider so javax.net.ssl.* system properties are respected by default


FWIW: It doesn't seem likely that this issue (SOLR-4509) is going to be backported to 6x since it has some incompatible solrj level changes (Configurer->Builder+SchemaProvider) but if I'm wrong and someone does decide to try and backport it, please note that SOLR-9028 has already been backported from master->6x and quite a few conflicts due to SOLR-4509 changes were resolved there that might cause new conflicts here.

If SOLR-4509 is backported, it might be easiest to:
# revert the branch_6x changes related to SOLR-9028
# backport SOLR-4509
# rebackport the master changes for SOLR-9028


Manually correcting fixVersion per Step #S6 of LUCENE-7271


This has broken support for specifying connection and read timeout values for UpdateShardHandler's http client. I opened SOLR-9262 to fix.

Commit 2b4420c4738bb3aed3ae759fd93b6cbbdbc1eefd in lucene-solr's branch refs/heads/master from [~shalin]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2b4420c ]

SOLR-9262: Connection and read timeouts are being ignored by UpdateShardHandler after SOLR-4509


Commit 51fde1cbf954b6f67283ad945525e8c6b5197fb9 in lucene-solr's branch refs/heads/master from [~shalin]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=51fde1c ]

SOLR-9262: Connection and read timeouts are being ignored by UpdateShardHandler after SOLR-4509


Discovered that while SolrCLI changed to 
{code}
String builderClassName = System.getProperty("solr.authentication.httpclient.builder");
{code}
the property set in {{bin/solr}} is still the old {{solr.authentication.httpclient.configurer}}
{code}
if [ "$SOLR_AUTHENTICATION_CLIENT_CONFIGURER" != "" ]; then
  AUTHC_CLIENT_CONFIGURER_ARG="-Dsolr.authentication.httpclient.configurer=$SOLR_AUTHENTICATION_CLIENT_CONFIGURER"
fi
{code}
and likewise in {{bin/solr.in.sh}}

Also looks like the Windows scripts lack support for these settings.

Covered in SOLR-9255

