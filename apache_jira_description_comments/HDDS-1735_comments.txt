[~elek] There are many dev-support scripts, which is not integrated with maven in Ozone.  Hadoop has already provided many of the plugins preconfigured such as findbugs, rat, unit tests.  There are many good parameter optimized for each of the plugins.  I see that Acceptance test is less detailed using Ozone approach than using Yetus.  Can we use the maven life cycle and maven plugins to accomplish the same thing as the dev-support scripts?

Sure, this is what we do. Please check the scripts. They execute the pre-configured maven commands...

[~elek] {quote}Sure, this is what we do. Please check the scripts. They execute the pre-configured maven commands...{quote}

Shell script wrapper on maven plugin goals is not exactly intended for a normal maven project setup.  Maven provides ability to run plugin goals or build life cycles (phases).  It looks like we are using shell script to wrap around maven goals to configure plugin to run a certain way for continuous integration.  This seems to redundant effort than writing pom.xml which describes plugin goals to run.  Can we try to use Maven intended model to avoid having to reinvent maven build life cycle, please?

To be honest I am confused a little bit.
 # Can you please clarify what is your suggestion? How should I improve the patch?
 # Can you please confirm that this patch doesn't block you in any of your work? (This is a fix for the _existing_ shell scripts which don't modify any part of the maven build just help to run certain build steps with maven)

[~elek] 
{quote}Can you please clarify what is your suggestion? How should I improve the patch?{quote}

The current direction for Ozone continuous integration is not moving in the direction that I like to see.  For example, user can perform:

{code}
mvn clean test
{code}

To perform unit test, and selectively launch test case base on:

{code}
mvn clean test -Dtest=[test-case]
{code}

Integration test can be performed by:

{code}
mvn clean integration-test
{code}

Selective integration test can be performed by:

{code}
mvn clean integration-test -Dtest=[test-case]
{code}

To run in environment specific build use profile:

{code}
mvn clean integration-test -Pk8s
{code}

I am questioning the direction of this issue is good for Ozone continuous integration because we are creating shell script that wrap around maven commands and many unique parameters to get Ozone build going.  This usually means the pom.xml and associated configurations are not properly written in pom.xml and will lead to problems when performing maven release or maven deploy.  This is the reason that I am concerned about introducing more shell script wrappers.  

I really like to have ability to use hadoop-ozone-project, instead of -f pom.ozone.xml to build the project.  This would be Ozone inline similar to other Hadoop sub-projects.  Precommit build can be minimize build time by building from hadoop-ozone-project.  I opened HDDS-1661 for that work.  Can we avoid reference to pom.ozone.xml in the meantime?

{quote}Can you please confirm that this patch doesn't block you in any of your work? (This is a fix for the existing shell scripts which don't modify any part of the maven build just help to run certain build steps with maven){quote}

This patch will add code that uses pom.ozone.xml, which I would like to move to hadoop-ozone-project.  It does impact my daily usage of maven, and more effort to clean up.

{quote}This patch will add code that uses pom.ozone.xml, which I would like to move to hadoop-ozone-project. It does impact my daily usage of maven, and more effort to clean up.
{quote}
It's an independent question. That's the whole point to use shell scripts: In case of a project restructure it's enough to updated the shell scripts. As of _now_ we have the pom.ozone.xml so I think it's not a big problem to use it.

Do you use these shell scripts? Based on your comments you prefer to use plain maven commends. If you don't use the shell script why is it a problem to improve them? The current maven build is not affected at all....

BTW maven lifecycle model is pretty limited compared to the graph based approach of gradle. For example if you have more than two type of tests, not just integration test and unit test, it doesn't fit very well. Also in your command both unit tests AND integration tests are executed. The only thing what I did is created shortcuts to make it easier to remember which maven flags should be used. If you have better shortcuts with exactly the same behavior, feel free to post it in a patch.

{quote}Do you use these shell scripts? Based on your comments you prefer to use plain maven commends. If you don't use the shell script why is it a problem to improve them? The current maven build is not affected at all....{quote}

If the scripts are offered, then dependencies introduced on the offered scripts. It would be more difficult to remove the scripts later.

{quote}BTW maven lifecycle model is pretty limited compared to the graph based approach of gradle. For example if you have more than two type of tests, not just integration test and unit test, it doesn't fit very well. {quote}

Incorrect assumption, you can use maven profile to trigger different type of tests.  For more comprehensive test suites, it can be written as sub-modules.
This avoid elementary mistakes like:

# adding test artifacts into distribution binaries. e.g. include smoke test in distribution tarball.
# doing integration test prior to package making.

Maven build life cycle is a production tested process.  It would be foolish to ignore the wisdom in the productive workflow.  Gradle provides simplified life cycle, and it can be powerful to put in the hand of seasoned developer.  However, it doesnot help inexperienced developer to sequence the build workflow.  In the end, gradle project may become just as messy as Ant project, which maven solved to make the build life cycle more process oriented.

{quote}If the scripts are offered, then dependencies introduced on the offered scripts. It would be more difficult to remove the scripts later.
{quote}
Fix me if I am wrong, but It doesn't seem to be a blocking problem.

If I understood well, you don't agree to use an existing committed/approved feature (pom.ozone.xml introduced by HDDS-1115), because you have an idea/plan to replace it with something else, sometime in the future.

{quote}If I understood well, you don't agree to use an existing committed/approved feature (pom.ozone.xml introduced by HDDS-1115), because you have an idea/plan to replace it with something else, sometime in the future.{quote}

Hadoop already provide it's own pre-commit build validation suites (yetus).  The optimization of building speed should not introduce a complete fork on how CI is done for Hadoop sub-project.  This is not a plan to replace it with something else, rather ask nicely to follow existing methodology.  This helps us to catch things like javadoc issues, checkstyle white space rules that are more aligned with Hadoop parent project conventions.

{quote}bq. Incorrect assumption, you can use maven profile to trigger different type of tests. For more comprehensive test suites, it can be written as sub-modules.
This avoid elementary mistakes like:

  1. adding test artifacts into distribution binaries. e.g. include smoke test in distribution tarball.
{quote}
We *absolutely* want these tests in the distribution tarball. We want every person to validate Ozone works before voting as well have the ability to verify the system they are deploying using the tests. Please understand that Ozone is a product under development. Many of the hadoop practices like not shipping tests only hamper ozone. 

{quote}Hadoop already provide it's own pre-commit build validation suites (yetus)
{quote}
Can you point me to an instruction – or documentation that explain how to even setup Yetus on my Mac ? Ozone avoids all these costly mistakes that make Hadoop very hard to use. So please don't hamper our efforts by insisting that we need to go back to Hadoop tool chain if we have a better experience in place.

Also, large parts of our tests run under K8s. Maven and Java-based infra are not very easy to support – but simple scripts work very well. So while Yetus is great for CI of Hadoop, we do need complimentary bash scripts. That is an uber statement; not specific to this JIRA, just an explanation of why we need to do this.

{quote}Hadoop already provide it's own pre-commit build validation suites (yetus). The optimization of building speed should not introduce a complete fork on how CI is done for Hadoop sub-project.
{quote}
If you see any fork in the patch, please let me know.
 # Did this patch modify anything related to maven lifecycle? Nope.
 # Did this patch modify anything related to Yetus? Nope.

We provided shortcuts to make it easier to execute some maven commands. Nothing more. And this is already done.

In this patch, I suggested to _improve_ the shortcuts (and cut one shortcut to separated unit + integration tests)

If you see any risk in making safer the *existing* scripts, please let me know (with the the definition how can I reproduce the problems, please).

 

[~anu] {quote}Can you point me to an instruction – or documentation that explain how to even setup Yetus on my Mac ? Ozone avoids all these costly mistakes that make Hadoop very hard to use. So please don't hamper our efforts by insisting that we need to go back to Hadoop tool chain if we have a better experience in place.{quote}

This is documented in https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute

Look for the section, Testing your patch.  This provides the same testing procedure as running patch testing with Yetus in Jenkins.

[~elek] {quote}If you see any risk in making safer the existing scripts, please let me know (with the the definition how can I reproduce the problems, please).{quote}

Ozone maven javadoc has been failing since March.  The percommit build test triggered acceptance test does not use the same javadoc rules as Hadoop.  More javadoc errors are getting generated without incentive to clean up.  Checkstyle, findbugs, rat plugins, shellcheck, Dockerfile checks are all using default or relaxed rules sets.  This creates more hidden problems in the code base.  Technical debts are building up.  This is my reasoning to advice against using one off shell script to run maven plugin goal without configuring the plugin configurations.

{quote}Look for the section, Testing your patch
{quote}
It tells how to run Hadoop on my Mac. My question was:
{quote}Can you point me to an instruction – or documentation that explain how to even setup Yetus on my Mac
{quote}
Completely different question.

 
{quote}Checkstyle, findbugs, rat plugins, shellcheck, Dockerfile checks are all using default or relaxed rules sets
{quote}
You are making assertions without data to back you up. I will personally assert what you are saying is completely wrong. To the point that I think you have no idea what you are talking about.
 # Let us do some tests – Take checkstyle – run it over Hadoop HDFS and Hadoop Common code base and post the result here.
 # Now run the same over Ozone and HDDS directory; post the result here.

I am sure you will not; since It will just expose your false claim.

 

Now let us go to findbugs, Do the same – run the findbugs – Then be a good citizen in the Hadoop world and fix the issues. I will throw my name in the gauntlet, if *you* can fix 10% of find bugs issues in HDFS/Hadoop Common code base, I will *personally* fix all find bugs issues in Ozone / HDDS code base.

 

Please do not make assertions without knowing what you are talking about.

 

Before we argue your *opinion* vs my *opinion*; let us measure, I really dare you to follow up on this; if you cannot, please hold you peace forever and stop trolling us.

 

 

 

 

[~anu] {quote}Completely different question.{quote}

The focus is to test code hygiene.  Please don't quote out of context to assert Yetus is only way to test patch.

{quote}Now let us go to findbugs, Do the same – run the findbugs – Then be a good citizen in the Hadoop world and fix the issues. {quote}

I ran this command to search for findbugs supression from hadoop source code top level:

{code}
[eyang@localhost hadoop]$ grep -R edu.umd.cs.findbugs.annotations.SuppressFBWarnings *|grep -v "Binary"
hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/AbstractFuture.java:import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeUsage.java:import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java:import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMPolicyProvider.java:import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateMap.java:import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java:import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMPolicyProvider.java:import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneClientAdapterFactory.java:import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/BucketEndpoint.java:import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/RandomKeyGenerator.java:import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
{code}

Findbug supression is only used once in HDFS, and none in other projects.  It's a growing occurrence in Ozone project.  There is one instance in HDFS because HDDS-1103 is another gift from Ozone developer.

Hadoop may not have clean findbugs report, but it is transparent.  Anyone can generate the report and fix problems.  It is a community effort to keep it clean.  One person might not be enough effort to keep the code clean.  Ozone code base tend to suppress warnings without address some of the code issues in the proper way.  I disagree with the approach to address problems, and the rest is up to you.

Exactly what I predicted. You chickened out. So here is the answer from the Ozone side.

I just ran this analysis on HDDS. I have 3 find bug issues, and 2 of them are experimental in Findbugs itself. I am using the latest find bugs. 

Obviously, you don't dare to do it on Hadoop or Common since your lies will be exposed.

Since you so blatantly bad mouth other people and complained about some specific lines in code I am thinking that you did not look at any of the Suppress annotations either. I did;

 As for the suppressFBWarning, Each one of them was written by a developer making a conscious decision; not by someone running a grep. if you think a specific case is wrong; you should tag that as a Jira, oh – but that requires work :).

Plus, I hope you know that we have 22 findbugsExclude files in Hadoop, which is another way of writing these suppressions. Full Disclosure, 8 of them are  from Ozone, something which you missed earlier.

I will upload my screenshot; if you want to continue this discussion produce the measurements on your side; otherwise I am going to presume that we have nothing material to discuss any further.

 

Uploaded the screenshot; since I did measure the stuff. I am walking the talk, would you like to do that? 

 

{quote}Exactly what I predicted. You chickened out. So here is the answer from the Ozone side.{quote}

I am already fixing Hadoop bugs and check style issues daily.  Your ask is going to take a while, and you should know that it takes more than one individual to build Hadoop.  I know that I am contributing my part to make it better on a daily basis.  I only expect you to do the same without resorting to SuppressFBWarnings to make it worse. 

{code:java}
./hadoop-common-project/hadoop-auth/dev-support/findbugsExcludeFile.xml
./hadoop-common-project/hadoop-common/dev-support/findbugsExcludeFile.xml
./hadoop-common-project/hadoop-nfs/dev-support/findbugsExcludeFile.xml
./hadoop-common-project/hadoop-minikdc/dev-support/findbugsExcludeFile.xml
./hadoop-common-project/hadoop-kms/dev-support/findbugsExcludeFile.xml
./hadoop-hdfs-project/hadoop-hdfs-httpfs/dev-support/findbugsExcludeFile.xml
./hadoop-hdfs-project/hadoop-hdfs-client/dev-support/findbugsExcludeFile.xml
./hadoop-hdfs-project/hadoop-hdfs-nfs/dev-support/findbugsExcludeFile.xml
./hadoop-hdfs-project/hadoop-hdfs/dev-support/findbugsExcludeFile.xml
./hadoop-hdfs-project/hadoop-hdfs-rbf/dev-support/findbugsExcludeFile.xml{code}
what are these but suppressions ? Allow me to show you some more examples:

Here is some code from one these files:
{code:java}
<Match>
    <Class name="org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder" />
    <Field name="nativeCoder" />
    <Bug pattern="UUF_UNUSED_FIELD" />
</Match>
<Match>
    <Class name="org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder" />
    <Field name="nativeCoder" />
    <Bug pattern="UUF_UNUSED_FIELD" />
</Match>
 <!-- 
   Further SaslException should be ignored during cleanup and
   original exception should be re-thrown.
 --> 
 <Match>
   <Class name="org.apache.hadoop.security.SaslRpcClient" />
   <Bug pattern="DE_MIGHT_IGNORE" />
 </Match>
 <!-- 
   Ignore Cross Scripting Vulnerabilities
 -->
 <Match>
   <Package name="~org.apache.hadoop.mapred.*" />
   <Bug code="XSS" />
 </Match>
 <Match>
   <Class name="org.apache.hadoop.mapred.taskdetails_jsp" />
   <Bug code="HRS" />
 </Match>
{code}
This is code written by someone who made a judgement call, they saw the warning message from findbugs and since they are experts on this domain made a judgement call that they know better than a tool. Unless you have evidence to prove that these lines are wrong; it would be better not to judge these developers. They contribute work, time and talent to our cause; and you are saying that grepping few lines over the code allows you to make a judgement call and deem that this is inferior code? I have shown your claim that Hadoop does not use suppressions – and the only one was contributed by [~elek]  is incorrect. The above lines prove that suppression is rampant in Hadoop code base. Much more than Ozone, if you care to look.

What I don't understand is  why have such blatant disregard and disrespect for the community members? if you find a bug, file it as JIRA, please stop making blanket statements and over-arching pronouncements on code quality. Let us be specific; if you see an issue; we welcome it; and thank you for your making our software better; the nature of software development is such that we will have issues. But let us do it with some consideration; and not say things like I grepped and found suppressions; therefore your code is bad.  if you don't understand that code; feel free to ask us; perhaps we are wrong; and you have found a bug; perhaps we have a perfectly valid reason to do it that way.

Once more; I stand by my earlier statement; unless you are willing to measure and show me numbers – I am going to presume that you have really no  issues to be solved.

Thank you for the conversation.

and I plan to commit this patch, as soon as the shell check issues are addressed. [~elek] thanks

[~elek] rat.sh runs maven command in hadoop-hdds and hadoop-ozone separately, but license errors are only checked for hadoop-ozone project.  This ignores any detection in hadoop-hdds project.  Can you confirm this is correct?


Latest patch is uploaded. ALL the shellcheck errors are fixed under hadoop-ozone/dev-support/checks.

The latest version is tested with argo based parallel test execution. Results are available here:

[https://github.com/elek/ozone-ci/tree/master/HDDS-1735]

(The feature branch also used to investigate HDDS-1764. In some of the builds the verbosity of the shell scripts are increased)

FTR: Findbugs errors are independent from this patch. The shell scripts are not introduced in patch, this patch is nothing more just a cleanup/improvement on top of the existing/accepted features.
{quote}rat.sh runs maven command in hadoop-hdds and hadoop-ozone separately, but license errors are only checked for hadoop-ozone project
{quote}
Thanks, this is also fixed in the latest patch.

Url is changed to:

[https://github.com/elek/ozone-ci/tree/master/hdds1735]

Argo build definitions available from here:

[https://github.com/elek/argo-ozone]

This is a good example how the (existing) scripts are used in a CI.

They are also used from Jenkinsfile and locally to do a quick local check before commit.

 

FAILURE: Integrated in Jenkins build Hadoop-trunk-Commit #16909 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/16909/])
HDDS-1735. Create separate unit and integration test executor (elek: rev 0bae9e8ec8b53a3b484eaa01a3fa3f177d56b3e4)
* (edit) hadoop-ozone/dev-support/checks/author.sh
* (edit) hadoop-ozone/dev-support/checks/checkstyle.sh
* (edit) hadoop-ozone/dev-support/checks/unit.sh
* (edit) hadoop-ozone/dev-support/checks/acceptance.sh
* (edit) hadoop-ozone/dev-support/checks/build.sh
* (edit) hadoop-ozone/dev-support/checks/rat.sh
* (add) hadoop-ozone/dev-support/checks/integration.sh
* (edit) hadoop-ozone/dev-support/checks/findbugs.sh


Thank you, I have committed this to the trunk. If you like I can cherry-pick this to the 0.4.1 branch. Please let me know. [~eyang] Thanks for the comments.

