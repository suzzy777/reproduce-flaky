Make sense to me. Although [~taklwu] and I have been trying to reduce the reliance on DistributedFileSystem.
I suspect we want to expose EC related APIs to Hadoop Common, eventually.

Note:
{code}
hdfs ec -setPolicy -path /hbase/data/default/usertable -policy xxxx
{code}
The command affects new files in the directory. It does not migrate existing files in the directory automatically. (we were planning to support this but got stalled)

Thanks for the input! Do you think that is a blocker for this work, or could we clean it up once the Hadoop Common API is available?
{quote}It does not migrate existing files in the directory automatically
{quote}
Yea the good thing here is this is already how all other storefile related settings work for hbase – changes to BLOOMFILTER, BLOCKSIZE, COMPRESSION, etc all require the user to follow-up with a major compaction. So I think it's ok to follow the same protocol for this new ERASURE_CODING_POLICY.

No that's fine. We're pursuing that in a separate branch HBASE-27740.
(reminder to myself: finish review&merge HBASE-27769 today)

I was under the impression that setErasureCodingPolicy requires HDFS admin user privilege. But checking the code again looks like it requires just the write privilege of that directory.


We recently added EC support in Apache Impala (check out Cloudera doc https://docs.cloudera.com/cdw-runtime/1.5.1/impala-reference/topics/impala-ec-policies.html the doc talks about Ozone EC but it works the same way for HDFS EC) IMPALA-11476 but we did not add the support for Impala to update table EC properties. It would be interesting to start thinking about giving applications more control over EC policies.

I’m trying to weigh two options: allow configuring EC at table level or family level. Thematically it makes sense at family level, and that would be nice for tables with families of different size. It’s also a lot more complicated to manage due to hbase directory structure of /hbase/data/namespace/table/region/family. Since family is the last component, we need to set the EC policy individually on every directory.

A table could have many thousands of regions. For example a table with 2 families and 100k regions would require setting the policy on 200k directories, and more as the table splits. 

On the other hand, if we only allow configuring on the table level then it’s just 1 directory to set it on (/hbase/data/namespace/table). 


[~weichiu] any concerns on the hdfs managing policies separately on that many directories?

I think simpler is always better.

In your example, configuring EC policy at family level requires 200k HDFS updates which is going to cause a big disruption to your service for sure. I also don't see the use case to configure EC policy at family level which is too fine grained.

 

Table level EC policy seems like the way to go.

Thanks for the input, that's fair. I've pushed a PR which supports just table-level EC.

Results for branch branch-3
	[build #107 on builds.a.o|https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-3/107/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-3/107/General_20Nightly_20Build_20Report/]




(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-3/107/JDK8_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(x) {color:red}-1 jdk11 hadoop3 checks{color}
-- For more information [see jdk11 report|https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-3/107/JDK11_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


Results for branch master
	[build #968 on builds.a.o|https://ci-hbase.apache.org/job/HBase%20Nightly/job/master/968/]: (/) *{color:green}+1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://ci-hbase.apache.org/job/HBase%20Nightly/job/master/968/General_20Nightly_20Build_20Report/]




(/) {color:green}+1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://ci-hbase.apache.org/job/HBase%20Nightly/job/master/968/JDK8_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(/) {color:green}+1 jdk11 hadoop3 checks{color}
-- For more information [see jdk11 report|https://ci-hbase.apache.org/job/HBase%20Nightly/job/master/968/JDK11_20Nightly_20Build_20Report_20_28Hadoop3_29/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}


I'm happy to take a look. Can you tell me what branch and test class you are seeing failing and maybe the exception? I see the branch-3 failure above, but I took a look at the output and it's not an undefined constant error.

I do think this might have broken the CommissioningTests because it changed the number of RS in the cluster from 1 to 3, and the test expects 1. I will look at fixing that, but want to make sure I catch what you're seeing as well. I can't reproduce an undefined constant issue.

Ah I deleted the comment thinking I am mistaken and creating noise, as it didn't fail when I checked out latest master. Possibly some change in my local which brought undefined constant error.

But later found the other test was failing since some time in flaky board and looked into it. I have made a fix for the test TestAdmin2, I still think the fix should be fine enough as the test is to just verify listing works good for decommissioning. But given the new context that RS count increased from 1 to 3,  makes me wonder why 3 servers got decommissioned if we are passing 1, see sample logs in PR. This could be some issue, will analyze more.

FYR HBASE-28275

 

 

