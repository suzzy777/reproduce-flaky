Thank for reporting and such a detailed analysis, [~gharris1727]! 

Yeah great analysis, thanks [~gharris1727] 

I'm a bit confused by point #4, however – is this a change in behavior (possibly related to KIP-848)? It's my understanding that the #onPartitionsAssigned callback is guaranteed to always be invoked regardless of whether the set of partitions being newly assigned is non-empty or not. This is in contrast with the #onPartitionsRevoked and #onPartitionsLost callbacks, which are only invoked when the set of partitions to act upon is non-empty.

I think one could argue that this inconsistency is not ideal, but the behavior of always invoking #onPartitionsAssigned is a stated guarantee in the public contract of ConsumerRebalanceListener. See [this paragraph|https://github.com/apache/kafka/blob/254335d24ab6b6d13142dcdb53fec3856c16de9e/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRebalanceListener.java#L67] of the javadocs. In other words, I don't think we can change this without a KIP, and if this behavior was modified recently then we need to revert that change until a KIP is accepted.

I just checked the current code and it looks like we do still respect the guarantee of invoking #onPartitionsAssigned in all cases. So I don't think step #4 is correct. Did you happen to see anything in the logs that would suggest the StreamThread was continuing in its regular loop and never stopping due to the rebalanceInProgress flag? Or is it possible that it's hanging somewhere in the shutdown process (or even in the rebalance itself)?

I'm just wondering if it might be related to the Producer, not the Consumer. I know we had some issues with the Producer#close hanging in the past, and that it was related to users deleting topics from under the app, which would be a similar situation to what you found here. I'm not sure if we ever fixed that, maybe [~mjsax] will remember the ticket for the Producer issue?

Found the ticket: https://issues.apache.org/jira/browse/KAFKA-9398

And yes, it's still unresolved. 

Given all the above, I think we can honestly just disable/remove the test, as the named topologies feature was never made into a real public API. I do know of a few people who are using it anyway but they're aware it was only an experimental feature and not fully supported by Streams. So imo we don't need to go out of our way to fix any flaky tests: provided we can demonstrate that the issue is specific to named topologies and not potentially an issue with Streams itself. Of course in this case it's actually the latter, but we've recognized the root cause as a known issue, so I don't think there's anything more this test can do for us besides be flaky and annoy everyone.

Thanks for digging into this! 

