As the test exists in 4.0, I tried to reproduce the failure in both 4.0 and 4.1
Managed to reproduce it when run with compression twice in 5000 times:
 - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1964/workflows/de6b4aa9-0a50-4488-b8fd-5e0da2718507/jobs/15521/tests#failed-test-0]
But I didn't manage in 10 000 runs on 4.0 - [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1966/workflows/d616e242-28f3-46d0-b345-30faf2f3ec24], [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1966/workflows/38a3ca64-97fe-4477-a05d-3aed705131b1]

Just with the regular _ant testsome_ target I didn't manage to reproduce it 5000 times on both branches. 

[~e.dimitrova] I've looked at all static inits and everything looks fine iiuc. The test can be ran locally with the {{RepeatableRunner}} but it won't repro and I didn't either by trying several things. All things considered I think this is probably some test env corner case and we can send it to RC and move it out of Beta imo. Wdyt?

I did try a 5K run with a few added asserts and test-compression and it didn't repro https://app.circleci.com/pipelines/github/bereng/cassandra/794/workflows/d643f1c8-e5b8-4e59-bfbf-63551e7018af/jobs/7228/parallel-runs/21?filterBy=ALL #justfyi

I did manage to reproduce the issue on 4.1 with Java 11 and without compression: [https://app.circleci.com/pipelines/github/adelapena/cassandra/2210/workflows/e2b62218-cfa4-4003-a946-c1823931bf64/jobs/22360]

There are two hits in 10K iterations:
{code}
.circleci/generate.sh -m \
  -e REPEATED_UTEST_TARGET=testsome \
  -e REPEATED_UTEST_COUNT=10000 \
  -e REPEATED_UTEST_CLASS=org.apache.cassandra.db.commitlog.CommitLogInitWithExceptionTest
{code}
So it seems that compression isn't the issue, and probably we will need more iterations to be sure of hitting it.

[~bereng] I didn't have the chance to work on this one, I only pushed repeated runs before I went off so we can see on which branches it fails, whether it is regression, etc. That is why I also didn't assign it as I am not actively working on it or familiar with the test.

With that said I wouldn't advocate for closing it as environmental issue without explanation just because it is hard to reproduce. We have a bunch of examples of legitimate bugs fixed from tests that were super rarely failing. But as I said, I don't know the details in this case so I will leave the decision to whoever assigns it and spends the time to figure out the details.

Whether this is a beta/rc blocker I don't know. I think it was Josh assigning version here when opening the ticket.

bq. Whether this is a beta/rc blocker I don't know.

This was one of the tickets excepted on the ML with a waiver, given that it was fixed in the next release, so blocking RC on this.

{quote}This was one of the tickets excepted on the ML with a waiver, given that it was fixed in the next release, so blocking RC on this.
{quote}
You are right as usual :) Thanks!

I did run it [today|https://app.circleci.com/pipelines/github/bereng/cassandra/796/workflows/be470790-e5bc-4fe7-97bc-bc2aaff554d1] 5K times on circle but I had decorated the test with the RepeatableRunner with 5K runs. So that is 5K*5K runs trying to assert where the null in {{CommitLog.instance.segmentManager.executor.isTerminated()}} exactly was an it didn't fail meh...

The good news are that [~adelapena] reproduced it and the logs are available in CircleCI

I caught one in [this run|https://app.circleci.com/pipelines/github/driftx/cassandra/673/workflows/6235dcf9-6a8e-4308-adfa-7b60299bb72f/jobs/7492] with [this patch|https://github.com/driftx/cassandra/commit/52bbf68c55d8eb2ea2eb8eb47ca07d96a72389c8] which indicates the executor is what is null... perhaps it's already gone.

{quote}I caught one in this run with this patch which indicates the executor is what is null... perhaps it's already gone.{quote}

Ah good this is what I was trying to get


I removed the 1000ms sleep in favor of awaiting termination on the executor [here|https://github.com/driftx/cassandra/commit/6d7b2aff20c8b3113fef72b186f2caadd39b3aeb].

||Branch||CI||
|[4.1|https://github.com/driftx/cassandra/tree/CASSANDRA-17928]|[circle 10k|https://app.circleci.com/pipelines/github/driftx/cassandra/679/workflows/45d0f69d-e99d-4f2b-b69c-af4ad7f8be18/jobs/7504]|
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-17928-trunk]|[circle 10k|https://app.circleci.com/pipelines/github/driftx/cassandra/680/workflows/51868e19-35d1-46fc-af02-9ec6f85bddd0/jobs/7506]



The patch looks good to me. I have started another pair of repeated runs for [4.1|https://app.circleci.com/pipelines/github/adelapena/cassandra/2328/workflows/50f33d9b-c7c6-4aa9-bac9-22ac78ad6b8c]  and [trunk|https://app.circleci.com/pipelines/github/adelapena/cassandra/2327/workflows/aa732ff3-304b-4f0d-ab3c-4662f0a9ebe2] since this one is hard to reproduce.

I think that [the test also exists in 4.0|https://github.com/apache/cassandra/blob/cassandra-4.0/test/unit/org/apache/cassandra/db/commitlog/CommitLogInitWithExceptionTest.java#L84-L93], although in that branch the thread is not wrapped. Should we port the fix, or the test doesn't fail in that branch?

I don't think it fails on 4.0, but we can see when it finishes [here|https://app.circleci.com/pipelines/github/driftx/cassandra/682/workflows/69d0d7ac-52bc-4f81-b7cd-fdfe7a75c2b7].

It seems that the run for 4.1 has hit a NPE: https://app.circleci.com/pipelines/github/adelapena/cassandra/2328/workflows/50f33d9b-c7c6-4aa9-bac9-22ac78ad6b8c/jobs/23224

The run for 4.0 also hits a failure, but this time not a NPE but an assertion error, probably due to a slow run: https://app.circleci.com/pipelines/github/driftx/cassandra/682/workflows/69d0d7ac-52bc-4f81-b7cd-fdfe7a75c2b7/jobs/7508/tests

bq. The run for 4.0 also hits a failure, but this time not a NPE but an assertion error, probably due to a slow run

Incidentally the await patch should solve this, but we're still left with the mysterious NPE in 4.1.

That might explain why I hit it easier in 4.1 than 4.0 :( 

Yep I had tried that approach to no avail as well. I am at a loss here as following the code, even trying to follow futures execution, etc I can't see where the hole is...

Ohhhh we're accessing CommitLog init in a multi-threaded way mixing static and non-static monitors. Hence data visibility and reordering are not warrantied. I pushed a proposal to see if that theory is right [here|https://github.com/bereng/cassandra/commit/718f831e6de0b0b88802018d5112fb7d3fe2cf40] with [10K CI|https://app.circleci.com/pipelines/github/bereng/cassandra/800/workflows/5dd074e9-051a-4cd0-bf28-00d84257cb67] running.

[~adelapena] it might be my fault or me being an idiot but
- I run generate.sh and it barfs all tests. We don't seem to have a simple way to bring up the help/usage instructions
- I run it with -m and it barfs all tests
- I run {{./generate.sh -m -e REPEATED_ANT_TEST_TARGET=test -e REPEATED_ANT_TEST_CLASS=CommitLogInitWithExceptionTest -e REPEATED_ANT_TEST_COUNT=10000 -e REPEATED_TESTS_STOP_ON_FAILURE=true}} and the config went empty but for the header of comments

That command, {{{}./generate.sh -m -e REPEATED_ANT_TEST_TARGET=test -e REPEATED_ANT_TEST_CLASS=CommitLogInitWithExceptionTest -e REPEATED_ANT_TEST_COUNT=10000 -e REPEATED_TESTS_STOP_ON_FAILURE=true{}}}, works for me.

Did you had any previous changes in {{config-2_1.yml}} that could have confused the script? What's is the output of that command? What version on {{circleci}} CLI tool are you using?

By the way, you can also use:
{code:java}
./generate.sh -m \
  -e REPEATED_UTESTS=org.apache.cassandra.db.commitlog.CommitLogInitWithExceptionTest \
  -e REPEATED_UTESTS_COUNT=10000 \
  -e REPEATED_TESTS_STOP_ON_FAILURE=true
{code}
That should create separate jobs for running the test with default config, compression, tries and system keyspace directories.

I'll try update the circleci when I have a gap...

{quote}I run generate.sh and it barfs all tests. We don't seem to have a simple way to bring up the help/usage instructions
{quote}
That's a bug, calling the script without options should print the help and exit without doing anything else. I'll open a ticket to fix it.

Opened CASSANDRA-17995 for fixing the issue when calling without options.

Well [this|https://github.com/apache/cassandra/compare/trunk...bereng:cassandra:CASSANDRA-17928-4.1] passes [10K|https://app.circleci.com/pipelines/github/bereng/cassandra/801/workflows/67a01d5c-5000-485a-9499-30565122afa1]. If we add the previous 10K runs that is 20K runs without a NPE.

The problem is that the static init on exit and the Thread.join should both warranty a happens-before with a memory barrier that should make all properly visible across threads. The only exception would be another class loader triggering the static init a second time. So I still don't understand who is shuffling things under our feet for the executor to be null.

I think this is still going to fail due to the awaitTermination call, where mine NPE'd ultimately in that same spot, but we shouldn't need that or the sleep if we spin wait.
Like [this|https://github.com/driftx/cassandra/commit/2d8c09678cb1f18c8103cabce1c07073dd435980] which is running 30k [here|https://app.circleci.com/pipelines/github/driftx/cassandra/684/workflows/7de84fef-0c04-49ee-9239-bdf88ddefd47/jobs/7510].

The spin wait timed out on 8 of those runs. :(

The spin wait timing out makes no sense as the exception should have terminated it. But neither does the NPE. Hence my suspicion sbd is shuffling things under our feet. Ofc I just added some debug and now everything passes :facepalm:

bq. Incidentally the await patch should solve this

Let's prove this out to make sure we understand everything on the 4.0 side.  We don't have the await method there but we can spin wait instead, which I did [here|https://github.com/driftx/cassandra/commit/cd2b04386fe4f22c0aff8ee32766df0b3200cb99].  Indeed, this passed [30k runs|https://app.circleci.com/pipelines/github/driftx/cassandra/686/workflows/3bc04364-b829-48ca-9e33-246899546db2/jobs/7514], where the equivalent in 4.1 previously did not.  The only commit against the test since 4.0 is CASSANDRA-16925, which is in line with the nature of the problem.

Some archaeology shows that:
 * The test consistently passed right before CASSANDRA-16924, at commit [2e2db4dc40c4935305b9a2d5d271580e96dabe42|https://github.com/apache/cassandra/commit/2e2db4dc40c4935305b9a2d5d271580e96dabe42]. CI [here|https://app.circleci.com/pipelines/github/adelapena/cassandra/2388/workflows/3f5e6b23-9e7c-46fc-bc15-88492e06bd02].
 * The test started to fail with a timeout at the call to {{initThread.join()}} after CASSANDRA-16924. However, the NPE wasn't hit back then. CI [here|https://app.circleci.com/pipelines/github/adelapena/cassandra/2387/workflows/6a003922-6bdc-4591-8e36-244982511b89].
 * The NPE started to appear with CASSANDRA-16925, together with the same timeout in {{{}initThread.join(){}}}. That was committed immediately after CASSANDRA-16924. CI [here|https://app.circleci.com/pipelines/github/adelapena/cassandra/2389/workflows/7a5fa11b-a79b-428b-a8d0-5e5439c5b979].

So it seems that both CASSANDRA-16924 and CASSANDRA-16925 broke the test, and the NPE comes from the latter.

Maybe [~benedict] or [~samt] can give some insights on this.

It looks like the problem is that the {{CommitLogSegmentManager}} assigns its {{executor}} variable after {{InfiniteLoopExecutor}} starts its thread, which may therefore process the exception immediately (i.e. before the executor is made visible via {{executor}}). Since the custom JVMKiller kills the thread that is starting the commit log, and this causes abnormal termination of the thread, this can prevent the field from being assigned entirely.

Why is this thread being killed here, though? This is not semantically equivalent to any execution that is possible in a normal system.

Who authored the test, perhaps their insight can be sought as to what the intention was?

This test was added in CASSANDRA-15295.

So it is not expressly discussed in that ticket, but AFAICT there is no reason to invoke {{Thread.stop}} in the JVMKiller. Since the real JVMKiller does not invoke Thread.stop this cannot be a critical part of the test. So a simple condition that is signalled by the JVMKiller, that the test waits on for some predetermined period (perhaps 10s) to confirm the JVMKiller has been invoked seems sufficient.

However, it might be nice to confirm with the original authors [~gzh1992n], [~djoshi], [~jwest]

[~benedict] unfortunately its been long enough I've lost the context. Giving it a quick read it seems like the goal is the clean up a resource the test created (I suppose it could also do this by marking the thread as a daemon instead)

bq. So a simple condition that is signalled by the JVMKiller, that the test waits on for some predetermined period (perhaps 10s) to confirm the JVMKiller has been invoked seems sufficient.

About like [this?|https://github.com/driftx/cassandra/commit/6b448fa9ccef58d9116d39a48444afb37594fa20]  Circle [passes 30k|https://app.circleci.com/pipelines/github/driftx/cassandra/692/workflows/28d800ad-9f19-4853-a8d8-88c2c20dbb6e/jobs/7522] iterations.

^Mmmm I like it

I ported this to 4.0 as well.  CI is still running.

||Branch||CI||
|[4.0|https://github.com/driftx/cassandra/tree/CASSANDRA-17928-4.0]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/696/workflows/97f32020-fc41-45ce-b8d4-a0e4cdfed84d], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/696/workflows/0b4f6404-3cb0-41ef-878b-0def1b845d2f]|
|[4.1|https://github.com/driftx/cassandra/tree/CASSANDRA-17928]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/701/workflows/a1093b26-9c80-455a-871c-da9493642571], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/692/workflows/7029bce2-ed30-4b93-831e-1efd831fc00f]|
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-17928-trunk]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/698/workflows/a65d9619-106f-449d-8cd3-1e4b9eba423e], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/697/workflows/13882701-c4ac-48a1-bff7-27dc7d9c58f8]|


The workaround looks good, +1 if it survives the run.

Nit: {{initThread}} can be a local variable now that is not used in {{setUp}} anymore.

Tip: For trunk or any other additional run, if you rebase the branches you can benefit from CASSANDRA-18000 to get faster repeated runs.

[~brandon.williams] sthg went south in circle bc we have 30K runs on 4.0 where it was already passing. The 4.1 run only has 500 repeats which is not enough and trunk has no repeats at all. I would suggest rebasing for CASSANDRA-18000 as well and do 30K repeats for 4.1 and trunk. Did I miss anything?

I've rebased the patches and started new runs:
||Branch||CI||
|[4.1 |https://github.com/adelapena/cassandra/tree/17928-4.1-review]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/2478/workflows/b1f45dd7-6f5b-45f5-806c-8d1e3ed907ae] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/2478/workflows/64e494ba-c496-40f1-9cf5-cf3e54869b7f]|
|[trunk|https://github.com/adelapena/cassandra/tree/17928-trunk-review]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/2479/workflows/1876a5e6-5d4b-4f63-b8db-37abe426c9b6] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/2479/workflows/e2701326-7fe8-4929-85b0-ccc84e35b0af]|

The config is generated with:
{code:java}
.circleci/generate.sh -m -e REPEATED_UTESTS_COUNT=30000
{code}
I have also taken the liberty of increasing the spin timeout to 30s instead of 10s, since I am afraid that with so many iterations we could hit an unusually long execution.

Looks like we hit that unusual long execution even at 30s

Trying with a higher timeout [here|https://app.circleci.com/pipelines/github/adelapena/cassandra/2484/workflows/7db9a37e-9866-4d6b-9445-3c5e1e751047].

That passed, should we go ahead with that timeout?

Yes, I think we can go ahead with the 2min timeout.

I've also run j11 and trunk with the increased timeout and all of them pass:
||Branch||CI||
|[4.1 |https://github.com/adelapena/cassandra/tree/17928-4.1-review]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/2484/workflows/7db9a37e-9866-4d6b-9445-3c5e1e751047] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/2484/workflows/4cc51390-8680-4638-a27c-81feb36b73e7]|
|[trunk|https://github.com/adelapena/cassandra/tree/17928-trunk-review]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/2486/workflows/3796fc67-ecfd-4e06-90cc-0751768cec25] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/2486/workflows/77e044a2-dde3-47ef-ac69-a36c4aca2153]|

30K iterations per config and branch is a lot of iterations. That increases our chances of hitting both timeouts and races. I guess that seeing that the test doesn't fail anymore when we increase the max wait time discards the possibility of an unknown race. I suspect that most tests using {{spinAssertEquals}} with a shorter timeout would also fail if we run them with such a high number of iterations.

bq. 30K iterations per config and branch is a lot of iterations

bq. I suspect that most tests using spinAssertEquals with a shorter timeout would also fail if we run them with such a high number of iterations.

I suspect so too, but that may just be the point where you are statistically bound to get bitten by an environmental problem.

Committed with the timeout changed to 120s.



