[~hanishakoneru], I have attached acceptance.zip including OM logs ({{hadoop-hadoop-om-\*}}), obtained from [PR run|https://github.com/apache/hadoop-ozone/pull/747/checks?check_run_id=552073925] for HDDS-3311.  It reproduced "failure 2", ie. _Restart OM and Verify Ratis Logs_ timeout.  Can you please check?

ozone-om-ha-s3, also fails for me with timeout.

_failure 1_ scenario:

{code:title=Stop Leader OM and Verify Failover}
06:53:20.688	INFO	Running command 'ozone sh volume create o3://omservice/volume1 2>&1'.	
06:53:29.143	INFO	${rc} = 255	
06:53:29.144	INFO	${output} = Couldn't create RpcClient protocol
{code}

I think this can happen if OM leader is not elected yet even during retry interval:

{code:title=log excerpt from each OM}
2020-04-02 06:53:29 INFO  Server:2726 - IPC Server handler 2 on 9862, call Call#0 Retry#5 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 172.18.0.3:50872
org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om3 is not the leader. Could not determine the leader node.
2020-04-02 06:53:29 INFO  Server:2726 - IPC Server handler 68 on 9862, call Call#0 Retry#6 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 172.18.0.3:34354
org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Could not determine the leader node.
2020-04-02 06:53:29 INFO  Server:2726 - IPC Server handler 2 on 9862, call Call#0 Retry#7 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 172.18.0.3:54958
org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om2 is not the leader. Could not determine the leader node.
{code}

Later _Test Multiple Failovers_ fails due to {{copyFromLocal: Volume volume1 is not found}}.

https://github.com/adoroszlai/hadoop-ozone/runs/554140106

I have no idea what's happening in _failure 2_ case:

{code:title=Restart OM and Verify Ratis Logs}
09:42:56.649	DEBUG	Test timeout 8 minutes active. 474.324 seconds left.	
09:42:56.651	INFO	Running command 'ozone sh key put o3://omservice/volume1/bucket1/testOMRestart_0_0 NOTICE.txt 2>&1'.	
09:50:50.974	FAIL	Test timeout 8 minutes exceeded.
{code}

{code:title=log excerpt from leader OM}
2020-04-02 09:42:52 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om3: nextIndex: updateUnconditionally 59 -> 1
2020-04-02 09:42:52 WARN  GrpcLogAppender:122 - om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-04-02 09:42:52 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 59 -> 58
2020-04-02 09:42:53 WARN  GrpcLogAppender:122 - om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-04-02 09:42:53 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 59 -> 58
2020-04-02 09:42:53 WARN  GrpcLogAppender:122 - om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-04-02 09:42:53 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 59 -> 58
2020-04-02 09:42:53 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om3: nextIndex: updateUnconditionally 59 -> 51
2020-04-02 09:42:54 WARN  GrpcLogAppender:122 - om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-04-02 09:42:54 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 59 -> 58
2020-04-02 09:42:54 WARN  GrpcLogAppender:122 - om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-04-02 09:42:54 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 59 -> 58
2020-04-02 09:42:55 WARN  GrpcLogAppender:122 - om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-04-02 09:42:55 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 59 -> 58
2020-04-02 09:42:55 WARN  GrpcLogAppender:122 - om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-04-02 09:42:55 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 59 -> 58
2020-04-02 09:42:56 WARN  GrpcLogAppender:122 - om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-04-02 09:42:56 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 59 -> 58
2020-04-02 09:42:56 WARN  GrpcLogAppender:122 - om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-04-02 09:42:56 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 59 -> 58
2020-04-02 09:42:56 WARN  GrpcLogAppender:212 - om1@group-D66704EFC61C->om3-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=71,entriesCount=8,lastEntry=(t:14, i:58)
2020-04-02 09:42:57 WARN  GrpcLogAppender:122 - om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-04-02 09:42:57 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 58 -> 57
2020-04-02 09:42:57 WARN  GrpcLogAppender:122 - om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2020-04-02 09:42:57 INFO  FollowerInfo:50 - om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 59 -> 58
{code}

https://github.com/adoroszlai/hadoop-ozone/runs/554504875

I am managing the 1.1.0 release and we currently have more than 600 issues targeted for 1.1.0. I am moving the target field to 1.2.0. 

If you are actively working on this jira and believe this should be targeted to 1.1.0 release, Please change the target field back to 1.1.0 before Feb 05, 2021. 

I am managing the 1.2.0 release and we currently have more than 600 issues targeted for 1.2.0. I am moving the target field to 1.3.0.

If you are actively working on this jira and believe this should be targeted for the 1.2.0 release, Please reach out to me via Apache email or Slack.

Ozone 1.3.0 had been released and we currently have more than 600 open issues targeted for 1.3.0. I am moving the target field to 1.4.0.

If there is anything needs to be discussed about the Target Version, Please reach out to me via Apache email or Slack.

