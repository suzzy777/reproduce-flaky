[~tedyu] Do you have the patch?

Looking at flaky test dashboard, TestAcidGuarantees has been hanging.

Addition of coverage should be done after making TestAcidGuarantees stable.

It seems {{TestAcidGuarantees}} exceeds the given timeout 900s. We should separate the {{TestAcidGuarantees}} by the policy. In my opinion, it can be addressed in this issue. [~tedyu] Iâ€™d like to deal with this issue if you don't work at it yet. 

chatted with [~tedyu] offline. He advised that we should find out why {{TestAcidGuarantees}} slows down. I run the {{TestAcidGuarantees}} with different commit one by one. The reason is that the HBASE-19200 make {{ConnectionImplementation}} use {{ZKAsyncRegistry}} which have to create the curator  and each thread in {{TestAcidGuarantees}} has its own {{ConnectionImplementation}}, hence {{TestAcidGuarantees}} takes some time to initialize all threads for each test case.

[~tedyu] any suggestions?




[~Apache9]:
Can you comment ?

Let me take a look first.

ConnectionImplementation needs to fetch the cluster id from zk even without HBASE-19200 so a zk connection is always necessary. So this means that curator initialization is much slower than creating a ZooKeeper instance directly? [~mdrob] is this expected sir?

And if connecting zk is the bottleneck, I think we can use the 'hbase.client.registry.impl ' to inject a special implementation to make the test run faster.

Thanks.

bq. inject a special implementation to make the test run faster.

We need to consider real life client behavior - not just in test environment.

What is the running time before HBASE-19200? It seems that we only have 5 read/writers for each test, not a big number.

Anyway I do not think it is necessary to start/stop mini cluster every time. We have 6 * 3 = 18 tests, and with one more parameter it will be 24, start/stop mini cluster will spend more than 10 mins...

I rewrite TestAcidGuarantees to only start mini cluster once and now it runs faster, and I think we should promote it to LargeTests. It does not make sense for a medium tests to run 10 mins or more. Will open a issue to upload the patch.

And another problem is about curator. I can see that we spend more than 1 second to retrieve the cluster id, too long I think. Will open a issue for this.

Thanks.

Please work on shortening the runtime of cluster id retrieval first (so that TestAcidGuarantees no longer times out).

Please see HBASE-19311, the time cost is more for starting mini cluster every time. Thanks.

And I'm still trying to find out the actual problem. In TestZKAsyncRegistry the getClusterId runs pretty fast...

I guess there is a retry in the background, the retry interval for curator is 1 second. Let me see if we can get the retry log.

QA would not run known flaky tests, including TestAcidGuarantees.
How about reverting HBASE-19200 first ?
This way, HBASE-19311 can run thru QA bot and you have enough time to find out what (from HBASE-19200) caused the test to run slower.

You can do what you like, i do not care. I will do my job. Thanks.

bq. What is the running time before HBASE-19200?
before: 655.004s
HBASE-19200: 865.749s

bq. I can see that we spend more than 1 second to retrieve the cluster id, too long I think. Will open a issue for this.
The cost i observed is same with you.:) I don't think the 1 second will kill us, and we don't have to revert HBASE-19200. The root cause is that {{TestAcidGuarantees}} is a parameterized test, so any small cost will make the executed time become even larger.

There are many solutions about speeding up {{TestAcidGuarantees}}.
# start mini cluster once (HBASE-19311)
# separate {{TestAcidGuarantees}}

#1 will be addressed by [~Apache9], and it is a good solution as the initialization of mini cluster is slower than any phases. #2 is necessary also I think as it not only speed up the {{TestAcidGuarantees}} but also help us to debug for different MemStore in the future.

[~tedyu] [~Apache9] Any suggestions about #2? 

+1 on #2. TestAcidGuarantees is already slow enough even without different memstore compaction policies...

[~chia7712]:
What's your plan for splitting TestAcidGuarantees ?

Should one subclass test 2 policies and the other subclass test two other policies ?

bq. Should one subclass test 2 policies and the other subclass test two other policies ?
Given that we may encounter the acid issue on different memory policy in the future, it will be better that each policy will be ran in different test class.
# NONE: TestAcidGuarantees
# BASIC: TestAcidGuaranteesWithBasicPolicy
# EAGER: TestAcidGuaranteesWithEagerPolicy
# ADAPTIVE: TestAcidGuaranteesWithAdaptivePolicy

Splitting TestAcidGuarantees also enable us to add more testing for compacting memstore. e.g {{IndexType}}


w.r.t. TestAcidGuaranteesWithBasicPolicy, the default policy is MemoryCompactionPolicy.BASIC.

Along your line of thinking, each TestAcidGuarantees should continue to be medium test.

Do you want to work out a patch ?

bq. w.r.t. TestAcidGuaranteesWithBasicPolicy, the default policy is MemoryCompactionPolicy.BASIC.
Do you mean the class name should be TestAcidGuaranteesWithDefaultPolicy?
bq. each TestAcidGuarantees should continue to be medium test.
yep
bq. Do you want to work out a patch ?
sure

TestAcidGuarantees can be named TestAcidGuaranteesWithNoInMemCompaction.

This way, people wouldn't mistakenly think that it uses default policy.

bq. TestAcidGuarantees can be named TestAcidGuaranteesWithNoInMemCompaction.
will roger that.

[~ted_yu] Do not piss off/abuse our best contributors. They do not need your direction.

[~eshcar] / [~anastas] Ted is filing issues for you it seems. Duo & Chia-ping are doing work to mitigate the extra burden introduced by the parameterization of TestAcidGuarantees. Ye might have input. Thanks.

bq. Do not piss off

I am just trying to see green Jenkins build. In the process, certain commits were identified as cause for test failure. The committer was not paying attention to the failing test(s).

I don't think I did anything wrong by alerting other committers of the failing tests which were broken by their commits.

I ran the test with adaptive policy where test output was filled with:
{code}
2017-11-22 16:52:39,731 DEBUG [RpcServer.default.FPBQ.Fifo.handler=0,queue=0,port=36600] ipc.CallRunner(141): callId: 327 service: ClientService methodName: Mutate size: 4.7 K connection: 172.18.128.12:54106 deadline: 1511369619718
org.apache.hadoop.hbase.RegionTooBusyException: Above memstore limit, regionName=TestAcidGuarantees,,1511369541883.abc7639b8ca2a5835888c2c62e803f9d., server=cn012.l42scl.hortonworks.com,36600,1511369038630, memstoreSize=528990, blockingMemStoreSize=524288
  at org.apache.hadoop.hbase.regionserver.HRegion.checkResources(HRegion.java:4167)
  at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:2921)
  at org.apache.hadoop.hbase.regionserver.RSRpcServices.mutate(RSRpcServices.java:2700)
  at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:41546)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:406)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:325)
{code}
Agree to disable eager and adaptive for now.

This also appeared in test output for adaptive policy:
{code}
2017-11-22 16:52:46,671 ERROR [MemStoreFlusher.1] regionserver.HRegion(1210): Asked to modify this region's (TestAcidGuarantees,,1511369541883.abc7639b8ca2a5835888c2c62e803f9d.) memstoreSize to a negative value which is incorrect. Current memstoreSize=384720, delta=-398460
java.lang.Exception
  at org.apache.hadoop.hbase.regionserver.HRegion.checkNegativeMemStoreDataSize(HRegion.java:1210)
  at org.apache.hadoop.hbase.regionserver.HRegion.decrMemStoreSize(HRegion.java:1203)
  at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2639)
  at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2355)
  at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2327)
  at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:2218)
  at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:498)
  at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:467)
{code}

{code}
+//TODO: HBASE-19266 disables this test as the eager policy causes the negative size of MemStore
+@Ignore
+public class TestAcidGuaranteesWithAdaptivePolicy extends TestAcidGuaranteesWithNoInMemCompaction {
{code}
The comment talks about eager policy while the test uses adaptive policy. Please adjust.
{code}
+  protected MemoryCompactionPolicy getPolicy() {
{code}
Is it possible another type of policy will be added to the test in the future ?
I don't think Java allows two methods with same name and parameters but different return values.
How about naming the method getMemoryCompactionPolicy() ?
{code}
+@Category({ FlakeyTests.class, MediumTests.class })
+public class TestAcidGuaranteesWithBasicPolicy extends TestAcidGuaranteesWithNoInMemCompaction {
{code}
I don't think the above test is flaky. Since basic policy is the default, we cannot afford this test to be flaky.
{code}
+@Category({ FlakeyTests.class, MediumTests.class })
+public class TestAcidGuaranteesWithNoInMemCompaction {
{code}
I don't think the above test is flaky either.

[~tedyu] Thanks for the log and reviews. Will address your comment in next patch.

bq. Since basic policy is the default, we cannot afford this test to be flaky.
I can't agree with you more. :)

bq. I don't think I did anything wrong by alerting other committers of the failing tests which were broken by their commits.

Alerting is not the issue. Its your innate talent for pissing people off w/ your tone and managerial attitude in an open source project made of volunteers. You've been informed of this multiple times by multiple characters but it does not register it seems. Above you are insisting that you have license to carry on w/ your tone-deaf behaviors though we see repercussions above and elsewhere in currently ongoing issues. Suggestion is to amend your behavior and how you address your cohort.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  9s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Findbugs executables are not available. {color} |
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 6 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 31s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 40s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  3s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  5m 52s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 27s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 56s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 54m 33s{color} | {color:green} Patch does not cause any errors with Hadoop 2.6.1 2.6.2 2.6.3 2.6.4 2.6.5 2.7.1 2.7.2 2.7.3 2.7.4 or 3.0.0-alpha4. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 94m 27s{color} | {color:red} hbase-server in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 21s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}167m 58s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:eee3b01 |
| JIRA Issue | HBASE-19266 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12898894/HBASE-19266.v0.patch |
| Optional Tests |  asflicense  javac  javadoc  unit  findbugs  shadedjars  hadoopcheck  hbaseanti  checkstyle  compile  |
| uname | Linux d1c0e0db3f85 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/component/dev-support/hbase-personality.sh |
| git revision | master / 194efe3e5a |
| maven | version: Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T07:58:13Z) |
| Default Java | 1.8.0_151 |
| unit | https://builds.apache.org/job/PreCommit-HBASE-Build/9976/artifact/patchprocess/patch-unit-hbase-server.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/9976/testReport/ |
| modules | C: hbase-server U: hbase-server |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/9976/console |
| Powered by | Apache Yetus 0.6.0   http://yetus.apache.org |


This message was automatically generated.



TestAcidGuaranteesWithBasicPoli didn't finish in the QA run.
This might be due to test environment.

w.r.t. 'memstoreSize to a negative value' error, the first occurrence is in TestAcidGuarantees#testMixedAtomicity

However, if I run the subtest alone, it passes with EAGER policy.

Note: the error is only a log:
{code}
      LOG.error("Asked to modify this region's (" + this.toString()
          + ") memstoreSize to a negative value which is incorrect. Current memstoreSize="
          + (memstoreDataSize - delta) + ", delta=" + delta, new Exception());
{code}
The above occurred multiple times during a run with EAGER policy. It would not cause the region server to shutdown.


Stepping back to the commit which added EAGER to TestAcidGuarantees,

HBASE-17575: Run critical tests with each of the Inmemory Compaction Policies enabled

'memstoreSize to a negative value' error can be observed in test output.

bq. The above occurred multiple times during a run with EAGER policy. It would not cause the region server to shutdown.
The error I mentioned is shown below.
{quote}
2017-11-22 17:24:53,493 FATAL [RS_CLOSE_REGION-asf911:51862-0] regionserver.HRegionServer(2376): ***** ABORTING region server asf911.gq1.ygridcore.net,51862,1511371099686: Assertion failed while closing store TestAcidGuarantees,,1511371469395.68d2d57885546cb47faa7361cbbbcca7. A. flushableSize expected=0, actual= dataSize=2290 , heapSize=7008. Current memstoreSize=-34350. Maybe a coprocessor operation failed and left the memstore in a partially updated state. *****
2017-11-22 17:24:53,493 FATAL [RS_CLOSE_REGION-asf911:51862-0] regionserver.HRegionServer(2382): RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]
2017-11-22 17:24:53,557 INFO  [RS_CLOSE_REGION-asf911:51862-0] regionserver.HRegionServer(2386): Dump of metrics as JSON on abort: {
{quote}
The detail log is [here|https://builds.apache.org/job/HBASE-Flaky-Tests/23491/artifact/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.TestAcidGuarantees-output.txt/*view*/]. The code about aborting the rs is shown below.
{code:title=HRegion.class}
        for (HStore store : stores.values()) {
          MemStoreSize flushableSize = store.getFlushableSize();
          if (!(abort || flushableSize.getDataSize() == 0 || writestate.readOnly)) {
            if (getRegionServerServices() != null) {
              getRegionServerServices().abort("Assertion failed while closing store "
                + getRegionInfo().getRegionNameAsString() + " " + store
                + ". flushableSize expected=0, actual= " + flushableSize
                + ". Current memstoreSize=" + getMemStoreSize() + ". Maybe a coprocessor "
                + "operation failed and left the memstore in a partially updated state.", null);
            }
          }
{code}

bq. TestAcidGuaranteesWithBasicPoli didn't finish in the QA run.
I have ran TestAcidGuaranteesWithBasicPolicy 50 times, and I don't encounter the hang...Let me try the v2 patch (asap)

{quote}
w.r.t. 'memstoreSize to a negative value' error, the first occurrence is in TestAcidGuarantees#testMixedAtomicity
However, if I run the subtest alone, it passes with EAGER policy.
{quote}
It seems the error happens by chance. I will dig in after committing this issue.



I searched the test output on my disk where I don't see the 'ABORTING region server' log.
This was probably due to my running eager compaction policy alone and with SSD, the negative memstore size never accumulated during the test run (I observed fluctuating memstore size across zero).

It seems separating the compaction policies into their own test would avert the aborting case. In production, it is unlikely that different policies would be used during the lifetime of the region server process.

lgtm
Drop the following print upon commit.
{code}
+    System.out.println("[CHIA] delete start");
     UTIL.deleteTable(TABLE_NAME);
+    System.out.println("[CHIA] delete end");
{code}

sure...:P

| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 10s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Findbugs executables are not available. {color} |
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 6 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 31s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 40s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  3s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  5m 53s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 27s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 48s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 50m 57s{color} | {color:green} Patch does not cause any errors with Hadoop 2.6.1 2.6.2 2.6.3 2.6.4 2.6.5 2.7.1 2.7.2 2.7.3 2.7.4 or 3.0.0-alpha4. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 87m 39s{color} | {color:green} hbase-server in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}165m 39s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:eee3b01 |
| JIRA Issue | HBASE-19266 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12899055/HBASE-19266.v1.patch |
| Optional Tests |  asflicense  javac  javadoc  unit  findbugs  shadedjars  hadoopcheck  hbaseanti  checkstyle  compile  |
| uname | Linux 8ef4d1bb6d4c 3.13.0-133-generic #182-Ubuntu SMP Tue Sep 19 15:49:21 UTC 2017 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/component/dev-support/hbase-personality.sh |
| git revision | master / 3e2941a49e |
| maven | version: Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T07:58:13Z) |
| Default Java | 1.8.0_151 |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/9991/testReport/ |
| modules | C: hbase-server U: hbase-server |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/9991/console |
| Powered by | Apache Yetus 0.6.0   http://yetus.apache.org |


This message was automatically generated.



Will commit it later without the ugly hack.

bq. Agree to disable eager and adaptive for now.

I maintain my previous stance. Once the 'memstoreSize to a negative value' error is eliminated, we can enable these two tests.

bq. I maintain my previous stance. Once the 'memstoreSize to a negative value' error is eliminated, we can enable these two tests.
Let me log it in another jira.

HBASE-19339

Push to branch-2 and master.
Thanks for the review. [~tedyu]

FAILURE: Integrated in Jenkins build HBase-2.0 #903 (See [https://builds.apache.org/job/HBase-2.0/903/])
HBASE-19266 TestAcidGuarantees should cover adaptive in-memory (chia7712: rev 66e650c39b7d794e9cd774add7b9f2be9d064e8f)
* (add) hbase-server/src/test/java/org/apache/hadoop/hbase/TestAcidGuaranteesWithEagerPolicy.java
* (add) hbase-server/src/test/java/org/apache/hadoop/hbase/TestAcidGuaranteesWithNoInMemCompaction.java
* (edit) hbase-server/src/test/java/org/apache/hadoop/hbase/AcidGuaranteesTestTool.java
* (delete) hbase-server/src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java
* (add) hbase-server/src/test/java/org/apache/hadoop/hbase/TestAcidGuaranteesWithAdaptivePolicy.java
* (add) hbase-server/src/test/java/org/apache/hadoop/hbase/TestAcidGuaranteesWithBasicPolicy.java


bq. Eshcar Hillel / Anastasia Braginsky Ted is filing issues for you it seems. Duo & Chia-ping are doing work to mitigate the extra burden introduced by the parameterization of TestAcidGuarantees. Ye might have input. Thanks.

Thank you [~stack], for paying my attention! I am sorry I missed this JIRA. Took a look on the entire chain just now. It looks like the solution is already committed. Please do ping us directly next some issue related to CompactingMemStore shows up. We would be happy to help, review and/or code the solution. 

The issue about negative size is traced by HBASE-19339. FYI [~anastas]

Thanks, will look on the negative size problem.

