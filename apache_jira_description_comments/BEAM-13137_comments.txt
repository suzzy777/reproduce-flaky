I suggest that we should be able to make the tests non-flaky by testing only current behavior.

This issue is assigned but has not received an update in 30 days so it has been labeled "stale-assigned". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.

This issue was marked "stale-assigned" and has not received a public comment in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.

[~egalpin] would you be able to work on this ?

[~echauchot] I'm wondering if the flakiness could be to do with non-determinism of whether data has been flushed from the translog prior to making a size estimate.Â  That's not something I'd want to introduce into the core functionality of the IO because I wouldn't want to alter what's happening in a user's ES cluster, but if it can stabilize tests I think that would be perfectly acceptable to add in the unit tests util methods. I'll play with that a bit now.

[~egalpin] Thanks for taking a look ! There use to be stats specific parameters in tests that were removed when passed to testContainers:
- _settings.put("index.store.stats_refresh_interval", 0)_ in the index settings in test. 
-       _request.addParameters(Collections.singletonMap("refresh", "wait_for"));_
Maybe it is the cause of more flakiness in stats related features (testSplit and testSizes).
They were only for test so did not impact production clusters of users.


Thanks [~echauchot] I've added the index.store.{{stats_refresh_interval}} setting back via default index template. Regarding refresh wait_for, I've opted for explicit use of flushAndRefreshAllIndices everywhere so as to have a single code path for consistent behaviour regardless of the API that would otherwise wait for refresh.  Open to thoughts on this approach though :) 

yes I saw in the PR. better indeed to make it explicit

Merged through [8c2e1fd|https://github.com/apache/beam/commit/8c2e1fd6109871496457c45ddcbfb10747c3cdcf]. Waiting for 2 weeks to monitor flakiness before closing the ticket. 100 local runs showed no flakiness, let's see with the load on jenkins servers.

Update:  so far I have not seen flaky failures in the past 7 days. 

[~egalpin] thx for the update. knowing the activity on the Beam project, I guess 1 week is enough to monitor flakiness, closing the ticket.

[~echauchot] Do you feel we could also close https://issues.apache.org/jira/browse/BEAM-5172 ?

[~egalpin] if you took a look at all ES tests for flakiness and if there was none also during the past day, I think we can close the ticket.

[~echauchot] I reintroduced even the tests previously marked as `Skip` due to their extreme flakiness.  Since merging https://github.com/apache/beam/pull/16507 I have not seen any ESIO failures. I'll make a note on BEAM-5172

ticket [BEAM-5172|https://issues.apache.org/jira/browse/BEAM-5172] closed

