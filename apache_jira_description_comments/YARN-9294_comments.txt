cc [~tangzhankun]

More detailed information:
*devices.allow* & *devices.deny*  are not readable (only writable).

*cgroup* is configured correctly (it seems) since CPU isolation is in place. However, the GPU devices isolation is flaky.

[~oliverhuhuhu@gmail.com],

Have you tried to do some manual cgroup isolation test without YARN to reproduce it?

Like create a directory under /sys/fs/cgroup/devices/hadoop-yarn and echo the values to cgroup device deny file and verify if the process is isolated as expected repeatedly?

I used to verify cgroup parameter with cgget and cgdelete. The tools can be installed by 
{code:java}
yum install libcgroup
yum install libcgroup-tools
cgget -r memory.limit_in_bytes -g memory:hadoop-yarn/container_1542945107795_0003_01_000002{code}
 

But I just verified in my Ubuntu VM, the "cgget" cannot show the denied devices although the isolation is working. Maybe the "cgget" also has this bug in REL.

From your description, it seems we should dig with reproducing the flaky GPU isolation first? Or try different OS kernel version?

 

Yes, cgget is the old API in RHEL 6 (libcgroup tookit) to get the allocation. It works for memory & CPU, but not for devices. I don't think it is a bug but it seems something not supported to pull device information. I did that manually in my desktop and it works.

My current suspicion is this _echo values to cgroup process_ might be flaky sometimes (either file io error, cgroup glitches, race conditions etc). Plan is to have a check to make sure the cgroup manipulation works before moving on to start the process in that cgroup.

[~oliverhuhuhu@gmail.com] , Yeah. Agree with the plan. And to find the stable reproducing steps seems a good start to me. We can write a script to create sub device cgroups and use "container-executor" to set parameter and then attach the running processes to verify if anyone sees the denied devices.

Confirmed it is a race condition in cgroups creation & executing command in the cgroups. We plan to go ahead with a safe check between these two privileged operations. Note the same issue should apply to 3.1+ as well. cc [~wangda] [~tangzhankun]

[~oliverhuhuhu@gmail.com] , good job! Looking forward to your patch.

After more debugging, we found the race condition is not because of flakiness with cgroup creation & launching job in the cgroup slice, but caused by an incompatibility with RHEL 7. Would love to hear if anyone is the community experienced the same issue with RHEL 7. Basically the existing logic to `mkdir container_123` && `echo taskId > container_123/tasks` doesn't work anymore. There is some sanity work in the OS that if the process is not registered in `/sys/fs/cgroup/systemd/`, the taskId will be removed from `container_123/tasks`.

There are a couple ways to fix the issue, one is to use RHEL 7 specific cgroups CLI like * systemd-run --unit=hu --slice=hadoop nohup /root/echo.sh* to start the container executor, but this won't be compatible with other operating systems. Still trying to figure out if there is a way to make it work for most OS.

[~oliverhuhuhu@gmail.com] , Do you mean that the current way of updating the cgroups in contain-executor ("fprintf" actually) is not working in RHEL 7?

