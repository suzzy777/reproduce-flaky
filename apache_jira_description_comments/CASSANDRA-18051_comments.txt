Looking closer into this one, it seems actually CASSANDRA-10584 added a test only for EACH_QUORUM, available only since 3.0

The other one tests all possible combinations, added as part of CASSANDRA-9102... I need to think more about this whether it is really a good idea to reduce the number of DCs

CC [~aweisberg] as I believe he was leading that effort to improve the testing in CASSANDRA-9102

I will look again into this in the new year

I would like to suggest we add a message in DTests for those two to be logged in case someone is using MIDRES in CircleCI and they fail, so they know the reason and move on. People can run them with HIGHRES, locally, in Jenkins. Raising the default MIDRES resources only for two tests is too expensive in my opinion.

It might be that less data centers testing is completely harmless but I do not believe people have the time now to look more in details in to that and considering that those tests were added to improve our coverage on critical path, I think it is best just to leave them alone for now. 

[~adelapena] , [~mck] any thoughts?

How much is the additional cost of running "Large DTests" in xlarge instead of large ? 

{quote}People can run them with HIGHRES, locally, in Jenkins.
{quote}
I understand that there were plans to remove HIGHRES and rename the current LOWRES and MIDRES to something like more related to their intended use. If we actually remove HIGHRES there won't be an option to easily run them on Circle, unless we can change them to use 2 DCs instead of 3. I don't see why those two tests need the 3 DCs, but I might be missing something.

[https://circleci.com/product/features/resource-classes]

XLarge spends double the credits we spend for Large per minute

[~adelapena] has a valid point that plans for getting rid of the HIGHRES and MIDRES config files in favor of one paid config are in place but I am afraid no one has assigned that ticket and works on that actively yet. 

{quote}XLarge spends double the credits we spend for Large per minute
{quote}
One might think that the larger resources would finish in less minutes, but that doesn't seem the case:
 * This old [MIDRES run|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2130/workflows/9df2d073-c26a-4750-82e4-8b6645f37f9c/jobs/16568/parallel-runs/1?filterBy=ALL] of large dtests that I have stolen from [~e.dimitrova] seems to take ~93 minutes when adding the times of its four runners. It uses large containers.
 * This [HIGHRES run|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2129/workflows/b451fdf4-e643-42e9-86a4-f849e57c559f/jobs/16685/steps] takes ~94 minutes to finish with its only runner. It uses xlarge containers.

Both runs contain 42 tests. I assume that the minutes we pay are the sum of minutes we see on the GUI for every runner.

So it seems that the larger resources double the price while keeping the total time unchanged.

Thank you for providing this context [~adelapena] . Yes, this is what was also our observation long ago when MIDRES config was created - it gives similar timing for less credits/money in many cases

What percentage of a pre-commit run is the dtest-large part? i.e. what is the size of the part we are doubling?

Quickly looking a pre-commit run (all parts added) 5.5 hours, and dtests-large is 0.5 hours. So it's a ~9% increase per run. (On the assumption there's the same amount of runners in all the stages, which there isn't – the increase looks to be much smaller)

Shouldn't we just leave dtest-large on xlarge for now, and focus on solving this ticket to reduce costs? And can't we use xlarge in MIDRES for just dtest-large (i.e. it's irrelevant if we otherwise remove HIGHRES)? Am I missing something here?

Would it make sense to create a new circle job 'dtest-that-need-high-resources' and put those 2 tests in with increased resources only there? If that were a waste of resource we make that one guarded by approval as jenkins should catch any glaring errors.

bq. Quickly looking a pre-commit run (all parts added) 5.5 hours, and dtests-large is 0.5 hours. So it's a ~9% increase per run. (On the assumption there's the same amount of runners in all the stages, which there isn't – the increase looks to be much smaller)
I think it matters what part is in large containers.

bq. Shouldn't we just leave dtest-large on xlarge for now, and focus on solving this ticket to reduce costs? And can't we use xlarge in MIDRES for just dtest-large (i.e. it's irrelevant if we otherwise remove HIGHRES)? Am I missing something here?

That is what I will suggest in my next comment

bq. Would it make sense to create a new circle job 'dtest-that-need-high-resources' and put those 2 tests in with increased resources only there? If that were a waste of resource we make that one guarded by approval as jenkins should catch any glaring errors.

Currently the large dtests are already behind approval so I would suggest to just move to large (considering that) and decide how to handle this (new jobs, refactoring, etc in the ticket where we  merge to one paid config)

I can push a patch either here or as part of CASSANDRA-18001 which I will push for final review today


I still think those tests can run with 2 DCs instead of 3. That is a trivial change in the dtests that would eliminate the need of running large dtests with xlarge.

If I'm missing something and the dtests actually need the 3 DCs, I think I would just first move the job to xlarge in midres so the job isn't broken, and then we can think on splitting the job when we eliminate highres.

{quote}bq. I still think those tests can run with 2 DCs instead of 3. That is a trivial change in the dtests that would eliminate the need of running large dtests with xlarge.
{quote}
Definitely quick and trivial change. Just those tests are added as part of an effort to raise the coverage on the critical path and the person who did that after code coverage runs, etc is not around anymore. Thus when talking about the critical path and we are pressed from other priorities now I would personally be more conservative, but I am open to different feedback. 
{quote}If I'm missing something and the dtests actually need the 3 DCs, I think I would just first move the job to xlarge in midres so the job isn't broken, and then we can think on splitting the job when we eliminate highres.
{quote}
Considering my previous comment this is probably the path I'd recommend to go for at this particular moment and revisit later when time is not a concern

If there are no objections should I move forward with the last suggestion?

Yep, fine with me. As for simplifying the two super-heavy dtests, we could later ask on Slack or the mail list to see if anyone sees a reason for them needing three DCs. I think that if no one does we shouldn't keep us tied to them.

{quote}Yep, fine with me.
{quote}
I will post it later today as part of the final patch for CASSANDRA-18001.

 

WFM too.

+1

Great, thank you all. I just posted that change as part of the final patches in CASSANDRA-18001.

It seems those are failing on 3.0 and 3.11 with 1 container XLarge, but not on other branches. For more info - CASSANDRA-18001.

I suggest we edit this ticket to fix those tests as I am not sure that the failures we see are related to the resources. Needs investigation

It was noticed on CASSANDRA-18001 that the tests fail even with XLarge containers on 3.0 and 3.11. I also noticed further to resources there might be some timing issue.

More to follow, but I am not sure when I will have the time for this so if anyone is interested to look into that - feel free to steal it. The tests are flaky for all branches.

I can post some more runs here on the weekend for information purposes and completeness

bq. The tests are flaky for all branches

So just for clarification. They fail on _all_ branches or on 3.0 and 3.11 only? Thx

Considering they fail with Large containers on all branches, I'd say we see issues on all branches, just on 3.0 and 3.11 even with XLarge containers

