I can replicate this locally in cdh5-trunk via {{/tests/run-tests.py failure}}, which takes only two minutes.

[~hsheinblatt_impala_e511], have you seen this before? Have our failure tests generally worked when run on their own, without the other e2e tests?

I had a look at this issue, the test waits for a list of metrics on the backend page to become zero:

"impala-server.backends.client-cache.clients-in-use",
"impala-server.hash-table.total-bytes",
"impala-server.io-mgr.num-open-files",
"impala-server.num-files-open-for-insert",

When running test_metrics_are_zero on my machine after the failure tests it will block for as long as 360 seconds on the first metric. The backend metrics page says the metric means the following: "The number of active Impala Backend clients. These clients are for communication with other Impala Daemons."

Do we expect these connections to be around for a longer time after the tests have run? Then we can either increase the timeout we wait for them to make sure they eventually go away. Or remove them from the list of metrics we wait on. If we expect them to close rather quickly then we should investigate why they don't


[~lv] - are you able to reproduce this locally, like [~jbapple] can? If so you should be able to see if there are {{TransmitData()}} RPCs in flight (by checking {{/rpcz}} during the timeout window). 

If so, then eventually in the Impala logs there should be a line like:

{code}
Datastream sender timed-out waiting for recvr for fragment instance <x:y> (time-out was: 120000ms)
{code}

If this is during the failure tests, this makes sense because queries that cancel can leave these 'zombie' fragments around for a while. To double check, you could set the amount of time waited to ~180s (the sender timeout should be 120s by default). 

If it _is_ the sender timeout, I'm not totally sure what we should do. It would be good not to lose coverage, but waiting for 2m for those tests to pass is annoying. We do have ideas about improving the zombie period, but probably not for 5.8. 

[~henryr] - yes, I am able to reproduce it. I measured the timeout for a couple of runs and it's consistently between 300 and 350 sec on my dev machine. I also found the RPCs in flight and the log message. Could it be that the RPCs queue up behind each other and multiple timeout periods add up that way?

We could remove "impala-server.backends.client-cache.clients-in-use" from the lists of metrics we wait on. There's already two other metrics where we did this with a reference to a Jira. Do we have some other tests to catch the case where clients don't disappear? 

Not to my knowledge. I think it would be good not to lose this coverage, but can't think of a good way around it right now. 

Are you able to tell if there are multiple successive RPCs that are 'queued'? The number of {{TransmitData()}} calls should go up in {{/rpcz}}, of course.



I observed that the number of {{TransmitData()}} calls started with 20 at the beginning of test_metrics_are_zero. It seemed to drop monotonously but at some point in time increased from 2 to 3, so I assume that's a sign of the 'queueing'. I also noticed that there were 13 fragments in flight while at the same time 20 calls to {{TransmitData()}} were reported.

I understood that the best thing we can do for now is disable that metric's check. I did so in this change, please let me know if we should do something else: http://gerrit.cloudera.org:8080/#/c/2788


IMPALA-3327: Disable metric validation for backend clients

After decoupling the startup of senders and receivers during fragment
execution, cancellation of queries can lead to 'zombie' fragments
waiting for the other end to get started. Those fragments will
time-out after 2 minutes, however due to queuing effects it can take
several minutes until all of them are gone. It is therefore currently
not feasible to wait for the
'impala-server.backends.client-cache.clients-in-use' metric in the
tests.

Change-Id: I67e8c5bd8ab4f71a3c10e9387ae165c0c1dd30f3
Reviewed-on: http://gerrit.cloudera.org:8080/2788
Reviewed-by: Dan Hecht <dhecht@cloudera.com>
Reviewed-by: Henry Robinson <henry@cloudera.com>
Tested-by: Internal Jenkins

