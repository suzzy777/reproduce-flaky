You could use a maintain a counter for the number of batches completed. That is a foreachRDD can increment a counter. And the testing code should wait for the counter to reach 2 before checking for the model. Alternatively, the check should be in an eventually loop.

Actually never mind, its already in eventually. The default timeout is 30 seconds. Then I dont get why this is failing. Could it be a thread race condition visibility issue?

CC: [~davies]  When you have a chance, can you please take a look?  Thanks a lot!
It's unfortunately hard to reproduce...just happens on Jenkins sometimes.

I've been able to reproduce this locally, but haven't found the exact cause yet.  Here is what I know so far

* It only happens in my env about 1 out of ~200 attempts, and seems to be pretty random
* When it fails, the centroids still have the initial values while predicting, and they are later updated

My thought is that there is some kind of race condition as [~tdas] mentioned, so somehow {{predictOn}} is occurring before {{trainOn}}.  I can keep looking into it, but I'd like to propose a slight change to the test (below).  

It basically tests the same thing, but is just more flexible about _when_ the model update happens.  Prediction on the first batch should yield the same regardless if centroids are at initial state or updated, so we can check the first element in {{predict_results}} for that.  Then the stream will repeat the second batch (as the default) and we just check the last element of {{predict_results}} which should yeild the correct result only after the model has been updated at least once.  So far, this does not seem to fail in my local env.

{noformat}
  def test_trainOn_predictOn(self):
    """Test that prediction happens on the updated model."""
    stkm = StreamingKMeans(decayFactor=0.0, k=2)
    stkm.setInitialCenters([[0.0], [1.0]], [1.0, 1.0])

    # Since decay factor is set to zero, once the first batch
    # is passed the clusterCenters are updated to [-0.5, 0.7]
    # which causes 0.2 & 0.3 to be classified as 1, even though the
    # classification based in the initial model would have been 0
    # proving that the model is updated.
    batches = [[[-0.5], [0.6], [0.8]]]
    default_batch = [[0.2], [-0.1], [0.3]]
    batches = [sc.parallelize(batch) for batch in batches]
    default_batch = sc.parallelize(default_batch)
    input_stream = self.ssc.queueStream(batches, default=default_batch)
    predict_results = []

    def collect(rdd):
      rdd_collect = rdd.collect()
      if rdd_collect:
        predict_results.append(rdd_collect)

    stkm.trainOn(input_stream)
    predict_stream = stkm.predictOn(input_stream)
    predict_stream.foreachRDD(collect)

    self.ssc.start()

    def condition():
      self.assertTrue(predict_results)
      self.assertEqual(predict_results[0], [0, 1, 1])
      self.assertEqual(predict_results[-1], [1, 0, 1])
      return True

    self._eventually(condition, catch_assertions=True)
{noformat}



User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/9670

Saw more failures recently and changed the priority to critical.

User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/10909

I was able to track down the cause of these failures, so here is an update with what I found.  The test {{StreamingKMeansTest.test_trainOn_predictOn}} has 2 {{DStream.foreachRDD}} output operations, 1 in the call to {{StreamingKMeans.trainOn}} and 1 with {{collect}} which has a parent {{DStream}} that is a {{PythonTransformedDStream}} returned from {{StreamingKMeans.predictOn}}, so 2 jobs are generated for each batch.  When the {{DStream}} jobs are generated, there is nothing to compute for the first job, which updates the model.  For generating the second job, {{PythonTransformedDStream.compute}} gets called which will then do a {{PythonTransformFunction}} callback that creates a {{PythonRDD}} and serializes the mapped predict function to a command, containing the current model.  

Next, the 2 jobs are scheduled in order - first to update the model and then collect the predicted result.  At this point, there is a race condition between completing the model update and generating the next set of jobs, which is running in a different thread.  If there is enough of a delay in the update, then the next set of jobs will be generated and the old model will be serialized to the {{PythonRDD}} command again.  Finally, the predict will be run against this old model causing the test failure.  

To sum it up, the underlying issue is that a func can be serialized with a value before a job is run that updates this value.  This doesn't appear to be an issue in the Scala code as the closure cleaner is run just before the job is executed, and it will get the updated values.

So far, the best solution I can think of would be to somehow delay the serialization of the model until it is needed, but I believe this would involve some big changes in {{PythonRDD}} as would any other solutions I could think of.  Is something that would be worth doing to correct this, or might there be an easier fix that I am not seeing?  It's not just a {{StreamingKMeans}} issue, so it would affect any PySpark streaming application with similar structure.  

I am attaching some simplified code used to reproduce the issue.  I also have a similar Scala version that produces the expected results.

Simple script [^flakyRepro.py] with similar operations to this StreamingKMeans test, used to reproduce the issue

I opened SPARK-13691 to describe the root issue in PySpark.

I'm removing the target version.  This is an important issue, but I don't see us getting a fix in for 2.0---and I have not heard of users encountering it (though the source of this bug might be hard to discern in practice).  Let's revisit for the next release.

The changes to the test I proposed earlier are still valid, and now that the root cause has been described in SPARK-13691, you could apply it and enable the test again to close this off.

