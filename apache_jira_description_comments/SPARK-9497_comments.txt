[~kayousterhout] Seems you added "repeatedly failing task that crashes JVM". Will you have time to take a look at this test?

When did you start seeing these failures?  I added the test 1.5 years ago, so it seems likely to be related to a recent change in the code.

It looks like this error is happening when we try to stop the AppClient from SparkDeploySchedulerBackend: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend.scala#L96; is it possible this started happening after [~zsxwing]'s recent change? https://github.com/apache/spark/commit/3bee0f1466ddd69f26e95297b5e0d2398b6c6268 (that changed the line of code in AppClient.scala that is failing: https://github.com/apache/spark/blame/master/core/src/main/scala/org/apache/spark/deploy/client/AppClient.scala#L251)

It started to fail about 2 to 3 days ago. Its history on hadoop 1.0 test can be found at https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/3117/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/testReport/junit/org.apache.spark/DistributedSuite/history/

I also see https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/3119/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.2,label=centos/testReport/junit/org.apache.spark.scheduler/ReplayListenerSuite/End_to_end_replay_with_compression/ [~zsxwing] maybe it is also related.

It looks like this has failed much longer ago than that; here's one from July 4th, for example: https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/2846/

I'm trying to figure out when this started but the history for the builds is super slow to load...will send another update shortly

A bunch of thoughts here:

The oldest failure I could find was July 2nd, and it seems to have been flakey since then.  The test seems to have been getting worse recently, which I'd guess is just because Jenkins is more heavily loaded right now. Maybe you already noticed this, but it looks like everything, DistributedSuite fails, and then all of the ShuffleNetty tests fail afterwards with "Only one SparkContext may be running in this JVM", which probably is because the DistributedSuite failure is during stop(), so presumably the SparkContext doesn't get shut down right, and then all of the later tests end up failing.

I also realized that the RpcTimeoutException isn't what causes things to fail, and the exception is caught in AppClient: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/client/AppClient.scala#L253

So it seems like the SparkContext is somehow getting into a bad state where it can't be shutdown.

One other thought is that what's different about this test (compared to the other tests in DistributedSuite) is that it causes the executor to fail.  Given that, this line in the recent change is a little suspicious: https://github.com/apache/spark/commit/3bee0f1466ddd69f26e95297b5e0d2398b6c6268#diff-a240aa7b4630dc389590147f96cf3431R174 and I wonder if it could be related.

I found the issue. The exception type of `RpcEndpoint.askWithRetry` is not `TimeoutException`. So now it won't be caught. I will fix it.

User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/7824

[~yhuai] Is the second link related? The failure test is "org.apache.spark.scheduler.ReplayListenerSuite.End-to-end replay with compression" and I cannot find "StopAppClient" in unit-tests.log.

I am not sure if it is related. Maybe now. I will keep an eye on ReplayListenerSuite and try to get more information about it.

