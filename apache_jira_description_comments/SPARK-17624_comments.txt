I cannot reproduce locally on my Mac laptop, Maybe you test machine is not so powerful to handle this unit test?

Having another look at this now as it still fails intermittently

[~tdas] hi, as author of this test and feature can you please explain why numRetries was chosen to be 1 instead of the default (2) -- is it important for this test? My concern is that there's something actually wrong/unreliable here and that the failure is legitimate

[~jerryshao] HW used is a 2x Intel(R) Xeon(R) CPU E5-2697 v2 @ 2.70GHz with 16 GB RAM (standard -Xmx3g used for the tests anyway), ext4 filesystem, Ubuntu 14 04 LTS

I figured out the problem. Submitting a patch. 

[~aroberts] To answer your question, the StateStore maintenance thread (in executor) communicates with the StateStoreCoordinator (in the driver) to identify whether a state store should be unloaded or not. When the SparkContext is stopped, the StateStore is supposed to not be able to talk to the StateStoreCoordinator and unload all loaded StateStores. By default, the RPC layer will attempt 3 times with large timeouts to talk to the StateStoreCoordinator before failing. I set the numRetries to 1 to fail faster in the "maintenance" test.

User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/15592

