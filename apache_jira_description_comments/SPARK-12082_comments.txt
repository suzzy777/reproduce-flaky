The test is "working", it's just not waiting long enough...

{noformat}
15/12/01 13:09:42.045 shuffle-server-0 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() on RPC id 6025146086366897917
java.lang.IllegalStateException: Expected SaslMessage, received something else (maybe your client does not have SASL enabled?)
        at org.apache.spark.network.sasl.SaslMessage.decode(SaslMessage.java:69)
        at org.apache.spark.network.sasl.SaslRpcHandler.receive(SaslRpcHandler.java:87)
15/12/01 13:09:43.285 ScalaTest-main-running-NettyBlockTransferSecuritySuite INFO NettyBlockTransferSecuritySuite:

===== FINISHED o.a.s.network.netty.NettyBlockTransferSecuritySuite: 'security mismatch auth off on client' =====

15/12/01 13:09:43.300 shuffle-client-0 ERROR OneForOneBlockFetcher: Failed while starting block fetches
java.lang.RuntimeException: java.lang.IllegalStateException: Expected SaslMessage, received something else (maybe your client does not have SASL enabled?)
        at org.apache.spark.network.sasl.SaslMessage.decode(SaslMessage.java:69)
        at org.apache.spark.network.sasl.SaslRpcHandler.receive(SaslRpcHandler.java:87)
        at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:149)
        at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
{noformat}


In this case it took more than 1 second (the test timeout) for the error to arrive at the client.

What's slightly odd to me is the huge variability in how long this test takes to run. In [most cases|https://spark-tests.appspot.com/tests/org.apache.spark.network.netty.NettyBlockTransferSecuritySuite/security%20mismatch%20auth%20off%20on%20client], it runs in less than 30ms, and rarely in under 200ms, so I wonder what causes it to take > 1 second in those failed tests.

If the build machines run on VMs, run multiple jobs, or have anything else that may lead to sporadic stalls / slowness, I wouldn't be surprised when timeouts turn out to be unreliable.

Our internal test machines are way more loaded than those used by Spark's jenkins, and we run into a bunch of different time outs that the Spark builds never see.

For now, I'm going to try bumping the timeout to something higher, like 10 seconds, and keep an eye on the test dashboard to see if that's a sufficient fix.

User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/10113

