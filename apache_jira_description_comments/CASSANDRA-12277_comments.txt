I'll admit that at face value that seems a weird and potentially dangerous thing to do to me. I can easily see us defaulting to such mechanism every time we get a flaky test and that hiding genuine problems more often than not. I don't know the details about that specific {{ReplicationAwareTokenAllocatorTest.testNewCluster}} test, but if a test knowingly fails a number of the times, this feels to me like the test isn't precise enough and should be improved.

Note that I can buy that exceptionally a test is such that making it pass 100% of the time is too much work to be worth the trouble but the test is still nice to have, but that should be exceptional and I'd rather handle that in a case-by-case basis, with very precise comment as to why this is allowed to fail some of the times, to make it clear it's a special case.   

bq. I can easily see us defaulting to such mechanism every time we get a flaky test and that hiding genuine problems more often than not
If we, collectively, don't have the discipline not to abuse something like this, we have bigger problems.

bq. this feels to me like the test isn't precise enough and should be improved
I'll let [~blambov] speak to that since this ticket was create from discussion with him.

bq. that should be exceptional and I'd rather handle that in a case-by-case basis
This ticket is about introducing infrastructure to allow us to formalize handling that on a case-by-case basis. If we leave a precise comment in the code, we still have a non-green test-board and the cognitive burden of filtering out "known flaky" failures when checking test results.

bq. If we, collectively, don't have the discipline not to abuse something like this, we have bigger problems.

We're all sometimes lazy, and it's not specific to this project, so I think not making it too easy to do the wrong thing (here, blindly ignoring a rare flaky test as long as it doesn't fail too often) is just smart project management. Also, test is the one area where I think we've been historically pretty bad at discipline, so while I'm all for improving our ways, I'm going to object on assuming discipline until I consider we've collectively and consistently shown discipline for a reasonable length of time.

bq. If we leave a precise comment in the code, we still have a non-green test-board and the cognitive burden of filtering out "known flaky" failures when checking test results.

You misunderstood what I meant. I'm not saying we let the test flake, I'm saying we manually modify the test to "run this thing N times, if M pass we're good" (and clearly explain why it's ok to do so), but without bothering adding infrastructure for it.

But let me also be clear I'm not suggesting we'd manually modify tests that way on any regular basis, that would be stupid.

Basically, I feel we're conflating 2 things here. It seems to me {{ReplicationAwareTokenAllocatorTest.testNewCluster}} is (potentially) a rare case where we *understand* why the test is flaky, but where 1) fixing the flakiness is not worth it and 2) we decide that we understand the flakiness well enough that we can trust the test to provide value _even_ if we ignore a few failed runs.

But I can't really see that being anything else than a very very rare situation (I'm not even entirely saying I'm fine with that one). So I don't think we should base our infrastructure for handling flaky tests on those assumptions. In general, a flaky test is a bug (probably of the test but still), and we should identify the reason of the flakiness and fix it. I'm fine marking tests with {{@flaky}} temporarily (when our educated guess is that it's probably a test problem), while we find time to fix it properly so it doesn't clutter the test result board. But I'm not convinced we should *replace* that by a different annotation that considers a flaky test is fine as long as it doesn't fail too often, which is what I understand from the description of this ticket.


If you ask me, the main question here is whether we want to have a test suite that adds value, or one that is perceived pure. I'd always choose the former over the latter, and the fastest way to get there is to have an infrastructure to deal with a very common problem in a very common way, similar to how countless other projects do it.

Tests flake, that's a fact of life. We either deal with it smartly as in, identify where you can allow it, and let it happen, or the way we have been doing it so far: not know when it is ok, always retry everything (at significant cost) and ignore real problems.

I'm mostly disagreeing than most of our flaky test fails for known and well understood reason (which was I understood the description to suggest, though I might have misinterpreted it). I think by and large, our flaky tests are flaky for reason we don't fully understood. They are tests for which our educated guess is that that it's likely an environment issue, but it's a guess nonetheless. The category of tests that flake for very well understood and controlled reasons may exist, but is imo comparatively tiny (and thus I just don't want to design the handling of flaky test based on assuming that 2nd category).

In general though, we're in agreement that 1) flakiness happens and will continue to happen and 2) we currently don't deal with it very well.

I'm completely on board changing how {{@flaky}} works, so that on a failure it automatically retries some N times and "pass" as long as it didn't failed more than M times (N and M to be defined). It's certainly much better than having {{@flaky}} skip tests altogether or fail the whole build, no questions about that.

But we should still, I think, consider those {{@flaky}} test for what they are: bugs that hopefully should be fixed over time. If we want another annotation to distinguish a few tests whose flakiness is well understood and accepted (and thus don't need any fixing), fine, but that's comparatively unimportant imo.

After trying for a day to find a quick solution to the underlying problem with this test, i.e. to find a way to algorithmically generate a good selection for the tokens of the first RF nodes, the thing I understand is that the problem needs a non-trivial time investment.

Being also too lazy to keep arguing about the benefits of an easy-to-use flaking test rerun infrastructure, I whipped out a rerun method that is not too difficult to apply on a case by case basis, and wrapped the test in it. The code is here:
|[code|https://github.com/blambov/cassandra/tree/12277]|[utests|http://cassci.datastax.com/view/Dev/view/blambov/job/blambov-12277-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blambov/job/blambov-12277-dtest/]|

The {{flakyTest}} method runs the given test, and if it fails reruns it for the specified number of times expecting it to succeed on all reruns. This should reduce the incidence of flakes on this test by at least an order of magnitude while immediately breaking if the test fails with higher than expected probability.

Mostly lgtm with the following minor remarks:
* I really think we need more explanation on why it's hard to fix the underlying problem. I'm sure it's obvious to you why randomness can cause the test to fail, why it's hard to fix it, and why the test is still valuable despite this, but it's not at all to me. I'm basically asking for some explanation of what the test does, where is the randomness involved and why it's not easy to control it better.
* A comment on {{flakyTest()}} explaining what it does, and hence why you shouldn't abuse it and should always justify it's use, wouldn't be superfluous imo.
* I believe we should be using {{%n}} instead of {{\n}} in format string for cross-platform compatibility.
* If the test failed after the first try, since we're running it as many time as asked anyway, I'd wrote a message on stderr for how many failures (over how many runs) we had (I think you could infer that from the stack trace since every failure is added as a suppressed exception, but a simple message is a bit more user friendly).

Changes are applied, please take another look.

To give you some food for thought, what do you think is a better test in this situation:
- one that fixes the seed (as I have been doing in other cases, including in {{testExistingCluster}} in this test file) and only always tests with one set of values, or
- one that lets random combinations play out, expecting a proportion of them to fail?

bq. Changes are applied, please take another look.

Lgtm, committed (to 3.0+), thanks.

bq. what do you think is a better test in this situation

In that particular situation, fixing the seed problably makes the test a lot less valuable so I'm fine with the committed solution.

