Hard to repro but seems sometimes we are sending MV mutations on MV creation before schema agreement and before MV build task:
Â 
{noformat}

DEBUG [OptionalTasks:1] 2020-06-02 17:31:36,119 CassandraDaemon.java:389 - Completed submission of build tasks for any materialized views defined at startup
INFO  [MigrationStage:1] 2020-06-02 17:31:46,145 ColumnFamilyStore.java:849 - Enqueuing flush of columns: 1.488KiB (0%) on-heap, 0.000KiB (0%) off-heap
INFO  [Messaging-EventLoop-3-9] 2020-06-02 17:31:46,159 NoSpamLogger.java:91 - 127.0.0.1:7000->127.0.0.2:7000-SMALL_MESSAGES-3d6869c3 incompatible schema encountered while deserializing a message
org.apache.cassandra.exceptions.UnknownTableException: Couldn't find table with id 2afc69c0-a4e6-11ea-a44e-7941109e8480. If a table was just created, this is likely due to the schemanot being fully propagated.  Please wait for schema agreement on table creation.
        at org.apache.cassandra.schema.Schema.getExistingTableMetadata(Schema.java:456)
        at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:638)
        at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:380)
        at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:398)
        at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:362)
        at org.apache.cassandra.net.Message$Serializer.deserializePost40(Message.java:770)
        at org.apache.cassandra.net.Message$Serializer.deserialize(Message.java:630)
        at org.apache.cassandra.net.InboundMessageHandler.processSmallMessage(InboundMessageHandler.java:320)
        at org.apache.cassandra.net.InboundMessageHandler.processOneContainedMessage(InboundMessageHandler.java:303)
        at org.apache.cassandra.net.InboundMessageHandler.processFrameOfContainedMessages(InboundMessageHandler.java:270)
        at org.apache.cassandra.net.InboundMessageHandler.processIntactFrame(InboundMessageHandler.java:255)
        at org.apache.cassandra.net.InboundMessageHandler.process(InboundMessageHandler.java:246)
        at org.apache.cassandra.net.FrameDecoder.deliver(FrameDecoder.java:320)
        at org.apache.cassandra.net.FrameDecoder.channelRead(FrameDecoder.java:284)
        at org.apache.cassandra.net.FrameDecoder.channelRead(FrameDecoder.java:268)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1408)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930)
        at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:792)
        at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:424)
        at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:326)
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
.
.
.
ERROR [MutationStage-2] 2020-06-02 17:31:46,414 Keyspace.java:621 - Attempting to mutate non-existant table 2afc69c0-a4e6-11ea-a44e-7941109e8480 (ks.t_by_v)
ERROR [MutationStage-1] 2020-06-02 17:31:46,414 Keyspace.java:621 - Attempting to mutate non-existant table 2afc69c0-a4e6-11ea-a44e-7941109e8480 (ks.t_by_v)
ERROR [MutationStage-2] 2020-06-02 17:31:46,415 Keyspace.java:621 - Attempting to mutate non-existant table 2afc69c0-a4e6-11ea-a44e-7941109e8480 (ks.t_by_v)
ERROR [MutationStage-4] 2020-06-02 17:31:46,415 Keyspace.java:621 - Attempting to mutate non-existant table 2afc69c0-a4e6-11ea-a44e-7941109e8480 (ks.t_by_v)
ERROR [MutationStage-1] 2020-06-02 17:31:46,415 Keyspace.java:621 - Attempting to mutate non-existant table 2afc69c0-a4e6-11ea-a44e-7941109e8480 (ks.t_by_v)
.
.
.
INFO  [MigrationStage:1] 2020-06-02 17:31:46,417 ColumnFamilyStore.java:398 - Initializing ks.t_by_v
.
.
.
DEBUG [MigrationStage:1] 2020-06-02 17:31:46,601 ViewBuilder.java:97 - Starting build of view(ks.t_by_v). Flushing base table ks.t
...
DEBUG [ViewBuildExecutor:1] 2020-06-02 17:31:46,778 ViewBuilderTask.java:127 - Starting new view build for range (-6661324248839560100,-6533221859438799530]

 {noformat}

I have to continue digging.

>  sending MV mutations on MV creation before schema agreement and before MV build task:

kind of a known issue. When a node receives schema propagation, it starts view building asynchronously, where other nodes may not receive schema propagation yet.

Hi [~jasonstack],

thx for the info. What I see is slightly different though. We do indeed [wait|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/view/ViewBuilderTask.java#L136] for schema agreement, but adding some logging on node1 we can see it _sees_ schema agreement while the other nodes2&3 don't know about the MV yet. Here is node1 under the wrong impression on schema agreement:

{noformat}
DEBUG [ViewBuildExecutor:1] 2020-06-02 17:31:46,137 Gossiper.java:2038 - Node: 127.0.0.3:7000 expectedVersion: null remoteVersion: a17bfd1c-15fd-30ad-9a5e-534b45062856
DEBUG [ViewBuildExecutor:1] 2020-06-02 17:31:46,137 Gossiper.java:2038 - Node: 127.0.0.2:7000 expectedVersion: a17bfd1c-15fd-30ad-9a5e-534b45062856 remoteVersion: a17bfd1c-15fd-30ad-9a5e-534b45062856
DEBUG [ViewBuildExecutor:1] 2020-06-02 17:31:46,137 Gossiper.java:2038 - Node: 127.0.0.1:7000 expectedVersion: a17bfd1c-15fd-30ad-9a5e-534b45062856 remoteVersion: a17bfd1c-15fd-30ad-9a5e-534b45062856
DEBUG [ViewBuildExecutor:1] 2020-06-02 17:31:46,137 Gossiper.java:2047 - Nodes agree on schema
{noformat}

I don't know if it's worth going down this race rabbit hole or if Is there a ticket around already I can mark this one a duplicate of? I can't seem to find one

Also do you think we should mark this test as a failure in dtests like it's suggested [here|https://issues.apache.org/jira/browse/CASSANDRA-14571?focusedCommentId=16549702&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16549702] until the underlying cause is fixed?

CASSANDRA-14571 should have avoided view builder starting before schema propagation..

According to the log [~Bereng] uploaded, it looks like node1 is in schema agreement with other nodes.. My hypothesis is when update view schema at node1, it starts view builder before updating schema digest..

In {{"Schema#transform"}}"
{code}
        merge(diff);   // starts view builder asynchronously
        updateVersion();  // update schema digest
{code}

bq. Also do you think we should mark this test as a failure in dtests like it's suggested here until the underlying cause is fixed?

Alternatively we can put a delay in view builder for dtests..

[~jasonstack] you rock! you saved me a few hours of painful debugging :-) I have tried to repro locally and I can't anymore. I will leave it running for lunch just in case as it was a difficult one to repro. I would like to get your opinion on how to best fix this given I don't know the codebase as well:
A. Make {{SchemaKeyspace#calculateSchemaDigest}} public and check schema agreement against that as well in {{Gossiper#nodesAgreeOnSchema}} (this is the current fix that seems to work)
B. Mark the schema somehow 'dirty' and take that into account in all calls to the version. Not sure how or how much work that would be
C. Block during the 'schema version update' window.

I think B and C sound very nice but I wonder into how much trouble and deadlocks can we get ourselves into and if it's worth it or reasonably doable. Maybe fixing {{Gossiper#nodesAgreeOnSchema}} with A is a good compromise and that method is going to be there for anybody that needs to. Wdyt? :thinking:

Ok I did run this for 1.5h continuously and it's all green except for 2 failures which are 'new' or different from how this was failing before this fix. I submitted a PR for review. Meanwhile the 'new failure' I would address in a new ticket.

New failure:
{noformat}
------------------------------------------------------------------------------------------------------------------------------ Captured stdout setup ------------------------------------------------------------------------------------------------------------------------------
15:47:37,189 ccm DEBUG Log-watching thread starting.
------------------------------------------------------------------------------------------------------------------------------- Captured log setup --------------------------------------------------------------------------------------------------------------------------------
15:47:37,151 conftest INFO Starting execution of test_populate_mv_after_insert_wide_rows at 2020-06-03 15:47:37.151758
15:47:37,152 dtest_setup INFO cluster ccm directory: /tmp/dtest-3_i9j_ku
-------------------------------------------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------------------------------------------
15:47:51,930 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.3 dc1> discovered
15:47:51,930 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.2 dc1> discovered
15:48:24,880 cassandra.protocol WARNING Server warning: Materialized views are experimental and are not recommended for production use.
15:48:25,920 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.3 dc1> discovered
15:48:25,921 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.2 dc1> discovered
15:48:25,947 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.3 dc1> discovered
15:48:25,947 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.1 dc1> discovered
15:48:25,984 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.1 dc1> discovered
15:48:25,984 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.2 dc1> discovered
15:48:27,973 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.3 dc1> discovered
15:48:27,973 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.2 dc1> discovered
15:48:28,108 cassandra.protocol WARNING Server warning: Aggregation query used without partition key
---------------------------------------------------------------------------------------------------------------------------- Captured stdout teardown -----------------------------------------------------------------------------------------------------------------------------
15:48:28,320 ccm DEBUG Log-watching thread exiting.
------------------------------------------------------------------------------------------------------------------------------ Captured stdout setup ------------------------------------------------------------------------------------------------------------------------------
15:48:29,170 ccm DEBUG Log-watching thread starting.
------------------------------------------------------------------------------------------------------------------------------- Captured log setup --------------------------------------------------------------------------------------------------------------------------------
15:48:29,134 conftest INFO Starting execution of test_populate_mv_after_insert_wide_rows at 2020-06-03 15:48:29.134170
15:48:29,134 dtest_setup INFO cluster ccm directory: /tmp/dtest-a84sugqx
-------------------------------------------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------------------------------------------
15:48:43,900 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.3 dc1> discovered
15:48:43,900 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.2 dc1> discovered
15:49:16,846 cassandra.protocol WARNING Server warning: Materialized views are experimental and are not recommended for production use.
15:49:17,911 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.3 dc1> discovered
15:49:17,911 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.2 dc1> discovered
15:49:17,946 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.3 dc1> discovered
15:49:17,946 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.1 dc1> discovered
15:49:17,977 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.1 dc1> discovered
15:49:17,977 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.2 dc1> discovered
15:49:20,349 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.3 dc1> discovered
15:49:20,350 cassandra.cluster INFO New Cassandra host <Host: 127.0.0.2 dc1> discovered
15:49:20,614 cassandra.protocol WARNING Server warning: Aggregation query used without partition key
---------------------------------------------------------------------------------------------------------------------------- Captured stdout teardown -----------------------------------------------------------------------------------------------------------------------------
15:48:28,320 ccm DEBUG Log-watching thread exiting.
---------------------------------------------------------------------------------------------------------------------------- Captured stdout teardown -----------------------------------------------------------------------------------------------------------------------------
15:48:28,320 ccm DEBUG Log-watching thread exiting.
---------------------------------------------------------------------------------------------------------------------------- Captured stdout teardown -----------------------------------------------------------------------------------------------------------------------------
15:49:21,329 ccm DEBUG Log-watching thread exiting.
===Flaky Test Report===

test_populate_mv_after_insert_wide_rows failed (1 runs remaining out of 2).
	<class 'AssertionError'>
	assert 12 == 0
 +  where 12 = Row(count=12).count
	[<TracebackEntry /home/bereng/work/repos/bdpWS/mvdtesttest/materialized_views_test.py:350>, <TracebackEntry /home/bereng/work/repos/bdpWS/mvdtesttest/materialized_views_test.py:171>]
test_populate_mv_after_insert_wide_rows failed; it passed 0 out of the required 1 times.
	<class 'AssertionError'>
	assert 1 == 0
 +  where 1 = Row(count=1).count
	[<TracebackEntry /home/bereng/work/repos/bdpWS/mvdtesttest/materialized_views_test.py:350>, <TracebackEntry /home/bereng/work/repos/bdpWS/mvdtesttest/materialized_views_test.py:171>]

===End Flaky Test Report===
=========================================================================================================================== 1 failed in 105.02 seconds ============================================================================================================================
{noformat}

Mmm the fix is no good {{ViewSchemaTest}} blocks now. So there might be legit cases for node schema agreement while the schema is being worked on. This needs further digging.

After speaking with [~jasonstack] we agreed this race condition is in the nature of the beast. It won't accommodate a quick and easy fix so better move it off 4.0 to be picked up later at a better time. Meanwhile a skip for the flaky failing test will be added.

