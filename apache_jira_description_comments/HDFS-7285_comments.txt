Thanks [~whjiang] for reporting this JIRA.

I'm attaching the first draft of the design doc and will be more than happy to incorporate feedbacks under this JIRA.

I'd also like to invite the community to a meeting at Cloudera headquarter (1001 Page Mill Road, Palo Alto) to discuss the design in more detail. My tentative plan is to hold it in the morning of the coming Friday (Oct. 31st). Please let me know if you are interested and if so, your availability. After soliciting feedbacks I will confirm logistic details, including the remote meeting URL.

Thanks Zhe for hosting the session.
bq.After soliciting feedbacks I will confirm logistic details, including the remote meeting URL.
Sounds great to have a local meeting to discuss the design and it can be efficient. For those that are not convenient to join it and the remote meeting, please provide the feedback and the logistic details here for broader further discussion. Thanks.

Particularly, as our design targets to support EC (erasure code) layered on top of HSM feature, it would be great to discuss this aspect to address questions like: what's the overall status of HSM? what's the gap to boost the EC support? Can we align between the two efforts? And etc.


A meeting has been scheduled:
* When: Friday Oct. 31st 10am~12pm
* Where: Cloudera Headquarter, 1001 Page Mill Road, Palo Alto. Both the lobby (for guests check-in) and the meeting room (Hadoop) are in building #2
* URL: https://cloudera.webex.com/cloudera/j.php?MTID=me26394d0a3559c7a9498f18ad7de8962
* Call-in: 1-650-479-3208 (US/Canada) with access code: 290 472 605. 

Please drop me a note (zhezhang@cloudera.com) if you prefer a different time.

Thanks [~drankye] for the suggestion. The interface of the erasure coding feature potentially has a close relationship with HSM (HDFS-2832) and archival storage (HDFS-6584). We'll make sure to cover this topic in the meeting and share the summary here.

Hi Zhe,

Thanks for organizing the meeting, and thanks all the folks for attending, it's a good one!

One thing I forgot to mention, we need to add a section in the design spec about the impact to distcp. There are multiple things to be addressed (I do not mean to make the list complete):
* copy between two clusters, one with EC support and one without, 
* should we restore data before copying or just copying the coded blocks.  The latter may not be feasible since the coded data may contain other files that are not to be copied; Even if the coded blocks belongs to the file to be copied, copying it to a target directory not tagged as EC  means some handling there. And copying to a cluster not supporting EC need special care too. 
* distcp has switch to preserve block size, if this switch is not specified, a different block size may be used at target, we need special handling here.
...

Thanks.


Hi all, here are some discussion notes from today's meeting. Thanks everyone for attending, and Zhe for presenting.

Attendees: Yongjun, Dave Wang, ATM, Eddy, Jing, Zhe, Matteo, Charles, Suresh, Todd, Michael C, Colin, Govind, myself

h2. EC cost
* Suresh: EC is much more CPU/network intensive than replication. Sanjay talked to Dhruba at FB and they didn't erasure code more than 10-20% of their data because of the impact on MR workloads.
* Todd: Systems like QFS and Colossus EC 100% of their data though, which shows this is possible. They have fast networks, which makes this possible.
* Andrew: Can also offload work to dedicated erasure coding nodes rather than doing it on DNs, like Facebook f4
* Suresh/ATM: Higher-level point, it would be good to have a way of delaying replication/EC work when an archival node or rack dies. There's already work for a "maintenance mode" for DNs, would be good to have the same thing at a rack-level. Then automatically kick an archival node or rack into maintenance mode on detecting a failure.
* Overall theme was that having throttling of EC work is a basic requirement, which is provided for in the design doc.

h2. Distributing work
* Suresh: Possible for lots of EC work and tracking to stack up at the NN. Would be good to push some of these responsibilities down to the DNs, let groups of DNs figure out EC work by themselves without NN involvement.
* Jing: Copyset work is related.
* Todd: Striping might alleviate NN load concerns, since it's a simpler in terms of metadata and coordination.
* This doesn't need to happen in the first cut, but just something to keep in mind for the future. This idea is being examined more generally for the blobstore work.

h2. Quotas
* This is an open issue. No quick agreement in the room whether to charge users for the replicated cost, or the EC cost.
* Charging the EC cost is good since it encourages users to save space
* However, can get in strange scenarios where user is at quota when things are EC'd, tries to append or otherwise convert to replication, are now over quota.

h2. Policy
* Agreement that StoragePolicy is currently an inflexible entity, hardcoded specification of what StorageTypes to use.
* Need some higher-level blob of code that looks at file attributes, access patterns, other higher-level metadata, and then based on that chooses the StorageType for the data.
* Ideally, flow looks like (file attributes and metadata) -> (policy engine) -> (data temperature) -> (storage types / EC / compression to use based on what's present in cluster).
* User would not be allowed to manually set the data temperature, but could query it. This prevents users and the policy engine from fighting each other.
* Users could possibly set some kind of "force" xattr though, which the policy engine would respect.
* Issue of keeping the policy consistent. Things like the balancer and mover need to be aware of the policy so they don't fight it. How is the policy distributed if it's not hardcoded, and is some code blob? Ultimately would be good to move these responsibilities back into the NN.
* Question of what if anything needs to be changed in branch-2.6. Since custom StoragePolicies are not allowed, we should be good as long as whatever future policy engine respects the current settings.

* AI: rename StoragePolicy to StorageTag or StoragePolicyTag or some other name.
* AI: potentially rename StoragePolicy#SSD to something else, naming confusion with StorageType#SSD
* AI: Potentially still some more discussion about naming and interfaces to be done. Might want a more complex struct for StorageTag to also encompass EC/replication, compressed/not, the different ways things can be EC or compressed.

h2. Misc other notes
* Suresh: in archival storage, data goes from hot to cold and never back, good simplification
* Suresh: Important to document the hardware / network topology requirements, i.e. RS(10,4) needing 14 racks.
* Thinking is that 90% of data on the cluster will be EC'd, which means failures are going to have a very large impact on performance. Hard to predict what will happen.
* Definitely need to handle small files well too, since most data is also <1 full block, like ~40MB.
* Suresh/Andrew: would be cool to rewrite BlockPlacementPolicy to be more general, handle the little twists that are necessary for node-striping, rack-striping, etc.

I'll file JIRAs for the above action items for further discussion.

I filed HDFS-7317 for renaming StoragePolicy, HDFS-7318 for renaming some policies in the default StoragePolicySuite that are named after a StorageType.

I think HDFS-7317 is also a reasonable place to further discuss the policy logic / StorageTag semantics. Since this stuff is currently in branch-2.6, it would be good to figure out any potentially incompatible changes so we don't block the release.

Thanks [~andrew.wang] for the great summary! Many helpful suggestions were brought up in the meeting, thanks everyone for attending. 

I will post an updated design doc to incorporate the feedbacks pretty soon. If you have any additional comments please post on the JIRA.

[~yzhangal] and I had an offline discussion. Erasure coding could reduce the data locality for distcp, similar to how it impacts other MapReduce jobs. This shouldn't be a significant performance degradation for warm/cold data.

{quote}
* User would not be allowed to manually set the data temperature, but could query it. This prevents users and the policy engine from fighting each other.
* Users could possibly set some kind of "force" xattr though, which the policy engine would respect.
{quote}

If a user (application) is aware - through its own statistics - of data temperature, it should be possible to hint this to the policy engine. 

How do applications plug into the policy engine machinery?

bq. Ideally, flow looks like (file attributes and metadata) -> (policy engine) -> (data temperature) -> (storage types / EC / compression to use based on what's present in cluster).

Why wouldn't the policy engine have the last word? I.e.  (file attributes and metadata, data temperature, cluster configuration metadata) -> (policy engine)

I think this feature development should go in a branch. Lets create a branch for this development?

Hi Uma, Yes, we will need the branch considering the changes involved in this feature.
I will create a branch soon and post it here. thanks

I have created the branch "HDFS-EC"

named the branch as "HDFS-EC" just to recognize the branch easily :)

Thanks

[~umamaheswararao] [~vinayrpet] Thanks, I agree we certainly need a feature branch for this. 

Thanks everyone for having the branch. We're currently working on the breakdown and sub tasks for the following implementation. 

Hi [~apurtell], I think you have good points. As the mentioned policy engine might not be very related to or coupled with the EC feature, we might be better to have separate JIRA to discuss and implement it. I will create one and track your points there. Thanks.

Hi [~andrew.wang], to address the policy engine aspects as discussed, just opened HDFS-7343 A comprehensive and flexible storage policy engine. It maybe not very coupled with this. Thanks.

[~zhz], can you please post an update design document for this work that considers comments from the meeting we earlier had?

[~sureshms] Sure, we are working on an updated design document and will post it soon.

Based on feedbacks from the meetup and a deeper study of file size distribution (detailed report to be posted later), *Data Striping* is added to this updated design, mainly to support EC on small files. A few highlights compared to the first version:
# _Client_: extended with striping and codec logic
# _NameNode_: {{INodeFile}} extended to store both block and {{BlockGroup}} information; optimizations are proposed to reduce memory usage caused by striping and parity data
# _DataNode_ remains mostly unchanged from the original EC design
# Prioritizing _EC with striping_ as the focus of the initial phase, and putting _EC with contiguous (non-striping) layout_ to a 2nd phase

To motivate and guide the design (especially data striping), we have analyzed several production clusters and generated what-if scenarios with different policies. Please refer to the attached report for full details. 

Hi Zhe Zhang. Do you plan to publish the tool you used to analyze the fsimage?

[~raviprak] Thanks for your interest. Sure, I can publish the program in a few days, after some basic code cleaning.

Thanks for posting the design doc.  It looks really nice!  Some comments/questions:
- Is a BlockGroup only used by one file, i.e. it cannot be shared by servel files?
- Suppose the answer to the above question is yes.  Then, how to encode small files?
- HDFS-3107 "HDFS truncate" is now committed.  We should revisit it.  (EC with truncation is still a non-goal in the design doc.)
- "Due to the complexity we will not support hflush or hsync for EC files at this phase."  Then what will happen if users call hflush/hsync?

- HDFS upgrade is not covered in the doc.  More specifically, if there are existing blocks using the block id's reserved for BlockGroup, how to upgrade the cluster?  And how to rollback?

Attaching the Python scripts used to generate the fsimage analysis report.

[~szetszwo] Thank you for reviewing the design doc; great questions!

bq. Is a BlockGroup only used by one file, i.e. it cannot be shared by servel files?
Each BlockGroup is used by only one file. In general, bundling multiple files in a single erasure coding stripe/group complicates file deletions.

bq. Suppose the answer to the above question is yes. Then, how to encode small files?
That's the benefit of striping: when divided into small units (64KB by default) and striped to multiple servers, even small files can be encoded. The fsimage analysis report quantifies the difference in space saving between striping and the traditional contiguous data layouts.

bq. "HDFS truncate" is now committed. We should revisit it. (EC with truncation is still a non-goal in the design doc.)
Good point. I will update the design doc to address this. We can either disallow truncate for encoded files, or convert the file (or at least the last block) into replication before truncating.

bq. "Due to the complexity we will not support hflush or hsync for EC files at this phase." Then what will happen if users call hflush/hsync?
Similar as above, we can either return an error telling the user hflush/hsync is not supported on the target file, or convert the file first (which sounds like too slow for frequent flush/sync operations). An ongoing optimization is to leverage the _incremental encoding_ feature from Intel's Storage Acceleration Library (ISAL) to flush parity data from partial stripe, and updating the parity data when more data is available.

bq. HDFS upgrade is not covered in the doc. More specifically, if there are existing blocks using the block id's reserved for BlockGroup, how to upgrade the cluster? And how to rollback?
This is a really good catch. IIUC {{SequentialBlockIdGenerator#LAST_RESERVED_BLOCK_ID}} is for this purpose? We can just divided the _unreserved_ ID space into regular blocks and BlockGroups. The latest patch under HDFS-7339 implements this logic ({{SequentialBlockIdGenerator}} and {{SequentialBlockGroupIdGenerator}}.

> ... We can just divided the unreserved ID space into regular blocks and BlockGroups. ...

Let me clarify my question.  The unreserved ID space currently is used only by blocks.  After the division, the IDs for BlockGroup could possibly be already used by some existing blocks.  How to upgrade the cluster to fix this problem?

> That's the benefit of striping: when divided into small units (64KB by default) and striped to multiple servers, even small files can be encoded. The fsimage analysis report quantifies the difference in space saving between striping and the traditional contiguous data layouts.

I thought the strip size is 1MB according to the figure in [Data Striping Support in HDFS Client|https://issues.apache.org/jira/secure/attachment/12687886/DataStripingSupportinHDFSClient.pdf].  Anyway, small files (say < 10MB) may not be EC at all since the disk space save is not much and the namespace usage is increased significantly. 

BTW, the fsimage analysis is very nice.  There are two more types of cost not covered, CPU overhead and replication cost (re-constructing the EC blocks).  How could we quantify them?

Some more questions:
- The failure cases for write seems not yet covered -- what happen if some datanodes fails during a write?
- Datanode failure may generate a EC strom -- say a datanode has 40TB data, it requires accessing 240TB data for recovering it.  It is in the order of PB for rack failure.  How could we solve this problem?

bq. The unreserved ID space currently is used only by blocks. After the division, the IDs for BlockGroup could possibly be already used by some existing blocks. How to upgrade the cluster to fix this problem?
OK I understand the issue now. Do you know when HDFS started to use sequentially generated (rather than random) block IDs -- starting from which version? I guess we can still attempt to allocate block group IDs from the second half of the unreserved ID space, and we check for conflicts in each allocation; if there is a conflict we move the pointer forward to the conflicting ID. In that scenario, to tell if a block is regular or striped, we need to parse out the middle part of the ID and check if it exists in the map of block groups. 

bq. I thought the strip size is 1MB according to the figure in Data Striping Support in HDFS Client.
Good catch. I need to update that design doc to match the 64KB default stripe cell size. BTW 1MB is the I/O buffer size (see parameters _C_ and _B_ on page 8 of the [master design doc | https://issues.apache.org/jira/secure/attachment/12687803/HDFSErasureCodingDesign-20141217.pdf].

bq. Anyway, small files (say < 10MB) may not be EC at all since the disk space save is not much and the namespace usage is increased significantly.
I agree it doesn't make much sense to encode files as small as a few MB. The current fsimage analysis didn't further categorize files under 1 block. But my guess is they only contribute a minor portion of space usage in most clusters. I'll try to run the analysis again to verify.

bq. BTW, the fsimage analysis is very nice. There are two more types of cost not covered, CPU overhead and replication cost (re-constructing the EC blocks). How could we quantify them?
Thanks and I really like the suggestion. CPU and I/O bandwidth usage is hard to simulate in a simple analyzer. We'll make sure it's included in the real system test plan.

bq. The failure cases for write seems not yet covered – what happen if some datanodes fails during a write?
I believe [~libo-intel] will share more details under HDFS-7545 soon. There is a range of policies we can adopt, the most strict being to return I/O error when any one target DN fails. In a "smarter" policy. the application can keep writing until _m_ DNs fail, which _m_ is equal to the number of parity blocks in the schema.

bq. Datanode failure may generate a EC strom – say a datanode has 40TB data, it requires accessing 240TB data for recovering it. It is in the order of PB for rack failure. How could we solve this problem?
I think this is an inevitable challenge with EC. The best we can do is to schedule EC recovery tasks together with {{UnderReplicatedBlocks}} with appropriate priority settings. This way blocks are recovered when the system is relatively idle. When lost blocks are accessed it can recovered on-the-fly, but traffic from online recovery should be much lower.

> ... Do you know when HDFS started to use sequentially generated (rather than random) block IDs – starting from which version? ...

It was done by HDFS-4645.

After some discussion with Jing, we think that block group ID is not needed at all -- we only need to keep the block group index within a file.  Will give more details later.

Thanks for clarifying.

bq. After some discussion with Jing, we think that block group ID is not needed at all – we only need to keep the block group index within a file. Will give more details later.
This is [discussed | https://issues.apache.org/jira/browse/HDFS-7339?focusedCommentId=14289868&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14289868] under HDFS-7339.

From HDFS-7353, posted by [~szetszwo], suggesting we use 'erasure' package name instead of 'ec'.
bq. ec also can mean error correcting. How about renaming the package to io.erasure? Then, using EC inside the package won't be ambiguous.
I'm not sure about this, but we'd better discuss this overall and have the conclusion. If decided, we should use it consistently in places regarding design, discussion, codes and etc. Currently, we all use EC/ec to mention about erasure coding. Does it conflict with error correction? Is there any related work about error correction? If not, I guess we could still use EC as we might not wish to change all the places. A better naming is good, being consistent is important for the big effort.

[~zhz], how about using  'erasure' package name instead of 'ec' since ec is ambiguous -- it could also mean error correcting (or elliptic curve)?

BTW, would you mind to share the design doc in a editable format?  Otherwise, I am going to post a new revised design doc.

I'm not an English speaker, but I guess we could also find many meanings for 'erasure' too. The perfect naming to resolve any ambiguity would be 'erasure coding' or 'erasure code' but they're a little verbose for a package name. When simplified form like 'ec' is desired, we are sure and able to find other meanings for it like the cases you mentioned, but does that mean we shouldn't use it? If so, we can probably find many cases in the world.

I might be bad to express what I thought, but I do not think it makes the great sense to change the overall design just for this. Would anyone give more thoughts?

I searched the code base and didn't find any existing package named 'ec' so there won't conflict actually.
As you might note:
1. In this JIRA descrition for this effort, it's explicitly stated as below:
 bq. Erasure Coding (EC) can greatly reduce the storage overhead without sacrifice of data reliability
2. Our branch is also named HDFS-EC.

If we do have other efforts for error correcting (or elliptic curve) for the project, then it's the point to avoid using 'ec' package name since it's already used here in this effort for 'erasure coding'.

> ... but I guess we could also find many meanings for 'erasure' too. ...

Since it is under the io package, it is much harder to have an ambiguous meaning.  However, io.ec still possibly means for error correcting (or arguably elliptic-curve cryptography).  Won't you agree that a two-letter acronym "ec" is much more ambiguous than the word "erasure"?

> In this JIRA descrition for this effort, it's explicitly stated as below:

Yes, it is very clear in the current context since there is no other project for error correcting.  But the package name sits there forever.  It we have a error correcting project later on, it may become a problem.

> Our branch is also named HDFS-EC.

Branch is temporary and invisible to users, we may delete, rename and reuse branch name as some point.  It is much harder to change package names since it is an incompatible change.

The "io.erasure" package name is just a minor suggestion.  I am fine if you insist using "io.ec" although I think it may lead to some unnecessary confusion.

I created a Google [doc | https://docs.google.com/document/d/12YbFDFQGJkx9aCPmtJVslgCIWC6ULnmJQxrIm-e8XFs/edit?usp=sharing] which is editable. If possible please login so I know who's making each comment and update.

[~szetszwo] The doc was last updated in mid December and doesn't contain some of the latest updates (mainly from HDFS-7339). If you don't mind the wait I plan to finish updating it before Wednesday. You can also go ahead with your updates assuming the HDFS-7339 discussions were incorporated.

Package naming is an interesting topic. *Erasure* doesn't sound very appropriate because it literally means "the act of erasing something" and is a bit ambiguous itself.  Actually erasure coding is [a type of error correction codes | http://en.wikipedia.org/wiki/Forward_error_correction#List_of_error-correcting_codes] so we don't need to worry about the conflict with "error correction". The only way to decrease ambiguity in general is to enlengthen the abbreviation. Two potential candidates came to mind: *ecc* standing for "error correction codes"; or *erc* "standing for "erasure coding" more specifically. Thoughts?

> I created a Google doc which is editable. If possible please login so I know who's making each comment and update.

Thanks for share it.  I do suggest that we only share it with the contributors who have intention to edit the doc.  Anyone who wants to edit the doc should send a request to you.  It could prevent accidental changes from someone reading the doc.  Sound good?

> ... If you don't mind the wait I plan to finish updating it before Wednesday. ...

Happy to wait.  Please take you time.

I will think more about the package name.

How about io.erasure_code?  Intel ISA-L also uses erasure_code as the directory name and library name.

It's good to use 'erasure_code' for a c library directory name, but not so elegant for a Java package name. How about "erasurecoding" in full.

The rational to use "erasurecoding" or "erasurecode" is simple, if no abbreviation sounds comfortable, then use the full words.

I am fine to use "erasurecode" although I prefer "erasure_code".

Thanks for your confirm. I would use "erasurecode" in the style like ones I found in the codebase, "datatransfer", "blockmanagement" and many. 

Hi [~zhz]

Design doc updates might be required for some points.

1. DISK-EC was added with the intention of identifying the Parity blocks in case of Non-strip encoding. But now, IMO the logical storage type DISK-EC is no longer required, as BLOCK can be identified either Parity/original using the BlockGroup.
2. blockStoragePolicydefault.xml is no longer in the code base and storage policies are no longer user configurable. It was removed before merging the HDFS-6584 to trunk, instead all the policies are hardcoded into BlockStoragePolicySuite.java
3. {quote}Transition between erasurecoded and replicated forms can be done by changing the storage policy and triggering the Mover to enforce the new policy.{quote}
I think this is not applicable for the striped design. This should be completely controlled by ECManager right?

4. {quote}Under this framework, a unique storage policy should be defined for each codec schema. For example, if both 3of5 and 4of10 ReedSolomon coding are supported, policies RS3of5 and RS4of10 should be defined and they can be applied on different paths.{quote}
This also may not be applicable for the striped design, as schema information also will be saved inside the BlockGroup itself. So IMO there is no need of separate policies for each of the schema.

Thanks [~szetszwo] for the suggestion. The Google [doc | https://docs.google.com/document/d/12YbFDFQGJkx9aCPmtJVslgCIWC6ULnmJQxrIm-e8XFs/edit?usp=sharing] has been updated with limited permission. Please let me know if you'd like to be added as an editor. Note that [~jingzhao] proposed and arranged an offline discussion today with Nicholas and myself. I'll make another update to the doc this afternoon.

[~vinayrpet] Good points on the storage policy section. Please take a look at the updated design doc.

bq. This also may not be applicable for the striped design, as schema information also will be saved inside the BlockGroup itself. So IMO there is no need of separate policies for each of the schema.
This is also related to the [discussion | https://issues.apache.org/jira/browse/HDFS-7349?focusedCommentId=14279639&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14279639] under HDFS-7349. Maybe we should wrap up that JIRA and incorporate the conclusion in the next rev of the design?

We had a very productive meetup today. Please find a summary below:
*Attendees*: [~szetszwo], [~zhz], [~jingzhao]

*NameNode handling of block groups* (HDFS-7339):
# Under the striping layout, it's viable to use the first block to represent the entire block group.
# A separate map for block groups is not necessary; {{blocksMap}} can be used for both regular blocks and striped block groups.
# Block ID allocation: we will use the following protocol, which partitions the entire ID space with a binary flag
{code}
Contiguous: {reserved block IDs | flag | block ID}
Striped: {reserved block IDs | flag | reserved block group IDs | block group ID | index in group}
{code}
# When the cluster has randomly generated block IDs (from legacy code), the block group ID generator needs to check for ID conflicts in the entire range of IDs generated. We should file a follow-on JIRA to investigate possible optimizations for efficient conflict detection.
# To make HDFS-7339 more trackable, we should shrink its scope and remove the client RPC code. It should be limited to block management and INode handling.
# Existing block states are sufficient to represent a block group. A client should {{COMMIT}} a block group just as a block. The {{COMPLETE}} state needs to collect ack from all participating DNs in the group.
# We should subclass {{BlockInfo}} to remember the block group layout. This is an optimization to avoid frequently retrieving the info from file INode.

*EC and storage policy*:
# We agreed that _EC vs. replication_ is another configuration dimension, orthogonal to the current storage-type-based policies (HOT, WARM, COLD). Adding EC in the storage policy space will require too many combinations to be explicitly listed and chosen from.
# On-going development can still use HDFS-7347, which embeds EC as one of the storage policies (it has already been committed to HDFS-EC). HDFS-7337 should take the EC policy out from file header and put it as an XAttr. Other EC parameters, including codec algorithm and schema, should also be stored in XAttr
# HDFS-7343 fundamentally addresses the issue of complex storage policy space. It's a hard problem and should be kept separate from the HDFS-EC project.

*Client and DataNode*:
# At this point the design of HDFS-7545 -- which wraps around the {{DataStreamer}} logic -- looks reasonable. In the future we can consider adding a simpler and more efficient output class for the _one replica_ scenario.

We also went over the *list of subtasks*. Several high level comments:
# The list is already pretty long. We should reorder the items to have better grouping and more appropriate priorities. I will make a first pass.
# It seems HDFS-7689 should extend the {{ReplicationMonitor}} rather than creating another checker.
# We agreed the best way to support hflush/hsync is to write temporary parity data and update later, when a complete stripe is accumulated.
# We need another JIRA for truncate/append support.

Thanks for posting the meeting note.  The meeting was very productive!

> ... The COMPLETE state needs to collect ack from all participating DNs in the group.

It should be collect ack from minimum number of DNs required for reading the data.  E.g. the min is 6 for (6,3)-Reed-Solomon.

{quote}
Good catch. I need to update that design doc to match the 64KB default stripe cell size. BTW 1MB is the I/O buffer size (see parameters C and B on page 8 of the master design doc 
{quote}
I think we need to  allow dynamic stripe cell size depends on the file size. If we only use small fixed value, for example 64KB as the stripe cell size, then for large file, we need much more ec block groups to store the entire file than the number of blocks we need using replication way, even as implemented in HDFS-7339, we only store the first ec block of the ec block groups in NN, but the NN memory consumption is a big issue since there are too many ec block groups. 

bq. If we only use small fixed value, for example 64KB as the stripe cell size, then for large file, we need much more ec block groups to store the entire file than the number of blocks we need using replication way,
The number of block groups is actually unrelated to the cell size (e.g. 64KB). For example, under a 6+3 schema, any file smaller than 9 blocks will have 1 block group.

A smaller cell size better handles small files. But data locality is degraded -- for example, it might be hard to fit MapReduce records into 64KB cells.

bq.I think we need to allow dynamic stripe cell size depends on the file size.
Good idea. Small strip cell size for small files in a zone, and large strip cell size for large files in another zone. For MR or data locality sensitive files, use larger cell size. As we're going to support various stripping and EC forms by configurable schema and file system zones, different stripping cell size is possible I guess. 

{quote}
The number of block groups is actually unrelated to the cell size (e.g. 64KB). For example, under a 6+3 schema, any file smaller than 9 blocks will have 1 block group.
A smaller cell size better handles small files. But data locality is degraded – for example, it might be hard to fit MapReduce records into 64KB cells.
{quote}
I think it's incorrect for normal file. For example, we have a file, and it's length is 128M. If we use 6+3 schema, and ec stripe cell size is 64K, then we need (128*1024K)/(6*64K) = 342 block groups. But if the ec stripe cell size is 8M, then we need 128/6*8 = 3 block groups. 
Obviously, small stripe cell size will cause much more NN memory for normal/big file, even we only store the first ec block of the ec block groups in NN. 

{quote}
Small strip cell size for small files in a zone, and large strip cell size for large files in another zone
{quote}
Right, for large file, using large stripe cell size can decrease NN memory consumption. Otherwise, ec feature will cause big issue for NN memory.
BTW, I have one thing not very clear, we need the concept of "zone"? The definition of "zone" is?

Yes we have EC zones, each zone actually represents a folder path and associates with an EC schema. Using the schema all the files in the zone will be in the form defined by it.

bq. I think it's incorrect. For example, we have a file, and it's length is 128M. If we use 6+3 schema, and ec stripe cell size is 64K, then we need (128*1024K)/(6*64K) = 342 block groups. 
Aah I see where the confusion came from. Sorry that the design doc didn't explain clearly the different parameters. When the client writes to a striped file, the following 3 events happen:
# Once the client accumulates 6*64KB data, it does _not_ flush the data to the DNs. The client buffers the data and starts buffering the next 6*64KB stripe.
# Once the client accumulates {{1024 / 64 = 16}} stripes -- that is 1MB for each DN -- it flushes out the data to DNs.
# Once the data flushed to each DN reaches 128MB -- that is {{128MB * 6 = 768MB}} data overall -- it allocates a *new block group* from NN.

Section 2.1 of the QFS [paper | http://www.vldb.org/pvldb/vol6/p1092-ovsiannikov.pdf] has a pretty detailed explanation too. 

{quote}
Yes we have EC zones, each zone actually represents a folder path and associates with an EC schema. Using the schema all the files in the zone will be in the form defined by it.
{quote}
OK, I see. It's fine for me. It's similar with storage policies for directories and files, and I think we don't need the concept of zone here. It's impressed me that we have restriction for the zone, for example, for an encryption zone, the files can't rename to other folders outside the zone and so on.

I'm not sure storage policy can cover all the cases and forms we're going to support considering strip support. I guess EC zone might not hurt. You're right about restriction for a EC zone, yes a file in a zone should not move outside or elsewhere without necessary transforming first.

{quote}
# Once the client accumulates 6*64KB data, it does not flush the data to the DNs. The client buffers the data and starts buffering the next 6*64KB stripe.
# Once the client accumulates 1024 / 64 = 16 stripes – that is 1MB for each DN – it flushes out the data to DNs.
# Once the data flushed to each DN reaches 128MB – that is 128MB * 6 = 768MB data overall – it allocates a new block group from NN.
{quote}
Yes, it makes sense now. Thanks.


*== Update ==*

I'm happy to update that Huawei has also interest in this erasure coding support and some engineering resources from the company will join the effort. They're most welcome. With their dedicated contributions we're sure to be able to move even faster. Today we had a PRC local meeting with Huawei engineers. The *attendees*: [~zhz] (Cloudera); [~hitliuyi], [~libo-intel] and [~drankye] (Intel); [~zhangyongxyz], [~tudandan] and some other team members (Huawei). They will start with two challenge tasks: 1) implementing EC customized block placement policy HDFS-7613; 2) investigating and implementing *Hitchhiker* erasure coding algorithm HDFS-7715. Thanks !


Revised the design doc as follows:
- Revised the sections for 
-* Saving I/O bandwidth
-* BlockGroup,
-* ErasureCodec,
-* ECClient, and
-* NameNode Memory Usage Reduction.
- Added new sections for
-* EC Writer,
-* Handling Datanode Failure during Write,
-* Reading a Closed File,
-* Reading a Being Written File,
-* Hflush,
-* Hsync,
-* Append,
-* Truncate,
-* BlockGroup States,
-* Generation Stamp,
-* BlockGroup Recovery,
-* EC Block Reconstruction,
-* Collision with Random Block ID.

Forgot to mention that the new design doc file is [HDFSErasureCodingDesign-20150204.pdf|https://issues.apache.org/jira/secure/attachment/12696562/HDFSErasureCodingDesign-20150204.pdf].

I tried to merge the trunk into the HDFS-EC branch and got a lot of conflicts. Looks like some of the trunk changes were applied manually to the EC branch? "git rebase origin/trunk" got several hundred commits diverge. Someone knows how to fix this? or we can create a different EC branch.

Since there are only a few patches committed, let's recreate the branch in order to fix the divergence.

[~jingzhao], [~szetszwo] I do the following every week (Monday) to merge trunk into HDFS-EC:
{code}
     git rebase apache/trunk
     git rebase apache/HDFS-EC
     git push apache HDFS-EC:HDFS-EC
{code}
Does it look correct? I just rebased again, didn't see any conflict.

Thanks [~szetszwo] for adding more details to the design doc. I see you are still editing so will hold on to my updates for now.

In general the added sections look good to me. I think they reflect what we discussed in the meetup. 

I didn't put too much detail on ECClient / EC Writer originally, because there's a separate [design | https://issues.apache.org/jira/secure/attachment/12687886/DataStripingSupportinHDFSClient.pdf] under HDFS-7545. When I make the next rev I can move all client related content there and refer it in the master design doc.

> ... I see you are still editing so will hold on to my updates for now.

Yes, just found some note which not yet added to the doc.

HDFSErasureCodingDesign-20150206.pdf:
- adds new sections for
-* Datanode Decommission, and
-* Appendix: 3-replication vs (6,3)-Reed-Solomon.

Thanks [~szetszwo] and [~zhz] a lot for taking care of and updating the main design doc.

bq.When I make the next rev I can move all client related content there and refer it in the master design doc.
Good point [~zhz]. Let's go this way as we previously discussed some time ago. The doc here is for the overall design and general considerations, with more focus on NameNode/ECManager part, for ECWorker, ECClient and ErasureCodec, it refers to their low level design docs attached elsewhere or to be attached here. I will take some time to update the design doc attached in HDFS-7337, move erasure codec details and consolidate all the related discussions into the doc. I think [~libo-intel] can do the similar thing for HDFS-7344. This way we could keep the overall design doc here relatively maintainable and easier readable. 

The current HDFS-EC branch needs double rebasing because both 'git merge' and 'git rebase' were used in its history. To resolve this issue and make maintenance a little easier, I will recreate the branch now.

Both 'git rebase' and 'git merge' have their own merits. Based on an offline discussion, [~andrew.wang] and myself prefer 'git rebase' and [~jingzhao] is OK with both. If you plan to contribute code and prefer 'git merge', please let me know. I usually import trunk changes into HDFS-EC every Monday, so let's reach an agreement within a week (before Feb. 16). Thanks!

Either 'git rebase' and 'git merge' is fine.  Thanks for taking care it.

We should keep "HDFS-EC" for a while.  How about using "HDFS-7285" as the name for the new branch?

bq. We should keep "HDFS-EC" for a while.
tszwo] Good point. I'd like to have keep using "HDFS-EC" to name the primary branch though. How about we rename the current branch as-is to "HDFS-EC-backup" or convert it to a tag?

bq.keep using "HDFS-EC" to name the primary branch
I would appreciate keeping "HDFS-EC" if it's possible or doable. It avoids we having to update relevant target/fix/affect versions in all the related issues. It would not also disturb our related discussions. Thanks !

> ... It avoids we having to update relevant target/fix/affect versions ...

We do not have to update individual JIRAs.  It is easy to rename HDFS-EC to something else.

> ... How about we rename the current branch as-is to "HDFS-EC-backup" or convert it to a tag?

It is fine if you really like "HDFS-EC".  It just looks different from most of other development branches, which simply use the JIRA number.

bq.It is easy to rename HDFS-EC to something else
Glad to know this. Thanks for clarifying this for me.
bq.It is fine if you really like "HDFS-EC". It just looks different from most of other development branches, which simply use the JIRA number.
It's great we can keep "HDFS-EC". I agree it looks kinds of different. "fs-encryption" is another exception.

The "branch-trunk-win", "fs-encryption" and "HDFS-EC" are the only exceptions.  There are 39 other branches use the JIRA number as the name (or the name prefix).  Why we do not follow the naming convention here?


I'm fine with changing to "HDFS-7285" to be consistent with other feature branches. So here's the summary:
# All future commits should go into HDFS-7285
# I will import trunk changes into HDFS-7285 with {{git rebase}} every Monday. If anyone else needs to rebase please do the same
# Let's keep HDFS-EC for a week to be safe. Meanwhile let's make all necessary changes, including JIRA fix/target version.

Sorry forgot to mention that the HDFS-7285 branch has been created with all HDFS-EC commits applied on top of the current trunk. [~drankye] and [~jingzhao] please let me know if I missed anything. Thanks!

Note: Since branch name changed HDFS-7285 now, I changed version name also HDFS-7285 in JIRA now. 

Thanks Uma! Seems it took care of the target version of all subtasks too. 

Per discussion above let's officially switch to the HDFS-7285 branch now. We have a nightly Jenkins job to monitor all incoming changes: https://builds.apache.org/job/Hadoop-HDFS-7285-nightly/

[~zhz], thanks for setting the Jenkins job!

I'm seeing a lot of conflicts when rebasing against trunk. Somehow git decides to re-apply HDFS-7723. Below is the output of {{git rebase -i apache/trunk}}.

{code}
  1 pick 5c27789 HDFS-7347. Configurable erasure coding policy for individual files and directories ( Contributed by Zhe Zhang )
  2 pick ae4e4d4 HDFS-7339. Allocating and persisting block groups in NameNode. Contributed by Zhe Zhang
  3 pick eb3132b HDFS-7652. Process block reports for erasure coded blocks. Contributed by Zhe Zhang
  4 pick 2477b02 Fix Compilation Error in TestAddBlockgroup.java after the merge
  5 pick 0ae52c8 HADOOP-11514. Raw Erasure Coder API for concrete encoding and decoding (Kai Zheng via umamahesh)
  6 pick f9e1cc2 HADOOP-11534. Minor improvements for raw erasure coders ( Contributed by Kai Zheng )
  7 pick c36a7a9 HADOOP-11541. Raw XOR coder
  8 pick 93fc299 Added the missed entry for commit of HADOOP-11541
  9 pick 2516efd HDFS-7716. Erasure Coding: extend BlockInfo to handle EC info. Contributed by Jing Zhao.
 10 pick e746443 HADOOP-11542. Raw Reed-Solomon coder in pure Java. Contributed by Kai Zheng
 11 pick 1611bb2 HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)
{code}

I'll just re-apply 1~10 on top of current trunk.

We had another meetup last Friday (2/20). Below is a summary, followed by a plan to generate a functional prototype.

*Attendees*: [~zhz], [~jingzhao], and [~drankye]

*Summary of BlockInfo extension*
# The following diagram illustrates the extension of {{BlockInfo}} to handle striped block groups (I recreated it from a whiteboard drawing). This is mainly contributed by Jing and thanks again for the great work!
{code}
                BlockInfo
               /   |     \
BlockInfoStriped   |      BlockInfoContiguous
       |           |            |
       |       BlockInfoUC?     |
       |       /         \      |
BlockInfoStripedUC       BlockInfoContiguousUC
{code}
# {{BlockInfoStriped}} and {{BlockInfoContiguous}} are already created under HDFS-7743 and HDFS-7716
# {BlockInfoStripedUC}} and {{BlockInfoContiguousUC}} are created under HDFS-7749. The current plan is to keep them separate despite the duplicate codes. A later effort will abstract out a common {{BlockInfoUC}} class.
# HDFS-7837 as well as part of HDFS-7749 handle persisting {{BlockInfo}} variants in multiple places:
#* BlockManager
#* INodeFile
#* FSImage
#* Editlog

*Remaining NameNode tasks*
# {{LocatedBlocks}} should be extended for striped reader (HDFS-7853)
# Initial XAttr structure for EC configuration (HDFS-7839)
# Other tasks, including HDFS-7369, do not block creating an initial prototype and should have a lower priority.

*DataNode high level thoughts*
# The NN will select a DN as the ECWorker in charge of recovering the lost data or parity block. That worker node might or might not be the same as the storage target (e.g., ECWorker should have powerful CPU)
# At this stage we should use a simple logic assuming ECWorker is the final target. It should construct the recovered block and store locally, before pushing to next targets if necessary

*EC policies*
# A set of default EC schemas should be embedded as part of HDFS
# An interface should be provided to define new EC schemas (either through command line or manipulate and refresh an XML file)
# EC and block layout (striping vs. contiguous) should be 2 orthogonal configuration dimensions: in the next phase we can enable contiguous+EC. At this phase we can assume striping layout when EC is enabled.

*Plan for a PoC prototype*
# An initial PoC prototype should contain the following features:
#* Configure a file to be stored in striping + EC format
#* Client requests to allocate and persist the striped block groups in NN
#* NN returns located striped block group
#* Client writes to the allocated DNs in striping fashion
#* NN correctly processes striped block reports
#* Blocks in the striped block group can go through the state machine of UC-COMMITTED-COMPLETE. {{UNDER_RECOVERY}} doesn't have to be supported at this stage.
#* Client can close the file
#* Client can read back the content correctly
#* _Optional_: File system states and metrics are correctly updated -- fsimage, edit logs, quota, etc.
# I think the following JIRAs should be resolved for the prototype:
#* HDFS-7749: need to fix a few Jenkins test failures
#* HDFS-7837
#* HDFS-7853
#* HDFS-7839
#* HDFS-7782

It's quite likely that the list is incomplete. So please feel free to add to it. Thanks!

Thanks [~zhz] a lot for scheduling the meetup discussion and the complete summary !
To make the first prototype complete and more solid, I guess we also need to incorporate HDFS-7349.

To follow up on the PoC prototype plan, I created a very rough test by manually applying the following patches, and it seems to work -- based on the [description | https://issues.apache.org/jira/browse/HDFS-7285?focusedCommentId=14339006&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14339006] above :)
# HDFS-7729 (this one needs major refactor after HDFS-7793)
# HDFS-7853
# HDFS-7782

A few bugs have been found and I'll post them under individual JIRAs.

Thank you for taking this and getting it work !!

This is the patch from trunk that was used in the PoC test. It demonstrates the changes we have made to support basic I/O in striping layout.

As discussed previously, I updated the document for the codec framework part, *Configurable and pluggable erasure codec* in HDFS-7337. Welcome more review and comments. Thanks !

I have a problem how to make EC files use specially designed placement policy. I need some help in decision making. Thanks. link HDFS-7068

Thanks [~walter.k.su] for raising the issue. I just commented with my thoughts there. Let's discuss there and see how it goes.

We have been discussing how to fit EC with other storage policies since the first [meetup | https://issues.apache.org/jira/browse/HDFS-7285?focusedCommentId=14192480&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14192480] and haven't reached a clear conclusion. This design is now blocking several ongoing JIRAs: HDFS-7068, HDFS-7349, HDFS-7839, HDFS-7866. I'd like to propose the following potential solution based on the ideas we have exchanged:

To reiterate the challenge: Multiple dimensions of storage policies could be applied to the same file. Across these dimensions we could have a large number of combinations -- easily over 50, could be over 100. Fitting them in a single dimension policy space is inefficient for the system to manage and inconvenient for admins to set / get.
* Storage-type preference: HOT / WARM / COLD
* Erasure coding schema: ReedSolomon-6-3 / XOR-2-1 (targeting 5~10)
* Block layout: Striping / contiguous
* Other potential policies, e.g. compression

We can setup a family of storage policy XAttrs, where each dimension can be independently set / get:
* {{system.hdfs.storagePolicy.type}}
* {{system.hdfs.storagePolicy.erasurecoding}}
* {{system.hdfs.storagePolicy.layout}}

Each dimension has a default value. So if an admin only wants to change the EC schema, the following command can be used. The {{getStoragePolicy}} should return policies on all dimensions unless an optional argument like {{-erasureCoding}} is used.
{code}
setStoragePolicy -erasureCoding RS63 /home/zhezhang/foo
getStoragePolicy /home/zhezhang/foo
{code}

Like the current storage policy semantics, the initial policy of a file or dir is inherited from its parent. Nested policy setting is allowed (/home is not ECed but /home/zhezhang is). A single file can have a storage policy without being in a zone.

Any feedbacks are very welcome. [~jingzhao], [~szetszwo], I think we should have another meetup to sync on this (and several other issues)?

I think we could pack all of this into a single xattr (i.e. {{system.storagePolicy}} as a protobuf. This will be more efficient, and also standardize the serde since xattrs values are just bytes.

We could also leave the storage type in the file header the way, since that's zero overhead, and just store the additional parameters into the xattr.

Thanks [~zhz] for the great post.
This documents existing relevant discussions well and gives a good proposal summary that unifies storage policy so EC and stripping can fit in in a much clean and elegant way. I would explicitly point out that in this way we would not use EC ZONE as previously discussed here and in other issues. We don't need to explicitly create and manage EC ZONEs. What is needed now is all about a storage policy for a file or folder. Once we all agree in this approach, we need to update the overall design here and rebase relevant issues as well. It would be great if we could gather as more as possible feedback and ideas this time.

[~andrew.wang] Thanks for the comment. I forgot to include those from our latest discussion. Yes, leaving the HSM policies as-is will work with this proposal and will just add some logic to combine data from XAttr and file header.

[~drankye] Good point. The semantics of EC configurations more closely resemble storage policies than _zones_. Like mentioned above, an EC policy can exist for a single file, and can be configured in a nesting manner.

Thanks for the summary, Zhe!

One question for the EC policy or EC ZONE is that whether we allow users to change the policy/schema of a file/dir. Currently for storage policy like COLD, WARM, and HOT, users can change a file/directory's policy and this change will be applied to new created/appended data, and can be enforced on existing files later by external tools like Mover. This lazy enforcement semantic also applies to renamed files.

However, for EC, since whether a file is EC'ed and its EC schema directly determines its read/write/append pattern, things become different and more complicated. If we allow changing the EC schema associated with a directory, we need to make sure for all the files inside its old EC schema can be found, which means we may need to associate the schema directly on the files or even blocks (which can be inefficient). And then how to handle new appended data and file rename also becomes challenge. If we disallow the schema changing or renaming across directories with different EC policies, in the end we may have a design like EC ZONE.

[~jingzhao] This is a great question to discuss and was missing in the above summary; thanks for bring it up!

In the initial design (page 6 of the latest [design doc | https://issues.apache.org/jira/secure/attachment/12697210/HDFSErasureCodingDesign-20150206.pdf]), EC policy changes will be lazily enforced by {{Mover}} or a similar tool. The rationales are:
# Conversion between replication and EC is a very important use case; so we do need to support changing EC policy on files and dirs
# The conversion should be done lazily for the same reason of lazily enforcing HSM policies: the purpose (saving space) is not urgent and the operation is expensive

bq. If we allow changing the EC schema associated with a directory, we need to make sure for all the files inside its old EC schema can be found, which means we may need to associate the schema directly on the files or even blocks (which can be inefficient).
Great point. Adding a little formality might help the discussion here. Essentially every file or dir has an _desired_ storage policy and an _actual_ one. Luckily, in the context of HSM we don't need to keep explicitly track of the _actual_ placement policy. EC policies are indeed more complicated in this perspective. I think we can solve it by doing the following:
# In the storage policy XAttr, always store the _actual_ policy instead of the desired one
# {{Mover}} (or a similar tool) should keep track of a queue of desired changes
# When converting an individual file, keep the old form until the block conversion is done. Then "flip" the XAttr
# Because of the above, when converting a directory we need to store the new policy XAttr in some of its files
# Appends should either be disallowed during a conversion, or with more advanced mechanism like appending to both old and new forms
# A renamed file should materialize and carry over the policy XAttr from the old dir. Then it will become a nested scenario, where the new dir has policy B and the moved file has policy A

Thanks for the comment, Zhe! Some comments inline:
bq. In the storage policy XAttr, always store the actual policy instead of the desired one

As you mentioned in #4, this may finally lead to the requirement that every file/block has to store its own "actual" storage policy. My main concern is that doing this in file level will lead to much harder management. Administrators have to check individual files to understand its storage scheme.

bq. Mover (or a similar tool) should keep track of a queue of desired changes

Considering Mover is just an external tool, if we use storage policy for EC files, clients (including Mover) will still directly talk to NN to set storage policy. And finally these desired changes still have to be handled/maintained by NN (or even a separate HDFS internal service).

bq. Conversion between replication and EC is a very important use case; so we do need to support changing EC policy on files and dirs

Agree. But different from HSM, where the migration only moves blocks across datanodes and keeps file->block metadata unchanged, the conversion between replication and EC will finally generate brand new blocks for the file. Therefore, a much simpler and maybe cleaner way for conversion can be just copying the files using the new scheme, deleting the old data after the copy , and renaming the new data using old names if necessary. This will not cause inefficiency since anyway we need to write new data. Also this can be simply adopted by a Mover like tool, and can avoid a lot of complexity when handling changes on files during the conversion.

In general, currently I prefer an EC-Zone design:
# Conversion is supported through copy
# EC policy is annotated on root directory of the zone as its XAttr
# No rename or EC schema change is allowed
This is very similar with specifying the EC schema in the volume level (if we later support volume).





Thanks Jing for the insights! I certainly agree there are non-trivial tradeoffs between these 2 options.

bq. Therefore, a much simpler and maybe cleaner way for conversion can be just copying the files using the new scheme, deleting the old data after the copy , and renaming the new data using old names if necessary. 
In that case, I guess the newly created file (with the old name) still needs to carry the new policy/schema? I don't see an easy way to avoid storing per-file policy info if the user chose to convert individual files between EC and replication forms. 

I think in most cases this conversion should happen in the directory level. For converting all the files contained in a directory, since all the files in the target directory (which can be a temporary directory before the later rename) share the same storage scheme, the storage schema only needs to be in the directory level if necessary. For converting an individual file from replication to EC, or between two EC schemas, in most use cases the file is just copied into another EC zone, and the schema should be already on the root of the zone.

[~zhz], Can you explain how to run your Python code? you don't have parameter specification.


Wow, why shows lots of repeated comments here?

[~azuryy] Good catch. The fsimage analyzer should be run following the steps below:
# Download both Python programs to the same directory
# ./ECAnalyzer <fsimage file name> <flag>, where the flag (-Dold or -Dnew) indicates whether the fsimage is in old (delimited) or new (xml) format.

I had another offline discussion with [~andrew.wang] around the storagePolicy vs. zone topic. We agreed that it's a difficult decision because it requires prediction of production usage patterns. The desired EC setup might not always align with directories. E.g., it is possible for a directory to contain both big files (suitable for striping) and small ones (will cause heavy NN overhead under striping). In this case, we can keep the directory policy to be non-EC, so only big files need to carry the EC policy in their XAttr -- it is a small NN overhead since only a small fraction of files are big. As a follow on optimization we can even setup a size-based policy for automatic conversion. I'll look at a few applications like HBase / Hive to get a better understanding.

I think we can follow an incremental development plan:
# We can start with a simple zone-like policy as Jing [proposed | https://issues.apache.org/jira/browse/HDFS-7285?focusedCommentId=14366293&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14366293] above. In this step we don't even need to fully implement the enforcement of zone constraints (empty directory, no nesting etc.).
# After collecting potential usage patterns (in terms of directory structure), we'll decide whether the use case of per-file and nested EC configuration is important enough. Based on that, we'll either fully implement zone constraints or implement fine grained EC policies.
# We'll finally decide whether and how to integrate with other storage policies

Thoughts?

Thanks for the update, Zhe! The plan sounds good to me. Let's finish the zone work first.

Good discussion and plan. But I'm still a little bit confused. [~zhz] was mentioning EC policies, and thinking about integrating them with other storage policies (HSM ones); [~jingzhao] said Let's finish the {{zone}} work first. What term or concept would we use as a final choice ? I'm worrying about this because it's kinds of messy, we need to choose one and use it consistently, update the overall design doc, sync with related issues. It also affects the implementation, for a example, {{setStoragePolicy}} or {{createZone}} for admin to set an EC policy for a directory...Let's have a conclusion. Thanks.

In my view, if we use something like {{extended storage policy}} (maybe better than {{EC policy}}), it would be easier to be unified and integrated into existing HSM storage policies, and also save some DFS commands to create EC zones. If we use {{EC Zone}}, it might not be so nature if we create a zone just for a file in case file level policy is needed in future. If we're likely to support file level EC policy, EC zone for directory sounds more nature. Since in medium future we only support directory level EC, either one is good, we just need one and the choice.

Thanks [~drankye] for the thoughts. 

I just talked to some HIVE folks about the HDFS directory structure in their typical workloads. In a nutshell, it looks like the following:
{code}
       warehouse
      /         \
     db1       db2
    /  \     /     \ 
 ...  ... table_1   table_2
                   /   |   \
              part_1 part_2 part_3 ...
{code}
Each DB table is represented as a directory (usually with huge fan-out), under which each partition is stored as a file. Each partition maps to a fixed range in the key space. I was told that it's quietly common to see skewed partitions. A likely scenario is thousands of small partitions along with a few outliers that are much larger than average. In the EC context, it indicates a potential need for per-file policies (e.g. EC for large files, replication for small files).

I still plan to look at a few more cases. At this stage, I think _extended storage policy_ is a good term to use in our APIs (maybe we can abbreviate it as {{XStoragePolicy}}). 

bq.At this stage, I think extended storage policy is a good term to use in our APIs (maybe we can abbreviate it as XStoragePolicy).
Sounds good to me. {{XStoragePolicy}} is nice. 

Per discussion with [~szetszwo], I updated ECWorker design doc in HDFS-7344, by incorporating latest discussions and thoughts scattered in related JIRAs, from [~libo-intel], [~zhz], [~jingzhao], [~szetszwo] and etc. It's break down and sub-tasks are also opened accordingly for all the parties to consider to take. Hope we can also move forward and make good progress in DataNode as well as we're going in NameNode, client and codec framework in this way.

Hi,
I think most of the commits to HDFS-7285 were not added to CHANGES-HDFS-EC-7285.txt.
This will help to update CHANGES.txt at the time of merging to trunk, and hence recording the contributions.
Very happy to see many new people Contributing to this work.

For all commits till now I have updated CHANGES-HDFS-EC-7285.txt through HDFS-8027.
Please take care for further commits.
Thanks.

Agree.  We should add an entry to CHANGES-HDFS-EC-7285.txt for each commit.

Yesterday we had another offline meetup. I think the discussion was very productive. Below please find the summary:
*Attendees*: Nicholas, Jing, Zhe

*Project phasing*
We went over the list of subtasks under this JIRA and separated them into 3 categories:
# Basic EC functionalities under the striping layout. Those subtasks were kept under this umbrella JIRA. The goal is for the HDFS-7285 branch to be ready for merging into trunk upon their completion.
# Follow-on tasks for EC+striping (including code and performance optimization, as well as support for advanced HDFS features). Those subtasks were moved under HDFS-8031. Following the common practice, those follow-on tasks are targeted for trunk, after HDFS-7285 is merged.
# EC with non-striping / contiguous block layout. Those subtasks were moved to HDFS-8030, which represents the 2nd phase of the erasure coding project.

Extending from the initial [PoC prototype | https://issues.apache.org/jira/browse/HDFS-7285?focusedCommentId=14339006&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14339006], the following _basic EC functionalities_ will be finished under this JIRA ([~szetszwo] please let me know if I missed anything from your list):
* A striped block group is distributed evenly on racks
* NN handles striped block groups in existing block management logics:
** Missing and corrupted blocks
** To-invalidate blocks
** Lease recovery
** DN decommissioning
* NN periodically distributes tasks to DN to reconstruct missing striped blocks
* DN executes the reconstruction task by pulling data from peer DNs
* Client can read a striped block group even if some blocks are missing, through decoding
* Client should handle DN failures during writing
* Basic command for directory-level EC configuration (similar to a zone)
* Correctly handle striped block groups in file system statistics and metrics
* Documentation
* More comprehensive testing
* _Optional_: instead of hard-coding, incorporate the {{ECSchema}} class with 1~2 schemas

*Key remaining tasks*
We think the following remaining tasks are _key_ in terms of complexity and amount of work:
# Client writing: the basic striped writing logic is close to complete (patch available under HDFS-7889), but it's challenging to handle failures during writing in an elegant way. 
# Client reading: the logic isn't too complex but amount of work is non-trivial
# DN reconstruction: logic is clean but work has not been started yet

*Client design*
We also dived into more details of the design of client reading/writing paths, and are synced on the overall approach. A few points were raised and will be addressed:
# Cell size in striping currently has default value of 1M. We should study its impact more carefully. Intuitively, a smaller value (like 128K) might be more suitable.
# Pread in striping format should always try to fetch data in parallel, when the requested range spans multiple striping cells.
# Stateful read in striping format should maintain multiple block readers to minimize overhead of creating new readers.

Here is the list we discussed.

h2. Phase 1 – Basic EC features
- Support (6,3)-Reed-Solomon
- Read 
-*  from closed EC files
-*  from files with some missing blocks
- Write
-*  Write to 9 datanodes in parallel
-*  Failure handling: continue writing with the remaining datanodes as long as #existing datanodes >= 6.
- EC blocks reconstruction 
-*  Scheduled by NN like replication
-*  Datanode executes block group reconstruction
- Block group lease recovery
-*  Datanode executes lease recovery
-*  Truncate at stripe group boundary
- NN changes
-*  EC block group placement
-*  EC zone
-*  Safemode calculation
-*  Quota
-*  Block report processing
-*  Snapshot
-*  Fsck
-*  Editlog/image
-*  Block group support
-*  EC file deletion
-*  Decommission
-*  Corrupted EC blocks
-*  ID collision
- Balancer/Mover
-*  Do not move EC blocks
- Documentation
- Testing



Thank you all for the comprehensive discussion and sorting these out ! It's very helpful.

I just finished weekly rebase. This time it's quite heavy, with several non-trivial changes. With this rebase, we finally have a stable Jenkins build with on test failures :)

If you have any ongoing work, please save the patch against your local repo and reapply on the new HDFS-7285 branch. Thanks!

> ... This time it's quite heavy, with several non-trivial changes. ...

Some of the conflicts probably is due to HDFS-8048.  I will make sure that it won't affect much on this JIRA.

Thanks Nicholas for the consideration! I think we also need to coordinate with HDFS-6200 (in particular, HDFS-8053 and HDFS-8054). [~wheat9] Could you advise when you plan to start/finish these 2 JIRAs? If necessary we can try to push our changes in {{DFSInputStream}} and {{DFSOutputStream}} to trunk.

bq.  If necessary we can try to push our changes in DFSInputStream and DFSOutputStream to trunk.

It should be unnecessary as git is smart enough to detect renames.

Thanks Haohui for the advice. You are right. In the most recent rebase git did merge changes to renamed files smoothly.

{quote}
Thanks for your great job about making erasure code native in HDFS. 
I am working on proactive data protection in HDFS by incorporating hard drive failure detection method based on collected SMART attributes into HDFS kernel and scheduling disk warning process in advance and want to have erasure code native supported by HDFS kernel instead of HDFS-RAID.
I have some questions below, but I don't know how to consult them , so I just list my questions here and hope it won't bother you so much.
1, I am wonderring whether and where i can download the project source code you are working on.
2, When this project will be accomplished, will it take a long time ?
3, Whether guys like me can join your group?
{quote}
Copying [comments | https://issues.apache.org/jira/browse/HDFS-8193?focusedCommentId=14539778&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14539778] from [~lpstudy] over here and please find my answers below:
# Erasure coding has been developed under the HDFS-7285 branch and the code can be accessed on github: https://github.com/apache/hadoop/tree/HDFS-7285
# We haven't explicitly discussed the target Hadoop release of the erasure coding feature. The plan will be discussed here.
# Sure! Contributions are always very welcome. Please feel free to file JIRAs on issues you see or take existing ones after checking with original assignee.

Hi all
I am a new comer , I want to know if I can add the HDFS-7285-initial-PoC.patch on the hadoop v2.2.0 ?
Thanks .

Hi [~Vincent.Wei], thanks for looking here.
bq.I am a new comer , I want to know if I can add the HDFS-7285-initial-PoC.patch on the hadoop v2.2.0 ?

That patch was still in development phase at that time and it was based on trunk code. I think lot has changed from v2.2.0.
Current progress is really good towards the completion of the Phase I of the EC feature.
Hope feature will be available soon in trunk/branch-2.


Uploading a test plan for phase I of the feature (thanks [~drankye] for filling in the details on codec testing). Any questions and comments are very welcome.

When we move on to follow-on optimizations (HDFS-8031) and phase II of the erasure coding feature (HDFS-8030) I will post additional test plans.

Since most planned functionalities for this phase is complete, we should perhaps start examining the entire consolidated patch in preparation for merging into trunk.

As you might have noticed, there have been a few trunk-based JIRAs trying to merge generic code refactors to trunk first, so as to minimize the consolidated patch: HDFS-8487, HDFS-8605, HDFS-8608, HDFS-8623, etc.. I'm working on a PoC [branch | https://github.com/zhe-thoughts/hadoop/tree/HDFS-EC-Merge] which rebases HDFS-7285 based on those efforts. I just finished a first pass, which (I think) includes all the changes except for fsimage/editlog supports.

In particular, the updated {{BlockInfo}} structure from HDFS-8487 will cause some non-trivial changes to the current HDFS-7285 code (hopefully making it cleaner). I'm attaching the proposed {{BlockInfoStriped}} and {{BlockInfoUCStriped}} patch. Comments and suggestions are very welcome.

Proposed structure for {{BlockInfoStriped}} and {{BlockInfoUCStriped}}. It's a little outdated. Please refer to my github branch for the latest proposed code.

bq. Proposed structure for BlockInfoStriped and BlockInfoUCStriped. It's a little outdated. Please refer to my github branch for the latest proposed code.

Below are some comments from the updated code from github POC branch, for the BlockInfo heirarchy.

{code}
  @Override
  BlockInfoUnderConstruction convertCompleteBlockToUC(
      HdfsServerConstants.BlockUCState s, DatanodeStorageInfo[] targets) {
    BlockInfoUnderConstructionContiguous ucBlock =
        new BlockInfoUnderConstructionContiguous(this,
            getBlockCollection().getPreferredBlockReplication(), s, targets);
    ucBlock.setBlockCollection(getBlockCollection());
    return ucBlock;
  }{code}
  {{BlockInfoStriped#convertCompleteBlockToUC(..)}} should return {{BlockInfoUnderConstructionStriped}} instance.


  {{StripedBlockStorageOp.java}} needs to have Apache licence header.

I am on will out of office  for biz trip form 6.23-6.26, I may reply e-mail slowly, please call me 13764370648 when there are urgent mater.



Thanks Vinay for reviewing the {{BlockInfo}} code! Those are good catches. I will update in the 2nd pass that I'm working on.

I just finished a pass rebasing all non-test changes. Attached is the consolidated PoC patch. The aforementioned github [repo | https://github.com/zhe-thoughts/hadoop/tree/HDFS-EC-Merge] also has the same changes. We have roughly:
# 5,600 LoC in {{hadoop-common}}, which is entirely new code 
# 2,000 LoC in {{blockmanagement}}. This is mainly to support block-level management of striping. Aside from a few new classes, most changes are in {{BlockManager}}
# 1,600 LoC in {{namenode}}. This is mainly to add striped blocks support in {{INodeFile}}, and fsimage/editlog. Biggest changes are on {{INodeFile}}, {{FSNameSystem}}, and {{FSDirWriteFileOp}}
# 1,100 LoC in {{datanode}}, 2,500 in client, and 1,000 in util. This is mainly new code.

Next I plan to make another pass and divide the consolidated patch into functional pieces (while doing that, add associated tests to each piece). 

Meanwhile, any comments / questions / suggestions on the patch are very welcome. 

Compared with the current HDFS-7285 branch, besides pre-merged refactor changes, the biggest difference is around {{INodeFile}}. With the new {{BlockInfo}} hierarchy introduced in HDFS-8499, we maintained the existing {{BlockInfo}} - {{BlockInfoUC}} structure in trunk; as a cost, we are losing the {{BlockInfoStriped}} - {{BlockInfoStripedUC}} inheritance in the current HDFS-7285 branch.

Consequently, {{INodeFile#blocks}} and {{FileWithStripedBlocksFeature#blocks}} can no longer use {{BlockInfoContiguous}} and {{BlockInfoStriped}} types because they contain both complete and UC blocks. [~hitliuyi]'s approach under HDFS-8058 is an option to solve the issue. Or for stronger type safety, we can build two additional interfaces, one for _striped complete or UC blocks_ and another one for contiguous ones.

bq. With the new BlockInfo hierarchy introduced in HDFS-8499....

Then why not bringing the BlockInfoXXX - BlockInfoXXXUC inheritance back and just make the inheritance structure like HDFS-7285 branch?

bq. INodeFile#blocks and FileWithStripedBlocksFeature#blocks can no longer use BlockInfoContiguous and BlockInfoStriped types because they contain both complete and UC blocks.
We can use BlockInfo as an abstraction for complete and UC blocks. We need to change FileWithStripedBlocksFeature#blocks to BlockInfo as well.

It's doable, because currently in trunk, INodeFile#blocks is BlockInfo and it works fine. Usually we don't cast BlockInfo to BlockInfoContiguous(or UC). (I didn't find such casting in trunk code, if we have we should worry type safety)
I saw you cast BlockInfo to BlockInfoStriped multiple times in BlockManager in github branch. They can be eliminated.

HDFS-8058 is irrelevant because in trunk INodeFile#blocks is already BlockInfo. HDFS-8058 is to reduce memory usage.

Thanks Jing and Walter for the helpful discussion!

bq. Then why not bringing the BlockInfoXXX - BlockInfoXXXUC inheritance back and just make the inheritance structure like HDFS-7285 branch?
This is because several places in trunk are relying on the {{BlockInfo}} - {{BlockInfoUC}} inheritance. As discussed under HDFS-8499, this multi-inheritance problem is fundamentally hard. HDFS-8499 patch keeps the {{BlockInfo}} - {{BlockInfoUC}} inheritance to minimize change to trunk. This structure also makes it easier to share common code because the code difference along the contiguous-striped dimension is smaller than the UC dimension. 

I'm open to revisiting the {{BlockInfo}} structure based on discussion here. With either structure discussed above, I think we should solve the {{BlockInfo}} multi-inheritance problem more completely as a follow-on.

bq. We can use BlockInfo as an abstraction for complete and UC blocks. We need to change FileWithStripedBlocksFeature#blocks to BlockInfo as well.
The PoC patch already does that. As Jing and myself commented under HDFS-8058, the downside is weaker type safety. For example, on the API level, {{setBlocks}} allows some other method to assign an array of {{BlockInfo}} to the INode; it's not easy to verify whether there are mixed types. My current thought is that we can create an abstraction {{BlocksInAFile}}, with a type and an array of {{BlockInfo}}. This will serve as a central place to control type safety. I'll post a patch under HDFS-8058 to demonstrate the idea.

bq. I saw you cast BlockInfo to BlockInfoStriped multiple times in BlockManager in github branch. They can be eliminated.
This is a good point. We can use {{isStriped}} and {{getStripedBlockStorageOp}} instead.


bq. As Jing and myself commented under HDFS-8058, the downside is weaker type safety.
Hmm. I was worrying mixed types of UC/non-UC. You are worrying mixed types of striped/contiguous. Isn't type check in {{setBlock}} enough? I don't know.

Comments on github branch(PoC-20150624.patch):
1. You missed merge {{INodeFile#getLastBlock}} and {{numBlocks}} to github branch.
2. I run test, and saw NPE. It'll fix after HDFS-8653 merges into trunk.
3. Please remove this line because NPE when lastBlock==null.
{code}
2437       Preconditions.checkState(!lastBlock.isStriped());  //FSNamesystem#appendFileInternal()
{code}

Thanks Walter! Will address in the next rev.

I just finished another pass of the consolidated patch. The github [repo | https://github.com/zhe-thoughts/hadoop/tree/HDFS-EC-Merge] has the latest code. I divided the consolidated patch into 13 sub-patches. Please let me know if the list looks reasonable to you.
{code}
    13. Balancer and mover support for striped block groups
    12. Support striped block groups in fsimage and edit logs
    11. Change fsck to support EC files
    10. Add striped block support in INodeFile.
    9. Datanode support
    8. Distribute recovery work for striped blocks to DataNode.
    7. Client side support
    6. Create LocatedStripedBlock abstraction to represent striped block groups.
    5. BlockPlacementPolicies for erasure coding
    4. Allocate and manage striped blocks in NameNode blockmanagement module.
    3. Extend BlockInfo to handle striped block groups.
    2. Support Erasure Coding Zones.
    1. HADOOP-COMMON side support for codec calculations.
{code}

I still haven't added tests to the sub-patches. Also the {{INodeFile}} implementation is still being discussed. They'll be addressed in the next pass.

In the next pass, to ensure we capture all latest branch changes, I will do the following:
# Take a latest consolidated patch from HDFS-7285
# Examine each file (if necessary, each diff in a file) in the patch and fit them into one of the 13 sub-patches (or discard as pre-merged changes).

The consolidated patch has roughly 25k lines of code. So any volunteered help is much appreciated :)

Thanks [~zhz] for the great work! I will have some time to look at some parts I'm familiar with.

Thanks Kai! Feel free to claim the component you'd like to look at.

There're some changes in the patch 1 (for codec) that should be placed elsewhere.
In {{FSOutputSummer}}, better for 9 (datanode support)
{code}
+  protected DataChecksum getDataChecksum() {
+    return sum;
+  }
{code}
In {{FsShell}}, better for 7 (client side support)
{code}
+  protected String getUsagePrefix() {
+    return usagePrefix;
+  }
{code}

I will continue to look at some other parts. And by the way, I'm going to move left codec issues planned for HDFS-7285 to the follow-on issue since I don't want to interrupt this pre-merge effort.

Attaching the consolidated-merge-patch.

1. Merged the latest trunk code to HDFS-7285 branch.
2. Taken diff against trunk.

Includes all changes done till date in the branch.

Thanks to [~zhz]. Most complicated BlockInfo heirarchy conflicts were resolved using the POC patch posted earlier.

Patch is pretty big. But also Intended to run jenkins.

\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  1s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12743065/HDFS-7285-merge-consolidated-01.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle shellcheck |
| git revision | HDFS-7285 / 0b7af27 |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/11561/console |


This message was automatically generated.

Attaching the same patch, with 'trunk' in the name.

If Jenkins still tries to apply against HDFS-7285, we can rename the patch to HDFS-EC-xxx

Thanks Vinay for the great effort! When I finish my current pass to divide the patch, we can compare the 2 consolidated patches and resolve all conflicts. 

Sure. Actually, I created this for this purpose only :). So that nothing should miss.

\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  1s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12743121/HDFS-7285-merge-consolidated-trunk-01.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle shellcheck |
| git revision | HDFS-7285 / 0b7af27 |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/11565/console |


This message was automatically generated.

Updating the same patch Vinay generated with a different name. Hope Jenkins tries it with trunk this time.

\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  1s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12743179/HDFS-EC-merge-consolidated-01.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle shellcheck |
| git revision | HDFS-7285 / 0b7af27 |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/11569/console |


This message was automatically generated.

Rebased patch.

I finished a complete pass of the patch (both {{main}} and {{test}} codes) and pushed to my github [repo | https://github.com/zhe-thoughts/hadoop/tree/HDFS-EC-Merge]. Also uploading the consolidated patch (renaming it again and hoping Jenkins applies it against trunk instead of our branch). 

I made some minor adjustments to the list of sub patches:
{code}
1. HADOOP-COMMON side support for codec calculations
2. Support Erasure Coding Zones.
3. Extend BlockInfo to handle striped block groups.
4. Allocate and manage striped blocks in NameNode blockmanagement module.
5. BlockPlacementPolicies for erasure coding
6. Create LocatedStripedBlock abstraction to represent striped block groups.
7. Distribute recovery work for striped blocks to DataNode.
8. Add striped block support in INodeFile.
9. Client side support
10. Datanode support
11. Change fsck to support EC files
12. Support striped block groups in fsimage and edit logs
13. Balancer and mover support for striped block groups
14. Additional unit tests for erasure coding
{code}
I also cherry-picked the 2 new patches from HDFS-7285 branch.

I created separate Jenkins builds for sub patches 1~4. Most tests passed. The remaining handful of failures seem to be resulted from the patch splitting (they pass in the final consolidated patch).

So *sub patches 1~4* are ready for reviews. Any comments and suggestions are much appreciated. In particular, sub patch 3 has the new implementation of {{BlockInfoStriped}} which has not been reviewed in the branch.

Thanks for the work, Zhe! So for the BlockInfoStriped related work, I suggest opening a separate jira under HDFS-7285 and post the main changes (from the current HDFS-7285 implementation) there. This can simplify the review. Also is it possible that we can defer this change after merging the remaining code?

Similar for the changes on INodeFile. In general I suggest let's try our best to separate the code change from the merging. We're now merging the changes from trunk back to HDFS-7285, thus it will be much faster and safer to make the merged code same with the current HDFS-7285 branch. For all the other changes we can do them later as subtasks under HDFS-7285. 

\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  1s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12744101/Consolidated-20150707.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle shellcheck |
| git revision | HDFS-7285 / 2c494a8 |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/11613/console |


This message was automatically generated.

Thanks Jing for the helpful comments! I've created HDFS-8728 focusing on the {{BlockInfo}} and {{INodeFile}} changes. 

We have revisited the {{BlockInfo}} structure and reverted HDFS-8499. I just rebased the consolidated patch.

I've also pushed the patch to my personal github repo and manually started a Jenkins [job | https://builds.apache.org/job/Hadoop-HDFS-7285-Merge/].

\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  1s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12749207/Consolidated-20150806.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle shellcheck |
| git revision | HDFS-7285 / 8799363 |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/11933/console |


This message was automatically generated.

[~zhz], could you describe the git rebase workflow we are using?  In a separated discussion, [~andrew.wang] claimed that our git rebase workflow requires committing some patches to trunk before merging the branch.  I wonder if you feel the same way.

[~szetszwo] I never said that, what I did say was the following:

* A rebase workflow gets very difficult without the ability to squash patches, since you would otherwise spend a lot of time fixing conflicts in intermediate commits that don't even show up in HEAD. We have 180 some commits in the EC branch now, and is definitely at the "very difficult" stage of rebasing.
* Moving refactors from branches to trunk is standard practice we've done many times before and is something I recommended we do here too.

Updating the patch to fix unit test failures. Now the rebased patch doesn't cause any additional failures compared with the current HDFS-7285 branch.

To better proceed with the open subtasks, I just created a branch {{HDFS-7285-merge}}. It applies the rebased consolidated patch ({{Consolidated-20150810.patch}}) on top of trunk; I will rebase the branch daily.

The branch basically provides an up-to-date {{HEAD}} for pending patches. After we finalize the above rebase workflow discussion we can always update the branch without changing the current {{HEAD}}.

So if you are working on one of the open subtasks, please kindly target the patch on {{HDFS-7285-merge}}. Ideally we should also commit to {{HDFS-7285}} branch in case we want to keep the branch to reserve development history. If you have a WiP patch that's non-trivial to rebase I'm very happy to help.

\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  1s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12749674/Consolidated-20150810.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle shellcheck |
| git revision | HDFS-7285 / fbf7e81 |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/11955/console |


This message was automatically generated.

> A rebase workflow gets very difficult without the ability to squash patches, since you would otherwise spend a lot of time fixing conflicts in intermediate commits that don't even show up in HEAD. We have 180 some commits in the EC branch now, and is definitely at the "very difficult" stage of rebasing.

If it is true, choosing the git rebase workflow seems a mistake.  We probably should switch to git merge workflow then.

> Moving refactors from branches to trunk is standard practice we've done many times before and is something I recommended we do here too.

The standard practice is to have a refactor patch committed to trunk first and then use git to merge to the branch, not the other way around, not using separated patches.  Committing similar code to different branches by different patches makes the merge hard since the branch and trunk are going to have a lot of conflicts as a result of such practice.

Moreover, if separated patches are needed, we usually ask the original contributors to contribute a merge patch for for committing patches to different branches so that the original contributors get the credits but not a different contributor.  An original contributor A did the hard work to come up patches which were only committed to the development branch.  Another contributor B did the easy work to post similar patches for committing to trunk.  After the development branch got merged to trunk, no one would care about the development branch and all the contributions in trunk are associated to B but not the original contributor A.  Although the contributor B may not have the intention, it does look like that B is stealing the credits from A.  Do you think that it is a problem, [~andrew.wang]?

> Zhe Zhang, could you describe the git rebase workflow we are using? ...

[~zhz], could you describe the workflow you are using?  We have a lot of confusion since there are so many merge patches; we now have a new branch; and we stopped rebasing the HDFS-7285 at some point.  It seems quite arbitrary.

[~szetszwo]:

* If you have issues with the rebase workflow, let's take it to common-dev. This topic applies beyond the scope of erasure coding.
* Regarding the refactors, you are talking about an SVN-style merge workflow rather than a git-style rebase workflow. In the case of BlockInfo, it went through a few mutations on the EC branch before arriving at the current state. This is because the understanding of BlockInfo+EC evolved over the course of development. It is prudent to wait until then to do the same refactoring in trunk, to avoid unnecessary churn on trunk.
* Regarding credit, the original contributor gets credit on the JIRA targeted for the branch, yes? JIRA assignee is how we credit contributors, and if there are multiple contributors, a JIRA comment saying as much. If you have examples, let's correct assignees or add comments to make sure that all contributors are being properly credited.

[~szetszwo]: Thanks for the question.

I posted my planned rebase workflow above: [link 1| https://issues.apache.org/jira/browse/HDFS-7285?focusedCommentId=14593827&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14593827], [link 2 | https://issues.apache.org/jira/browse/HDFS-7285?focusedCommentId=14600282&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14600282],  and a few other places. Based on that plan I already finished rebasing the branch with 14 squashed sub-patches. Then before pushing to HDFS-7285 branch we started revisiting HDFS-8499 and the rebase workflow.

The new branch was created simply because we haven't reached an agreement on the rebase workflow. It enables us to commit pending patches regardless of the final conclusion on rebase workflow.

Please let me know if that answers your question. 

bq. The new branch was created simply because we haven't reached an agreement on the rebase workflow.

I don't quite get what you are trying to achieve here. If the whole point of the branch, however, is about merging it back to the mainline, at the very least you will need consensuses with other contributors anyway. No?


Looks like the current situation is little bit confusing :)  The new feature branch Zhe proposed here is for merging the changes from trunk to EC dev. Still doing git rebase may be too hard now.

For synchronizing branch with trunk, we can use 'git merge'. For merging branch to trunk, we can use 14 squashed sub-patches the way [~zhz] did. Is it weird that one commit has a huge commit message? We commit one sub-patch, and keep many jira numbers and contributors as a whole commit message for this one commit log. In this way, we can avoid the painful process of rebasing every out-of-date commit, and also can keep the contributor infos as [~szetszwo] suggested.
I don't know if it's weird. At least it's an option.
bq. So if you are working on one of the open subtasks, please kindly target the patch on HDFS-7285-merge. Ideally we should also commit to HDFS-7285 branch 
So after HDFS-8854. An open subtask has to prepare 2 patches. a 'ECPolicy' version for HDFS-7285-merge, and a 'ECSchema' version for HDFS-7285? Or we commit HDFS-8854 to both of them first? Can we just sync HDFS-7285 with trunk?

[~wheat9] [~jingzhao] [~walter.k.su] Thanks for the discussion. 

bq. at the very least you will need consensuses with other contributors anyway.
Absolutely. I created the {{HDFS-7285-merge}} branch exactly because we haven't reached a consensus on whether / how to squash the commits in the current HDFS-7285 branch when rebased against trunk. Meanwhile, we do need to commit patches of the pending subtasks. Committing to the current HDFS-7285 branch will cause additional rebasing efforts. 

bq. The new feature branch Zhe proposed here is for merging the changes from trunk to EC dev. Still doing git rebase may be too hard now.
Thanks Jing for helping explaining it. Yes the new branch is just *HDFS-7285 rebased against trunk* (or from another angle, merging trunk changes to EC). We can continue discussing how to split the git history, but the end result (the {{HEAD}}) won't be affected.

bq. Can we just sync HDFS-7285 with trunk?
{{HDFS-7285-merge}} is actually already synced with trunk. If we all agree, it's actually clearer if we push it as {{HDFS-7285}} and move the current {{HDFS-7285}} as a backup branch. 



As explained below, I think each subtask should *at least* be committed to {{{{HDFS-7285-merge}}. 

For HDFS-8854, since we already have 2 versions, I think we should commit to both.

I have tried to rebase current {{HDFS-7285}} branch against the current trunk using {{git rebase trunk}}. It was not smooth as expected. Since I did not wanted to push the rebase directly onto {{HDFS-7285}}, created one more branch {{HDFS-7285-REBASE}}. This branch is just for reference purpose.

The advantage of this is, it retained all the commits along with message,date and author details, even after resolving conflicts. I skipped one commit purposefully HDFS-8787 to be in sync with trunk. it was just rename of files. other than this, no other commits got squashed.

There were 192 commits to be rebased against trunk, including the intermediate merge conflict resolved commits. Since I couldnt edit each and every commit to resolve compilation errors after each commit, resolved remaining compilation errors at the end, with one more commit.

If anyone wants to verify, please checkout the branch HDFS-7285-REBASE. and can compare against the Consolidated patch.

Since this is only for trying to check the possibility of rebase, I am not saying this should be considered as final branch.
If everyone feels good to go like this approach, I could do some more detailed rebase next week, ( may be verify the compilation after each commit ? Not sure whether its possible to stop for each commit rebase)

-Thanks

Many thanks to Vinay for the great effort! I just cherry-picked the 2 new commits (HDFS-8854 and HDFS-8220) to the branch. Also created a Jenkins [job | https://builds.apache.org/job/Hadoop-HDFS-7285-nightly/].

I'll also compare {{HDFS-7285-REBASE}} with the consolidated patch. After that and verifying Jenkins results, I'll push it as {{HDFS-7285}} so we can better proceed with pending subtasks. I'll also move the current {{HDFS-7285}} branch as a backup, in case we want to reconcile differences in individual commits.

Thanks for starting jenkins job. 
I prefer to wait for sometime for others also to take a look at the rebase. 
Then we can decide about moving this as HDFS-7285 Branch.

bq. I'll also move the current HDFS-7285 branch as a backup.
bq. Then we can decide about moving this as HDFS-7285 Branch.
Great. 

I have some question [~zhz]. 
1. Will you rebase HDFS-7285 weekly after squashes it?
2. Should we squash HDFS-8854 and HDFS-8833 as well? To make the future rebasing easier, also to try to avoid a second squash.
3. The commit message is inaccurate because HDFS-7285 is not finished yet.
{noformat}
commit ecf3634830fdbf228c056e3d4ae77ae0def17683
...
    HDFS-7285. Erasure Coding Support inside HDFS.
{noformat}

Thanks Walter for the questions.

bq. Will you rebase HDFS-7285 weekly after squashes it?
Yes that's my plan. Actually since we are close to merging I plan to rebase more frequently. Currently I'm rebasing HDFS-7285-merge daily

bq. Should we squash HDFS-8854 and HDFS-8833 as well? To make the future rebasing easier, also to try to avoid a second squash.
[~andrew.wang] has started a discussion thread on common-dev regarding the rebase workflow. I'll wait until we reach a consensus there before squashing the 2 new big patches.

bq. The commit message is inaccurate because HDFS-7285 is not finished yet.
Good point. I'll reword it in next rebase.

[~vinayrpet] Sure, let's solicit more feedback.

Your rebase doesn't cause any additional failures: [Jenkins results | https://builds.apache.org/job/Hadoop-HDFS-7285-REBASE/]. Did you run Jenkins before posting the branch? Otherwise, a nice ace :)

Given the feedbacks from the common-dev [discussion | http://mail-archives.apache.org/mod_mbox/hadoop-common-dev/201508.mbox/%3cCAGB5D2bPWeV2Hk+67=DamWpVfLTM6nkjb_wG3n4=WAN890zqfA@mail.gmail.com%3e], I think we should use "git merge" to sync HDFS-7285 branch with trunk now. Please comment if you have any concern.

I will do the following before updating the main HDFS-7285 branch:
# Run "git merge" on current HDFS-7285 branch agains trunk, and add one merge commit to resolve all conflicts so far
# Reuse the current HDFS-7285-merge branch to carry the merged result, and run Jenkins
# Compare the merged result with the rebased result from [~vinayrpet] on HDFS-7285-REBASE branch

If no major discrepancy is found, I will push the merged result to HDFS-7285 branch and delete the intermediate branches.

I apologize for not foreseeing the complexity and size of the EC branch at the beginning of the project, and adjusting the git workflow accordingly. Hope we have cleared confusions through the above discussions. Let me know if you still have questions about the workflow.

I just finished step #1 and #2 above, and pushed the result of "git merge" to the {{HDFS-7285-merge}} branch. Jenkins [job | https://builds.apache.org/job/Hadoop-HDFS-7285-Merge/] has been triggered.

Because HDFS-8801 requires major changes to the branch, this "git merge" was against HDFS-6407, which immediately precedes HDFS-8801 in trunk. [~jingzhao] has created a patch under HDFS-8909 which should be able to merge HDFS-8801 to the branch (I think we should keep it as a separate JIRA because it does more than just merging).

Comments on HDFS-7285-merge branch.
1. Unused import.
{code}
+import com.google.common.base.Preconditions; //DFSInputStream.java
{code}
{code}
+import org.apache.commons.logging.Log;  //DFSOutputStream.java
+import org.apache.commons.logging.LogFactory;
{code}

3. Please replace it with {{stat.getErasureCodingPolicy()==null}}
{code}
+    if(stat.getReplication() == 0) {  //DFSOutputStream.java
+      throw new IOException("Not support appending to a striping layout file yet.");
+    }
{code}

4. redundant space character.
{code}
-public abstract class BlockInfo extends Block  //BlockInfo.java
+public abstract class  BlockInfo extends Block
{code}

5. indent
{code}
-  private LocatedBlock createLocatedBlock(final BlockInfo blk, final long pos) {  //BlockManager.java
+  private LocatedBlock createLocatedBlock(final BlockInfo blk, final long pos
+  ) throws IOException {
{code}

6. truncation is file level. Maybe assert !file.isStriped() is less confusing? And also {{createNewBlock(false)}} can be less confusing.
{code}
+     BlockInfo oldBlock = file.getLastBlock();  //FSDirTruncateOp.java
++    assert !oldBlock.isStriped();
...
++          fsn.createNewBlock(file.isStriped()) : new Block(
{code}

7. unnecessary change
{code}
-    fsn.getBlockManager().setBlockToken(lBlk, //FSDirWriteFileOp.java
-        BlockTokenIdentifier.AccessMode.WRITE);
+    fsn.getFSDirectory().getBlockManager().
+        setBlockToken(lBlk, BlockTokenIdentifier.AccessMode.WRITE);
{code}
8. unnecessary change. (wiki says using [Sun's conventions|http://www.oracle.com/technetwork/java/javase/documentation/codeconventions-136091.html], which says "Break before an operator")
{code}
 -          if (blockInfo.getBlockCollection().getStoragePolicyID()  //FSNamesystem.java
 -              == lpPolicy.getId()) {  // feature branch
 +          BlockInfo blockInfo = getStoredBlock(b);
-           if (blockInfo.getBlockCollection().getStoragePolicyID() == lpPolicy.getId()) {  // trunk
++          if (blockInfo.getBlockCollection().getStoragePolicyID() ==
++              lpPolicy.getId()) {
{code}
Thanks [~zhz] for the great work!

Thanks Walter for reviewing the branch! I made the above changes and pushed to {{HDFS-7285-merge}} branch.

Jenkins [reported | https://builds.apache.org/job/Hadoop-HDFS-7285-Merge/95/testReport/] 9 failures. They are all on hadoop-common, which shouldn't be affected by EC changes. They all pass locally.

I'll compare the consolidated patches from this merge and Vinay's branch and report findings here.

Inspired  by [^HDFSErasureCodingPhaseITestPlan.pdf] created by [~zhz].  We has implemented system tests(Functional Tests) according to this test plan [^HDFSErasureCodingSystemTestPlan-20150824.pdf]. Latest test report will be posted separately soon.    Please feel free to discuss and add more test cases or scenarios to this system test plan.

Thanks [~demongaorui] for uploading the HDFSErasureCodingSystemTestPlan-20150824.pdf.

AFAIK following can be discussed and can be considered for testcases..

1) snapshot feature 
2) balancer feature
3) block placement policy based testing like, diferent racks etc..

I feel, better to consider following also..

4) parallel writes 
5) parallel reads

I have rebased the {{HDFS-7285-REBASE}} branch that [~vinayrpet] created on top of HDFS-6407, the same starting point as {{HDFS-7285-merge}}. I then compared the HEAD of these 2 branches. As can be seen from the attached {{Compare-consolidated-20150824.diff}} file, most differences are cosmetic. I've incorporated necessary changes and updated the {{HDFS-7285-merge}} branch and started a Jenkins [job | https://builds.apache.org/job/Hadoop-HDFS-7285-Merge/102/] for verification. From the past several Jenkins runs, {{HDFS-7285-merge}} is not introducing new failures compared to {{HDFS-7285}}.

Once verified by Jenkins I'll push {{HDFS-7285-merge}} as the main {{HDFS-7285}} branch and move the current {{HDFS-7285}} branch as a back-up.

Please let me know if you see any issues with the plan. Thanks!

Latest Jenkins [run | https://builds.apache.org/job/Hadoop-HDFS-7285-Merge/102/] is stable. I just updated the {{HDFS-7285}} branch as described above.

[~demongaorui] Thanks for the great addition to the test plan! The list of system functional tests looks good to me. A few questions that I think we can further discuss:
# Good points from [~brahmareddy], I suggest that we also add HSM/mover tests to the list.
# In reading tests we can distinguish stateful read and pread. Maybe we should test seek-and-read scenario too.
# It seems each test scenario in the "Tips for EC Writing/Reading" section is systematically labeled. Will the labels be used to drive automatic testing?

[~vinayrpet] Could you take a look at the results comparing the two rebase/merge efforts, and see if we should delete the {{HDFS-7285-REBASE}} branch? Thanks!

It's great to have the complete functional system tests for the striping support. Thanks guys!

In our side, we're working on end to end performance tests while implementing new erasure coders and optimizing striping codes. Related tasks: 
* Implementing ISA-L coder (about 25X than the Java coder, HADOOP-11887, HADOOP-11540);
* Implementing a new Java RS coder that’s both faster (6X than the existing Java coder) and compatible with ISA-L coder (HADOOP-12041).
* Optimizing client striping input/output stream codes (HDFS-8668 and related), consolidating client stateful and positional reads (HDFS-8957 and related);
* Performance benchmark test tools (HDFS-8968 and HADOOP-11588).

We're running the perf tests while tuning and optimizing the codes, and will provide the report when it's ready.

Thanks [~zhz] for the hard taking and I understand we may take some time to make the merging to trunk happen. Before that, I thought it would be good to keep moving. Think about so many follow-on issues listed in HDFS-8031 even for phase I, not to say we have another phase for non-striping erasure coding support. Having some branches for the effort, I'm wondering which branch better to use for the community to continuously work on and commit the follow-on tasks. Any thought? Thanks.

[~drankye] Thanks for raising the question. Since we have reached the conclusion to allow "git merge" in the workflow, I have merged trunk changes into the main HDFS-7285 branch. Going forward let's just use {{HDFS-7285}} branch for all commits.

I have actually deleted the intermediate {{HDFS-7285-merge}} branch. [~vinayrpet] Do you think we should delete the {{HDFS-7285-REBASE}} branch too, given that the result has been compared against and incorporated into the current {{HDFS-7285}} branch?

I just finished another pass of "git merge" to bring HDFS-7285 branch in sync with trunk. There were quite a lot of conflicts so I pushed to my personal github [repo | https://github.com/zhe-thoughts/hadoop/tree/HDFS-7285-20150827] first. Also triggered a Jenkins [job | https://builds.apache.org/job/Hadoop-HDFS-7285-Merge/]. Once verified I will update the main HDFS-7285 branch. Meanwhile let me know if you see any issues. Most conflicts were with HDFS-8823 and recently committed subtasks of HDFS-6200, in case anyone wants to take a look at how conflicts were resolved.

Based on latest version of branch HDFS-7285, we implemented system test according to the test plan [^HDFSErasureCodingSystemTestPlan-20150824.pdf]. We failed to test some scenarios in EC file writing/reading test case because of problems which is not related to HDFS, but ssh issues of the test script. We will figure out the problem, and implement remaining test scenarios ASAP. Thanks [~jingzhao] and [~szetszwo]] for help [~tfukudom] and our team during the whole process of test planning and implementation. 

Thank you very much [~brahmareddy], [~zhz]. 

{quote}
   1) snapshot feature 
   2) balancer feature
{quote}
may will be developed in future EC work, we could add these into the system test plan, and implement the test later. 
{quote}
   4) parallel writes  
   5) parallel reads
{quote}
I think {{parallel reads}} means more than one client try to read the same EC file form HDFS, right? What is {{parallel writes}} refer to, in EC system testing? Could you explain the scenario?

{quote}
1. Good points from Brahma Reddy Battula, I suggest that we also add HSM/mover tests to the list.
2. In reading tests we can distinguish stateful read and pread. Maybe we should test seek-and-read scenario too.
3. It seems each test scenario in the "Tips for EC Writing/Reading" section is systematically labeled. Will the labels be used to drive automatic testing?
{quote}

We can also add {{HSM/mover}} to the test plan, and implement it in future work. 

For the reading distinguish, we currently implement system test by using FSShell command in terminal, like {{CopyFromLocal}} and {{CopyToLocal}}. Can we set the client to read EC file in particular mechanism like stateful read and pread by terminal command? 

The labels in EC Writing/Reading tests were generated by test script during the test process, but it is also possible to drive automatic testing by the scenario labels vice versa.


The merge result in HDFS-7285-20150827 LGTM. except that,
1. We have removed BlockInfo(BlockInfo from) constructor
2. Here It should be {{blocksmap.getStoredBlock(b)}}, and it could return {{null}};
{code}
 +  public BlockCollection getBlockCollection(Block b) { //BlockManager.java
-     return blocksMap.getBlockCollection(b);
++    BlockInfo storedBlock = getStoredBlock(b);
++    return namesystem.getBlockCollection(storedBlock.getBlockCollectionId());
++  }
{code}
I think it's better to not to call {{getStoredBlock}} inside {{getBlockCollection}}. It's wired to have another {{getBlockCollection}} which only used in {{SequentialBlockGroupIdGenerator}}. Can it just call blockManager.blocksMap.getStoredBlock(..) ?

3. wrong intent
{code}
-     if (file.isStriped()) {  //FSDirAttrOp.java
-       throw new UnsupportedActionException(
-           "Cannot set replication to a file with striped blocks");
-     }
++      if (file.isStriped()) {
++        throw new UnsupportedActionException(
++            "Cannot set replication to a file with striped blocks");
++      }
{code}

Parallel write/read meant for bench-marking the perf aspects and observing any concurrency issues.( different clients writing different files,read files..), I think, we can take care in (HDFS-8968 and HADOOP-11588).

[~brahmareddy] - Yeah I'm working on the two issues to support parallel read/write benchmarking.

Thanks Walter! I updated the {{getBlockCollection}} in my personal github repo and the Jenkins job has been pretty stable so far. I'll incorporate your suggestions and update the main HDFS-7285 branch.

Thanks Rui! I guess you are referring to HDFS-8968?

Yep. We intend to create a throughput benchmark tool in HDFS-8968. HADOOP-11588 is something like a micro-benchmark, which benchmarks performance at an API level.

With HDFS-8833 committed, all remaining subtasks are about write pipeline error handling (with very little change to existing trunk code) and minor improvements. I suggest we start reviewing the feature branch in the context of merging to trunk.

I'm attaching the current consolidated patch ({{HDFS-7285-Consolidated-20150911.patch}}). Feedbacks are very welcome!

Thanks Zhe! Also IMO the write pipeline error handling may not be a blocker for merging the feature branch to trunk (but may be for branch-2).

I agree with Jing that write pipeline error handling can be moved to follow-on. We should also think about whether to merge EC to branch-2, and if so, what should be the target version (current target version is 3.0.0).

I just moved sub-tasks on write pipeline error handling to the follow-on umbrella, HDFS-8031. If we decide to change the target version we can move some of them back.

I just made another pass of the open subtasks. None of them looks like blockers for merging the feature branch to trunk.

The nightly Jenkins [job | https://builds.apache.org/job/Hadoop-HDFS-7285-nightly/] is relatively stable. Most runs with massive failures are caused by stale dependencies (e.g. {{NoClassDefFoundError}} on {{ECSchema}}, after HDFS-8833 removed dependency on it). The only real test issue is {{TestWriteStripedFileWithFailure}}, which is flaky. But we already know that it depends on a series of open JIRAs on error handling in the write path (HDFS-8704, HDFS-9040). We under-estimated the required efforts on those JIRAs so decided to commit the test first. I think we can revert the patch at this phase and redo the change later.

I also just made another pass of merging trunk changes into the feature branch locally, and pushed the result to my personal github [repo | https://github.com/zhe-thoughts/hadoop/tree/HDFS-7285-merge]. A Jenkins [job | https://builds.apache.org/job/Hadoop-HDFS-7285-Merge/] has been started. The main conflict was with HDFS-9111.

The Jenkins [job | https://builds.apache.org/job/Hadoop-HDFS-7285-Merge/] is not showing any new failed tests. I just updated the feature branch with latest trunk changes. It was a force push because HDFS-8920 was committed while I test the git merge result locally. So I just cherry-picked HDFS-8920; there was no conflict.

The [vote | http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201509.mbox/%3cCAAjoTXnx+79aGB=wV+NxQA9jy7KS9aw_b=19WMf9hBKu0tEVAA@mail.gmail.com%3e] on hdfs-dev and hadoop-dev mail lists to merge this feature branch to trunk has just passed. I will work with Andrew to merge the branch.

I just did a final {{git merge}} to sync with trunk and [~andrew.wang] helped push the HDFS-7285 branch to trunk. Resolving this JIRA now; let's keep working on follow-on tasks under HDFS-8031.

Thanks very much for all contributors to EC phase I, as well as the helpful discussions in the community.

FAILURE: Integrated in Hadoop-Hdfs-trunk #2379 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2379/])
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev af8eaacdb40afba327b776e2b4f374be624289fc)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev 2c277802c1635837ec926862523be02e05e5649b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt


FAILURE: Integrated in Hadoop-trunk-Commit #8548 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/8548/])
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev af8eaacdb40afba327b776e2b4f374be624289fc)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev 2c277802c1635837ec926862523be02e05e5649b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt


FAILURE: Integrated in Hadoop-Yarn-trunk #1203 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/1203/])
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev af8eaacdb40afba327b776e2b4f374be624289fc)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev 2c277802c1635837ec926862523be02e05e5649b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt


FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #473 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/473/])
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev af8eaacdb40afba327b776e2b4f374be624289fc)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev 2c277802c1635837ec926862523be02e05e5649b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt


FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #439 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/439/])
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev af8eaacdb40afba327b776e2b4f374be624289fc)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev 2c277802c1635837ec926862523be02e05e5649b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt


FAILURE: Integrated in Hadoop-Mapreduce-trunk #2408 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2408/])
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev af8eaacdb40afba327b776e2b4f374be624289fc)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev 2c277802c1635837ec926862523be02e05e5649b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt


FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #465 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/465/])
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev af8eaacdb40afba327b776e2b4f374be624289fc)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt
HDFS-8027. Erasure Coding: Update CHANGES-HDFS-7285.txt with branch (zhezhang: rev 2c277802c1635837ec926862523be02e05e5649b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt


Hi there,

    I planed to apply this patch(Consolidated-20150810.patch),could anyone please tell me the branch which this patch can be applied to?
    And my goal is hadoopV2.7.3, is it possible to work? 

    Looking forward to the answers.

Best wishes!



Hi Doris, this patch was meant to apply to trunk. Getting this (and all the related EC patches) backported to 2.7 will be a lot of work, since 2.7 and trunk differ quite a bit.

Thanks a lot, Andrew. I see. Since such a situation, do you have any idea about the realese time of stable 3.0.0?

Hi Doris, please see here for the current release plan:

https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+3.0.0+release

If you're interested in EC, please do try out the recent 3.0.0-alpha2 release, we're actively working to improve the feature.

Dear all!

First of all let me introduce ourselves: we are Chocolate Cloud from Denmark, we use erasure coding to improve storage solutions. We already have Reed-Solomon and Random Linear Network Coding backends for Liberasurecode, and now we are at the final stage of developing our RS plugin to HDFS-EC. The performance of our plugin is similar to ISA-L's, in some configurations we are better, in others we are worse (our initial speed comparison charts can be found here: https://www.chocolate-cloud.cc/Plugins/HDFS-EC/hdfs.html).

We would like our plugin to become officially supported in Hadoop 3.0. We can already provide a preliminary version of our (native) library and a patch with the necessary glue code for the next alpha release.

I'd like to know your thoughts about whether it's possible and how it could be achieved.

P.S: I'm happy to share more details if there's interest

That sounds exciting, [~sw0rdf1sh]. There's a wiki [here|https://wiki.apache.org/hadoop/HowToContribute] that describes the patch process. If you need help please don't hesitate to reach out on [hdfs-dev|http://hadoop.apache.org/mailing_lists.html#HDFS].

Thanks [~sw0rdf1sh]. In addition to what Chris pointed out, you might also want to take a look at the discussions under HADOOP-11264; since your library is on the RS layer, sounds like the library can be plugged in with our coder framework.

The Hadoop EC framework is designed to be pluggable, however, the native EC codec loader assumes it uses ISA-L library (ErasureCodeNative.java), so that probably need to be changed.

Thanks for the help guys, we'll proceed with creating a sub-task and submitting our patch. We should use *3.0.0-alpha2* release as the base, right?

Hi Marcell,

I recommend basing the patch off of the "trunk" branch, we've been pretty actively working on EC in preparation for alpha3.

I looked at the benchmark results, very interesting! I'm guessing the X axis is the EC cell size? We've chosen the RS parameters for our built-in EC policies somewhat arbitrarily, but maybe we should tune them for better performance.

Are there any patterns you see wrt the RS parameters? It looks like the #1 factor is how many parity blocks there are, though there's some weirdness like 10+4 being faster than 9+4. There seem to be differences based on the cell size, but no pattern I can see.

Another question, is the Chocolate Cloud RS implementation byte-for-byte compatible with the Hadoop ISA-L and Java RS coder implementations? If not, it'll be harder for users to experiment with CC RS since it'll require rewriting data. We'll also need to add a new set of EC policies, which increases the configuration and maintenance overhead. In this case, I'd like to see more clear wins and a stable native code release before we put this in a release.

In the meanwhile, we can also set up a development branch so you can work on the Hadoop integration. We've also made some progress toward pluggable EC policies and coder framework, though it's not done yet. If this gets finished, it'd be a nice way of making your RS implementation accessible to users without requiring it to be shipped with Hadoop itself.

Hi Andrew!

Alright, we'll use the "trunk". 

Yes, the X axis is the cell/chunk size. These charts reflect preliminary speeds that we quickly made just to see an approximate comparison against ISA-L. We are currently working on a more reliable benchmark, I'll post the results when they are in. Theoretically our encode speed is mostly dominated by the number of data units and reconstruct speed by the number of lost parity units that needs to be repaired. 

Today we are not compatible with ISA and the built-in plugins (since our plugin uses the Vandermonde coefficient matrix), but we identified this problem as well and working actively on the Cauchy matrix implementation. Our goal is to be compatible and we will reach in the next few weeks.

Our RS plugin, similarly to ISA-L, is implemented in C++ and offered as a native binary library. The only thing needs to be shipped within the Hadoop framework is the glue code. Our understanding is that this is the intended way of shipping new native EC plugins. Some glue code is always necessary in the framework (like with Liberasurecode). Isn't this the case?

Hi Marcell,

I've filed HDFS-11503 so we can continue discussion off of this umbrella JIRA with many watchers. I'll respond to your above comment over there.

I have no permission to upload attachment.Please visit csdn web

!image-2021-04-20-17-26-45-834.png! the url of image->[https://img-ask.csdnimg.cn/upload/1619363340018.png]

I can not understand it.For example,if 2-missing,the value of column named 3-replication  is 1LR+1RR.Why not 1LN+1RR?What it means?

[~iostream2008@163.com]: the image cannot be viewed. Can you upload again? Thanks

[~zhz]

I have no permission to upload attachment.Please visit csdn web

 the url of image->[https://img-ask.csdnimg.cn/upload/1619363340018.png]

[~iostream2008@163.com] Unfortunately I don't have permission to the [link|[https://img-ask.csdnimg.cn/upload/1619363340018.png]] you just pasted. It shows "403 Forbidden". Could you find a way to upload the image to an accessible location, or describe the issue? Thanks

[~zhz] https://img-ask.csdnimg.cn/upload/1619363340018.png

You can see this picture by coping the URL address to the browser.Don't open this url link directly

Below is https://img-ask.csdnimg.cn/upload/1619363340018.png :
 !1619363340018.png! 

