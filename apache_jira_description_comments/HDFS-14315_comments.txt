Submitting the initial patch.

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 14s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m 20s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 11s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 16s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 49s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 13 new + 19 unchanged - 0 fixed = 32 total (was 19) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 21s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 22s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 85m  5s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 37s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}143m 52s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.ha.TestBootstrapStandby |
|   | hadoop.hdfs.server.namenode.TestPersistentStoragePolicySatisfier |
|   | hadoop.hdfs.web.TestWebHdfsTimeouts |
|   | hadoop.hdfs.server.datanode.TestBPOfferService |
|   | hadoop.hdfs.server.namenode.TestDecommissioningStatus |
|   | hadoop.hdfs.qjournal.server.TestJournalNodeSync |
|   | hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | HDFS-14315 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12959992/HDFS-14315.000.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux ad07bb16029f 4.4.0-138-generic #164~14.04.1-Ubuntu SMP Fri Oct 5 08:56:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 5e91ebd |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/26320/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/26320/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/26320/testReport/ |
| Max. process+thread count | 3160 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/26320/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



[~daisuke.kobayashi] thanks for working on this.
{code:java}
import org.apache.hadoop.fs.*;
import org.apache.hadoop.hdfs.*;
{code}
We are usually avoiding to use wildcard import.
{code:java}
//Wait for block replication
try { Thread.sleep(10000); } catch (InterruptedException ie) {}
{code}
Just sleeping makes test flaky or unnecessary long. DFSTestUtil#waitForReplication or similar should be used if you can.
{code:java}
      //File is in snapshots. Decreasing replication factor, but not the requested value                                                                                                                                                                      
      if (oldBR > targetReplication && targetReplication > replication &&
              iip.getLatestSnapshotId() != Snapshot.CURRENT_STATE_ID) {
        FSDirectory.LOG.info("Decreasing replication from {} to {} for {}" +
                             ". Requested value is {}, but {} is the maximum in snapshots",
                             oldBR, targetReplication, iip.getPath(), replication, targetReplication);
{code}
This looks error prone for future change of INodeFile#getPreferredBlockReplication. FSDirAttrOp should not be aware of implementation details of snapshot as far as possible. Just adding requested replication facter to existing log would work for you?

Thanks for reviewing the patch [~iwasakims]. All the comments make sense. Will post the second patch, later.

Submitting the second patch which addressed the points made by Masatake.

{code:java}
     if (oldBR != -1) {
-      if (oldBR > targetReplication) {
+      //File is likely in snapshots. Decreasing replication factor, but not the requested value
+      if (oldBR > targetReplication && targetReplication > replication) {
+        FSDirectory.LOG.info("Decreasing replication from {} to {} for {} " +
+                             "whereas requested value is {}. The file is likely in snapshots",
+                             oldBR, targetReplication, iip.getPath(), replication, targetReplication);
+      //File is not in snapshots. Decreasing replication factor
+      } else if (oldBR > targetReplication){
         FSDirectory.LOG.info("Decreasing replication from {} to {} for {}",
                              oldBR, targetReplication, iip.getPath());

{code}
The comment and log message should not mention snapshot. INodeFile#getPreferredBlockReplication would be fixed due to additional reason for adjusting number of replicas. My suggestion is just always logging given replication rather than adding conditionals like this.
{code:java}
         FSDirectory.LOG.info(
             "Decreasing replication from {} to {} (over {}) for {}",
             oldBR, targetReplication, replication, iip.getPath());
{code}

Comments for TestRplicationSnapshot:

{code}
   public void testReplicationWithSnapshot() throws Exception {
-    short fileRep = 1;
+    short fileRep = 2;
     // Create file1, set its replication to 1
     DFSTestUtil.createFile(hdfs, file1, BLOCKSIZE, fileRep, seed);
     Map<Path, Short> snapshotRepMap = new HashMap<Path, Short>();
+    FileStatus fileStatus = hdfs.getFileStatus(file1);
+    long len = 0;
     // Change replication factor from 1 to 5. In the meanwhile, keep taking
     // snapshots for sub1
{code}
Changing initial value of fileRep makes following comments inconsistent.

{code}
+      for( BlockLocation locations : hdfs.getFileBlockLocations(fileStatus, 0, len) ) {
+        //assertTrue(locations.getHosts().length == fileRep);
+        assertEquals(fileRep, locations.getHosts().length);
{code}
The line commented out should be removed.

{code}
+    // At this point in time, number of replicas (fileRep) is 5 and the latest snapshot
+    // was taken when the replication factor was 4.
+
+    //Change replication factor back to 3, but it's set to 4.
+    hdfs.setReplication(file1, (short) 3);
{code}
I think there is confusion about terminology in the test. "fileRep" means replication factor set in inode. DistributedFileSystem#setReplication sets the given number to inode regardless of snapshots. INodeFile#getPreferredBlockReplication returns number of replicas adjusted considering snapshots. The number of replicas is called "blockRep" in the test.

{code}
+      //Wait for block replication
+      DFSTestUtil.waitForReplication(hdfs, file1, fileRep, 5000);
+
+      // Check number of replicas in datanodes, which should be 5.
+      len = fileStatus.getLen();
+      for( BlockLocation locations : hdfs.getFileBlockLocations(fileStatus, 0, len) ) {
+        //assertTrue(locations.getHosts().length == fileRep);
+        assertEquals(fileRep, locations.getHosts().length);
+      }
{code}
The check is redundant since DFSTestUtil#waitForReplication do the same thing. "should be 5" is wrong since fileRep is changing in the loop.

{code}
+    //Wait for block invalitation in datanodes, but expect to receive the TimeoutException.
+    try {
+      DFSTestUtil.waitForReplication(hdfs, file1, (short) 3, 10000);
+    } catch (Exception e) {
+      assertEquals(TimeoutException.class, e.getClass());
+    }
{code}
TestReplication could be a reference if you want to check that there is no invalidation work without waiting 10s, while I think just checkSnapshotFileReplication is enough here.

Thanks for the comments! I will sort them out and send a new patch soon.

Hi here,

I have ran into this issue durning investigating the same behaviour, and I wanted to add some additional information that I found out, that may change the direction of the ticket as well a bit.

I ran into the following scenario:
 Given:
 * a file that consists from one block with replication factor 3
 * a snapshot created that contains this file

When:
 * the file replication is set to 2 with hdfs dfs -setrep 2 /path/to/file
 * and then removing the snapshot holds the file with rep factor 3

 Then:
 * the file has 3 block replicas until the next FBR arrives from all the DNs holding the block replicas for the file
 * after a NameNode restart all 3 replica is marked to be in a stale state according to hdfs fsck <blockID> and in stale state until the FBR arrives, and effectively this is what prevents the removal of the excess replica according to this log message and the code around the log message:
 DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* rescanPostponedMisreplicatedBlocks: Re-scanned block <blockID>, result is POSTPONE
 * before the NameNode restart the replicas are not in stale state

I think the problem is that the block is in an unexpected state that prevents the excess replica be removed, and that we might examine further but may be in an other JIRA it is up to you. I am not confident enough to decide whether the described behaviour is expected or is a bug, I just wanted to add here to discuss this as well and decide if we want to fix something here.

Also this might be related to HDFS-11146 but I am not 100% sure on that either again, limited knowledge about block replica states... :S

Thanks for sharing your findings here [~pifta]. Yup I was aware of that behavior too, but was not aware of HDFS-11146! 
 While HDFS-11146 gets resolved in near future, however, users are still hard to notice the replicas for files within snapshots don't get decreased immediately, under a particular condition, based on the current namenode's design. Hence my goal here is to add more hint into the namenode log. Does this make sense to you?

