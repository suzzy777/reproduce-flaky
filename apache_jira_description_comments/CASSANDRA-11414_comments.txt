I'd say changing the stress write CL to TWO would fix this (we must keep the read CL at ONE though)

Created [pr|https://github.com/riptano/cassandra-dtest/pull/889] with above suggestion.

[~pauloricardomg] If you like, you can try that fix out using this CI job: http://cassci.datastax.com/view/Parameterized/job/parameterized_dtest_multiplexer/build?delay=0sec

Nice! Can you please give permission to the paulo.motta user? Also, does it work with custom github repositories? I only see the CUSTOM_DTEST_BRANCH field. How do I run for the pauloricardomg/11414 branch?

It's not built to run from other gh repos yet, but that might be possible later. For now you'd need to push to a branch on riptano/cassandra-dtest if you have permissions to do that.

I haven't seen the stress failure since that PR was merged, so that's good. However, I'm still seeing some failures where a node times out waiting for CQL:

http://cassci.datastax.com/job/trunk_dtest/1081/testReport/junit/bootstrap_test/TestBootstrap/resumable_bootstrap_test/

So it appears there can sometimes be issues on the receiving node when the stream dies, causing that node to fail to start listening for clients.

A successful run: https://gist.github.com/ptnapoleon/b999e42e3e0a39de233d875ac00ee79b
A failed run: https://gist.github.com/ptnapoleon/00e1e5739ad867b9dfb10fbf16da9022

Here is a collection of several hundred runs of this test, to give an idea of how often it flakes, and how.
http://cassci.datastax.com/view/Parameterized/job/parameterized_dtest_multiplexer/68/testReport/

It seems there could be a few races when a stream session is interrupted, I also faced some of those on CASSANDRA-3486. There are also some potential issues with the dtest that can make it non-deterministic, so I addressed those on this [PR|https://github.com/riptano/cassandra-dtest/pull/1051] (currently under review).

I addressed the most visible of these races on the {{StreamSession}} and surroundings, and submitted a new multiplexer run based on [this dtest branch|https://github.com/riptano/cassandra-dtest/pull/1051]: https://cassci.datastax.com/view/Parameterized/job/parameterized_dtest_multiplexer/148/

The patch and dtests for 3.0 are available below:
||3.0||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-11414]|
|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11414-testall/lastCompletedBuild/testReport/]|
|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11414-dtest/lastCompletedBuild/testReport/]|

(will set to PA and give more details about the improvements if multiplexer results look good)

Since this test kills streaming at random points, it was causing various errors or race conditions causing the test to fail, so the basic idea here is to improve synchronization to avoid these races when a node is randomly killed in the middle of a streaming. With that said, I made the following improvements:

* 2.2+
** Add null protection on ConnectionHandler.signalCloseDone
** Stream session was not being failed on {{SocketException}}, what could cause it to hang on broken connections

* 3.0+
** Synchronize access to transaction on {{StreamReceiveTask}}
** Abort {{SSTableWriter}} if received after {{StreamReceiveTask}} is finished
** Abort {{SSTableWriter}} if there's a failure during finalization on {{StreamReceiveTask}}
** Synchronize access to {{StreamSession}} methods: {{prepareReceiving}}, {{addTransferFiles}} and {{addTransferRanges}}, so they don't race with {{onError}}, since that will try to abort active tasks.
*** Throw exception if any of these are executed after stream session is finished (added tests on {{StreamReceiveTask}}))

After these were addressed, the number of failures have gone down from 28/100 to 11/100 on this [multiplexer job|https://cassci.datastax.com/view/Parameterized/job/parameterized_dtest_multiplexer/149/].

The remaining failures are due to bad timing on dtest, so I [updated the dtest|https://github.com/riptano/cassandra-dtest/pull/1051/commits/51ed5f55c85a3a1c339b265ac4b056137215e5fd] to address those and submitted a new multiplexer run (still queued).

Patch and tests available below:
||2.2||3.0||3.9||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-11414]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-11414]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.9...pauloricardomg:3.9-11414]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-11414]|[branch|https://github.com/riptano/cassandra-dtest/compare/master...pauloricardomg:11414]|
|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-11414-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11414-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.9-11414-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-11414-testall/lastCompletedBuild/testReport/]|
|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-11414-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11414-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.9-11414-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-11414-dtest/lastCompletedBuild/testReport/]|

Will set to PA once new multiplexer and CI run looks good.

[multiplexer run|https://cassci.datastax.com/view/Parameterized/job/parameterized_dtest_multiplexer/151/] looks good, setting to Patch Available.

Patch LGTM, +1.

Committed as {{00e7ecf1394f8704e2f13369f7950e129459ce2c}}.

This doesn't look right to me, someone take a look?

protected void signalCloseDone()
        {
            if (closeFuture == null)
                close();

            closeFuture.get().set(null);


seems like it should be

            if (closeFuture.get() == null) 
                close();


(closeFuture will never be null)

Thanks Dave, you are right.
Fixed in ninja commit {{e983590d303c9c19577b3bd5b5c95adc9f5abb8a}}.

