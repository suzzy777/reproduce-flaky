-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537078/accesslogger-v1.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDatanodeBlockScanner

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2855//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2855//console

This message is automatically generated.

Fix javadoc (copy & paste ftl).

-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537098/accesslogger-v2.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks
                  org.apache.hadoop.hdfs.TestDFSClientRetries

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2862//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2862//console

This message is automatically generated.

Tests are passing locally for me, so I guess they are flaky?

Those tests aren't implicated by this change, so no worries.

Thank you very much for coding up this plugin interface!  I like the core ideas.

What is the expected use case for {{FSAccessLogger}}?  Suppose I want to send a message on a socket (syslog, or any of the message bus protocols).  Then my FSAccessLogger implementation's {{.initialize}} method probably has to do a fairly heavyweight setup.  It seems a bit unfortunate to do that in every {{FSNamesystem}} constructor.  It's not too unusual to have lots of FSNamesystem objects within a single JVM, and having many sockets (or whatever) open would be a resource hog.

I'm somewhat inclined to say the FSAccessLogger should be a static singleton in the FSNamesystem class, largely preserving the existing semantics. I don't know of a usecase where per-FSNamesystem logging configurability would be useful.  But I can imagine that one might exist, so:

Alternatively we could simply document that FSAccessLogger has to be cheap and recommend that plugins use a service object or similar (sorry, I don't know the Design Patterns terminology for this technique, but you make one object that does the heavy lifting and let all the lightweight client objects find it through a Factory or whatever).

I don't like the hackiness of your change to {{logAuditEvent}}; I would prefer to see the existing StringBuilder-based file logging implementation turned into the premier plugin to the FSAccessLogger interface.  As a bonus, this gives us a feel for how good the plugin interface is to code against.

bq. It's not too unusual to have lots of FSNamesystem objects within a single JVM

I think you're confusing FSNamesystem with another class, Andy. There's only one per NN. So I don't think it should be a singleton at all.

bq. I don't like the hackiness of your change to logAuditEvent; I would prefer to see the existing StringBuilder-based file logging implementation turned into the premier plugin to the FSAccessLogger interface. As a bonus, this gives us a feel for how good the plugin interface is to code against.

I vaguely agree - but that implies that you might want to have multiple plugins installed at once. I can see, for example, wanting to continue to log to the existing audit logs, but additionally save to a database, etc.

I quickly scanned the patch, and agree the implementation should probably use a chain of {{FSAccessLoggers}} instead of hardcoding log4j + one more.  OTOH, I think implementing additional loggers is fraught with peril.  For instance, it makes it too easy for someone to write a custom logger which imposes an artificial performance penalty (like blocking on a db, instead of queueing message for another thread to update in the db).  Sub-par loggers may be attributed as NN performance problems which may tarnish hadoop's reputation...


Daryn: decent point. Do you think we should just attach a big warning to the interface saying "make sure you do queueing", or should we build the queue into the core code itself?

The issue with doing our own queueing is that it's still nice to have the log4j logs interleave in-order with the other related log messages. I wouldn't want the log to be async. But definitely reporting to a JDBC-backed DB should be async via a queue.

Another thing to think through with the queue: we probably don't want it to be unbounded for fear of OOME. And I imagine some organizations may have a strict audit requirement that limits the possibility of lost audit events. In such a case, operations probably _should_ block on a full queue - else events could be permanently stuck in the queue if the underlying DB is down, and then lost if the NN crashes.

Marcelo, any thoughts?

Todd: good points as well about log ordering and whether the impl should enforcing queueing, if so, bounded/unbounded queueing.  I'm not sure there's a one-size-fits-all, and flexibility could become very complex.  I'm a bit squeamish of no queueing and leaving it up to the user since the logging is inside the namesystem lock.  It'll be all too easy to add latency or even stall the NN entirely trying to log a write op.

While conceptually this jira is good idea, this is in a very critical portion of the NN that risks dire performance impacts. I'm inclined to think/propose other audit loggers should post-process the audit file in a separate process.




bq. While conceptually this jira is good idea, this is in a very critical portion of the NN that risks dire performance impacts. I'm inclined to think/propose other audit loggers should post-process the audit file in a separate process.

While I understand very well the desire to not let users shoot themselves in the performance foot, I don't think that the case of writing a custom logger implementation is a place where users need to have their hand held. Not a ton of folks will want to write a custom logger implementation, and those who do should understand the potential impacts this will have on the daemons where they're installed.

Thanks for the comments everyone. Good to know FSNamesystem is a singleton, so no need to worry about that issue.

As for queuing / blocking, I understand the concerns, but I don't see how they're any different than today. To do something like this today, you'd do one of the following:

(i) Process logs post-facto, by tailing the HDFS log file or something along those lines.

This would be the "completely off process" model, not affecting the NN operation.

(ii) Use a custom log appender that parses log messages inside the NN.

This is almost the same as what my patch does; except it's tied to the log system implementation.

Both cases suffer from turning a log message into something expected to be a "stable" interface; the second approach (which is doable today, just to make that clear) adds on top of that all the concerns you guys listed.

Does anyone know how the different log systems behave when using file loggers, which I guess would be the vast majority of cases for this code? Do they do queuing, do they block waiting for the message to be written, what happens when they flush buffers, what if the log file is on NFS, etc? Lots of the concerns raised here are similar to those questions.

I agree that implementations of this interface can do all sorts of bad things, but I don't see how that's any worse than today. Unless you guys want to forgo using a log system at all for audit logging, and force writing to files as the only option, having your own custom code to do it and avoid as many of the issues discussed here as possible.

The code could definitely force queuing on this code path; since not everybody may need that (the current log approach being the example), I'm wary of turning that into a requirement.

So, those out of the way, a few comments about other things:
. audit logging under the namesystem lock: that can be hacked around. One ugly way would be to store the audit data in a thread local, and flush it in the unlock() methods.

. using the interface for the existing log: that can be easily done; my goal with not changing that part was to not change the existing behavior. I could use the "AUDITLOG access logger" as the default one, that would be very easy to do. A custom access logger would replace it (or we could make the config option a list, this allowing the use of both again).

Allow multiple loggers to be defined; still logs under the namesystem lock.

-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537915/hdfs-3680-v3.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestFsck
                  org.apache.hadoop.hdfs.server.namenode.TestAuditLogs
                  org.apache.hadoop.hdfs.TestDatanodeBlockScanner

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2906//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2906//console

This message is automatically generated.

Fixed default configuration (and TestAuditLogs in the process).

-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12538037/hdfs-3680-v4.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS
                  org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
                  org.apache.hadoop.hdfs.TestPersistBlocks

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2912//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2912//console

This message is automatically generated.

Early comments:

Existing LOG could handle different levels, such as trace, debug, info, warn etc. I understand we probably use info level logs now. Should we consider adding such levels to FSAccessLogger?

Some comments:
# FSAccessLogger.java
#* Why call it FSAccessLogger and not AuditLogger? AuditLogger seems to be a more generic name.
#* You cannot make this InterfaceAudience.Public given HdfsFileStatus and UserGroupInformation are not Public.
#* Add InterfaceStability.Unstable to FSAccessLogger
#* Remove from class javadoc implementation details about FSNamesystem registration etc. Also the note "... cannot influence the result" is unnecessary.
#* "Called after an auth check is" is not relevant for method logAccess.  "logAuditEvent" may be a more appropriate name.
# FSNamesystem.java
#* Do not catch blanket Exception. Instead catch the specific exception you want to handle.
#* Make initializing accessLoggers into a separate method.
#* Please add javadoc to FSAccessLogger#logAuditEvent() indicating the performance of this method can affect the system. The implementations must return quickly.
#* Make DefaultAccessLogger class private
# TestFSNameystem.java
#* Please consider adding a separate test instead of adding this to TestFSNamesystem.java
#* Mock may be better than TestAccessLogger implementation. If you still want to use TestAccessLogger make it private class.


Hi Suresh, thanks for the comments. Replies below.

bq. Existing LOG could handle different levels, such as trace, debug, info, warn etc. I understand we probably use info level logs now. Should we consider adding such levels to FSAccessLogger?

IMO "trace", "debug" et al are related to the logger implementation, not the audit event. The audit event already has information about what it's about; e.g., access allowed / denied, etc. The logger can choose to map that information to something that makes sense in the target; for example, logging denied events with "warning" level. But such level wouldn't make much sense in a different implementation (say, for example, writing to a database). 

bq. Why call it FSAccessLogger and not AuditLogger? AuditLogger seems to be a more generic name.

Fair enough, will change.

bq. You cannot make this InterfaceAudience.Public given HdfsFileStatus and UserGroupInformation are not Public.

That poses an issue, though. Would there be resistance to make those two classes public? The problem with them not being public is that it would then require the information to be exposed in some other way: either a new class that just provides the same information (= code duplication, overhead to create the copy), or a string (difficult to parse, overhead to create the string).

bq. Do not catch blanket Exception. Instead catch the specific exception you want to handle.

Are you OK with catching RuntimeException? I'm really being paranoid here, because I don't want a buggy AccessLogger to suddenly cause the NameNode to go down. Alternatively, the access logger could be disabled when it throws an exception (similar to how HBase disables coprocessors when they throw unexpected exceptions).

bq. Please consider adding a separate test instead of adding this to TestFSNamesystem.java

Any particular reason why? The test is testing functionality of FSNamesystem (instantiating and using custom AccessLoggers), so it makes sense to me that it's part of FSNamesystem's test.

bq. Mock may be better than TestAccessLogger implementation. If you still want to use TestAccessLogger make it private class.

I can't mock because FSNamesystem instantiates the access logger using Class.forName(). I also believe I cannot make it private for the same reason: FSNamesystem trying to call the (now private) constructor would cause an IllegalAccessException.


bq. That poses an issue, though. Would there be resistance to make those two classes public?
The only issue is, we are now limiting our ability to change these classes. I think for now, we should keep the AuditLogger it self as non Pulic. Any one implementing it understands that these interfaces are currently not public.

bq. Are you OK with catching RuntimeException?
Now I see the issue. I think catching Exception in that case is case. Adding a comment to describe the reason would be great. Also we should handle a problem where a logger is not functional/degraded and causes exceptions all the time.

bq. Any particular reason why?
The test is for AuditLogger functionality and not FSNamesystem. The detail that logger list is handled and used in FSNamesystem is an implementation detail.

bq. ... FSNamesystem trying to call the (now private) constructor would cause an IllegalAccessException.
Makes sense.

Applying review feedback; I chose to remove misbehaving audit loggers on the first exception, instead of keeping track of how many exceptions or the rate of exceptions being thrown.

Any idea why the Hadoop QA bot hasn't picked up the new patch?

Not sure, but I just kicked it for you manually.

I have not looked at the patch. I will review it later. 

Thinking more about the approach, just removing audit logger on the first exception has many issue we need to consider.
# With this approach, should namenode run if for some reason we have removed all the audit loggers from the list? My answer would be no, given the importance of audit log.
# Does the system need a mechanism to add/remove audit loggers? When a failed logger is fixed, do we need a way to refresh the audit logger so it is picked up by the Namenode again?
# When you have multiple audit loggers, is there a need to keep them in sync or out of sync audit loggers okay?
# Alternatively, should we consider a separate daemon that runs off of the audit log written to disk and updates other syncs instead of doing it inline in the namenode code?

Answers inline.

bq. With this approach, should namenode run if for some reason we have removed all the audit loggers from the list? My answer would be no, given the importance of audit log.
bq. Does the system need a mechanism to add/remove audit loggers? When a failed logger is fixed, do we need a way to refresh the audit logger so it is picked up by the Namenode again?

Being of the opinion that making it a list wasn't really needed to start with (I can't really see a scenario where you'd have more than one custom logger, which is what my original patch did), I don't think the NameNode should be stopped. Removing the audit logger from the list means the audit logger is buggy, which means that we'd be stopping the NameNode because code outside the control of the NameNode did something bad, which goes back to previous discussions about how this can end up being blamed on the HDFS code while it's not HDFS's fault.

I could, though, always fall back to having the default logger in the list if it ever becomes empty. That would still not generate any audit logs if the logging system is not configured for it.

bq. When you have multiple audit loggers, is there a need to keep them in sync or out of sync audit loggers okay?

Not sure what you mean by "in sync", but my answer here is the same regardless: there is no need to do anything other than what's already being done. Custom loggers, once called, do what they want with the data, and it's out of the NameNode's control at that point.

bq. Alternatively, should we consider a separate daemon that runs off of the audit log written to disk and updates other syncs instead of doing it inline in the namenode code?

See my comment on 20/Jul/12 . I think that's overkill and creates more problems than it solves.


+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12538458/hdfs-3680-v5.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2935//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2935//console

This message is automatically generated.

bq. I can't really see a scenario where you'd have more than one custom logger
Well now that it is list, we cannot predict on how it gets used.

bq. Not sure what you mean by "in sync"
Given that on the first exception we throw away a logger, a logger could miss whole bunch of audit logs. One could envision an audit log that cannot miss entries. So a mechanism to bring the out of sync audit logger "in sync" will be needed. 

bq. See my comment on 20/Jul/12 . I think that's overkill and creates more problems than it solves.
I read through the comments dated 20/Jul. I am not sure I understand why it creates more problems. As regards to overkill, we are introducing pluggability in a critical part. It could result in service availability issue. So lets give due consideration even if it is overkill.

bq. Does the system need a mechanism to add/remove audit loggers? When a failed logger is fixed, do we need a way to refresh the audit logger so it is picked up by the Namenode again?
Please address this previous comment as well.

bq. Given that on the first exception we throw away a logger, a logger could miss whole bunch of audit logs.

You're talking about letting audit loggers recover in some way. I don't think you can have it both ways: either misbehaving loggers are removed, or they're always called, and we just eat the exceptions and let them misbehave for as long as they want to, and potentially recover by themselves. Anything more complicated is, well, more complicated, and I don't think it's worth the extra complexity, even if there is something that could possibly be done.

This covers the "add / remove" audit loggers question too. I don't think that's needed. It's a configuration option; this is really not something that should be changing during runtime.

bq. So a mechanism to bring the out of sync audit logger "in sync" will be needed. 

Again you're assuming that we should try to fix broken audit loggers, something that we can't do. I'm treating broken audit loggers as just that: broken. If they want to not lose audit messages, they should make sure they don't throw exceptions, and handle any unexpected situations internally.

bq. I am not sure I understand why it creates more problems.

Well, let me rephrase. Talking about a separate daemon to serve audit logs is moot because, well, it wouldn't be HDFS code anymore. Anyone can do that without requiring a single line change in HDFS.

My point with submitting this patch is that I think it's worth it to have an interface closer to the core for audit logs. It provides loggers with richer information about the context of the logs, information that may not exist in the text messages written to a log file. It also provides an interface that is more stable than a string in a log file (as in, if you change it, the compiler will yell at you).

Just to reiterate my previous point, it is possible today to have a custom audit logger inside the NameNode without my patch; you just have to implement a log appender for the log system in use, and configure the logging system accordingly. With that, you get all the issues with custom loggers (they run synchronously in a critical section of the NameNode, they can bring down the whole process, etc) without any of the benefits of a proper interface (they have to parse a string to get any context data about the log, potentially losing info). Which is what led me to propose a cleaner way of doing it.

Given the designed use case for audit loggers -- they are creating business critical immutable logs of filesystem access -- I do not agree that the NN should always continue to operate when an audit logger fails.

The purpose of audit logging is, "*every* time a user accesses a file, an audit log entry is generated".  Not "well, most of the time, unless some system had a hiccup, in which case we just silently allow access without audit logs".  Think of it as similar to an authentication system; if PAM is unable to access the LDAP server, it returns DENY, not ALLOW.

Yes, this adds points of failure to the system -- if a fail-closed audit log fails, then the NN will stop allowing access. For customers that need this capability, that may be an acceptable trade-off; that's a decision that should be left to the customer, and for which we should establish sensible defaults.

I think the simplest solution is for the NN to simply shutdown if the AuditLogger fails.  This probably needs a warning in fairly big letters.

If there's user demand for fail-open audit logs, that can be done in the AuditLogger implementation, by catching all exceptions and swallowing them.  Or we could add separate configurations for "fallible audit loggers" and "critical audit loggers".

(I'm willing to listen to contrary opinions in this area -- if you think I'm wrong about this, please do say so!)

Just to be clear, I don't have any strong opinion over whether the NN should fail or continue if an access logger fails. I just implemented the "stop logging on failure" thing because there were comments about third party code bringing the NN down and that being seen as an HDFS bug. The config route is an option, but I'm always loth to add more config options unless strictly necessary.

The only thing I'm against is building a complicated system where we try to, in some way, "fix" the audit logger (by, let's say, re-instantiating it). I think that sort of logic belongs in the audit logger implementation itself.

bq.  I don't think you can have it both ways: either misbehaving loggers are removed, or they're always called...
I think you are missing my point. Misbehaving loggers should be removed. But given the importance of audit logging, if the error condition associated with that logger is fixed, we need a mechanism to reinstate that logger back without having to restart the namenode. By the same token, we also need to discuss how a logger that was removed can get back in sync with the logs it might have missed. If it is complicated to implement, then lets discuss how important such a functionality is.

bq. Well, let me rephrase. Talking about a separate daemon to serve audit logs is moot because, well, it wouldn't be HDFS code anymore. Anyone can do that without requiring a single line change in HDFS.
If we conclude that it is a better solution, why not.

bq.  it is possible today to have a custom audit logger inside the NameNode without my patch
Now I see your earlier point. However since this is adding support in HDFS, lets consider the use cases we need to support. 

bq. we could add separate configurations for "fallible audit loggers" and "critical audit loggers"
I was thinking along the same lines. But why would one use fallible audit logger, to build what functionality, and what guarantees they can provide with such a solution. Are there valid "best effort only", "might be incomplete" solutions around audit logging? This made me conclude that only thing that is really useful is "critical audit loggers".
 


bq. This made me conclude that only thing that is really useful is "critical audit loggers".

+1. I believe this is the case with log4j -- if the current log4j volume fills up, I'm pretty sure I've seen the NN just abort itself, rather than keep going without logs. I prefer this behavior.

bq. I think you are missing my point. Misbehaving loggers should be removed. But given the importance of audit logging, if the error condition associated with that logger is fixed, we need a mechanism to reinstate that logger back without having to restart the namenode.

How can you know that the error condition was fixed, if you have no idea of what it was in the first place? How can you know that a logger that threw a NullPointerException will now stop throwing it?

Which is why I took the simpler approach, which is not to make assumptions. The only thing that needs to be settled, in my view, is what to do when the logger throws an exception: ignore (and log it), and hope it will recover by itself in subsequent calls, or log, loudly, that the logger failed, and stop using that logger.

I understand the importance of audit logging, but I think that anybody who's implementing an audit logger should understand that too, and make their logger resilient to errors. There's very little the FSNamesystem code can do other than the two options above, from what I can see.

bq. what to do when the logger throws an exception

I think the right answer to this question is "abort the NN". We cannot continue to provide NN service without the audit logger that the admin configured.

Just throwing a raw idea out there that might address many of the concerns:
* The NN has an option for an external logger daemon
* HDFS provides the optional audit daemon maybe as a simple RPC server
* The audit data is encoded in a protobuf, json, etc to avoid unreliable parsing
* The custom audit logger(s) implement a interface similar to that in the patch

Advantages for the NN:
* Isolated from dangerous code that might call System.exit, segfault in native code, etc
* Isolated from leaky code
* If RPC is used, we can leverage secure auth, configurable retries, perhaps failover, etc

Hi Daryn,

bq. The NN has an option for an external logger daemon

So, this implies some sort of IPC, either to another local process or possibly even a remote one. Which raises different questions, some of which have been asked before:

* Are audit logs written synchronously?
* If yes, is the extra latency for the RPC call acceptable?
* If no, how do we reliably know that audit logs have been written?
* What's an acceptable timeout for the RPC call?

And it doesn't really answer the question of "what to do on failure", although the consensus there seems to be to "abort". I'm not against this solution per se, I just don't think that, in the end, it's much different than allowing 3rd party code into the NN. I'll refer back to ATM's comment that we expect people who are writing audit loggers to know what they're doing.


On a different note, I played with the "let exceptions flow" approach, and it seems that the NN catches the exception at some point and morphs it into a RemoteException. I actually like that better than shutting down the NN; it fails the particular operation, without shutting down the NN itself, which allows the logger to recover if it can. Of course a really broken logger will just keep throwing exceptions, but then IMO the user is asking for it by installing a broken custom logger.

Apologies if I wasn't clear, but by RPC I meant using the existing RPC server framework used by all hadoop daemons.  That's where we can leverage secure auth, retries, timeouts, failover, etc.

* Are audit logs written synchronously?

Consensus seems to be yes, I'm inclined to agree.

* If yes, is the extra latency for the RPC call acceptable?

I'd assume it can't be that bad since most of hadoop uses it.

* If no, how do we reliably know that audit logs have been written?

The RPC proxies implement synchronous calls.

* What's an acceptable timeout for the RPC call?

It's configurable, so it's based on what the particular user will tolerate?

bq.   I just don't think that, in the end, it's much different than allowing 3rd party code into the NN

Sure it is, as noted before, it buffers the NN against 3rd party code calling exit, segfaulting in JNI, memory and fd leaks, OOM, etc.  If the daemon misbehaves or blows up then it shouldn't compromise the integrity of the namespace.

bq. Sure it is, as noted before, it buffers the NN against 3rd party code calling exit, segfaulting in JNI, memory and fd leaks, OOM, etc. If the daemon misbehaves or blows up then it shouldn't compromise the integrity of the namespace.

The consensus seems to be that if you can't log an audit event the NN should blow up (either fail the request or just shut itself down entirely). So even though the daemon is in a different address space, in that view it would still affect the functionality of the NN.

Yes, a faulty customer logger will affect the functionality of the NN.  The question is what is level of risk to the NN, and is it acceptable?  Let's say the db driver segfaults or scribbles on memory.  If the logger is running within the NN, the namespace integrity is potentially compromised.  If it's in a daemon, the NN will notice the RPC server crashed and may initiate a clean shutdown.

bq. Yes, a faulty customer logger will affect the functionality of the NN. The question is what is level of risk to the NN, and is it acceptable? 

I'll reiterate my previous point: this is not arbitrary users writing and installing custom logger implementations into the NN. This will likely be a handful of people writing these, and operators will have to consciously install them into the NN. The people who are involved in writing a custom logger should be aware of the inherent risks of doing so and should write defensive code. This is not the place in the Hadoop code where we should be holding the hand of the users.

I think this point is further supported by your observation that people seem to agree that shutting down the NN is the right answer if an event fails to get logged by a custom logger. If that's what we intend to have happen in the event of custom audit log failure, then the worst case scenario of a custom audit logger seg faulting or calling System.exit really isn't that bad. The NN already has to handle an ungraceful shutdown and maintain data integrity, so the marginal increase of danger should be low.

bq. If it's in a daemon, the NN will notice the RPC server crashed and may initiate a clean shutdown.

This is one potential implementation of a custom audit logger that would potentially reduce the risk, but we should not force this design on the writers of custom loggers.

bq. I'd assume it [latency of RPC call] can't be that bad since most of hadoop uses it.

I agree with Marcelo that the latency of doing an RPC per audit log is likely unacceptably high. Just because the latency of the Hadoop Server/Client implementations is acceptable for FS/MR job operations doesn't mean it's sufficient for audit logging. My guess would be that it's not acceptable for anything but the smallest use cases. At the very least, it seems to make the performance near acceptable we couldn't actually do an RPC per log event, but instead would have to buffer and group the events into fewer calls, effectively group commit on the log events. That sort of complexity should be left up to the custom logger author, if using a separate daemon is actually required.

For clarification, I was throwing out ideas and concerns to think about.  I don't intend to review and/or block this jira.

Change code to let exceptions from audit loggers propagate to the caller methods.

+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12539080/hdfs-3680-v6.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2945//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2945//console

This message is automatically generated.

Made AuditLogger interface public, avoiding usage of UserGroupInformation and HdfsFileStatus classes.

Only concern with this change is that exposing just the user name misses a lot of information on the principal held by UGI; don't know how interesting that information is for audit logs, though, since the service configuration should provide a lot of the details (like auth type, realm, etc).

-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12541843/hdfs-3680-v7.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestReplication

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3060//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/3060//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3060//console

This message is automatically generated.

Sorry for letting this sit for so long.

The latest patch looks pretty good to me. One small comment: instead of passing the result of UserGroupInformation#getUserName, I suggest you pass UserGroupInformation#toString, which has some extra salient information for audit purposes. You should probably also change the formal parameter name to something like "userString" instead of "userName", to indicate that it might contain more information than just the username.

+1 once this is addressed.

Comments:
# The patch needs to document the newly added parameter. The document should include how to set it up, the expectation from the audit log implementation and the impact of this configuration when things do not work correctly.
# dfs.namenode.access.logger should be dfs.namenode.audit.logger
# TestAuditLogger - add javadoc and @link to the functionality being tested
# Minor - there is a mention of FSAccessLogger in DefaultAuditLogger javadoc
# what is the reason symlink is being done in logAuditEvent? Why is it a part of this jira?
# How does one add DefaultAuditLogger with a custom audit loggers? How does isAuditEnabled() method work if you add an ability to setup DefaultAuditLogger?
# java.security.principal unnecessary import in FSNamesystem.java
# FSNamesystem#auditLog should be moved to DefaultAuditLogger. Also why is auditLog used for logging in method getFileInfo and mkdirs still? Why not new auditloggers used for that?
# Should AuditLogger#logAuditEvent consider throwing IOException to indicate error?
# Sorry I have not caught up all the comments - what is the final decision on how to handle logger errors? Currently the client gets an exception when logAuditEvent fails. That does not seem to be correct.



Hi Suresh, I think I addressed all your concerns (will upload new patch after testing).

bq. what is the reason symlink is being done in logAuditEvent? Why is it a part of this jira?

What do you mean "symlink is being done"? I'm not creating any symlinks. I'm creating a FileStatus object based on an HdfsFileStatus, which is a private audience class and thus cannot be used in the public audience AuditLogger.

bq. How does one add DefaultAuditLogger with a custom audit loggers? How does isAuditEnabled() method work if you add an ability to setup DefaultAuditLogger?

I hope I addressed this in the new documentation. For your second question, you should take a look at the "isDefaultAuditLogger" in FSNamesystem.java.

bq. FSNamesystem#auditLog should be moved to DefaultAuditLogger.

Since that field is public and used in other places, I'd rather not touch that.

bq. Should AuditLogger#logAuditEvent consider throwing IOException to indicate error?

IOException would be one of many exceptions custom audit loggers could throw. So I don't see why it should be special-cased here. My opinion is that audit loggers in general shouldn't throw exceptions; if they really want to, having to throw a RuntimeException indicates that it's not really an expected case.

bq. Sorry I have not caught up all the comments - what is the final decision on how to handle logger errors? Currently the client gets an exception when logAuditEvent fails. That does not seem to be correct.

I don't think there was an ultimate decision; my current patch follows the "things work just like before" approach: currently, if the logging system throws some kind of exception, it fails the request, just like the new code does. If that's not desired, then it can be changed, but I think that's outside the scope of this patch.


{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12547810/hdfs-3680-v8.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3268//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3268//console

This message is automatically generated.

bq. What do you mean "symlink is being done"? I'm not creating any symlinks. I'm creating a FileStatus object based on an HdfsFileStatus, which is a private audience class and thus cannot be used in the public audience AuditLogger.

Why is the following code part of this patch?
{noformat}
+    if (stat != null) {
+      Path symlink = stat.isSymlink() ? new Path(stat.getSymlink()) : null;
+      Path path = dst != null ? new Path(dst) : new Path(src);
+      status = new FileStatus(stat.getLen(), stat.isDir(),
+          stat.getReplication(), stat.getBlockSize(), stat.getModificationTime(),
+          stat.getAccessTime(), stat.getPermission(), stat.getOwner(),
+          stat.getGroup(), symlink, path);
+    }
{noformat}

bq. IOException would be one of many exceptions custom audit loggers could throw. So I don't see why it should be special-cased here. My opinion is that audit loggers in general shouldn't throw exceptions; if they really want to, having to throw a RuntimeException indicates that it's not really an expected case.
Given loggers are going to some kind of io, to database, or some server etc. IOException should be expected and seems like a logical thing to throw and not RunTimeException. The user of audit logger needs to deal with that exception.

bq. I don't think there was an ultimate decision; my current patch follows the "things work just like before" approach: currently, if the logging system throws some kind of exception, it fails the request, just like the new code does. If that's not desired, then it can be changed, but I think that's outside the scope of this patch.
I do not think it is outside the scope of this patch. Current logger could fail, on system failures. However here it may fail because poorly written code, network errors etc. I thought we decided NN will abort [here|https://issues.apache.org/jira/browse/HDFS-3680?focusedCommentId=13427039&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13427039], [here|https://issues.apache.org/jira/browse/HDFS-3680?focusedCommentId=13427043&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13427043] and you seem to agree [here|https://issues.apache.org/jira/browse/HDFS-3680?focusedCommentId=13427550&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13427550]. Let me know if I missed any discussion that decided contrary to that.

hdfs-default.xml document does not cover my previous comment:
bq.  The document should include how to set it up, the expectation from the audit log implementation and the impact of this configuration when things do not work correctly.


bq. Why is the following code part of this patch?

See my previous response:

"I'm creating a FileStatus object based on an HdfsFileStatus, which is a private audience class and thus cannot be used in the public audience AuditLogger."

bq. Given loggers are going to some kind of io, to database, or some server etc. IOException should be expected and seems like a logical thing to throw and not RunTimeException.

You're making assumptions about the implementation of the logger. Why would it throw IOException and not SQLException? What if my logger doesn't do any I/O in the thread doing the logging at all? Saying "throws IOException" would just make implementors wrap whatever is the real exception being thrown in an IOException instead of a RuntimeException, to no benefit I can see. If FSNamesystem should handle errors from custom loggers, it should handle all errors, not just specific ones.

bq. I do not think it is outside the scope of this patch. Current logger could fail, on system failures. However here it may fail because poorly written code

We can't prevent badly written code from doing bad things (also see previous comments from ATM that this is not an interface that people will be implementing willy nilly - people who'll touch it are expected to know what they are doing). The reason I say it's out of the scope of this patch is because it's a change in the current behavior that's unrelated to whether you have a custom audit logger or not; if audit logs should cause the name node to shut down, that's a change that needs to be made today, right now, independent of this patch going in or not.

. hdfs-default.xml document does not cover my previous comment

It covers details related to configuration. Details about what the implementation is expected to do should be (and are) documented in the interface itself, which is the interface that the person writing the implementation will be looking at.

If you're talking about the "what about when things don't work correctly part", I'll wait for closure on the other comments.

bq. You're making assumptions about the implementation of the logger. Why would it throw IOException and not SQLException? What if my logger doesn't do any I/O in the thread doing the logging at all? Saying "throws IOException" would just make implementors wrap whatever is the real exception being thrown in an IOException instead of a RuntimeException, to no benefit I can see. If FSNamesystem should handle errors from custom loggers, it should handle all errors, not just specific ones.
Generally this type of interface could be implemented by implementation that can connect to local or remote service. Even though only remote implementations throw IOException, the interfaces are defined to throw IOExceptions. See our RPC interface examples.

That said if you do not want to throw IOException, which I recommend, is fine. However you need to make sure you throw a checked exception from this interface, to design it correctly, to force the caller to handle the error condition. RTE which typically indicates programming error is not the choice.

bq. We can't prevent badly written code from doing bad things (also see previous comments from ATM that this is not an interface that people will be implementing willy nilly - people who'll touch it are expected to know what they are doing). The reason I say it's out of the scope of this patch is because it's a change in the current behavior that's unrelated to whether you have a custom audit logger or not; if audit logs should cause the name node to shut down, that's a change that needs to be made today, right now, independent of this patch going in or not.

I disagree. You are allowing audit logger to be pluggable in this patch. What is the impact of making logging pluggable and how namenode must deal with is the relevant questions for this patch. Not sure you looked at this comment from Todd, that I agree with:
bq. +1. I believe this is the case with log4j  if the current log4j volume fills up, I'm pretty sure I've seen the NN just abort itself, rather than keep going without logs. I prefer this behavior.

Other thing I have not completely thought through but bother me is, when these failures happen, a failure response is sent to the client, though it is successful on Namenode and is recorded on editlog. I would like others opinion on this.

bq. Details about what the implementation is expected to do should be (and are) documented in the interface itself, which is the interface that the person writing the implementation will be looking at.
I think you still need this in the hdfs-default.xml. The programmers who implements this interface and an administrator who configures it could be different. The side effects of this configuration change should be documented for an administrator so he understands the impact of this.


bq. That said if you do not want to throw IOException, which I recommend, is fine. However you need to make sure you throw a checked exception from this interface, to design it correctly, to force the caller to handle the error condition. RTE which typically indicates programming error is not the choice.

My main point in resisting declaring any checked exception here is that not doing that makes it clear that loggers are not expected to throw exceptions, and thus should avoid doing so whenever possible. From FSNamesystem's point of view, there's no point in handling checked and unchecked exceptions differently; and audit log failure should be handled the same way regardless of the nature of the failure, unless you want to do crazy things like have a "RetriableFailureException" to indicate that FSNamesystem can call the method again to see if it works.

bq. I disagree. You are allowing audit logger to be pluggable in this patch. What is the impact of making logging pluggable and how namenode must deal with is the relevant questions for this patch. Not sure you looked at this comment from Todd, that I agree with:

I'm making it easier, but as I've mentioned before, it's already possible to use custom loggers by doing it at the log system layer (e.g. a custom log4j appender; just use http://wiki.apache.org/logging-log4j/JDBCAppenderConfiguration, for example, and you have many more failure cases than just writing to a local file). 

That's why I say that if you're worried about that case, that particular change should be made in the current code regardless of this patch. But if you feel strongly about it, I can add a try...catch that just shuts down the namenode.

(BTW, unless log4j itself shuts down the process by calling System.exit(), which my experience says is not the case, I don't see where this "shutdown on audit log error" code is.)

bq. Other thing I have not completely thought through but bother me is, when these failures happen, a failure response is sent to the client, though it is successful on Namenode and is recorded on editlog. I would like others opinion on this.

That's independent of this patch (ignoring the "broken audit logger" argument). The problem is that the audit log is written after the operation succeeds; to fix that, the permission check and implementation of each operation should be separated, so that you could write the audit log before executing the operation.

bq. The side effects of this configuration change should be documented for an administrator so he understands the impact of this.

What side effects are you talking about? The only one would be "if the audit logger fails, namenode will shut down", if that behavior is implemented. Otherwise, any side-effect is implementation-specific and it's impossible to say anything other than "there might be side effects".


Just to end the "what does log4j do" discussion, here's what happens when log4j appenders throw exceptions/errors:

2012-10-05 16:32:05,748 WARN org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo from 172.29.110.222:48751: error: java.lang.RuntimeException: testing exception handling.
java.lang.RuntimeException: testing exception handling.
        at test.AppenderBase.append(AppenderBase.java:72)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.log(Category.java:856)
        at org.apache.commons.logging.impl.Log4JLogger.info(Log4JLogger.java:199)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logAuditEvent(FSNamesystem.java:258)

2012-10-05 16:32:08,820 WARN org.apache.hadoop.ipc.Server: IPC Server handler 11 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo from 172.29.110.222:48754: error: java.lang.Error: testing error handling.
java.lang.Error: testing error handling.
        at test.AppenderBase.append(AppenderBase.java:74)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.log(Category.java:856)
        at org.apache.commons.logging.impl.Log4JLogger.info(Log4JLogger.java:199)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logAuditEvent(FSNamesystem.java:258)

log4j appenders in general use log4j's ErrorHandler when errors occur, instead of throwing exceptions. The default one (http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/helpers/OnlyOnceErrorHandler.html) just drops errors after printing the first one to stderr. So the NameNode is not shut down; it just keeps running, and audit logs are silently dropped. Which is another reason why I'll maintain that the "what to do when logging an audit fails" issue is not particular to my patch.

Re-based and re-tested (a.k.a. "ping").

Suresh, are you satisfied with Marcelo's responses to your comments? If so, I'll go ahead and review this patch for commit.

{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12553408/hdfs-3680-v9.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3496//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3496//console

This message is automatically generated.

The latest patch looks pretty good to me. The only little issue that I noticed is that the word "explicitly" is misspelled here:
{code}
+   * Default AuditLogger implementation; used when no access logger is
+   * defined in the config file. It can also be explocitly listed in the
{code}

I'm +1 on this patch once this is addressed.

Suresh - if you have any more comments about this latest patch, please chime in. I'd like to commit this in the next day or two, but I'll obviously hold off if you've got more comments/concerns.

rebased + corrected typo.

{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12555381/hdfs-3680-v10.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3574//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3574//console

This message is automatically generated.

+1, the latest patch looks good to me.

I'm going to commit this tomorrow unless anyone has further comments in the meantime. (Suresh?)

Aaron, sorry for the delay to look at this jira. I will post a comment by the end of the day today.

OK, I just committed this to trunk and was in the process of committing it to branch-2 when you commented. I'll revert the trunk commit and wait for your comments.

I've just reverted the trunk commit, but you'll still probably see some Hudson integration comments here. Please just ignore them.

Suresh - thanks for taking another look. I think that Marcelo's investigation has probably addressed all of the concerns you've raised previously, but it will be good to hear your thoughts.

Integrated in Hadoop-trunk-Commit #3077 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3077/])
    Reverting initial commit of HDFS-3680 pending further comments. (Revision 1415797)
HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin. (Revision 1415794)

     Result = SUCCESS
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1415797
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java

atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1415794
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java


bq. Suresh - thanks for taking another look. I think that Marcelo's investigation has probably addressed all of the concerns you've raised previously, but it will be good to hear your thoughts.
No problem. If there are further comments I will post them.

Integrated in Hadoop-Yarn-trunk #53 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/53/])
    Reverting initial commit of HDFS-3680 pending further comments. (Revision 1415797)
HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin. (Revision 1415794)

     Result = SUCCESS
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1415797
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java

atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1415794
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java


Integrated in Hadoop-Hdfs-trunk #1243 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1243/])
    Reverting initial commit of HDFS-3680 pending further comments. (Revision 1415797)
HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin. (Revision 1415794)

     Result = FAILURE
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1415797
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java

atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1415794
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java


Integrated in Hadoop-Mapreduce-trunk #1274 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1274/])
    Reverting initial commit of HDFS-3680 pending further comments. (Revision 1415797)
HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin. (Revision 1415794)

     Result = SUCCESS
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1415797
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java

atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1415794
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java


Suresh, any comments here?

I have no further comments. The patch looks good.

I've just committed this to trunk and branch-2.

Thanks a lot for the contribution, Marcelo, and thanks to Todd, Andy, Daryn, and Suresh for the reviews.

Integrated in Hadoop-trunk-Commit #3093 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3093/])
    HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin. (Revision 1418114)

     Result = SUCCESS
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1418114
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java


Integrated in Hadoop-Yarn-trunk #58 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/58/])
    HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin. (Revision 1418114)

     Result = SUCCESS
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1418114
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java


Integrated in Hadoop-Hdfs-trunk #1247 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1247/])
    HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin. (Revision 1418114)

     Result = FAILURE
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1418114
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java


Integrated in Hadoop-Mapreduce-trunk #1278 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1278/])
    HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin. (Revision 1418114)

     Result = SUCCESS
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1418114
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java


