Seems this is the last enemy.

This is what I have found now. Hope it helps.

{noformat}
2015-03-03 22:17:04,776 INFO  [asf900.gq1.ygridcore.net,35522,1425421016549-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-4] master.RegionStateStore(207): Updating row testRegionTransitionOperations,eee,1425421019650.9d08f4e7ce46a76e88b9e5265b81ff83. with state=PENDING_CLOSE
2015-03-03 22:17:04,776 INFO  [PriorityRpcServer.handler=7,queue=1,port=45988] regionserver.RSRpcServices(1347): Open testRegionTransitionOperations,yyy,1425421019650.e2998ed5019dc219cc3c0112541c0cdf.
{noformat}

Grep e2998ed5019dc219cc3c0112541c0cdf, you can see that this should be the last stage when moving a region. So it means we begin to disable the table when there are region transitions that have not finished yet.

So this maybe an insufficient waitForRITtoBeZero implementation. But I think disable table should deal with this situation. It is not acceptable that disable a table will fail if there are regions in transition I think.

{noformat}
2015-03-03 22:17:04,715 DEBUG [Thread-268] master.AssignmentManager(1295): Starting unassign of testRegionTransitionOperations,yyy,1425421019650.e2998ed5019dc219cc3c0112541c0cdf. (offlining), current state: {e2998ed5019dc219cc3c0112541c0cdf state=OPEN, ts=1425421021849, server=asf900.gq1.ygridcore.net,44226,1425421017118}

2015-03-03 22:17:04,758 INFO  [MASTER_TABLE_OPERATIONS-asf900:35522-0] handler.DisableTableHandler(123): Attempting to disable table testRegionTransitionOperations

2015-03-03 22:17:04,765 INFO  [MASTER_TABLE_OPERATIONS-asf900:35522-0] handler.DisableTableHandler(166): Offlining 26 regions.
{noformat}

There is only one log line for unassigning e2998ed5019dc219cc3c0112541c0cdf and it is before we disabling testRegionTransitionOperations so it should be the region moving(offline and then online). The number of regions in log is right(26 regions), but seems we skip the offline process if region is in transition.
{code:title=DisableTableHandler.java}
    @Override
    protected void populatePool(ExecutorService pool) {
      RegionStates regionStates = assignmentManager.getRegionStates();
      for (HRegionInfo region: regions) {
        if (regionStates.isRegionInTransition(region)
            && !regionStates.isRegionInState(region, RegionState.State.FAILED_CLOSE)) {
          continue;
        }
        final HRegionInfo hri = region;
        pool.execute(Trace.wrap("DisableTableHandler.BulkDisabler",new Runnable() {
          public void run() {
            assignmentManager.unassign(hri);
          }
        }));
      }
    }
{code}
So the problem maybe the code how we deal with the regions in transition state when disabling a table? I have not found the code yet.

That is the same situation here, I think.
https://issues.apache.org/jira/browse/HBASE-13076

bq. Table become ENABLED, thats leads to regions instructed to assign immediately on onRegionClosed. BulkDisabler will not know about that and will wait indefinitely, because it will not issue unassign for newly opened regions.


The region moving of e2998ed5019dc219cc3c0112541c0cdf is not triggered by client, it should be the balancer.
{noformat}
2015-03-03 22:17:04,713 INFO  [Thread-268] master.HMaster(1171): balance hri=testRegionTransitionOperations,yyy,1425421019650.e2998ed5019dc219cc3c0112541c0cdf., src=asf900.gq1.ygridcore.net,44226,1425421017118, dest=asf900.gq1.ygridcore.net,45988,1425421016996
{noformat}
[~octo47], I do not think it is caused by table enabling because this is a region moving, not just assigning.

There are 7 regions moving trigger by client
{noformat}
2015-03-03 22:17:04,011 INFO  [Thread-268] master.HMaster(1278): Client=null/null move hri=testRegionTransitionOperations,,1425421019650.f0fadadca3541ee837977b545ecabba2., src=asf900.gq1.ygridcore.net,45988,1425421016996, dest=asf900.gq1.ygridcore.net,44226,1425421017118, running balancer
2015-03-03 22:17:04,105 INFO  [Thread-268] master.HMaster(1278): Client=null/null move hri=testRegionTransitionOperations,ccc,1425421019650.00fe88fb2e6ed600a05c96283ba13055., src=asf900.gq1.ygridcore.net,45988,1425421016996, dest=asf900.gq1.ygridcore.net,44226,1425421017118, running balancer
2015-03-03 22:17:04,121 INFO  [Thread-268] master.HMaster(1278): Client=null/null move hri=testRegionTransitionOperations,ddd,1425421019650.55adc74733629e36d79051d5ca9150b1., src=asf900.gq1.ygridcore.net,45988,1425421016996, dest=asf900.gq1.ygridcore.net,44226,1425421017118, running balancer
2015-03-03 22:17:04,130 INFO  [Thread-268] master.HMaster(1278): Client=null/null move hri=testRegionTransitionOperations,fff,1425421019650.a44832b7d6de27281a976a74dfff9c7b., src=asf900.gq1.ygridcore.net,45988,1425421016996, dest=asf900.gq1.ygridcore.net,44226,1425421017118, running balancer
2015-03-03 22:17:04,140 INFO  [Thread-268] master.HMaster(1278): Client=null/null move hri=testRegionTransitionOperations,jjj,1425421019650.6012302985744f8f91e8082742d1a033., src=asf900.gq1.ygridcore.net,45988,1425421016996, dest=asf900.gq1.ygridcore.net,44226,1425421017118, running balancer
2015-03-03 22:17:04,149 INFO  [Thread-268] master.HMaster(1278): Client=null/null move hri=testRegionTransitionOperations,nnn,1425421019650.30840d6dbbfc8512a13014e60ab758b3., src=asf900.gq1.ygridcore.net,45988,1425421016996, dest=asf900.gq1.ygridcore.net,44226,1425421017118, running balancer
2015-03-03 22:17:04,160 INFO  [Thread-268] master.HMaster(1278): Client=null/null move hri=testRegionTransitionOperations,ooo,1425421019650.44abfcf09b9f08a8dbdfe9185747db34., src=asf900.gq1.ygridcore.net,45988,1425421016996, dest=asf900.gq1.ygridcore.net,44226,1425421017118, running balancer
{noformat}
No e2998ed5019dc219cc3c0112541c0cdf. We turn on balancer at the end of test and call master.balance, so it is possible that there is region in transition when we disabling the table.

And I think the direct reason is we skip regions in transition when deploy tasks, but we do not ignore these regions in waitUntilDone, so only a timeout can let us deploy the new tasks for remaining regions and the default timeout is 5 minutes thus we get a test timeout.

But I blamed the related code, seems there is nothing changed recently, so I do not know the root reason why it becomes flaky only recently...


I think the problem maybe HBASE-11611. HBASE-11611 cleaned up zk related codes. But in nodeDeleted method, we will check if the onlined region is belonged to a disabled table. If so, we will offline the region.
{code:title=AssignmentManager.java}
                  if (disabled) {
                    // if server is offline, no hurt to unassign again
                    LOG.info("Opened " + regionNameStr
                        + "but this table is disabled, triggering close of region");
                    unassign(regionInfo);
                  }
{code}
So the unit test is only flaky on master.
See the full code here: https://github.com/apache/hbase/blob/branch-1/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java

I have not found codes with same function in master, so this maybe the reason? [~stack] [~jxiang]



Oh sorry, I found the code.

{code:title=AssignmentManager.java}
  private String onRegionOpen(final RegionState current, final HRegionInfo hri,
      final ServerName serverName, final RegionStateTransition transition) {
    ...
    if (getTableStateManager().isTableState(hri.getTable(),
            TableState.State.DISABLED, TableState.State.DISABLING)) {
      invokeUnAssign(hri);
    }
    ...
  }
{code}
So let me see if there is a race condition.

Yeah, same with HBASE-13076, it is because we reset the table state to ENABLE so the above code does not work.
{noformat}
2015-03-03 22:17:04,764 DEBUG [AM.-pool2-t13] master.AssignmentManager(1020): Setting table testRegionTransitionOperations to ENABLED state.
{noformat}
Let me see why we call assign during disabling...

OK, is this a possible race condition?

When moving a region, we will call invokeAssign at the end of onRegionClosed. This will schedule a async assign task.

There are lots of places where we check for table disable, but we can pass all the check and arrive here
{code:title=AssignmentManager.java}
  /**
   * Caller must hold lock on the passed <code>state</code> object.
   * @param state
   * @param forceNewPlan
   */
  private void assign(RegionState state, boolean forceNewPlan) {
        ...
        // In case of assignment from EnableTableHandler table state is ENABLING. Any how
        // EnableTableHandler will set ENABLED after assigning all the table regions. If we
        // try to set to ENABLED directly then client API may think table is enabled.
        // When we have a case such as all the regions are added directly into hbase:meta and we call
        // assignRegion then we need to make the table ENABLED. Hence in such case the table
        // will not be in ENABLING or ENABLED state.
        TableName tableName = region.getTable();
        if (!tableStateManager.isTableState(tableName,
          TableState.State.ENABLED, TableState.State.ENABLING)) {
          LOG.debug("Setting table " + tableName + " to ENABLED state.");
          setEnabledTable(tableName);
        }
        ...
  }
{code}
Notice that here we only hold the region lock, not table lock. So let's stop this thread, and start the DisableTableHandler Thread. In prepare method, it will set table state to DISABLING under the protect of table lock. And in handleDisableTable, we will set it DISABLING one more time(without any lock)
{code:title=DisableTableHandler.java}
  public DisableTableHandler prepare()
      throws TableNotFoundException, TableNotEnabledException, IOException {
      ...
      // There could be multiple client requests trying to disable or enable
      // the table at the same time. Ensure only the first request is honored
      // After that, no other requests can be accepted until the table reaches
      // DISABLED or ENABLED.
      //TODO: reevaluate this since we have table locks now
      if (!skipTableStateCheck) {
        if (!this.assignmentManager.getTableStateManager().setTableStateIfInStates(
          this.tableName, TableState.State.DISABLING,
          TableState.State.ENABLED)) {
          LOG.info("Table " + tableName + " isn't enabled; skipping disable");
          throw new TableNotEnabledException(this.tableName);
        }
      }
      ...
  }

  private void handleDisableTable() throws IOException {
    // Set table disabling flag up in zk.
    this.assignmentManager.getTableStateManager().setTableState(this.tableName,
      TableState.State.DISABLING);
    ..
  }
{code}
And after this, the DisableTableHandler will enter a 'while(true)' loop and break until all regions are offline. Now let us stop the DisableTableHandler thread. Since there is no lock protection so the assign thread is free to wake up. So it will do the check and find that the table is not in ENABLED or ENABLING state so it will set it to ENABLED. So, the DisableTableHandler will wait forever.

Here is the log
{noformat}
2015-03-03 22:17:04,754 DEBUG [RS_CLOSE_REGION-asf900:44226-2] handler.CloseRegionHandler(122): Closed testRegionTransitionOperations,yyy,1425421019650.e2998ed5019dc219cc3c0112541c0cdf.
2015-03-03 22:17:04,758 INFO  [MASTER_TABLE_OPERATIONS-asf900:35522-0] handler.DisableTableHandler(123): Attempting to disable table testRegionTransitionOperations
2015-03-03 22:17:04,758 DEBUG [AM.-pool2-t13] master.AssignmentManager(1203): Found an existing plan for testRegionTransitionOperations,yyy,1425421019650.e2998ed5019dc219cc3c0112541c0cdf. destination server is asf900.gq1.ygridcore.net,45988,1425421016996 accepted as a dest server = true
2015-03-03 22:17:04,759 DEBUG [AM.-pool2-t13] master.AssignmentManager(1243): Using pre-existing plan for testRegionTransitionOperations,yyy,1425421019650.e2998ed5019dc219cc3c0112541c0cdf.; 
2015-03-03 22:17:04,764 DEBUG [AM.-pool2-t13] master.AssignmentManager(1020): Setting table testRegionTransitionOperations to ENABLED state.
2015-03-03 22:17:04,765 INFO  [MASTER_TABLE_OPERATIONS-asf900:35522-0] hbase.MetaTableAccessor(1430): Updated table testRegionTransitionOperations state to DISABLING in META
2015-03-03 22:17:04,765 INFO  [MASTER_TABLE_OPERATIONS-asf900:35522-0] handler.DisableTableHandler(166): Offlining 26 regions
2015-03-03 22:17:04,770 INFO  [AM.-pool2-t13] hbase.MetaTableAccessor(1430): Updated table testRegionTransitionOperations state to ENABLED in META
2015-03-03 22:17:04,770 INFO  [AM.-pool2-t13] master.AssignmentManager(1023): Assigning testRegionTransitionOperations,yyy,1425421019650.e2998ed5019dc219cc3c0112541c0cdf. to asf900.gq1.ygridcore.net,45988,1425421016996
{noformat}

[~Apache9] let me review the above in (my) morning...

[~zhangduo] at first I tried to make thread BulkUnassign more intelligent, it peeks regions and later rechecks that all regions not OPENED, if so it rescheduled. But later I found piece of code removed in HBASE-13076 in this test never failed in my setup (2/3 times it failed before) flackery 

Yes, I think remove the code above in assign method could solve the problem of this test.
But we need to confirm that removing it is safe?

Good analysing. I think we are good to remove that part as Andrey did in HBASE-13076.  We should not change (write) a table state in assigning any region. We only need to check (read) the state instead.

Thanks [~jxiang] I committed HBASE-13076. Lets see if it helps here. Will leave the issue open a while as we watch it.

Resolving as fixed. TRUNK is blue now (mostly). Giving it to [~Apache9] since he did bunch of the footwork with [~octo47] help.

