Attaching a truncated (at 1/2 the lines) version of one of the JVMs' {{.spill}} files for one of these jobs.  The full file exceeds the 60MB Jira attachment limit.  The lines near the end of this truncated file look like the end of the full file, so I think (didn't check) that the remainder of the file is just a repetition of this, from the end of the truncated file:

{noformat}
33700791 WARN  (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@726e0f26) [    ] o.a.h.h.s.d.DataNode 127.0.0.1:36261:DataXceiverServer: 
java.nio.channels.ClosedChannelException: null
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:235) ~[?:1.8.0_191]
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100) ~[?:1.8.0_191]
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:141) ~[hadoop-hdfs-2.7.4.jar:?]
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:135) [hadoop-hdfs-2.7.4.jar:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]
33700793 WARN  (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@726e0f26) [    ] o.a.h.h.s.d.DataNode 127.0.0.1:36261:DataXceiverServer: 
java.nio.channels.ClosedChannelException: null
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:235) ~[?:1.8.0_191]
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100) ~[?:1.8.0_191]
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:141) ~[hadoop-hdfs-2.7.4.jar:?]
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:135) [hadoop-hdfs-2.7.4.jar:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]
33700793 WARN  (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@726e0f26) [    ] o.a.h.h.s.d.DataNode 127.0.0.1:36261:DataXceiverServer: 
java.nio.channels.ClosedChannelException: null
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:235) ~[?:1.8.0_191]
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100) ~[?:1.8.0_191]
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:141) ~[hadoop-hdfs-2.7.4.jar:?]
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:135) [hadoop-hdfs-2.7.4.jar:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]
{noformat}

Commit bd104ffd48859f625c999856768b9f14bb6fe1f2 in lucene-solr's branch refs/heads/branch_7x from [~steve_rowe]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=bd104ff ]

SOLR-13060: set suite timeout on non-terminating HDFS Nightly tests to one hour


Commit ec1bd0da2f784a39fbe5dc21d78349c41bfdaec2 in lucene-solr's branch refs/heads/master from [~steve_rowe]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ec1bd0d ]

SOLR-13060: set suite timeout on non-terminating HDFS Nightly tests to one hour


As I mentioned I would on the dev list, I've committed suite timeouts (set to one hour) on these three tests.

[~hossman] wrote on the dev list:

{quote}
There was a logging change that miller made on master ~Nov29 (and 
backported to 7x a few days later) that i just dialed down from INFO to 
DEBUG ... was causing an INFO log message once a second from every solr 
core, which can add up in long running tests.

might explain the recent up tick...

de65b45fca00445d5dde8f25ee5bc67f791a84f7
a61c9f2c22362692b47e4d38dd32953f3a7156d1
{quote}

But the repeating log snippet in my previous comment is at WARN level, so I don't think ^^ applies (at least in the case of the one test I took this from)?

[~dawid.weiss] wrote on the dev list:

{quote}
Unless the logging system is writing to raw io descriptors (or is
initialized earlier than test rules, which I don't think it should),
you could also use the:

@TestRuleLimitSysouts.Limit(bytes = ....)

to limit the amount of logs to a sensibly high limit.
{quote}

Good info, I'll wait to do this until the problem resurfaces.

I looked at TestRuleLimitSysouts again (it's been a while). It only triggers an error after the test completed (and successfully). So it's not going to interrupt/ stop a running test from dumping all the output it wants. Perhaps we should make an optional "hard limit" there so that tests that spill megabytes to sysouts are terminated earlier. This may not be always possible -- for example a logger will most likely catch an exception from the appender and just ignore it. 

Another crude workaround would be to not delegate sysouts to actual output after a certain hard limit (and still throw IOException in such a case). This would, in the worst case, wait until the test is over, but wouldn't fill the disk.

There is also the "SuppressSysoutChecks" annotation which is used in a number of tests (and parent classes). I think we should remove this annotation entirely and just give each test a large hard limit. I recall people arguing they do have the need for gigabytes of logs from tests, but I'd argue if the problem isn't captured in the first 20 gigs or so then the rest probably isn't that useful anyway...

A tentative patch at hard limiting the output in LUCENE-8604

Commit ec1bd0da2f784a39fbe5dc21d78349c41bfdaec2 in lucene-solr's branch refs/heads/jira/http2 from [~steve_rowe]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ec1bd0d ]

SOLR-13060: set suite timeout on non-terminating HDFS Nightly tests to one hour


Same thing happened again, AFAICT *after* my suite timeout commit, on https://builds.apache.org/job/Lucene-Solr-NightlyTests-master/1721/  - two test suites continued to print HEARTBEAT messages long after the hour (3600s) timeout I set on them.  [~dweiss] do you understand what's happening?

From .../1721/consoleText:

{noformat}
Checking out Revision ef2f0cd88c6eb4b662aea06eaeb3b933288b23eb (refs/remotes/
origin/master)
[...]
   [junit4] HEARTBEAT J2 PID(17526@lucene1-us-west): 2018-12-13T05:07:17, stalled for 48000s at: MoveReplicaHDFSTest (suite)
   [junit4] HEARTBEAT J0 PID(17515@lucene1-us-west): 2018-12-13T05:07:17, stalled for 49652s at: HdfsCollectionsAPIDistributedZkTest (suite)
{noformat}

From {{git log}}:

{noformat}
commit ef2f0cd88c6eb4b662aea06eaeb3b933288b23eb
Author: Jan Høydahl <janhoy@apache.org>
Date:   Wed Dec 12 11:33:32 2018 +0100
[...]
commit ec1bd0da2f784a39fbe5dc21d78349c41bfdaec2
Author: Steve Rowe <sarowe@apache.org>
Date:   Tue Dec 11 18:49:06 2018 -0800
{noformat}

From {{MoveReplicaHDFSTest}} and {{HdfsCollectionsAPIDistributedZkTest}} at commit {{ef2f0c}}:

{code:java}
@ThreadLeakFilters(defaultFilters = true, filters = {
    BadHdfsThreadsFilter.class, // hdfs currently leaks thread(s)
    MoveReplicaHDFSTest.ForkJoinThreadsFilter.class
})
@Nightly // test is too long for non nightly
@TimeoutSuite(millis = TimeUnits.HOUR)
public class MoveReplicaHDFSTest extends MoveReplicaTest {
{code}

{code:java}
@Slow
@Nightly
@ThreadLeakFilters(defaultFilters = true, filters = {
    BadHdfsThreadsFilter.class // hdfs currently leaks thread(s)
})
@TimeoutSuite(millis = TimeUnits.HOUR)
//commented 23-AUG-2018  @LuceneTestCase.BadApple(bugUrl="https://issues.apache.org/jira/browse/SOLR-12028") // 12-Jun-2018
public class HdfsCollectionsAPIDistributedZkTest extends CollectionsAPIDistributedZkTest {
{code}


Hi Steve. No, I don't have a clue. I'll download those event files and then I'll be able to tell you more.

So my best guess is this: the suite didn't terminate due to a timeout because it completed (with a failure). There are numerous messages about threads that leaked the suite scope and many attempts to send interrupts to these threads. Fairly early on this happens as well:

{code}
[
 "APPEND_STDERR",
 {
  "chunk": "177769 WARN  (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@6734d615) [    ] o.a.h.h.s.n.NameNodeResourceChecker Space available on volume '/dev/sdb1' is 0, which is below the configured reserved amount 104857600%0A"
 }
] 
{code}

The subsequent 31 gigs of logs is repetitive this:
{code}
[
 "APPEND_STDERR",
 {
  "chunk": "49851804 WARN  (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2d347651) [    ] o.a.h.h.s.d.DataNode 127.0.0.1:42148:DataXceiverServer: %0Ajava.nio.channels.ClosedChannelException: null%0A%09at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:235) ~[?:1.8.0_191]%0A%09at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100) ~[?:1.8.0_191]%0A%09at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:141) ~[hadoop-hdfs-2.7.4.jar:?]%0A%09at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:135) [hadoop-hdfs-2.7.4.jar:?]%0A%09at java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]%0A"
 }
]
{code}

I am not sure why the JVM doesn't terminate eventually because forked process does halt() after all suites are processed. Without a stack trace it's impossible to tell where it got stuck or why... Lack of clean environment because of those thread leaks doesn't make the diagnosis easier. I'll take another look tomorrow, time permitting, but it's such a mess on border conditions (leaked unknown threads crossing test/ runner scope boundaries, 0 disk space, timeouts, 40 gig logs...) that any analysis is very time consuming, if at all possible.

I think limiting the amount of sysouts will be a good start; the patch I posted a few days ago is pretty much ready, I'll commit it in over the weekend.



Hi [~steve_rowe]. Just FYI: the upgrade of randomizedtesting does fix the suite timeout problem (I just tested it on by running SOLR-13074 with a suite timeout of 10 seconds...). I think one hour is very generous  for the sysout loop in SOLR-13074, so it'll be enough to fill the disk anyway. I'll work on truncating sysouts up to at most 1 gig, test it on that SOLR-13074, then maybe to fix the underlying cause of leaking threads. 

Until this is solved, I don't think it makes sense to run hdfs tests at all -- they will hang and fill up disk space on jenkins.

+1 for disabling these tests and not re-enabling until they work properly. 

 

bq. Just FYI: the upgrade of randomizedtesting does fix the suite timeout problem (I just tested it on by running SOLR-13074 with a suite timeout of 10 seconds...).

Awesome, thanks!

bq. I think one hour is very generous for the sysout loop in SOLR-13074, so it'll be enough to fill the disk anyway.

All the examples I've seen have HEARTBEAT messages runing for 40k-50k seconds, an order of magnitude higher, which is why I set it to an hour.

bq. I'll work on truncating sysouts up to at most 1 gig, test it on that SOLR-13074, then maybe to fix the underlying cause of leaking threads.

Great!

bq. Until this is solved, I don't think it makes sense to run hdfs tests at all – they will hang and fill up disk space on jenkins.

+1, I'll apply the {{@AwaitsFix}} annotation.

Commit 459f05da0c85baf3e76286b22005e633a0b1144d in lucene-solr's branch refs/heads/branch_7x from [~steve_rowe]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=459f05d ]

SOLR-13060: Annotate never-terminating HDFS Nightly tests with AwaitsFix


Commit a62b5941b028cc56bf5c35ab9d9ea70b9edb9853 in lucene-solr's branch refs/heads/master from [~steve_rowe]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a62b594 ]

SOLR-13060: Annotate never-terminating HDFS Nightly tests with AwaitsFix


{quote}
bq. Until this is solved, I don't think it makes sense to run hdfs tests at all – they will hang and fill up disk space on jenkins.
+1, I'll apply the @AwaitsFix annotation.
{quote}

FYI I only applied the annotation to the three test suites mentioned in this issue's description, since in all 9 or so instances I've seen, only those tests have displayed the problem.  (There are another 25 or so HDFS test suites that I did not annotate.)

This is probably easy to fix... but not for me. The NPE problem here can be easily avoided by adding cloud config (this is a result of Mark's changes):

https://github.com/apache/lucene-solr/compare/master...dweiss:SOLR-13060

this, however, results in further errors from tests, such as:
{code}
  2> 17471 ERROR (qtp1816967642-985) [n:127.0.0.1:37271_solr c:MoveReplicaHDFSTest_failed_coll_true s:shard2 r:core_node7 x:Mov
eReplicaHDFSTest_failed_coll_true_shard2_replica_n4] o.a.s.h.RequestHandlerBase org.apache.solr.common.SolrException: Error CRE
ATEing SolrCore 'MoveReplicaHDFSTest_failed_coll_true_shard2_replica_n4': Unable to create core [MoveReplicaHDFSTest_failed_col
l_true_shard2_replica_n4] Caused by: Unrecognized lockType: native
{code}

I'm guessing I initialize the collection wrong, because the cloud-hdfs config set has it right.... But I know too little about the subject of hdfs and Solr in cloud mode to fix it properly. All the tests that try to read this config set (cloud-hdfs) seem like they have no way of working (and are currently disabled, in one way or the other), so addressing it properly in one place should give an example to people like me on how to fix it elsewhere.

I also can't run those tests in Windows at all. Hadoop home isn't properly initialized and I don't know whether it's a requirement to set it up properly manually (?):
{code}
7680 ERROR (SUITE-MoveReplicaHDFSTest-seed#[8489A45D88EB8B8A]-worker) [    ] o.a.h.u.Shell Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
{code}

I'm curious if these issues still exist with Hadoop 3 which was just merged to master/8x in SOLR-9515. I can take a look at the 3 annotated tests.
 * MoveReplicaHDFSTest
 * HdfsCollectionsAPIDistributedZkTest
 * HdfsAutoAddReplicasIntegrationTest

MoveReplicaHDFSTest is just plain broken to me at infrastructural level (how things are started up/ cleaned up).

[~dweiss] - Thanks. Yea I commented about MoveReplicaHDFSTest in SOLR-13074.

I see a bunch of "namenode low on available disk space" messages before failures for HdfsCollectionsAPIDistributedZkTest.
{code:java}
ant test  -Dtestcase=HdfsCollectionsAPIDistributedZkTest -Dtests.seed=B6F9375F80C2F760 -Dtests.nightly=true -Dtests.slow=true -Dtests.awaitsfix=true -Dtests.badapples=true -Dtests.locale=en-SG -Dtests.timezone=America/Winnipeg -Dtests.asserts=true -Dtests.file.encoding=UTF-8
{code}
The above fails for me repeatedly.

Similar errors with HdfsAutoAddReplicasIntegrationTest.
{code:java}
ant test  -Dtestcase=HdfsAutoAddReplicasIntegrationTest -Dtests.seed=BD8B35758F984DD8 -Dtests.nightly=true -Dtests.slow=true -Dtests.awaitsfix=true -Dtests.badapples=true -Dtests.locale=ar-QA -Dtests.timezone=Antarctica/Palmer -Dtests.asserts=true -Dtests.file.encoding=UTF-8
{code}
The above also fails for me repeatedly.

So I looked at this and the way the HDFS versions of the tests are written doesn't really make any sense. I have at least the HdfsCollectionsAPIDistributedZkTest test passing it looks like now (turns out it had some parts of MoveReplica in it???). HdfsAutoAddReplicasIntegrationTest is still a bit flaky but it does pass sometimes (might be a timing issue?). 

MoveReplicaHDFSTest I'll look at in SOLR-13074.

Attached patch that tries to address the issues laid out here. Basically the HDFS* versions now just setup HDFS with the right configset.

| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 36s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} Release audit (RAT) {color} | {color:green}  1m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} Check forbidden APIs {color} | {color:green}  1m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} Validate source patterns {color} | {color:green}  1m 36s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 40m 53s{color} | {color:green} core in the patch passed. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 46m 49s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | SOLR-13060 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12957397/SOLR-13060.patch |
| Optional Tests |  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  |
| uname | Linux lucene1-us-west 4.4.0-137-generic #163~14.04.1-Ubuntu SMP Mon Sep 24 17:14:57 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | ant |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-SOLR-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh |
| git revision | master / 1a23ab0 |
| ant | version: Apache Ant(TM) version 1.9.3 compiled on July 24 2018 |
| Default Java | 1.8.0_191 |
|  Test Results | https://builds.apache.org/job/PreCommit-SOLR-Build/290/testReport/ |
| modules | C: solr/core U: solr/core |
| Console output | https://builds.apache.org/job/PreCommit-SOLR-Build/290/console |
| Powered by | Apache Yetus 0.7.0   http://yetus.apache.org |


This message was automatically generated.



Patch still looks good. Will rebase on master and run through tests again to double check. Planning to push this if things go well locally.

Commit 6a886b274d6cc24dd1b84a7379ee8a07ccae3ce8 in lucene-solr's branch refs/heads/master from Kevin Risden
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=6a886b2 ]

SOLR-13060: Improve HdfsAutoAddReplicasIntegrationTest and HdfsCollectionsAPIDistributedZkTest

Signed-off-by: Kevin Risden <krisden@apache.org>


Commit 129de3f5e9f45d79711853b087fce86358a63307 in lucene-solr's branch refs/heads/branch_8x from Kevin Risden
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=129de3f ]

SOLR-13060: Improve HdfsAutoAddReplicasIntegrationTest and HdfsCollectionsAPIDistributedZkTest

Signed-off-by: Kevin Risden <krisden@apache.org>


Closing after the 9.0.0 release

