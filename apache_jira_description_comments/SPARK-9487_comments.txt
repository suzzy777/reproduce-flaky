Hey [~mengxr],

What would be the preferred num. worker threads? Should we set all of them to local[2] to stay consistent with the Scala/Java side?

Thanks

This will maybe break some tests in the process but it would probably be good. I'd go with 4 rather than 2 just for the old streaming tests (so if we want to be consistent 4 everywhere). Is this something people are interested in pursuing? If so maybe we should make it a starter issue?

Hello All,
Can I help with this in anyway?
Thanks

I think you can just make the change and see what happens?

+1 to [~srowen]'s comment. I would not be surprised to see some test failures because of the implicit change in the default partitioning as a result - but for most of those just updating the results will be the right course of action. Let me know if you have any questions [~kanjilal] and welcome to PySpark :)

Synched the code, am familiarizing myself first with how to run unit tests and work in the code , [~srowen][~holdenk], next steps will be to run the unit tests and report the results here, stay tuned.

Great, thanks for taking this issue on :)

[~holdenk] finally getting time to look at this, so I am starting small, I made the change inside ContextCleanerSuite and HeartbeatReceiverSuite from local[2] tp local[4], per the documentation here (http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version) I ran mvn -P hadoop2 -Dsuites= org.apache.spark.HeartbeatReceiverSuite test--- looks like everything worked

I then ran mvn -P hadoop2 -Dsuites= org.apache.spark.ContextCleanerSuite test-- looks like everything worked as well

See the attachments and let me know if this is the right process to run single unit tests,  I'll start making changes to the other Suites , how would you like to see the output, should I just have attachments or just do a pull request from the new branch that I created?
Thanks

PS
Another question, running single unit tests like this takes forever, are there flags I can set to speed up the builds, even on my 15 inch macbook pro with SSD the builds shouldnt take this long :(.  


Let me know next steps to get this into a PR.





Ping on this, [~holdenk] can you let me know if I can move ahead with the above approach.
Thanks

See https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
You should open a pull request to show the changes so far.

The tests take hours to run in their entirety. You can run just python tests with pyspark/run-tests

PR attached here: https://github.com/apache/spark/pull/15689
I only changed everything to local[4] in core and ran unit tests, all unit tests ran sucessfully


This is a WIP so once have folks review this initial request and signed off I will start changing the python pieces

[~holdenk][~sowen]  let me know next steps

Again have a look at https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark -- you need to update your PR title to link it.

User 'skanjila' has created a pull request for this issue:
https://github.com/apache/spark/pull/15689

[~srowen] Yes I read through that link and adjusted the PR title, however please do let me know if I can proceed adding more to this PR including python and other parts of the codebase.

Added org.apache.spark.mllib unitTest changes to pull request

[~srowen], [~holdenk]  what are the next steps to drive this to the finish line, should I continue adding to this pull request and keep making the local[2]->local[4] changes through all of the codebase, would love some insight.

Yes, keep going, why not?

added local[4] to repl, sparksql, streaming, all tests pass, pull request is here: https://github.com/apache/spark/compare/master...skanjila:spark-9487


ok I have moved onto python, I am attaching a log that contains test errors upon changing local[2] to local[4] on the ml module in python



Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).

[Stage 49:>                                                         (0 + 3) / 3]SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

                                                                                
**********************************************************************
File "/Users/skanjila/code/opensource/spark/python/pyspark/ml/clustering.py", line 98, in __main__.GaussianMixture
Failed example:
    model.gaussiansDF.show()
Expected:
    +--------------------+--------------------+
    |                mean|                 cov|
    +--------------------+--------------------+
    |[0.82500000140229...|0.005625000000006...|
    |[-0.4777098016092...|0.167969502720916...|
    |[-0.4472625243352...|0.167304119758233...|
    +--------------------+--------------------+
    ...
Got:
    +--------------------+--------------------+
    |                mean|                 cov|
    +--------------------+--------------------+
    |[-0.6158006194417...|0.132188091748508...|
    |[0.54523101952701...|0.159129291449328...|
    |[0.54042985246699...|0.161430620150745...|
    +--------------------+--------------------+
    <BLANKLINE>
**********************************************************************
File "/Users/skanjila/code/opensource/spark/python/pyspark/ml/clustering.py", line 123, in __main__.GaussianMixture
Failed example:
    model2.gaussiansDF.show()
Expected:
    +--------------------+--------------------+
    |                mean|                 cov|
    +--------------------+--------------------+
    |[0.82500000140229...|0.005625000000006...|
    |[-0.4777098016092...|0.167969502720916...|
    |[-0.4472625243352...|0.167304119758233...|
    +--------------------+--------------------+
    ...
Got:
    +--------------------+--------------------+
    |                mean|                 cov|
    +--------------------+--------------------+
    |[-0.6158006194417...|0.132188091748508...|
    |[0.54523101952701...|0.159129291449328...|
    |[0.54042985246699...|0.161430620150745...|
    +--------------------+--------------------+
    <BLANKLINE>
**********************************************************************
File "/Users/skanjila/code/opensource/spark/python/pyspark/ml/clustering.py", line 656, in __main__.LDA
Failed example:
    model.describeTopics().show()
Expected:
    +-----+-----------+--------------------+
    |topic|termIndices|         termWeights|
    +-----+-----------+--------------------+
    |    0|     [1, 0]|[0.50401530077160...|
    |    1|     [0, 1]|[0.50401530077160...|
    +-----+-----------+--------------------+
    ...
Got:
    +-----+-----------+--------------------+
    |topic|termIndices|         termWeights|
    +-----+-----------+--------------------+
    |    0|     [1, 0]|[0.50010191915681...|
    |    1|     [0, 1]|[0.50010191915681...|
    +-----+-----------+--------------------+
    <BLANKLINE>
**********************************************************************
File "/Users/skanjila/code/opensource/spark/python/pyspark/ml/clustering.py", line 664, in __main__.LDA
Failed example:
    model.topicsMatrix()
Expected:
    DenseMatrix(2, 2, [0.496, 0.504, 0.504, 0.496], 0)
Got:
    DenseMatrix(2, 2, [0.4999, 0.5001, 0.5001, 0.4999], 0)
**********************************************************************
2 items had failures:
   2 of  21 in __main__.GaussianMixture
   2 of  20 in __main__.LDA
***Test Failed*** 4 failures.



[~srowen][~holdenk]  thoughts on next steps, should this pull request also contain code fixes to fix the errors that occur when changing local[2] to local[4] or should we break up the pull request into subcomponents, one focused on the scala pieces already submitted and the next focused on fixing the python code to work with local[4], thoughts on next steps?

I'm not against just committing the Scala (or Java) changes separately, though that tends to make some tests less consistent while making others more consistent. Right now the WIP PR doesn't make all of the Scala changes yet, right? are there similar issues?

 It wouldn't hurt to figure out these test failures if that's all there is in Python and get them all done at once. I think some of it is just due to expected variations due to different distiributions of the data, but bears some reading of the tests to see if that makes sense.

I definitely want to figure out these test failures as a next step, however I think I'd like for folks to have the benefit of the changes to the Scala and Java changes independent of the python work.  If that makes sense what are the next steps to commit this pull request with only the scala/java changes?  To that end I will create a sub-branch focused on the python stuff and merge in those changes into my current branch once the tests are fixed.


[~srowen], let me know your thoughts and if the above makes sense

I don't see a hurry since the tests aren't broken now. The PR doesn't contain all the Scala/Java changes right now.

Sorry forgot to reply to your other question, from my checks I believe I had made all the java and scala changes as doing a simple find in IntelliIdea only shows the python changes being outstanding. 

Don't think so ... I searched now and the first 2 hits even weren't included. For example WriteAheadLogBackedBlockRDDSuite and FlumePollingStreamSuite

Ok , for some odd reason my local branch had the changes but weren't committed, PR is here:  https://github.com/skanjila/spark/commit/ec0b2a81dc8362e84e70457873560d997a7cb244

I added the change to local[4] to both streaming as well as repl, based on what I'm seeing locally all Java/Scala changes should be accounted for and unit tests pass, the only exception is the code inside spark examples in PageViewStream.scala, should I change this, seems like it doesn't belong as part of the unit tests.


My next TODOs:
1) Change the example code if it makes sense in PageViewStream
2) Start the code changes to fix the python unit tests

Let me know thoughts or concerns.

User 'skanjila' has created a pull request for this issue:
https://github.com/apache/spark/pull/15848

[~srowen] if there's no further issues I will : 1) start working on another pull request to fix all the python unit test issues 2) in this other pull request I will include the fixes for the example to work as well as the TestSQLContext.   Any objections to merging the pull request above?

[~srowen] given this is my first patch, I wanted to understand a few things, I was looking at this:  https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/68529/consoleFull and its not clear that these unit tests are in any way related to my changes, any insight on this, are these tests that happen to fail due to other dependencies missing, if so is someone else working to fix these?

I will move onto working on python unit tests under the same PR next.

All tests are run. Some of the test failures do look related. Do they fail for you locally?
Please let's focus on getting this subset done before making another PR.

No they don't which is why I asked, I will dig into these and resubmit.  Point taken around making another PR

Because the same tests have failed twice, I suspect it is a real failure. 

ok so I've spent the last hour or so doing deeper investigations into the failures, I used this https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/68531/ as a point of reference, listed below is what I found



LogisticRegressionSuite  (passed successfully on both my master and feature branches)                                                                                  
OneVsRestSuite    (passed successfully on both my master and feature branches) 
DataFrameStatSuite   (passed successfully on both my master and feature branches)                                                                                  
DataFrameSuite     (passed successfully on both my master and feature branches)                                                                                  
SQLQueryTestSuite  (passed successfully on both my master and feature branches)                                                                                   
ForeachSinkSuite  (passed successfully on both my master and feature branches)                                                                                      
JavaAPISuite       (failed on both my master and feature branches)


The master branch does not have any code changes from me and the feature branch of course does 

I am running individual tests by issuing commands like the following from the root directory based on the documentation:  ./build/mvn test -P... -DwildcardSuites=none -Dtest=org.apache.spark.streaming.JavaAPISuite


Therefore my conclusion so far based on the above jenkins report is that my changes have not introduced any new failures that were not already there, [~srowen] please let me know if my methodology is off anywhere



No, it's almost certain that your changes introduced the test failure. It keeps failing. JavaAPISuite does not fail on Jenkins in master.
The problem is that it's not 100% certain that a (real) failure in Jenkins is reproducible in your different, local environment. This can make debugging quite hard. Still it's worth trying to figure out how the test would fail based on Jenkins output and try to fix it; we can't merge a change that breaks tests for the build system of reference.

Understood, so I wanted a fresh look at this from a different dev environment, so on my macbook pro I tried changing the setting to local[2] and local[4] for JavaAPISuite, it seems that they both fail so yes mimicing the real Jeankins failure will be hard, should I close this pull request till this is fixed and resubmit a new one, I have no idea at this point how long debugging this or even replicating this will take, thoughts on a suitable set of next steps?

How are you running tests? I'd try to run the tests exactly like Jenkins does -- try running the whole suite, with same args. It passes on my Macbook modulo the occasional flaky tests. Until you can get the tests passing with no changes, yeah it will be pretty hard to proceed to develop changes to the tests.

I am running the tests by the following command: ./build/mvn test -P... -DwildcardSuites=none -Dtest=org.apache.spark.streaming.JavaAPISuite , is that not the way Jenkins runs the tests , I noticed that locally I am getting the same error as Jenkins shown here (https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/68529/consoleFull) regardless of whether I set the configuration to local[2] or local[4]:  expected:<[[(hello,1), (world,1)], [(hello,1), (moon,1)], [(hello,1)]]> but was:<[[(hello,1), (world,1)], [(moon,1), (hello,1)], [(hello,1)]]>

It may be a slightly flaky test and your environment exposes that in a way that others don't. The question is, is the output from the test actually just as correct as the test's expected answer? if so the test should be loosened. Same question for the others.

Sean,
I took a look at the code and here it is:

List<List<String>> inputData = Arrays.asList(
      Arrays.asList("hello", "world"),
      Arrays.asList("hello", "moon"),
      Arrays.asList("hello"));

    List<List<Tuple2<String, Long>>> expected = Arrays.asList(
        Arrays.asList(
            new Tuple2<>("hello", 1L),
            new Tuple2<>("world", 1L)),
        Arrays.asList(
            new Tuple2<>("hello", 1L),
            new Tuple2<>("moon", 1L)),
        Arrays.asList(
            new Tuple2<>("hello", 1L)));

    JavaDStream<String> stream = JavaTestUtils.attachTestInputStream(ssc, inputData, 1);
    JavaPairDStream<String, Long> counted = stream.countByValue();
    JavaTestUtils.attachTestOutputStream(counted);
    List<List<Tuple2<String, Long>>> result = JavaTestUtils.runStreams(ssc, 3, 3);

    Assert.assertEquals(expected, result);


As you can see the expected is assuming that the contents of the stream get counted accurately for every word, the output that gets generated through the flakiness just has hello,1 moon,1 reversed which I dont think matters, unless the goal of the test ist o identify words in order of how they enter the stream the expected and the actual answer are correct.  Therefore net net the test is flaky, should I refactor the test to actually look at the word count and not the order, thoughts on next steps?


Agree, it seems like it should not be sensitive to ordering within each batch. This could convert the List<Tuple2<...>> to Set<Tuple2<...>> so that they are compared without regard to order. If that's the nature of the difference, then yes it's the test that should be fixed as part of this change.

Ok fixed the unit test, didnt have to resort to using Sets, was able to compare the contents of each of the lists to certify the tests, pull request is here:  https://github.com/apache/spark/pull/15848

Once pull request passes I will start working on fixing all the examples and the python code.  Let me know next steps

Ok guess I spoke too soon :), onto the next set of challenges, jenkins build report is here:  https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/68897/


I ran each of these tests individually as well as together as a suite locally and they all passed, any ideas on how to address these?

[~srowen] following up, thoughts on how to proceed on these, I looked through , for example I looked at LogisticRegresionSuite and I dont see anything about even specifying local[2] versus local[4]?  Thoughts on how to proceed on these, they all succeed locally as I mentioned

Look at its superclass (trait) that defines "sc", which sets local[2].

This may be hard to update all tests to make them work with a slightly different number of threads. It's great if we can get this done, but if you're having difficulty troubleshooting the various failures that may occur, I think it may be best to abandon this. It's hard to assist with every failure here.

I really want to finish the effort that I started for helping the community and will do my best to debug all the issues moving forward, for now I will skip ahead to the python tests to get those working and then come back and troubleshoot and fix all the test failures, sound reasonable?

Given the latest thread on the devlist thoughts [~srowen][~rxin] on next steps for this?

(Which thread?) I think that if you can get all tests in one language updated uniformly to pass with a different number of threads, that would be a sufficient unit of work to commit. I know it's not small. Or, maybe even a couple modules along with associated test improvements that make them robust to the number of threads. If you can get a significant logical chunk of improvement working we can commit it as a step towards a resolution.

[~srowen] I think the above plan is great minus one fundamental flaw, I already have tests passing uniformly across multiple components locally, the issue I am running into is trying to get the tests working in jenkins, currently every change I've made locally passes unit tests.    Until the issue with my local environment and jenkins gets resolved I don't see a clever way to get tests to pass , let me know your thoughts on a good way to get past this.  After we figure this out I can pick a set of components to work with a uniform number of threads.

Yes, that probably means the test changes aren't quite robust in their new form. Getting them to pass locally and on Jenkins indicates they're at least general enough to pass across both envs. And of course we have to get them to pass on Jenkins. It can be hard to debug; try a different machine? try loosening conditions? you can push changes to a WIP PR to see how Jenkins treats them. I think we need to bring this to a conclusion though. Right now I'm not clear this solves enough of a problem to bother with, so I'm inclined to close it.

I'm ok closing it actually but it does outline issues with robustness around the unit tests, should we open up another jira or reframe this effort to make the unit tests more robust, that may require some more thought/redesign to produce identical results locally as well as in jenkins, my vote would be to close this out and recreate another jira that I can take on to make the unit tests more robust for 1 specific component with very narrowly defined goals, what do you think?

Well, this JIRA is implicitly about making a test or two more robust in order to effect this change. I don't see what opening another JIRA does. 

This isn't a must-have JIRA anyway. I think it's solvable and we've discussed here general strategies for debugging failures, and on the PR I suggested specific fixes to specific failures. I don't think anything is blocking this other than just doing it. It's not trivial. But if it isn't something you or anyone else can get working with reasonable effort i think it's better to just abandon this.

Then I would suggest keeping it open and focus on a particular module and make the unit tests robust in that module, is there a specific module that's in dire need of robustness of unit tests, I was thinking of picking the sql module and moving forward to make the unit tests under that be more robust as a first goal, thoughts?

I think this is going around in circles. You already have an open invitation to improve tests in any logical subset of the project in order to accomplish this change in number of worker threads. You're saying you are unable to get them to pass on Jenkins and unwilling to debug. I don't think there is more guidance to give here; either you can effect this change or not. If nobody can or seems willing to try, I think it should be closed, because this really isn't an error to start with, nor even that suboptimal (excepting that it has revealed a couple tests could be a little more robust)

User 'skanjila' has created a pull request for this issue:
https://github.com/apache/spark/pull/16498

[~srowen]  build/tests has passed in jenkins, what do you think?  Should we commit a little at a time to keep this moving forward or should I make the next change, I prefer to create new pull requests for each unit test area that I fix so my preference would be to commit this little change for ContextCleaner.

