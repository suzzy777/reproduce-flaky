It looks like you've touched this test, so I thought you might be a good person to investigate.

I think my change is unrelated to the part which failed. This failure may just be a race with HDFS which ended up showing stale number of
cached entries even after the cached table is dropped. Re-assigning to Dimitris to confirm as he knows catalog way better than I do.

I see a small clean up (drop uncached_local_tbl too) which can be done but it's unrelated to this failure as that table is never cached in the test. The clean up method will eventually drop the test database which will remove that table too.


Removed broken-build because this is not really blocking the builds.  It's a flaky test that doesn't repro easily.  Downgrading.

Happened again http://sandbox.jenkins.cloudera.com/job/impala-asf-master-core-repeated-runs/28/testReport/junit/query_test.test_hdfs_caching/TestHdfsCachingDdl/test_caching_ddl_exec_option____disable_codegen___False___abort_on_error___1___exec_single_node_rows_threshold___0___batch_size___0___num_nodes___0____table_format__text_none_/

Happened again http://sandbox.jenkins.cloudera.com/job/impala-cdh5-2.7.0_5.9.0-core-non-partitioned-joins-and-aggs/10/

Happened again: http://sandbox.jenkins.cloudera.com/job/impala-external-gerrit-verify-merge-ASF/435/consoleFull

This failed again: 

{code}
01:21:41 =================================== FAILURES ===================================
01:21:41  TestHdfsCachingDdl.test_caching_ddl[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none] 
01:21:41 query_test/test_hdfs_caching.py:208: in test_caching_ddl
01:21:41     assert num_entries_pre == get_num_cache_requests()
01:21:41 E   assert 9 == 10
01:21:41 E    +  where 10 = get_num_cache_requests()

{code}

I've seen this 5x in the last week. I'm going to upgrade it to critical, but it's close to being a broken build.

I've seen this several more times this week. Priority upped to blocker.

Happened again: http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-asf-master/job/impala-asf-master-core-integration/197/

Failed again: http://jenkins.impala.io:8080/job/ubuntu-14.04-from-scratch/686/consoleFull

Saw this again on an internal build.

[~dtsirogiannis], this has a target version of 2.9. How's the investigation going?

[~jbapple], I haven't looked at it yet. 

[~mjacobs] saw this: http://jenkins.impala.io:8080/job/gerrit-verify-dryrun/249/

Downgrading to "Critical", since these are not regressions from 2.8.0

We hit this again: http://jenkins.impala.io:8080/job/ubuntu-14.04-from-scratch/1068/

[~srus] - I re-tagged this as broken build since it keeps hitting the automated tests. Do we have a better label for this?

I saw this again in a private Jenkins run.

Change-Id: I3ec4ba5dfae6e90a2bb76e22c93909b05bd78fa4
Reviewed-on: http://gerrit.cloudera.org:8080/6603
Reviewed-by: Alex Behm <alex.behm@cloudera.com>
Tested-by: Impala Public Jenkins
---
M tests/query_test/test_hdfs_caching.py
1 file changed, 22 insertions(+), 6 deletions(-)

Approvals:
  Impala Public Jenkins: Verified
  Alex Behm: Looks good to me, approved

This was seen again on an ASAN build.

I assume that its the same issue as before, its just the that timeout that was added isn't long enough for how slow ASAN is.

Again: http://jenkins.impala.io:8080/job/ubuntu-14.04-from-scratch/1503/

I added an ASAN-specific timeout to be much higher than the non-ASAN builds, at this patch.

https://gerrit.cloudera.org/#/c/7115/


As a minimum step, we should consider changing this test to log in case of error the expected and found cache directives to see which is the problematic one. 

Saw this again:
{noformat}
query_test/test_hdfs_caching.py:212: in test_caching_ddl
    assert num_entries_pre == get_num_cache_requests()
E   assert 9 == 10
E    +  where 10 = get_num_cache_requests()
{noformat}

Hit this again during private build:
{noformat}
11:22:28 FAIL query_test/test_hdfs_caching.py::TestHdfsCachingDdl::()::test_caching_ddl[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 5000, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: text/none]
11:22:28 =================================== FAILURES ===================================
11:22:28  TestHdfsCachingDdl.test_caching_ddl[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 5000, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: text/none] 
11:22:28 query_test/test_hdfs_caching.py:212: in test_caching_ddl
11:22:28     assert num_entries_pre == get_num_cache_requests()
11:22:28 E   assert 9 == 10
11:22:28 E    +  where 10 = get_num_cache_requests()
11:22:28 =============== 1842 tests deselected by "-m 'execute_serially'" ===============
{noformat}

Hit this again.

Commit 532a0d7afcb56217d2aae45c1c97890d6788251b in impala's branch refs/heads/master from [~tarmstrong@cloudera.com]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=532a0d7 ]

IMPALA-3040: add logging to test_caching_ddl

We don't currently have enough information to reconstruct why the test
failed, so lets add logging with timestamps to understand which timeout
we're actually hitting.

Change-Id: Iabc30445440e0fb358856da407d833f5ae975213
Reviewed-on: http://gerrit.cloudera.org:8080/9579
Reviewed-by: Thomas Tauber-Marshall <tmarshall@cloudera.com>
Tested-by: Impala Public Jenkins


Commit e7fa31ebbc3e469072f5adbacacceb36d228e61b in impala's branch refs/heads/2.x from [~tarmstrong@cloudera.com]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=e7fa31e ]

IMPALA-3040: add logging to test_caching_ddl

We don't currently have enough information to reconstruct why the test
failed, so lets add logging with timestamps to understand which timeout
we're actually hitting.

Change-Id: Iabc30445440e0fb358856da407d833f5ae975213
Reviewed-on: http://gerrit.cloudera.org:8080/9579
Reviewed-by: Thomas Tauber-Marshall <tmarshall@cloudera.com>
Tested-by: Impala Public Jenkins


What are the next steps here?

[~tarmstrong] I will take a look.

In the case I looked into the problem is that the cache directive /test-warehouse/cachedb.db/cached_tbl_local/j=1 never got removed. The relevant namenode log:
{noformat}
2018-05-29 22:31:55,636 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 3000 milliseconds
2018-05-29 22:31:55,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 14 directive(s) and 7 block(s) in 1 millisecond(s).
2018-05-29 22:31:58,636 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 3000 milliseconds
2018-05-29 22:31:58,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 14 directive(s) and 7 block(s) in 1 millisecond(s).
2018-05-29 22:32:01,636 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 3000 milliseconds
2018-05-29 22:32:01,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 14 directive(s) and 7 block(s) in 1 millisecond(s).
2018-05-29 22:32:03,483 INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: addDirective of {path: /test-warehouse/cachedb.db/cached_tbl_local/j=1, replication: 1, pool: testPool, expiration: 26687997791:19:48:13.951} successful.
2018-05-29 22:32:03,679 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning because of pending operations
2018-05-29 22:32:03,680 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 15 directive(s) and 7 block(s) in 1 millisecond(s).
2018-05-29 22:32:04,738 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /hbase/WALs/localhost,16203,1527647516612/localhost%2C16203%2C1527647516612.default.1527658324725. BP-1128346793-127.0.0.1-1527647465315 blk_1073755283_14459{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-45a1fee3-50a1-44cc-8230-916d08f9b968:NORMAL:127.0.0.1:31001|RBW]]}
2018-05-29 22:32:04,740 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /hbase/WALs/localhost,16201,1527647513859/localhost%2C16201%2C1527647513859.default.1527658324731. BP-1128346793-127.0.0.1-1527647465315 blk_1073755284_14460{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-223dfda7-738f-4020-bf4c-29a4642750e1:NORMAL:127.0.0.1:31002|RBW]]}
2018-05-29 22:32:04,748 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/localhost,16203,1527647516612/localhost%2C16203%2C1527647516612.default.1527658324725 for DFSClient_NONMAPREDUCE_-454483218_1
2018-05-29 22:32:04,748 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/localhost,16201,1527647513859/localhost%2C16201%2C1527647513859.default.1527658324731 for DFSClient_NONMAPREDUCE_1562981572_1
2018-05-29 22:32:04,755 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:31002 is added to blk_1073755263_14439{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-223dfda7-738f-4020-bf4c-29a4642750e1:NORMAL:127.0.0.1:31002|RBW]]} size 83
2018-05-29 22:32:04,755 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:31001 is added to blk_1073755264_14440{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-45a1fee3-50a1-44cc-8230-916d08f9b968:NORMAL:127.0.0.1:31001|RBW]]} size 83
2018-05-29 22:32:04,756 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/localhost,16203,1527647516612/localhost%2C16203%2C1527647516612.default.1527654724633 is closed by DFSClient_NONMAPREDUCE_-454483218_1
2018-05-29 22:32:04,756 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/localhost,16201,1527647513859/localhost%2C16201%2C1527647513859.default.1527654724642 is closed by DFSClient_NONMAPREDUCE_1562981572_1
2018-05-29 22:32:04,765 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /hbase/WALs/localhost,16202,1527647515279/localhost%2C16202%2C1527647515279.default.1527658324757. BP-1128346793-127.0.0.1-1527647465315 blk_1073755285_14461{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-223dfda7-738f-4020-bf4c-29a4642750e1:NORMAL:127.0.0.1:31002|RBW]]}
2018-05-29 22:32:04,771 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/localhost,16202,1527647515279/localhost%2C16202%2C1527647515279.default.1527658324757 for DFSClient_NONMAPREDUCE_-1203988066_1
2018-05-29 22:32:04,778 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:31000 is added to blk_1073755265_14441{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-32944451-dc57-41dc-8020-5c9c1e27be16:NORMAL:127.0.0.1:31000|RBW]]} size 83
2018-05-29 22:32:04,778 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/localhost,16202,1527647515279/localhost%2C16202%2C1527647515279.default.1527654724676 is closed by DFSClient_NONMAPREDUCE_-1203988066_1
2018-05-29 22:32:06,679 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 3000 milliseconds
2018-05-29 22:32:06,680 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 15 directive(s) and 7 block(s) in 1 millisecond(s).
2018-05-29 22:32:09,495 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /hbase/WALs/localhost,16202,1527647515279/localhost%2C16202%2C1527647515279.meta.1527658329232.meta. BP-1128346793-127.0.0.1-1527647465315 blk_1073755286_14462{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-45a1fee3-50a1-44cc-8230-916d08f9b968:NORMAL:127.0.0.1:31001|RBW]]}
2018-05-29 22:32:09,679 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 3000 milliseconds
2018-05-29 22:32:09,986 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 15 directive(s) and 7 block(s) in 307 millisecond(s).
2018-05-29 22:32:10,069 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/localhost,16202,1527647515279/localhost%2C16202%2C1527647515279.meta.1527658329232.meta for DFSClient_NONMAPREDUCE_-1203988066_1
2018-05-29 22:32:10,500 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:31000 is added to blk_1073755266_14442{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-32944451-dc57-41dc-8020-5c9c1e27be16:NORMAL:127.0.0.1:31000|RBW]]} size 83
2018-05-29 22:32:10,500 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/localhost,16202,1527647515279/localhost%2C16202%2C1527647515279.meta.1527654729154.meta is closed by DFSClient_NONMAPREDUCE_-1203988066_1
2018-05-29 22:32:12,680 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 3001 milliseconds
2018-05-29 22:32:12,680 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 15 directive(s) and 7 block(s) in 0 millisecond(s).
2018-05-29 22:32:15,681 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 3001 milliseconds
2018-05-29 22:32:15,681 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 15 directive(s) and 7 block(s) in 0 millisecond(s).
2018-05-29 22:32:18,682 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 3001 milliseconds
2018-05-29 22:32:18,682 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 15 directive(s) and 7 block(s) in 0 millisecond(s).
2018-05-29 22:32:19,469 INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: removeDirective of 17 successful.
2018-05-29 22:32:19,471 INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: removeDirective of 19 successful.
2018-05-29 22:32:19,472 INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: removeDirective of 18 successful.
2018-05-29 22:32:19,473 INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: removeDirective of 23 successful.
2018-05-29 22:32:19,476 INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: removeDirective of 22 successful.
2018-05-29 22:32:19,478 INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: removeDirective of 21 successful.
2018-05-29 22:32:19,479 INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: removeDirective of 25 successful.
2018-05-29 22:32:19,480 INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: removeDirective of 24 successful.
{noformat}
The cache directive ID is assigned by an ever-increasing counter, so we can know that /test-warehouse/cachedb.db/cached_tbl_local/j=1 is assigned id=26 and is not removed. Catalogd log at this time doesn't have anything interesting, so I added some log in https://gerrit.cloudera.org/c/10640/ to see if catalogd called removeDirective() at all.

We can also consider logging cache directive activity at INFO level because it's not an frequent operation, and because HDFS does it.

Commit 6682e4a09463ee547dc603aa2a4863e85e14efcc in impala's branch refs/heads/master from [~tianyiwang]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=6682e4a ]

IMPALA-3040: Print logs around removeDirective()

I observed that some caching directives on partitions don't get
removed. HDFS would throw an exception if removeDirective() operation
fails on its side, so I suspect that removeDirective() isn't called by
catalogd. This patch prints logs around removeDirective() if it's
skipped.

Change-Id: I9d9f6eb1951c77e095d5029e8bbdc67e9b39b814
Reviewed-on: http://gerrit.cloudera.org:8080/10640
Reviewed-by: Tim Armstrong <tarmstrong@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Commit 7f0a7b1384d8c833bda68cfd1eb5ea766cf9b324 in impala's branch refs/heads/2.x from [~tianyiwang]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=7f0a7b1 ]

IMPALA-3040: Print logs around removeDirective()

I observed that some caching directives on partitions don't get
removed. HDFS would throw an exception if removeDirective() operation
fails on its side, so I suspect that removeDirective() isn't called by
catalogd. This patch prints logs around removeDirective() if it's
skipped.

Change-Id: I9d9f6eb1951c77e095d5029e8bbdc67e9b39b814
Reviewed-on: http://gerrit.cloudera.org:8080/10640
Reviewed-by: Tim Armstrong <tarmstrong@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Commit ac4acf1b77ccad95528741c255834d8ccdb84518 in impala's branch refs/heads/master from [~tianyiwang]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=ac4acf1 ]

IMPALA-3040: Remove cache directives if a partition is dropped externally

HdfsTable.dropPartition() doesn't uncache the partition right now. If
the partition is dropped from Hive and refreshed in Impala, the
partition will be removed from the catalog but the cache directive
remains. Because Impala directly uses HMS client to drop a
table/database, the cache directive won't be removed even if the table
is dropped in Impala, if the backgroud loading is run concurrenty with
the HMS client RPC call. This patch removes the cache directive in
dropPartition() if the partition is removed from HMS.

Change-Id: Id7701a499405e961456adea63f3592b43bd69170
Reviewed-on: http://gerrit.cloudera.org:8080/10792
Reviewed-by: Bharath Vissapragada <bharathv@cloudera.com>
Tested-by: Tianyi Wang <twang@cloudera.com>


I haven't seen this for months. Closing it.

Commit 45d2ce7d3b953fbc325608bc378ee38a1b506a49 in impala's branch refs/heads/2.x from Tianyi Wang
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=45d2ce7 ]

IMPALA-3040: Remove cache directives if a partition is dropped externally

HdfsTable.dropPartition() doesn't uncache the partition right now. If
the partition is dropped from Hive and refreshed in Impala, the
partition will be removed from the catalog but the cache directive
remains. Because Impala directly uses HMS client to drop a
table/database, the cache directive won't be removed even if the table
is dropped in Impala, if the backgroud loading is run concurrenty with
the HMS client RPC call. This patch removes the cache directive in
dropPartition() if the partition is removed from HMS.

Change-Id: Id7701a499405e961456adea63f3592b43bd69170
Reviewed-on: http://gerrit.cloudera.org:8080/10792
Reviewed-by: Bharath Vissapragada <bharathv@cloudera.com>
Tested-by: Tianyi Wang <twang@cloudera.com>


