Same as IMPALA-9405?

Yes, looks like its same as IMPALA-9405.

Spend sometime on this and almost made it to work. We can still make it work but it made me think that we are trying to solve 2 conflicting requirements. Our FE unit tests spin up their own CatalogServiceCatalog instances (see CatalogServiceTestCatalog for example). Testing can become flaky if we make MetastoreClientPool a singleton since all the FE tests run within a single process and that would mean they will share the MetastoreClientPool. We currently rely on Catalog#close() call in the tests to shutdown the pool. This works ok for most of the tests except the ones which rely on {{createTransientTestCatalog}} which uses a EmbeddedHMS service. Currently MetastoreClientPool should have one to one mapping with the Catalog instances. The MetastoreClientPool in {{DirectMetaProvider}} should ideally never get instantiated after we fix IMPALA-9375. We should only have either CatalogMetaProvider or DirectMetaProvider running but not both.

I am now inclined to abandon this patch and close this JIRA as "wont fix". [~stakiar] [~stigahuang] any thoughts?

Commit 0cb44242d20532945e5fb09f5bbef6c65415a753 in impala's branch refs/heads/master from Vihang Karajgaonkar
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=0cb4424 ]

IMPALA-9791: Support validWriteIdList in getPartialCatalogObject API

This change enhances the Catalog-v2 API getPartialCatalogObject to
support ValidWriteIdList as an optional field in the TableInfoSelector.
When such a field is provided by the clients, catalog compares the
provided ValidWriteIdList with the cached ValidWriteIdList of the
table. The catalog reloads the table if it determines that the cached
table is stale with respect to the ValidWriteIdList provided.
In case the table is already at or above the requested ValidWriteIdList
catalog uses the cached table metadata information to filter out
filedescriptors pertaining to the provided ValidWriteIdList.
Note that in case compactions it is possible that the requested
ValidWriteIdList cannot be satisfied using the cached file-metadata
for some partitions. For such partitions, catalog re-fetches the
file-metadata from the FileSystem.

In order to implement the fall-back to getting the file-metadata from
filesystem, the patch refactor some of file-metadata loading logic into
ParallelFileMetadataLoader which also helps simplify some methods
in HdfsTable.java. Additionally, it modifies the WriteIdBasedPredicate
to optionally do a strict check which throws an exception on some
scenarios.

This is helpful to provide a snapshot view of the table metadata during
query compilation with respect to other changes happening to the table
concurrently. Note that this change does not implement the coordinator
side changes needed for catalog clients to use such a field. That would
be taken up in a separate change to keep this patch smaller.

Testing:
1. Ran existing filemetadata loader tests.
2. Added a new test which exercises the various cases for
ValidWriteIdList comparison.
3. Ran core tests along with the dependent MetastoreClientPool
patch (IMPALA-9824).

Change-Id: Ied2c7c3cb2009c407e8fbc3af4722b0d34f57c4a
Reviewed-on: http://gerrit.cloudera.org:8080/16008
Reviewed-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Resolving as "won't fix" due to complications in FE tests when changing MetastoreClientPool into singleton. Currently, this patch is not critical since we make sure that coordinator and catalog instantiate one pool each.

[~vihangk1] sorry for the delayed response. I'm not super familiar with these tests. AFAICT there are 4 places where we create a {{MetaStoreClientPool}}:
{code:java}
stakiar @ stakiar-desktop -bash ~/Impala 2020-06-19 13:30:21 master
 [51] → grep -iIR 'new MetaStoreClientPool' fe/src/main/
fe/src/main/java/org/apache/impala/service/Frontend.java:      metaStoreClientPool_ = new MetaStoreClientPool(1, 0);
fe/src/main/java/org/apache/impala/catalog/Catalog.java:    this(new MetaStoreClientPool(0, 0));
fe/src/main/java/org/apache/impala/catalog/CatalogServiceCatalog.java:        new MetaStoreClientPool(INITIAL_META_STORE_CLIENT_POOL_SIZE,
fe/src/main/java/org/apache/impala/catalog/local/DirectMetaProvider.java:      msClientPool_ = new MetaStoreClientPool(cfg.num_metadata_loading_threads, {code}
It seems each one can grow up to a max size of 32 connections. I think the usage in Frontend.java is only initialized for coordinators and the rest are all initialized in the catalog?
 Right, so after IMPALA-9375 the usage in DirectMetaProvider will go away, and we will have the usage in CatalogMetaProvider and Catalog remaining? Is there a bound on how many connections the Catalog and CatalogMetaProvider end up using, or can they both potentially create 32 connections depending on the load?

Linking [https://gerrit.cloudera.org/#/c/16030/] for reference

[~stakiar] no worries.

Yes, the constructor in Frontend.java is instantiated only for coordinators. The second in that list is called from ImpaladCatalog which is used in the legacy Catalog mode. It uses 0 initial connections and I am not aware of any paths in there which would talk to HMS. Hence I don't expect that pool to grow at all.

The upper bound on number of connections is 32 which would happen if there are 32 concurrent threads trying to do HMS operations. This max bound should not happen in Catalog since we limit to maximum 16 concurrent metadata load operations in CatalogService using the config {{num_metadata_loading_threads}}. I can see that Frontend's MetastoreClientPool can in theory have 32 open connections to HMS if there are 32 concurrent insert queries being planned on the transactional table on that coordinator. It also depends on the kind of operations being done in HMS since long lasting operations will reserve the client for a longer duration and hence its is more likely that a new connection will need to be instantiated to serve the requests. In Frontend we mostly do open, abort or commit transaction which are not expected to take a long and hence I think its unlikely that we will see 32 open connections from each coordinator. It probably is a good idea to make the max pool size configurable so that for coordinator nodes we set it a lower value.


