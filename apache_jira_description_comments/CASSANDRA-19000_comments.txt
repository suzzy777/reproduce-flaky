This one reminds me of CASSANDRA-17283

Might need just an exclusion, to be investigated. 

Okay, this is suspicious; the last run failed also on trunk... let me see.

[https://ci-cassandra.apache.org/job/Cassandra-trunk/1766/testReport/org.apache.cassandra.tools/BulkLoaderTest/testBulkLoader_WithArgs2_cdc_jdk17_arch_x86_64_python2_7/]

EDIT: I ran successfully locally BulkLoaderTest class on the latest 5.0 and trunk. So, it seems it is just yet another flaky test.

I cannot reproduce either with [5.0|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2557/workflows/be404529-e81a-4504-8ced-aaaedc647902], or on [trunk|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2558/workflows/44a50540-f04a-4459-9353-7d8ee46b8834].

However, I found the logs in nightlies from the failed test run in Jenkins - [https://nightlies.apache.org/cassandra/trunk/Cassandra-trunk-test-cdc/1826/Cassandra-trunk-test-cdc/jdk=jdk_17_latest,label=cassandra,split=7/build/test/logs/cdc.jdk17.arch=x86_64.python2.7/]

Cassandra drivers docs point to threads with names: <cluster_name>- nio-worker -<n>

And the logs - both of passing and the not passing test run contain:
{code:java}
DEBUG [cluster3-nio-worker-0] 2023-11-02 18:57:49,939 Defuncting Connection[/127.9.9.1:9042-1, inFlight=0, closed=false] because: [/127.9.9.1:9042] Cannot connect
DEBUG [cluster3-nio-worker-0] 2023-11-02 18:57:49,939 [/127.9.9.1:9042] preventing new connections for the next 1000 ms
DEBUG [cluster3-nio-worker-0] 2023-11-02 18:57:49,939 [/127.9.9.1:9042] Connection[/127.9.9.1:9042-1, inFlight=0, closed=false] failed, remaining = 0
DEBUG [cluster3-nio-worker-0] 2023-11-02 18:57:49,939 Connection[/127.9.9.1:9042-1, inFlight=0, closed=true] closing connection
DEBUG [main] 2023-11-02 18:57:49,940 [Control connection] error on /127.9.9.1:9042 connection, no more host to try
com.datastax.driver.core.exceptions.TransportException: [/127.9.9.1:9042] Cannot connect
    at com.datastax.driver.core.Connection$1.operationComplete(Connection.java:226)
    at com.datastax.driver.core.Connection$1.operationComplete(Connection.java:192)
    at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
    at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
    at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
    at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
    at com.datastax.shaded.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
    at com.datastax.shaded.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
    at com.datastax.shaded.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
    at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
    at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
    at com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
    at com.datastax.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at com.datastax.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: com.datastax.shaded.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /127.9.9.1:9042
Caused by: java.net.ConnectException: Connection refused
    at java.base/sun.nio.ch.Net.pollConnect(Native Method)
    at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
    at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
    at com.datastax.shaded.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
    at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
    at com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
    at com.datastax.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at com.datastax.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:833){code}
So I guess seeing those pop up was a matter of time.

My understanding of CASSANDRA-9054 is that tools do not have many of the services initialized during regular startup and the test verifies that we do not do that when using the BulkLoader and nothing new unneeded has popped up. 

I suggest we add an exclusion as [this |https://github.com/ekaterinadimitrova2/cassandra/commit/f14c92889eaeb9432e2ec716495b5bd313e969fe]. (I also fixed some indentation that was off)

Tested locally, same patch for 5.0 and trunk. 

+1

My understanding, reading the ticket, is that this has not been seen on circleci.  Therefore running the patch on circleci has little value.  On that, I agree with the hypothesis and committing this eagerly.

I have run it on the CircleCI multiplexer, with and without CDC and with all JDK combinations:
||Branch||CI||
|4.1  |[j8 |https://app.circleci.com/pipelines/github/adelapena/cassandra/3282/workflows/b42a6316-5fcb-45d2-8397-261e6017ebe6] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/3282/workflows/ed23c598-5e48-403b-b07c-0b734a70f7fd]|
|5.0  |[j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/3284/workflows/f888629b-3150-401c-843b-f9e8f00a2ac8] [j17|https://app.circleci.com/pipelines/github/adelapena/cassandra/3284/workflows/ffd44fb4-92ec-4c60-89c6-cc1c44bdb02b]|
|trunk|[j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/3283/workflows/42e9671b-6ab4-45a6-80de-74ee1dc29353] [j17|https://app.circleci.com/pipelines/github/adelapena/cassandra/3283/workflows/82992a46-33e4-4f63-b6f7-4cfedb4211d8]|

That's 3*2*1000 runs on each branch, and no one has reproduced the failure.
 

Thank you both!
The test also exists on previous branches, and I can see those threads coming from the drivers pooling, presented in the logs when running the tests with any jdk, cdc, etc.

I tested locally the tiny patch with all branches 4.0+. I suggest we commit the exclusion of 

cluster[0-9]-nio-worker-[0-9] in the test for all branches. WDYT? 

bq. I suggest we commit the exclusion of cluster[0-9]nio-worker[0-9] in the test for all branches. WDYT? 

Yes, I agree.  Even if it ends up being but a checkpoint commit, it's safe.

Agree, the exclusion should work. One might think that the {{close()}} calls for the driver's cluster and session in the try-with-resources at [{{NativeSSTableLoaderClient#init()}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/NativeSSTableLoaderClient.java#L70] should synchronously stop all the driver threads. But that doesn't seem to be the case according to the Jenkins-only test failure. I guess it's a driver thing and we can just exclude those threads from the test, maybe adding a comment?

{quote}Agree, the exclusion should work. One might think that the {{close()}} calls for the driver's cluster and session in the try-with-resources at [{{NativeSSTableLoaderClient#init()}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/NativeSSTableLoaderClient.java#L70] should synchronously stop all the driver threads. But that doesn't seem to be the case according to the Jenkins-only test failure. I guess it's a driver thing and we can just exclude those threads from the test, maybe adding a comment?
{quote}
 

That is a good point, to be sure I am not just covering a driver bug, I just pinged [~absurdfarce] in Slack. If it is a driver bug we need a ticket and we do not want to cover it with exclusion. I will update the ticket when I hear from [~absurdfarce] 

I confirmed with Bret McGuire that, in this case, it is expected behavior in driver version 3.11.5 (the current Java driver version is 5.0+).

Looking into 3.11.0, the same explanation [1] applies there too. The release notes show minor changes unrelated to what we are looking at here - [https://github.com/apache/cassandra-java-driver/tree/3.11.5/changelog.] (looking at the commit log, there are no other changes not mentioned in the official change log)

[1] From Bret about 3.11.5: 
{quote}ConnectionCloseFuture.force() is the relevant logic and the [use of the listener off of the channel.close() future|https://github.com/apache/cassandra-java-driver/blob/3.11.5/driver-core/src/main/java/com/datastax/driver/core/Connection.java#L1460C1-L1471] suggests that anybody who waits on the future via get() would be guaranteed that cleanup had completed. But Connection.closeAsync() only returns the future (which seems right) and the various methods on HostConnectionPool either [call closeAsync() without returning anything|https://github.com/apache/cassandra-java-driver/blob/3.11.5/driver-core/src/main/java/com/datastax/driver/core/HostConnectionPool.java#L644-L646] or [just call force() directly|https://github.com/apache/cassandra-java-driver/blob/3.11.5/driver-core/src/main/java/com/datastax/driver/core/HostConnectionPool.java#L225-L231].

HostConnectionPool.closeAsync() does return the future, so there's at least the possibility somebody could call this and wait. Without a stack trace of the actual shutdown it's hard to say for sure, but based on everything I'm seeing here I would definitely say this behaviour is expected within the current driver.

I wouldn't characterize this as a bug. The intent here is clearly for best efforts work at eventual completion. [CloseFuture.force()|https://github.com/apache/cassandra-java-driver/blob/3.11.5/driver-core/src/main/java/com/datastax/driver/core/CloseFuture.java#L55-L57] even explicitly states as much.
{quote}
If no one has concerns here, I can commit it next week (Andres is already OOO, so I do not expect an answer from him before Monday next week)

Also, this is confirmed to be test issue and not a 5.0 blocker, I just acknowledged it in the fix versions.

Thanks for the detailed explanation. IMO an exclusion with perhaps a brief comment like "the driver isn't expected to terminate threads on close synchronously" should do.

Thanks [~adelapena] , I will commit the patch, together with the suggested comment over the weekend, at latest early next week. 

Committed to https://github.com/apache/cassandra.git

   7fdb88d10a..55fecfb65e  cassandra-4.0 -> cassandra-4.0

   6a282b50b9..08d9b70b4d  cassandra-4.1 -> cassandra-4.1

   91a242fe00..a2911c7391  cassandra-5.0 -> cassandra-5.0

   1b7e895f56..316a239c7c  trunk -> trunk

