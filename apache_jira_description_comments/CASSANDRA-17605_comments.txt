The problem seemed to be TTL expiring. It can be reproed easily locally by setting TTL to 1. The fix is to check if TTL has expired hence the assert doesn't apply.

Moving this to urgent as it was seen on the 4.1 branch.

Looks good overall, here are repeated CI runs for all involved branches:
||Branch||CI||
|3.0|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1528/workflows/33c5bc80-ff45-4172-be5c-a33735545d38]|
|3.11|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1530/workflows/c66b8d14-a2d4-4f0e-98e9-052a29c5816b]|
|4.0|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1529/workflows/fa0ea980-dc10-4ec1-88fd-9548d3af7515] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1529/workflows/6d9fcc35-689b-4a74-8799-ec7a23d69e1a]|
|4.1|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1532/workflows/321b76e2-92a1-44bb-98c3-95f5cefb42f1] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1532/workflows/c9bca385-b757-45a7-87ed-bf87ca438bad]|
|trunk|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1531/workflows/c179ef60-4d3b-4ab7-bbff-1b4663901b68] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1531/workflows/b7f08bf6-df70-4f85-bffa-a99970d3b492]|

The suggested changes ignore the read failures if each block of operations takes longer than the TTL. If that happens we wouldn't have managed to properly test the desired operations due to env conditions, so IMO the test would be inconclusive. It might make sense to just skip the test if we find such scenario:
{code:python}
except AssertionError as ae:
    if (time.time() - start) < 50:
        raise ae
    else:
        pytest.skip("Skipping test because it took longer than the 50s TTL")
{code}
 Or we might continue without immediately skipping the test but just setting a boolean signalling that some check has failed and, at the end of the test, consider it skipped:
{code:python}
except AssertionError as ae:
    if (time.time() - start) < 50:
        raise ae
    else:
        skipped = True
...
if skipped:
    pytest.skip("Skipping test because some checks took longer than their specified TTL, "
                "probably due to an unusually slow environment")
{code}
Also, it could be useful to increase the TTLs to reduce the likelihood of that problematic scenario. Currently we use TTLs of 50, 40 and 30 seconds, we could for example use 100, 90 and 80 seconds.
 

Thinking it again, it might be a better idea to just increase the TTLs and use the {{@flaky}} annotation to just repeat the test a number of times until it succeeds. We could keep the current catching of exceptions and time check to throw a custom exception when the checks fail due to a slow environment. Then we would use a [{{rerun_filter}}|https://github.com/box/flaky/blob/master/flaky/flaky_decorator.py#L18-L30] to only repeat the test if it has failed in the expected conditions, similarly to what is done with [{{test_failure_due_to_timeout}}|https://github.com/apache/cassandra-dtest/blob/trunk/dtest.py#L202-L231]. That way we would be giving more time to the slow environment instead of just skipping the tests. wdyt?

[~adelapena] thanks for your feedback. In fact it trigerred me to push what I believe is an overall better solution: If TTL expires fail the test asking the user to bump TTL again. That way it's deterministic and crystal clear what the problem and the solution are. Wdyt?

I didn't test all branches as the behavior of the test didn't change. Let's see how CI goes [this time|https://app.circleci.com/pipelines/github/bereng/cassandra/655/workflows/badbea6a-00e2-40ed-aacf-89b491229d0a/jobs/5952]

Increasing TTLs and throwing a clear test error message also works for me, that's better that just ignoring the checks. If it fails again we can go back and either further increase the TTLs or add a more robust retry policy.

Is that a +1 to merge? :-)

It is, once we fix the four inequalities commented on the PR, so we don't intercept a legitimate assertion error while we are still within the TTL. I would also start either a full run of dtests or a separate repeated run of {{{}test_mv_with_default_ttl_without_flush{}}}, since the latter is also a caller of the modified {{{}_test_mv_with_default_ttl{}}}.

Note that the provided test repetitions on CircleCI are useful to verify that we haven't inadvertently introduced a new failure, but they aren't able to reproduce the original failure conditions. This is shown by [this successful run|https://app.circleci.com/pipelines/github/adelapena/cassandra/1533/workflows/1f6ab942-7b4a-4be9-959f-483700c3aea1] of {{test_mv_with_default_ttl_with_flush}} against an unpatched dtest repo. We could try to increase the number of repetitions, but I doubt that we are going to be able to reproduce the original slow env conditions on Circle. Probably this is a Jenkins-only issue.

Hi [~adelapena]: Inequalities changed, I included a full dtests run + a repeated run of the other test and the best repro of the original issue we have is the ones I did locally which matched the original stack traces. I think this is everything we need?

Looks good to me +1. There are a couple of unrelated test failures on [the provided CI run|https://app.circleci.com/pipelines/github/bereng/cassandra/656/workflows/ef05cbed-9c9f-4451-8f96-9ad7f9a441eb] that are due to the dtest branch missing CASSANDRA-17456. After rebasing [this new run|https://app.circleci.com/pipelines/github/adelapena/cassandra/1541/workflows/9e376d6d-6b8c-410f-a2c9-81afa3c796dd] looks as green as a four-leaved-clover. Same for [trunk|https://app.circleci.com/pipelines/github/adelapena/cassandra/1542/workflows/1514ce71-00a3-4940-92f6-4e2c6d9eb044]. I think we're ready to commit.

bq. looks as green as a four-leaved-clover

:-DDD

