Slightly better with the debug trace included

{code}
018-04-06 16:35:23,048 [main] DEBUG s3a.S3AFileSystem (S3AFileSystem.java:innerDelete(1760)) - Delete path s3a://hwdev-steve-new/ - recursive true
2018-04-06 16:35:23,048 [main] DEBUG s3a.S3AFileSystem (S3AFileSystem.java:innerDelete(1765)) - delete: Path is a directory: s3a://hwdev-steve-new/
2018-04-06 16:35:23,048 [main] INFO  s3a.S3AFileSystem (S3AFileSystem.java:rejectRootDirectoryDelete(1837)) - s3a delete the hwdev-steve-new root directory of true
rm: `s3a://hwdev-steve-new/': Input/output error
2018-04-06 16:35:23,050 [pool-2-thread-1] DEBUG s3a.S3AFileSystem (S3AFileSystem.java:close(2492)) - Filesystem s3a://hwdev-steve-new is closed

{code}

Proposed: 

# improve info message. downgrade?
# log @ error before throwing the exception



Created an initial patch based on the idea that {{org.apache.hadoop.fs.s3a.S3AFileSystem#innerDelete}} returns what returned from {{org.apache.hadoop.fs.s3a.S3AFileSystem#rejectRootDirectoryDelete}}, and the same is returned to {{org.apache.hadoop.fs.s3a.S3AFileSystem#delete}} which ultimately returned to {{org.apache.hadoop.fs.shell.Delete.Rm#processPath}} and we have the {{PathIOException}} which outputs the {{Input/output error}}. 
Based on that, I found that the best places to add the messages are the ones I used.


| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 10s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 21m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 22s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 10s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 23s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 44s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  4m 30s{color} | {color:green} hadoop-aws in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 27s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 60m 10s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | HADOOP-15370 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12949076/HADOOP-15370.001.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 6b7cfeba495c 4.4.0-134-generic #160~14.04.1-Ubuntu SMP Fri Aug 17 11:07:07 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 34b6fa7 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC1 |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/15550/testReport/ |
| Max. process+thread count | 342 (vs. ulimit of 10000) |
| modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/15550/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.



LGTM. As usual, where did you test it, what s3guard settings, etc?

mvn verify ran against eu-west-1. s3guard: dynamo, non-auth and auth after that.

Results are not looking good. For non-authoritative:
{noformat}
[ERROR] Failures:
[ERROR]   ITestDynamoDBMetadataStore>MetadataStoreTestBase.testDeleteSubtree:321->MetadataStoreTestBase.deleteSubtreeHelper:346->MetadataStoreTestBase.assertEmptyDirectory:927->MetadataStoreTestBase.assertDirectorySize:882->Assert.assertNotNull:712->Assert.assertTrue:41->Assert.fail:88 Directory /ADirectory2 is null in cache
[ERROR]   ITestDynamoDBMetadataStore>MetadataStoreTestBase.testDeleteSubtreeHostPath:326->MetadataStoreTestBase.deleteSubtreeHelper:346->MetadataStoreTestBase.assertEmptyDirectory:927->MetadataStoreTestBase.assertDirectorySize:882->Assert.assertNotNull:712->Assert.assertTrue:41->Assert.fail:88 Directory s3a://cloudera-dev-gabor-ireland/ADirectory2 is null in cache
[ERROR]   ITestDynamoDBMetadataStore.testProvisionTable:594->Assert.assertEquals:631->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 expected:<20> but was:<10>
[ERROR]   ITestDynamoDBMetadataStore>MetadataStoreTestBase.testPutNew:243->MetadataStoreTestBase.assertEmptyDirs:932->MetadataStoreTestBase.assertEmptyDirectory:927->MetadataStoreTestBase.assertDirectorySize:882->Assert.assertNotNull:712->Assert.assertTrue:41->Assert.fail:88 Directory /da2 is null in cache
[ERROR] Errors:
[ERROR]   ITestDynamoDBMetadataStore.testBatchWrite:318->doTestBatchWrite:365 NullPointer
[ERROR]   ITestDynamoDBMetadataStore>MetadataStoreTestBase.testPruneUnsetsAuthoritative:737 ? NullPointer
[ERROR]   ITestS3GuardToolDynamoDB>AbstractS3GuardToolTestBase.testDestroyNoBucket:336->AbstractS3GuardToolTestBase.run:115 ? IllegalArgument
[ERROR]   ITestS3AMiniYarnCluster.setup:68 ? NoClassDefFound org/bouncycastle/jce/provid...
{noformat}

I've seen {{ITestS3AMiniYarnCluster#testWithMiniCluster}} and {{ITestS3GuardToolDynamoDB#testDestroyNoBucket}} but not the others.

These failures/errors are not related to the patch (obviously). I did a regression testing with clean trunk, and got the same results. 

Failures/errors with non-authoritative:
{noformat}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.912 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.yarn.ITestS3AMiniYarnCluster
[ERROR] testWithMiniCluster(org.apache.hadoop.fs.s3a.yarn.ITestS3AMiniYarnCluster)  Time elapsed: 19.697 s  <<< ERROR!
java.lang.NoClassDefFoundError: org/bouncycastle/jce/provider/BouncyCastleProvider
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:840)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1270)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:328)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.initResourceManager(MiniYARNCluster.java:348)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$200(MiniYARNCluster.java:128)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceInit(MiniYARNCluster.java:497)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:316)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.fs.s3a.yarn.ITestS3AMiniYarnCluster.setup(ITestS3AMiniYarnCluster.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.bouncycastle.jce.provider.BouncyCastleProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 27 more
{noformat}

{noformat}
ERROR] Tests run: 40, Failures: 4, Errors: 2, Skipped: 0, Time elapsed: 177.303 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore
[ERROR] testBatchWrite(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 1.262 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore.doTestBatchWrite(ITestDynamoDBMetadataStore.java:365) (...)

[ERROR] testProvisionTable(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 11.394 s  <<< FAILURE!
java.lang.AssertionError: expected:<20> but was:<10> (...)
org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore.testProvisionTable(ITestDynamoDBMetadataStore.java:594) (...)

[ERROR] testPruneUnsetsAuthoritative(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 2.323 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.fs.s3a.s3guard.MetadataStoreTestBase.testPruneUnsetsAuthoritative(MetadataStoreTestBase.java:737) (...)

[ERROR] testDeleteSubtree(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 2.377 s  <<< FAILURE!
java.lang.AssertionError: Directory /ADirectory2 is null in cache (...)

[ERROR] testPutNew(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 1.466 s  <<< FAILURE!
java.lang.AssertionError: Directory /da2 is null in cache (...)

[ERROR] testDeleteSubtreeHostPath(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 2.378 s  <<< FAILURE!
java.lang.AssertionError: Directory s3a://cloudera-dev-gabor-ireland/ADirectory2 is null in cache (...)
{noformat}

{noformat}
[ERROR] testDestroyNoBucket(org.apache.hadoop.fs.s3a.s3guard.ITestS3GuardToolDynamoDB)  Time elapsed: 1.921 s  <<< ERROR!
java.lang.IllegalArgumentException: No DynamoDB table name configured (...)
{noformat}

I've created a gist with the results for authoritative and non-authoritative: https://gist.github.com/bgaborg/e0d8092659cf009fcd2985ca95c574be


ITestS3AMiniYarnCluster#testWithMiniCluster could fail because of HADOOP-15832.
I'll check if we have created issues for these errors, and create if we haven't yet.

* NPEs are new
* Bouncy castle, yes. I've got a fix for that in HADOOP-14556, I'm not worried there: [fix|https://github.com/steveloughran/hadoop/blob/s3/HADOOP-14556-delegation-token/hadoop-tools/hadoop-aws/pom.xml#L509]. I'd like that patch in, but we could tease that bit out for an early fix.
* I've been hitting some other standalone s3guard test failures; filed some bugs against that

w.r.t NPE. that {{status.getPath()}} in the log could trigger it. Check for it not being null before expanding

I've created HADOOP-15947 and fixed the 4 issues in {{ITestDynamoDBMetadataStore}}, submitted a patch.

+1 on this patch. But I am also seeing a pretty scary number of (presumably unrelated) failures that we really need to get under control. I ran all tests in parallel with 8 threads, but with and without this patch, HADOOP-15947, and HADOOP-15798. I tried with S3Guard disabled, with -Ds3guard, -Ds3guard -Ddynamo, and finally -Ds3guard -Ddynamo -Dauth. The bouncycastle issue always shows up regardless, so I'm fine to ignore that until Steve's fix goes in. I hit HADOOP-14927 with -Ds3guard against the local metadata store. And I hit this issue once with S3Guard disabled, but I haven't been able to reproduce it:

{code}[ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 172.151 s <<< FAILURE! - in org.apache.hadoop.fs.contract.s3a.ITestS3AContractDistCp
[ERROR] testUpdateDeepDirectoryStructureToRemote(org.apache.hadoop.fs.contract.s3a.ITestS3AContractDistCp)  Time elapsed: 24.869 s  <<< ERROR!
java.io.FileNotFoundException: Expected file: not found s3a://mackrory/fork-0002/test/ITestS3AContractDistCp/testUpdateDeepDirectoryStructureToRemote/remote/DELAY_LISTING_ME/outputDir/inputDir/file1 in s3a://mackrory/fork-0002/test/ITestS3AContractDistCp/testUpdateDeepDirectoryStructureToRemote/remote/DELAY_LISTING_ME/outputDir/inputDir
        at org.apache.hadoop.fs.contract.ContractTestUtils.verifyPathExists(ContractTestUtils.java:940)
        at org.apache.hadoop.fs.contract.ContractTestUtils.assertPathExists(ContractTestUtils.java:918)
        at org.apache.hadoop.fs.contract.ContractTestUtils.assertIsFile(ContractTestUtils.java:826)
        at org.apache.hadoop.fs.contract.ContractTestUtils.verifyFileContents(ContractTestUtils.java:235)
        at org.apache.hadoop.tools.contract.AbstractContractDistCpTest.distCpDeepDirectoryStructure(AbstractContractDistCpTest.java:499)
        at org.apache.hadoop.tools.contract.AbstractContractDistCpTest.testUpdateDeepDirectoryStructureToRemote(AbstractContractDistCpTest.java:223)
        ... 16 omitted by me
Caused by: java.io.FileNotFoundException: No such file or directory: s3a://mackrory/fork-0002/test/ITestS3AContractDistCp/testUpdateDeepDirectoryStructureToRemote/remote/DELAY_LISTING_ME/outputDir/inputDir/file1
        at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2280)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2174)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2112)
        at org.apache.hadoop.fs.contract.ContractTestUtils.verifyPathExists(ContractTestUtils.java:934)
        ... 21 more{code}

All the really scary results are enumerated here: https://gist.github.com/mackrorysd/81eb96e91af1d05f59db9871da60b178

That's just too much variation for us to commit with confidence. We need to figure out some central way to track these failures and make a concerted, combined effort to get that down. [~stevel@apache.org] [~gabor.bota] What do you think about starting a Google Spreadsheet or something where we can keep a tally / dates or failures currently occurring in trunk. Short-term, we can consult that to see if a failure is just flaky and not a sign of a regression in the current patch. Medium-term, we try to get that list down to nothing by prioritizing fixing the ones that crop up most often.

Usually, these tests are failing if the location I run tests against is really remote (like running against us-west from Budapest office). What target are you running your tests against?

Sure we can create a google doc for these failures. Please create one [~mackrorysd] with the tests that are failing, and the configuration you use. I'll test it and figure out what's happening.

So my bucket and DDB table are both in us-west-2, which is the closest to me. I also started running tests on a machine in us-west-2 and saw little difference. My table was a bit under-provisioned, but I'm back at the defaults now, again with little difference. I'll be tracking failures in this spreadsheet from now on until things seem more settled on my end. Everyone should have view access, and I'll gladly add any ASF contributor to the edit permissions to also track their reports if they'd like. I'll do more test runs tomorrow. Without S3Guard and with local metadatastore, the runs are pretty clean except for issues that are known and documented with fixes on the way.

https://docs.google.com/spreadsheets/d/1Z9dkg5yC7Hu7VQ5G2Wz40hG1pb0DhLuLjJ3PZe7WL3Q/edit?usp=sharing

I had a bucket load of failures a while back, did look @ some of them but nothing coalesced. I think the HADOOP-15843 had 

* having a shared DDB table rather than one mapped to the bucket itself confused tests, to the extent that the DDB table actually got deleted at one point, I believe I recall
* s3guard + local was broken: some tests running when they shouldn't. I rarely test that configuration. Maybe I should do it more often.
* we've a failing multipart upload test in: HDFS-13713. I think we should just get that final patch in with some tuning of the final spec.

Thanks [~stevel@apache.org]. Yeah I've barely touched s3a since we added the separate test property and had forgotten the purpose behind was precisely to solve that problem. I'm not using a separate table for that property. I've also cranked up the read / write limits on my tables and lowered my test concurrency to 4. I'm still getting a couple of scaling exceptions, which seems very wrong, but things are at least deterministic enough that I can see that the set of 3 patches I just reviewed fixes the issues they aim to, and do not introduce regressions. I'm gonna keep that spreadsheet up to date with more recent results until I can get to the bottom of all of those.

+1, committed this patch.

SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #15517 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/15517/])
HADOOP-15370. S3A log message on rm s3a://bucket/ not intuitive. (mackrorysd: rev 5d96b74f33ca716c9fe4fadb046f79ed488a3059)
* (edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java


