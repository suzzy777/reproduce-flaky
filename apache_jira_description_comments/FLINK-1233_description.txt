The test seems to trigger a race condition in the scheduler.

My first guess is that it is the structure of the job that frees slots from the slot sharing group (when the final aggregation is running with parallelism 1) and then tries to add slots to that sharing group again from unoccupied slots.

{code}
java.lang.Exception: Cannot schedule the receivers, not enough resources
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.lookupConnectionInfoAndDeployReceivers(ExecutionGraph.java:591)
	at org.apache.flink.runtime.jobmanager.JobManager.lookupConnectionInfo(JobManager.java:558)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.flink.runtime.ipc.RPC$Server.call(RPC.java:420)
	at org.apache.flink.runtime.ipc.Server$Handler.run(Server.java:947)
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Not enough free slots available to run the job. You can decrease the operator parallelism or increase the number of slots per TaskManager in the configuration. Resources available to scheduler: Number of instances=1, total number of slots=4
	at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleTask(Scheduler.java:220)
	at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleImmediately(Scheduler.java:135)
	at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:203)
	at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:342)
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.lookupConnectionInfoAndDeployReceivers(ExecutionGraph.java:585)
{code}