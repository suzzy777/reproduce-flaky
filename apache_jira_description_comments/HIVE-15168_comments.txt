Sadly my wonderful fix managed to prevent the testJobSubmission test from failing for one, single, sad day....
I think I found a second racecondition, would you mind taking a second look at it [~xuefuz], [~lirui]?
I hope this time my fix will be a tiny bit more permanent...

In the meantime I'll try running the test a couple of hundred times in a loop to see if it breaks again.

Could we have a few words describing the problem and the fix? It's not obvious while reading code diff. Thanks.

Also, please attach the patch here as well.

The problem is similar to the one fixed before for this test, we have a race condition between the listeners being registered (rpc.addListenre and promis.addListener) and the submit of the message through RPC.
If you take a look at SparkClientImpl#ClientProtocol#submit you will see that currently the driverRpc.call is invoked before the listeners are registered.
To reproduce the test failure one needs to add for example a Thread.sleep after the driverRpc.call and before the listeners are being registered.
This would probably never or almost never happen in real life, because the execution of the spark job and the network latency should easily take longer than the time needed for the code with the listeners to run. But in the unit test it is a cause of intermittent failures.

Hi [~xuefuz],
did you by any chance had the time to take a look at my proposed refactoring in the spark client? If you don't like the current approach then I can try to come up with something that would only impact the junit tests.



Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12838339/HIVE-15168.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 10699 tests executed
*Failed tests:*
{noformat}
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=103)
	[bucketsortoptimize_insert_4.q,multi_insert_mixed.q,vectorization_10.q,auto_join18_multi_distinct.q,join_cond_pushdown_3.q,custom_input_output_format.q,skewjoinopt5.q,vectorization_part_project.q,vector_count_distinct.q,skewjoinopt4.q,count.q,auto_join7.q,parallel.q,union33.q,union_lateralview.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=106)
	[union_remove_1.q,ppd_outer_join2.q,groupby1_noskew.q,join20.q,smb_mapjoin_13.q,multi_insert.q,groupby_rollup1.q,temp_table_gb1.q,vector_string_concat.q,smb_mapjoin_6.q,metadata_only_queries.q,auto_sortmerge_join_12.q,groupby_bigdata.q,groupby3_map_multi_distinct.q,innerjoin.q]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_acid_non_acid] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=145)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2222/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2222/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2222/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12838339 - PreCommit-HIVE-Build

I will take a look at the TestSparkCliDriver failures.

Spark related tests are running locally with a minimal variance (one batch was 2% slower than master). Reuploading patch to see if issue is reproducible.

Sorry for the delay. I will take a look today. Thanks.



Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12839795/HIVE-15168.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10715 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=43)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_schema_evol_3a] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_acid_non_acid] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=91)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=93)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2226/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2226/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2226/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12839795 - PreCommit-HIVE-Build

There seems to be test failures.

i'm not sure these failures are related.
These are known flaky tests:
explainanalyze_2 - https://issues.apache.org/jira/browse/HIVE-15084
transform_ppr2 - https://issues.apache.org/jira/browse/HIVE-15201
orc_ppd_schema_evol_3a - https://issues.apache.org/jira/browse/HIVE-14936
These are failing rather consistently:
union_fast_stats - https://issues.apache.org/jira/browse/HIVE-15115
join_acid_non_acid - https://issues.apache.org/jira/browse/HIVE-15116

auto_sortmerge_join_2 - is not identified as flaky, but the failure is in MR, I don't think my changes to the SparkClient could have caused it. And it did not fail on the first run.
{code}
< FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
< ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
{code}

This leaves TestSparkCliDriver, which I cannot repro locally.
From the test report it seems to me that the error is happening during the test setup:
{code}
java.lang.AssertionError: Failed during createSources processLine with code=3
{code}
But the hive log has a different failure:
{code}
2016-11-21T08:36:57,751  INFO [stderr-redir-1] client.SparkClientImpl: 16/11/21 08:36:57 WARN TaskSetManager: Lost task 0.0 in stage 46.0 (TID 53, 10.234.144.78): java.io.IOException: Failed to create local dir in /tmp/spark-8a7bd913-fca5-4990-ad09-c9eff4dacae0/executor-e85f9833-b0ab-47ed-bb95-56dc9ef64177/blockmgr-044ca916-76f1-402f-80ed-c4ec5fd4d544/2b.
{code}
I'm not sure if either can be related to the refactoring in the SparkClientImpl.

[~zsombor.klara], not sure if I correctly understand your explanation about the race condition, but looking at the doc of addListener,
{code}
    /**
     * Adds the specified listener to this future.  The
     * specified listener is notified when this future is
     * {@linkplain #isDone() done}.  If this future is already
     * completed, the specified listener is notified immediately.
     */
    Future<V> addListener(GenericFutureListener<? extends Future<? super V>> listener);
{code}
Seems the listener will be notified even if it's registered after the future has completed?

That is indeed a very good point. Maybe I misunderstood the race condition here. I will cancel the patch until I can investigate a bit further. You are right that according to the javadoc the listeners should get notified even if they are registered after the future has completed.

I had another look at this and you are correct, the listeners are being called even if the task has completed. Unfortunately that isn't enough for us because in the JobHandleImpl the state change will not reset the state of a job to queued if it is already cancelled/succeeded/failed. And if we never set the state to queued then the onJobQueued method will never be called on the JobHandleListener.
{code}
 /**
   * Changes the state of this job handle, making sure that illegal state transitions are ignored.
   * Fires events appropriately.
   *
   * As a rule, state transitions can only occur if the current state is "higher" than the current
   * state (i.e., has a higher ordinal number) and is not a "final" state. "Final" states are
   * CANCELLED, FAILED and SUCCEEDED, defined here in the code as having an ordinal number higher
   * than the CANCELLED enum constant.
   */
  boolean changeState(State newState) {
    synchronized (listeners) {
      if (newState.ordinal() > state.ordinal() && state.ordinal() < State.CANCELLED.ordinal()) {
        state = newState;
        for (Listener<T> listener : listeners) {
          fireStateChange(newState, listener);
        }
        return true;
      }
      return false;
    }
  }
{code}

I think that this code is correct and should not be changed. Once a job has transitioned to a terminal state we should not revert it to queued. But it also means that we must ensure that the state changes happen sequentially.

[~zsombor.klara], thanks for the investigation. I also tried adding some sleep in the listener before changing the state to QUEUED, then the test fails consistently.
Based on that, I think we have two choices. One is to remove {{verify(listener).onJobQueued(handle)}} in the test. Because it's not guaranteed to be called. Seems we can keep {{verify(listener).onJobStarted(handle)}} - at least on the RemoteDriver side we're sending JobStarted and JobResult sequentially.
The other one is try to detect the missing state changes. E.g. if the current state is SENT and we're told to change to SUCCEEDED, then we must have missed QUEUED and STARTED. And we can notify the listeners of the missing state changes before we change to SUCCEEDED.
[~xuefuz] what's your opinion on this?

I'm fine with either approach. Mostly because I think this is only an issue for the unit test.

I'd prefer to remove the verification in the test - don't want to add extra logic to make test pass.
[~zsombor.klara] let's do this unless [~xuefuz] thinks otherwise.

Uploading new (and much simplified) patch based on [~lirui]'s comments.



Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12840260/HIVE-15168.1.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 10733 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_acid_non_acid] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=145)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2261/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2261/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2261/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12840260 - PreCommit-HIVE-Build

Agree with Rui that we should just fix the test case. It would be also good if we put some comments in the code saying a potential race condition exists however unlikely in production, for future reference.

[~zsombor.klara], would you mind update the patch to address Xuefu's comments?

Added a comment in the listener where we set the QUEUED state clarifying that the state transition may not happen if the job succeeds before it is called.

The javadoc on the JobHandleImpl#changeState(State newState) is already clear enough I think.



Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12840402/HIVE-15168.2.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10684 tests executed
*Failed tests:*
{noformat}
TestMiniSparkOnYarnCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=157)
	[infer_bucket_sort_num_buckets.q,gen_udf_example_add10.q,insert_overwrite_directory2.q,orc_merge5.q,bucketmapjoin6.q,import_exported_table.q,vector_outer_join0.q,orc_merge4.q,temp_table_external.q,orc_merge_incompat1.q,root_dir_external_table.q,constprog_semijoin.q,auto_sortmerge_join_16.q,schemeAuthority.q,index_bitmap3.q,external_table_with_space_in_location_path.q,parallel_orderby.q,infer_bucket_sort_map_operators.q,bucketizedhiveinputformat.q,remote_script.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=106)
	[union_remove_1.q,ppd_outer_join2.q,groupby1_noskew.q,join20.q,smb_mapjoin_13.q,multi_insert.q,groupby_rollup1.q,temp_table_gb1.q,vector_string_concat.q,smb_mapjoin_6.q,metadata_only_queries.q,auto_sortmerge_join_12.q,groupby_bigdata.q,groupby3_map_multi_distinct.q,innerjoin.q]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_schema_evol_3a] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_acid_non_acid] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=145)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=93)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2283/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2283/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2283/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12840402 - PreCommit-HIVE-Build

+1. The latest failures are not related

Committed to master. Thanks Barna for the patch and Xuefu for the review.

