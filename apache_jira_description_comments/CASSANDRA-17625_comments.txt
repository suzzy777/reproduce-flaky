I haven't been able, despite much thinking, of coming up with a better theory than a legit timeout. I've seen updates take more than 30s on overloaded dtests so...

The links expired, I don't see this failure in Butler on 4.1 or trunk.

Trying to reproduce without the patch [here|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=17625-4.1&filter=all] using:

 
{code:java}
.circleci/generate.sh -m \
-e REPEATED_DTEST_NAME=auth_test.py::TestAuth::test_system_auth_ks_is_alterable \
-e REPEATED_DTEST_VNODES=false \
-e REPEATED_DTEST_COUNT=500 \
-e REPEATED_DTEST_STOP_ON_FAILURE=false
{code}
 

This failure is not reproducible and I think it was random timeout. I would suggest we close the ticket without raising the 30s further for now. 

If we still want to raise it, I wouldn't make it double though... maybe 40s at most

[~e.dimitrova] iirc I think I managed to repro adding sleeps and playing with timeouts to make sure I hit the scenario I was after. Does this help you repro it?

Also I have seen a simple update on dtests take 30s, so raising it it's still a good idea imo. I don't know if 40s will make a difference though being this a schema change rather than a single simple update. 60s, finger in the air, sounded better. Wdyt?

{quote}[~e.dimitrova] iirc I think I managed to repro adding sleeps and playing with timeouts to make sure I hit the scenario I was after. Does this help you repro it?
{quote}
Can you share the steps, please? I guess you still have them as you tested the patch. Thanks in advance

Not only I don't have them. But on Ubuntu 22 seems we can't run dtests. I am swimming in a sea of exceptions while trying to run dtests bc of python vs pytest vs libs vs the universe. Which worries me as more people will start using Ubuntu 22 soon.

Anyway let me see if this helps: There is a python wrapper that will force a wait on schema agreement and it observes {{max_schema_agreement_wait}}. This is forced through so trying a '1' here and seeing what happens would be my first test. That would, if your PC is slow enough, render the error we're after. If that doesn't work you can always play with {{wait_for_schema_agreement}} to shave or add a few seconds here and there to artificially repro the error.

This is why on a very heavily loaded system you can hit this imo as after much looking around I didn't find any problems unless I missed sthg. Hence giving more time for the schemas to agree, which seems was already a concern of the original author also, seems the best bet and matches the repro and the stack traces imo.

Ok I managed to crawl myself out of the libs hell

{noformat}
11:33:16,761 ccm DEBUG Log-watching thread exiting.
===Flaky Test Report===

test_system_auth_ks_is_alterable failed and was not selected for rerun.
	<class 'cassandra.DriverException'>
	Keyspace metadata was not refreshed. See log for details.
	[<TracebackEntry /home/bereng/work/repos/bdpWS/dtests/auth_test.py:69>, <TracebackEntry /home/bereng/work/repos/bdpWS/dtests/tools/metadata_wrapper.py:10>, <TracebackEntry /home/bereng/work/repos/bdpWS/dtests/tools/metadata_wrapper.py:57>, <TracebackEntry /home/bereng/work/repos/bdpWS/dtests/src/cassandra-driver/cassandra/cluster.py:2162>]
{noformat}

This is by forcing {{wait_for_schema_agreement}} to return False to simulate a timeout on schema agreement. My 2cts.

I still do not think we should raise it now. Moreover, there were a lot of CI issues and now things are back to normal and this test is not seen being failing.

Neither  we can reproduce it after hundreds of runs without skipping the wait time. I am +0 on this change

Well this failed on jenkins not circle iiuc. And we know sometimes they repro on one place but not another. Also after all the effort I put into it it would be a pity imo we don't take this chance to harden this now that it's free money on the table imo. Like I don't see any disadvantages merging this but only advantages.

I am a strong +1 on merging this so unless you disagree I still want to merge it. Otherwise we can ask sbdy else to break the tie sort to speak? wdyt?

{quote}Well this failed on jenkins not circle iiuc. And we know sometimes they repro on one place but not another.
{quote}
True,  but it doesn't fail in Jenkins too after the infra issues were solved. 

If currently the other tests go with default 10 and we don't see a need to raise it, but this one starts failing with more than 3 times longer delay I would love to know why tbh. Now you spent the time to confirm that it was just our infra. The infra is fixed and I don't see a reason to make it 6 times the default delay.

Maybe [~jmckenzie] as a reporter or anyone else can also express opinion here. 

bq. True,  but it doesn't fail in Jenkins too after the infra issues were solved. 

Flakies will come and go. That it didn't fail recently is not proof it may not fail again and it's been just a few days since those were fixed. Also that is orthogonal to merging work that has already been done, cost has been incurred already and has no downside.

bq. starts failing with more than 3 times longer delay I would love to know why tbh

If it's due to a slow env, which is my guess, it can hit at any random time and some operations will be more affected than others. And there's no other thread to pull iiuc.

bq. Now you spent the time to confirm that it was just our infra

I disagree here. I only found a repro so far.

bq. The infra is fixed and I don't see a reason to make it 6 times the default delay

It's not 6 times. It was 30s and now it will be 60s. As I said I have seen other tests take 30s to insert a row. So 60s for a complex operation like a schema agreement to complete sounds reasonable to me.

All you pointed are valid questions to ask oneself. But I don't see any reason not take the free money on the table as the cost to tackle the ticket has already been incurred. [~jmckenzie] I am still a strong +1 on merging the effort put here and I don't see any disadvantage to it.




Honestly, to me our arbitrary setting of timeouts on various tests and operations is a giant headache.

If a test has a timeout of 30 seconds in it, that implies that the contract with the end user we're trying to test and enforce is that the operation will complete in 30 seconds or less on hardware >= the specs of the ASF Jenkins env (our likely weakest CI env).

On those grounds, I'd advocate for all transient intra-test timeouts to be pushed up to whatever the global test timeout is (10 minutes, 15, etc), and only to enforce lower timeouts on operations in the very specific cases we are committing to an operation taking place in <= a certain amount of time.

Chasing flaky timeouts like this is not a value-add activity for the project IMO.

{quote}If a test has a timeout of 30 seconds in it, that implies that the contract with the end user we're trying to test and enforce is that the operation will complete in 30 seconds or less on hardware >= the specs of the ASF Jenkins env (our likely weakest CI env).
{quote}
Hey [~jmckenzie], this is wait for schema agreement not a test timeout so definitely I wouldn't advocate for setting it to 10 minutes. :) 
{quote}It's not 6 times. It was 30s and now it will be 60s. 
{quote}
It was 10s default, raised to 30 long time ago, so now it will be 6 times and it is currently not even flaky anymore.

With all that said, I am +0 and moving away from this ticket. It's up to you and whoever assigns themselves as a reviewer.

I expressed my opinion.

{quote}I wouldn't advocate for setting it to 10 minutes.
{quote}
I'm advocating for the concept of "wait for a timeout" to always point to a global constant that we use in _literally every place_ we wait for a timeout in a test so things like this don't constantly drain our time and energy. It was 10s, we raised it to 30 a long time ago, and if 1/100 test runs it takes 31 seconds we're going to waste our time on it.

It certainly could make single pathologically bad test runs take a long time on ASF Jenkins but I think that's a reasonable price to pay.

How we should improve our handling of timeouts, which could really help long term given our history, it's certainly sthg to take into account. We've had this discussion before and wondered if that would help us go green given the high number of timeouts we've historically had. Certainly a point to discuss given the drain of effort this represents. But on this particular ticket this price has already been payed.

So given this ticket has already been investigated, repro'ed and a commit proposed, which has no drawbacks, I would like to merge it so my effort doesn't go to waste. Hence why I'd like to get a +1 to merge, given it's a 1loc change it's a small ask and review imo.

Oh yeah for sure; sorry, I have a bad habit of making meta observations in JIRA comments which then reads as if I'm blocking work on something based on that. Totally not the intent here.

We can follow up on the dev ML about the bigger picture once we're at green on 4.1.

[~jmckenzie] apologies for the late reply. I've been OOO.

Yes 1y ago I was proposing raising and/or reworking timeouts. I did create a branch and ran some tests and everything looked good. I think it would indeed be a good idea to raise that point once CI is back on track.

Regarding this particular ticket I am not sure if you're saying you've reviewed, +1'ed and are happy I merge?

Formally +1'ing this after barging in here like that is the least I can do.

+1 to taking timeout from 30-60. Patch looks good.

lol Thx! Let's try to remember one or the other to revisit timeouts from a generic pov once 4.1 is out and CI is back to normal again.

