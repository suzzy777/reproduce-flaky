And, if one e2e test fails, the remaining ones still run. This means that the person who should be alerted to the failure finds out later, sometimes hours later.

Why would you want to stop all other tests from running? Assuming there are no dependancies, I'd rather have a full list of the tests that failed rather than fixing one at a time and trying again. Obviously, if all or more than N tests start failing it may make sense to abort. In any case, I think the behavior should be configurable. 

Well, for one thing, the test output is so verbose, when I know that some test failed, I hate having to comb through the output to see which one(s). For another, it delays alerting the monitoring system (or monitoring human).

That's not entirely true. For instance, there is a plugin that we currently use that summarizes the results for all the frontend tests (look at Test Results link in any job that runs tests), so you don't always have to comb through the logs. I don't believe it's a good idea to prevent all tests from running because the test output is too verbose. Maybe we should fix that first. 

We used to stop end-to-end tests when the first one failed, but we changed that because it killed our test coverage.  If there was one flaky test, we'd loose coverage of all other tests for that run.

I believe there's still an option to fail fast that you can use for local/private runs, though.

It's not so much determining the allowable number of failing end-to-end tests -- 1 versus 10 -- that I was questioning. I guess I was more so surprised that if any of the *backend* tests fail, that we go on to run the end-to-end tests at all.

What prompted me to file this bug? [~mikesbrown] noticed that [http://sandbox.jenkins.cloudera.com/job/impala-umbrella-build-and-test/4839/] failed because of this backend test failure:

{noformat}
21:29:51       Start 38: tmp-file-mgr-test
21:29:54 38/84 Test #38: tmp-file-mgr-test ................***Failed    3.73 sec
{noformat}

It's not just a matter of the test output being extremely verbose (although Jim is correct -- one really has to look hard to find this failure.) In my experience, it's a common testing assumption that unit tests -- which is what I understand the back end and front end tests to be -- need to pass before attempting any kind of system/integration/performance/etc. tests. If your unit tests can't pass, than what kind of faith can you have in your end-to-end tests? 

Perhaps I have a misunderstanding with regard to the purpose of the backend and front end tests though?

I don't have a strong opinion about with the proposed change of making end-to-end depend on frontend and backend passing (I agree in theory; in practice our unit tests may still fail too much due to test bugs, but this has improved).  My comment was in regards Jim's first comment -- we should not stop executing the rest of the e2e tests if one fails because there is no strong dependency between any two e2e tests.

I'm not a big fan of terminating our test runs early, even if there is a failure. We had several variants of that in the past and none of them worked all too well for our typical dev workflow.
The main problem is that when there are several test failures it takes a very long time and repeated runs to fix all of them. There is no "resume the test runs from here" feature, so most of the time most of the tests need to be re-run from scratch (including compiling, data loading, etc. etc.)

On the other hand, it is possible to monitor the progress of a job to find out about failures early (if that is desired).

I did not expect that this would be a controversial idea, but I also understand that there's an issue with our test setup being so expensive.

I'm curious -- do we actually require the data to be loaded (or even for the minicluster to be running) before the BE and FE tests will pass?


I believe the BE tests will run without data, but the FE tests definitely require tables and data.

