I'm not sure what else we can really do here. It's a timeout that didn't reach schema agreement, it knows it didn't, and reported it.

Well, I guess we need to improve the test. Didn't we have similar issue with the Python DTests?

What is weird about this test is that it shows me it was run only twice in Butler (when it failed) and it wasn't run otherwise?

bq. Didn't we have similar issue with the Python DTests?

I think in those the issue was not waiting for agreement, where here it has, and failed after waiting 70s.

bq. What is weird about this test is that it shows me it was run only twice in Butler (when it failed) and it wasn't run otherwise?

That is odd, I don't know what's going on there.

I've tried to reproduce it on the multiplexer:
{code}
.circleci/generate.sh -p \
  -e REPEATED_JVM_DTESTS=org.apache.cassandra.distributed.test.CASMultiDCTest \
  -e REPEATED_JVM_DTESTS_COUNT=100
{code}
https://app.circleci.com/pipelines/github/adelapena/cassandra/3198/workflows/0ee48e01-1c84-44cd-a783-6ec6b487b029

It doesn't hit that failure with 100 runs, but it finds a different failure in {{testLocalSerialLocalCommit}}:
{code}
junit.framework.AssertionFailedError: numWritten: 2 < 3
	at org.apache.cassandra.distributed.test.CASMultiDCTest.testLocalSerialCommit(CASMultiDCTest.java:111)
	at org.apache.cassandra.distributed.test.CASMultiDCTest.testLocalSerialLocalCommit(CASMultiDCTest.java:121)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}

I haven't been able to repro. {{testLocalSerialLocalCommit}} seems to be flaky [in 4.1 as well|https://ci-cassandra.apache.org/job/Cassandra-4.1/417/testReport/org.apache.cassandra.distributed.test/CASMultiDCTest/] according to jenkins despite logs being lost by now already. #justfyi

Oh wait the schema disagreement seems to be 5.0 only [this|https://ci-cassandra.apache.org/job/Cassandra-5.0/33/testReport/junit.framework/TestSuite/org_apache_cassandra_distributed_test_CASMultiDCTest__jdk11/] is a recent failure

The problem is that on a slow env the node does indeed take more than 70s to start so schema change agreement can't be reached. In the attached log  [^TESTS-TestSuites.xml.xz] we can see the test starts and 70s later it fails which matches the [70s timeout|https://github.com/apache/cassandra/blob/cassandra-5.0/test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java#L968] of the change monitor.

If you grep the log for 'Schema updated' or 'd03783d7'  you can see the propagation of schemas being correct but not being complete. Also the test goes silent for 47s which is probably the root cause blowing the timeouts: 

{noformat}
INFO  [node4_ScheduledTasks:1] node4 2023-09-13 02:08:29,935 StatusLogger.java:121 - system_auth.role_permissions                 0,0
DEBUG [node1_ScheduledTasks:1] node1 2023-09-13 02:09:16,769 MigrationCoordinator.java:267 - Pulling unreceived schema versions...
{noformat}


We can only raise the timeout to 140s, see PR, if we agree the diagnostic is correct. Wdyt?

Do we keep on seeing these failures? Is it every time the same reason? 
What is the timeout we agreed on in the Python DTests? 

bq. Do we keep on seeing these failures?

In the comment [above|https://issues.apache.org/jira/browse/CASSANDRA-18707?focusedCommentId=17765001&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17765001] you can see that yes, we keep seeing them

bq. Is it every time the same reason? 
 
The ones I've seen yes.

bq. What is the timeout we agreed on in the Python DTests? 

In dtests you can pass in a timeout. But some tests like [this one|https://github.com/apache/cassandra-dtest/blob/trunk/secondary_indexes_test.py#L294] do already have 120s.



{quote}In the comment above you can see that yes, we keep seeing them
{quote}
Well, I meant more than 1 random failure. Anyway :)  I don't see it in Butler on any branch and it seems no one reproduced it so far.

So your idea is actually to make it double the wait time for all tests on 4.0? I see comments about 5.0 and 4.1, too. I guess you waited for feedback before propagating to all other branches, and this should be 4.0+ change. (covering all branches)

Also, what do we do with the other failure [~adelapena] mentioned? 

Considering this silence of 47s happens rarely and randomly only in Jenkins, I suspect it is just the environment, and raising it to 140 should be okay. 

[~drohrer] , I noticed you were the one who tested and extended the wait time from 60 to 70 due to a race with a hard-coded 60-second timeout in MigrationManager {{MIGRATION_DELAY_IN_MS. Any reason why 140 wouldn't be a good idea here? }}

{quote}Doug Rohrer , I noticed you were the one who tested and extended the wait time from 60 to 70 due to a race with a hard-coded 60-second timeout in MigrationManager MIGRATION_DELAY_IN_MS. Any reason why 140 wouldn't be a good idea here? {quote}

I'd suggest that I should have made it a configurable value back then, and we should probably make it configurable now. Bumping the default to 140 seems reasonable, but I think we should make it easier for folks with their own CI environment to potentially bump it if they need to without having to change the code again? Thoughts on adding a new property and defaulting _that_ to 140?

I do hate to just bump the timeout without understanding why the cluster just hung for 47 seconds, but if we've tried to reproduce it a bunch of times and it's just not happening it seems like the only reasonable option for now. By making it configurable, we can try locally to reproduce and diagnose the root cause (if there is one)

{quote}I'd suggest that I should have made it a configurable value back then, and we should probably make it configurable now. Bumping the default to 140 seems reasonable, but I think we should make it easier for folks with their own CI environment to potentially bump it if they need to without having to change the code again? Thoughts on adding a new property and defaulting _that_ to 140?
{quote}
+1 on making it configurable and further experimenting.
{quote}I do hate to just bump the timeout without understanding why the cluster just hung for 47 seconds, but if we've tried to reproduce it a bunch of times and it's just not happening it seems like the only reasonable option for now. By making it configurable, we can try locally to reproduce and diagnose the root cause (if there is one)
{quote}
[~drohrer], I should admit that you have a point. I was against raising it for the Python tests a while ago, but the community decided the opposite, considering our unstable and slow environment. However, making one more attempt to reproduce the problem by lowering the timeouts sounds reasonable. Especially when there is a concern the cluster hung for 47 seconds... Technically, we do such experiments sometimes, even without having an explicit config parameter. [~Bereng], did you happen to try that already?

 

bq.  I guess you waited for feedback before propagating to all other branches

Yep let's agree first and then it goes into all 4.0+ branches.

bq. Also, what do we do with the other failure Andres de la Peña mentioned? 

I opened CASSANDRA-18851 to handle that one

bq. I'd suggest that I should have made it a configurable value back then

Seems like an easy change to fold into this ticket. Added to the PR.

bq. Berenguer Blasi, did you happen to try that already?

Nope. And I am not following I am afraid. 60s pauses on jenkins are common going from my experience fixing previous flakies. I have seen it before. The logs are just empty for that period, there's absolutely nothing to go about. So as Brandon said it is just a timeout and I agree it's bc of the env imo. If we reduce the timeout we'll just make it fail more times, stare at an empty log for that timeout period and then there's nothing we can do about it right? I'm probably missing sthg but if it failed again what do you do without logs?

{quote}If we reduce the timeout we'll just make it fail more times, stare at an empty log for that timeout period and then there's nothing we can do about it right? I'm probably missing sthg but if it failed again what do you do without logs?
{quote}
I haven't played with this particular test, but usually, we add additional logging and/or enable tracing during debugging Python DTests. Staring at an empty log is not the way, I agree.

That is clear. But it doesn't repro bc imo you're trying to repro a jenkins env freeze just during schema agreement. No amount of added logging will make a difference bc we have no insight into that. So I am with Brandon I don't know what else to do here. I'd merge and if it happens again maybe sbdy else sees sthg... :shrug:

Sorry, got pulled into something else. I will be looking into this soon

No hurries and thanks for your time

A few observations on my end:
 * The test exists only 4.1+. Thus, even if we decide to raise the timeout to deal with the flaky test, I don't see a reason to do it on 4.0.
 * I can fail the test locally only with timeout 0s, not even with 1ms... I have no clue what Jenkins is doing...
 * As the test was looped only 100 times as per previous comment, I tried 500 in Circle CI in both 4.1 and 5.0 branches, and I can reproduce only the other failure from CASSANDRA-18851
 * I don't see any new failures in Butler. I am tempted to use this ticket to make configurable the timeout as suggested but not raise it until we see the failure again. WDYT? 

So iiuc we're basically saying the same:

bq. I don't see a reason to do it on 4.0.

Let's do it in 4.0 +1000 imo. We don't want to be repeating this analysis in the future. As the failure is related to jenkins it may affect any branch and test anytime.

bq. I have no clue what Jenkins is doing

Agreed

bq. I can reproduce only the other failure from CASSANDRA-18851

Agreed

bq. make configurable the timeout as suggested

Agreed and was pushed to the PR

bq. but not raise it until we see the failure again. WDYT? 

I strongly disagree here. We don't want to do this analysis again or worse, have sbdy else do it again from scratch, specially when it can hit any random test. It would be wasting the effort put here imo.

Reasons why I am not convinced we should raise it everywhere:
- 4.0 is a stable branch where we do not see many changes or this type of failures (so let's say it would be probably just with/without - it is ok)
- we see this timeout with a specific test, and not with many, which makes it suspicious.
- if anyone gets back to the issue, they will find the previous conversation on this ticket and not waste time from scratch.

In Python DTests we opted to raise the timeout per tests or in create_cf.  However it seems the majority opted in for 120, so let's just accept your suggestion here and move on. I guess that the suspicious test will pop in again if there is something bigger. 
+1, assuming the patch is the same on all branches and there is green CI run per branch



bq. 4.0 is a stable branch where we do not see many changes

Correct. The problem is that we have no insight as to what is happening in jenkins at the time. It might hit it as well

bq. we see this timeout with a specific test, and not with many, which makes it suspicious.

I disagree here. I have seen schema changes many times go over 60s in jenkins. That was a common scenario when we were on the 4.0 flaky fixing frenzy and that's why at the time I did already propose to raise timeouts already. That was declined and I found myself debugging through logs the same problem 2 or 3 extra times. This is just another one.

bq. not waste time from scratch

Correct. But you you just have an indication of what have happened. You still have to wade through logs to confirm it is indeed the same scenario.

bq. In Python DTests we opted to raise the timeout

Correct and I think it was a good move. I will change the 140s to 120s for consistency and OCD lol

Thanks I'll put PRs up


Pushed all PRs + CI links. LGTM if I am not missing anything.

Thanks. The posted runs LGTM. (I didn't check the patch as I relied it was the same one everywhere?) I also suggest running the Java upgrade tests as they exercise those paths too. 

Also, we need runs with both JDK versions, as usual. 

I couldn't find but maybe I just missed it - is there a repeated run with this patch for the test we were looking at? To verify if something has changed.

Yes the patch it's the same on all branches and yes I should have ran upgrade tests I missed than one! I was trying to spare CI cycles  so I didn't run all jdks but I added them now for completion. It's all done now, same links.

The repeated runs I did didn't repro the issue. So repeating with a higher timeout will only waste CI imo? wdyt?

{quote}I was trying to spare CI cycles so I didn't run all jdks, but I added them now for completion.
{quote}
I think I may be looking in the wrong place, so I am posting the links I used to verify CI here: 
[4.0 |https://app.circleci.com/pipelines/github/bereng/cassandra?branch=CASSANDRA-18707-4.0] hit CASSANDRA-18158, but I do not see J11 run. 
[4.1 |https://app.circleci.com/pipelines/github/bereng/cassandra/1089/workflows/6cbb8ed4-221a-4ae4-88b6-bd08579c144f/jobs/31124] - I do not see J11 runs
[5.0 |https://app.circleci.com/pipelines/github/bereng/cassandra/1090/workflows/8bcb9c45-7a72-4ae2-a62b-970e9f66a1d3/jobs/31125] - I do not see J17 runs
[trunk |https://app.circleci.com/pipelines/github/bereng/cassandra?branch=CASSANDRA-18707-trunk] - I do not see J17 runs

I also just realized (absolutely my fault, so leave it for a follow-up ticket, please, apologize) that the timeout is not configurable with -D or so - being in CASSANDRA_RELEVANT_PROPERTIES, for example, as [~drohrer] suggested. Though it can be changed per test in the codebase, you can still not change it per the CI system as a configuration parameter. 
{quote}The repeated runs I did didn't repro the issue.
{quote}
Yes, but the raised timeout can have a side effect. I spun the test locally on 5.0 100 times with your patch applied, and there were no issues, so I suggest we skip that in CI.

+1 on green CI (the missing JDK runs)

You're looking in the right places. The links are on the PRs. What I did yesterday is add runs for the mixed jdks case only, again to prevent extra CI which is expensive. I added what you requested now and all is green. I don't see what side effects a timeout increase can have, in this case, in regards to the underlying jdk version but there it goes for completion.

Ah yes we spoke on Slack about that. I did make it configurable per test but not as a global property...

Everything seems green. I'll merge tomorrow to give some time.

