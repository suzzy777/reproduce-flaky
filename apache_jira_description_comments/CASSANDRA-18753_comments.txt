This is a great idea. I was thinking about configuring cassandra.yaml with all new stuff enabled and leave legacy behind. The amount of new features which are going to be introduced in 5.0 is quite big and I would love to run it all with latest and greatest.

I guess there are two possible reasons for having the very conservative defaults that we have:
 # Compatibility: we have things disabled, old sstable formats, etc. because they can mess with rolling upgrades. We could have an alterantive yaml for new deployments that don't care for backwards compatibility. It could still keep some new optional features disabled.
 # Stability: all new features are disabled by default, no matter how well-tested they are. We could have a yaml for the brave, enabling new features, provided that we consider them mature enough. This yaml could also keep backward compatibilty.

I guess this draws three possible new yamls:
 # New installs, no new features
 # New installs, all new features enabled
 # Upgrade, new features enabled if compatible

Can you give an example of a feature that 1 and 3 should treat differently?

Sure, {{storage_compatibility_mode}} in 1 would be {{NONE}}, whereas in 3 it would be either {{UPGRADING}} or {{NONE}}, not sure.

The point on 1 would be disabling new optional features, like dynamic data masking or SAI. More fundamental things that are disabled only for compatibility, like modern sstable formats, could be enabled.

3 instead would remain tied to compatibility, but it could enable new optional features that don't mess with existing use cases, like dynamic data masking.

2 could be more aggressive and even enable guardrails with some recommended values.

Of these configs, 1 is probably the less useful so I'd discard it. 2 seems the more valuable to me. 3 is in between, I guess.


Somebody mentioned a startup param and I like that idea, along the lines of {{-XX:+UnlockCommercialFeature}}:

- Multiple yaml files
- cassandra starts in compatibility mode by default
- cassandra -UnlockNewFeatures starts C* with new features enabled
- If we had 2 startup scripts, one per flavor, we could provide a symlink to whatever script is your choice.

I wonder how we would give continuity to a new config file for new deployments.

Let's say that a user deploying Cassandra 5 for the first time decides to use that file for new deployments. At some point in the future that user will want to upgrade to Cassandra 6. That future version of Cassandra will have (at least) two versions yaml file. One will be the classic one offering backward compatibility since the beginning of the times. The other will be the yaml for new deployments, including whatever is new in Cassandra 6.

What yaml should the user choose? The backwards-compatible yaml won't include the new Cassandra 5 stuff that they originally enabled. The yaml for new deployments will include stuff for new Cassandra 6 deployments. It seems that every time we add a yaml for new deployments we start a new timeline that will need backwards compatibility.

I would not mix mulitple yamls with startup param...

I'm not sure we should worry about people upgrading.  You can't just drop in a new config file anyway, you need to migrate your existing one, and I feel like at that point experienced users will dig through everything anyway, making the files we ship rough guides.

> I would not mix mulitple yamls with startup param...
This.

What about an install script? Something that walks people through 2 meta phases:
1: Broad strokes. What major profile do you want on the node? (conservative, full, compatible)
2: Granular. Walk through some few highlights of what's in cassandra.yaml w/explanations of them (could just pull out comments) and prompt the user for values
3: GC / profile: what kind of workload do you expect on the cluster? (read heavy, write heavy, mixed). Tune GC / params accordingly

I'm thinking something along the lines of what most other linux applications do where they prompt you for what you want values to be and provide defaults, as well as providing for a headless install option.

How would that work in a cluster of 100 nodes?

My gut feeling is that would get complicated pretty fast. What if there are some combinations of features which are not compatible with each other? I mean ... Cassandra would work but their combination would not be optimal. So we may unintentionally generate a configuration file which would perform even worse then the default one.

I would not focus on the performance at all. I would leave options / settings related to that out. I would just focus on "do you want this feature we turned off by default because of compatibility to be turned on now?" 

to answer to [~adelapena] to his last comment, this is a good insight. I think that users would need to be aware of the fact that this would be just for "evaluation purposes" and if they are going to upgrade to the next major it (probably) wont work ... 

Another question I have is - how can we ensure that these two configuration files are synchronized? What I mean by that is that if we introduce a new configuration parameter to the "official yaml", everytime we do that we also need to update the "optimal" one? I can imagine this will be out of sync in one month :) 

bq. Another question I have is - how can we ensure that these two configuration files are synchronized?

We could generate one from the other since we're just flipping some parameters.

What would be interesting to try is to generate cassandra.yaml programmatically. If we move all comments from cassandra.yaml for each option to its Java counterpart, e.g. like some annotation, then we could go over Config, parse the fields and their annotations, even its order (driven by some annotation again) and we might dump the config into yaml.

So the generation of that yaml when we do that interactively would be rather easy. We would just dump it "differently". 

Just an idea thought. 

A Junit loading all the yamls and failing when params are missing, mismatched, etc should be easy to do.

This is probably a separate topic, but maybe we could consider creating a schema for cassandra.yaml so that autocompletion and validation could be done in the editor?


bq. How would that work in a cluster of 100 nodes?
Same as today. Get params in one place the way you want them, then on your own distributing them.

bq. What if there are some combinations of features which are not compatible with each other? I mean ... Cassandra would work but their combination would not be optimal. So we may unintentionally generate a configuration file which would perform even worse then the default one.
Well, this is no different than the status quo if our recommended method of configuring your C* node is to open a cassandra.yaml file and manually change a bunch of stuff right?

Jumping in… and trying to simplify so we get this in 5.0-beta…

I'm for reducing this down to two options:
 - {{cassandra.yaml}}
 - {{cassandra_latest.yaml}}

This removes option (3) from Andres' suggestion.  I think that should be better tackled with documentation on upgrading, and general documentation on "whats new".  (side suggestion: over time we can also add startup info messages informing of when legacy non-default options are still be used (post complete cluster upgrade).)

I'm for the unit test approach to ensure these two files don't drift in bad ways, instead of embarking on the auto-generate yaml from config mission (which is out of scope IMHO).  I also suggest this can be done separately in 5.0.x instead of 5.0-beta.

The goal and value delivered by this ticket is primarily addressing first impressions of Cassandra.  We want new users to experience Cassandra at its best.  This is about performance and capacity over compatibility and operations. 

As this is becoming an urgent issue and we don't have time to prepare a complex solution in time for the release, I am starting to prepare a patch that introduces an alternative configuration in {{{}cassandra_latest.yaml{}}}. I am also going to make a corresponding junit configuration (replacing "test-tries"), include it in pre-commit runs, and look into doing the same for dtests.

If the costs of running this version of tests is deemed too high, we could change the J17 tests to run "test-latest" leaving J11 ones at the legacy config.

Draft [pull request|https://github.com/apache/cassandra/pull/2896] linked. I'm interested if there are other settings that should be switched, e.g. if we want to turn on any guardrails.

Tests have quite a few failures which I'll address in the next few days.

[~blambov]  We plan to do this soon (1), (2), so the config will probably change a little bit. Maybe you could wait for that so we do not need to change the stuff you do right now. I think this will be the last configuration change in cassandra.yaml before 5.0.0.

(1) https://lists.apache.org/thread/nhp6vftc4kc3dxskngxy5rpo1lp19drw
(2) https://issues.apache.org/jira/browse/CASSANDRA-19021

> disk_access_mode: mmap_index_only

I was thinking of using this setting in the _latest config. Having it changed in both is even better.

In addition to this, I intend to set commit log to direct IO (CASSANDRA-18464) in the _latest config.

Could {{disk_access_mode}} be moved under a dedicated sstable format configuration so that we could have different defaults for legacy big table format and optimized settings for trie as there is no legacy settings for it?

... and even split for index/data, read/write?

I think it is too late to do that for 5.0.0. Maybe later?

CASSANDRA-19021 is done.

Created tickets for the types of test failures that mark actual differences with the new configuration. Started another test run and looking at what needs to be done to enable dtests-latest.

The best way I could think of to add dtest support is to introduce an option to CCM to specify the name of the configuration YAML to use, so that we can pass "cassandra_latest.yaml" there, something along the lines of the two patches that I have attached.

I don't know the procedure to get a change to python dtests which requires a corresponding CCM patch. Where can I read up on this / who can help with it?

The [latest test round|https://app.circleci.com/pipelines/github/blambov/cassandra/559/workflows/00ce0a94-f86f-4b6b-9d91-95602b14c850] appears as good as can be expected (the 11 failures in {{utests_latest}} are covered by the linked tickets).

Should we go ahead with committing this or wait for dtest support?

Patch the dtest repo and pass its url and branch to circleci parameters.

For ccm patches, you change the ccm coordinates in the dtest requirements.txt [here|https://github.com/apache/cassandra-dtest/blob/trunk/requirements.txt#L12].

bq. Should we go ahead with committing this or wait for dtest support?

Split out the tickets, wfm.

[~blambov] I haven't used it but did you check env var {{CASSANDRA_CONF}} for py dtests? I see it in use [here|https://github.com/apache/cassandra-dtest/blob/5f725067532589161ae8c5ca60c05782480a452c/repair_tests/deprecated_repair_test.py#L222] so it might work?

DTest support has been added.

The python dtests require pull requests for [CCM|https://github.com/riptano/ccm/pull/760] and [cassandra-dtest|https://github.com/apache/cassandra-dtest/pull/243] to be merged. It works by passing an argument to ccm to make it read the configuration from "cassandra_latest.yaml". The new configuration replaces {{{}dtest_offheap{}}}, as the offheap setting for memtables is also turned on in the latest configuration.

I'm not happy at all with how the in-jvm dtests are configured at this point (directly including the settings in code), but I could not think of a quick way to get them to load a configuration file. The latest config is combined with vnodes to lighten the testing load.

Test results to appear [here|https://app.circleci.com/pipelines/github/blambov/cassandra/567/workflows/aa84b1f1-b138-42a8-8e81-dd149c87224e].

bq. The new configuration replaces dtest_offheap

Perfect, I came here to mention that, and there it was :D

Gone for a first pass of the review. The approach seems ok to me. There seems to be dtests failures probably needing similar changes as the junits.

I was hoping to avoid CCM changes (which iirc imply docker images changes) and have proven difficult. But CASSANDRA_CONF isn't read as an env var as I saw and tested so we have no alternative there.

The patches are ready for final review.

I believe I fixed all the test issues I found and could address, and have opened tickets for everything that appears to be a genuine problem or a test that requires specific knowledge to fix (see the linked issues).

Maybe we can refactor testing a bit (I don't insist on this ticket, but for 5.0 at least). My point is that if we decide to modify something in say latest configuration, the change will have be carefully applied to:
- the main configuration file
- the lastest diff file for testing
- the dtest configuration

I think this is error prone and it will be easy to oversee something. One more place which was overlooked in this PR I think is {{UnitConfigOverride}} which is used for the new fuzz tests. 

To add to that, testing stuff in IDE with a particular configuration was a pain so far, it is not going to get better with this change.

I'd like to propose something:
- have a class which provides config overrides as a simple map for different configurations
- have an Ant task which takes either main cassandra.yaml or latest cassandra.yaml and apply overrides on it, and then generates a final configuration yaml
- don't use test/conf/cassandra.yaml - we should base on the main cassandra/latest.yaml and only change what we need
- the same overrides can be used for all kinds of unit and distributed tests and the configuration can be easily generated any time for any tool
- when we want to run a test with a certain configuration, we can simply add one statement in static initializer which will generate the configuration, save it somewhere and set the config property URL

I do think we should apply a similar approach for system properties

CASSANDRA-19126 shows some problems related to this

cc [~dcapwell]

Could you please open a separate ticket for these refactorings? It is important that we land the additional testing this adds to ensure problems are fixed in the 5.0 release.

Of course, we can do that later

Merged CCM and DTest patches (they do not change anything unless the {{--configuration-yaml}} flag is used).

[The state of failing tests at the moment|https://app.circleci.com/pipelines/github/blambov/cassandra/595/workflows/ed598605-6af6-443e-9336-aaa47ae27e43]:
 - JUnit tests in compatible mode (which changes to use {{{}heap_buffers{}}}):
 -- {{CQLVectorTest}} (CASSANDRA-19167)
 -- {{VectorUpdateDeleteTest}} (CASSANDRA-19168)
 - JUnit tests in latest mode:
 -- repair fuzz tests {{{}ConcurrentIrWithPreviewFuzzTest{}}}, {{{}FailedAckTest{}}}, {{{}FailingRepairFuzzTest{}}}, {{{}HappyPathFuzzTest{}}}, {{SlowMessageFuzzTest}} (CASSANDRA-19042)
 -- {{RepairJobTest}} (CASSANDRA-19043)
 - JVM dtests in latest mode:
 -- {{RepairTest}} (CASSANDRA-19085)
 -- {{SSTableLoaderEncyptionOptionsTest}} (CASSANDRA-19126)
 -- {{QueriesTableTest}} (CASSANDRA-19046)
 - Python dtests in latest mode:
 -- {{TestWriteFailures.testPaxos}} (CASSANDRA-19145)
 -- {{TestReplaceAddress}} (CASSANDRA-19144)
 -- {{TestSnapshot}} (CASSANDRA-19126)
 -- {{TestClientRequestMetrics}} (CASSANDRA-19046)

Several {{TestBootstrap}} tests seems to be failing in all configurations, some already marked as flaky; this likely is not caused by this patch. There are also some timeouts (e.g. {{ActiveCompactionsTest}} times out when run repeatedly due to longer {{{}testActiveCompactionTrackingRaceWithIndexBuilder{}}}).

Please review [the PR|https://github.com/apache/cassandra/pull/2896].

