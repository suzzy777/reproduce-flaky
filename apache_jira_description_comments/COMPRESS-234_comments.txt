I have added a rather large patch that removed TarBuffer.java all together.  It was unnecessary and added overhead and complexity.  I substituted it out with just a regular BufferedInputStream/BufferedOutputStream.  These changes pass all current unit tests.

There is one unit test to add, though I'm not sure how to do it.  My implementation passed all unit tests but failed, in practice, on large TAR files.  It's because my "skip" functions were not skipping as I had thought.  Skipping a long distance is what brought out the problem, I would call "skip" once, and be done with it.  With the small test files, it was able to skip it in one jump.  For larger files, it required a while(skip() > 0) type of fix.  I don't think you want to be shipping around a 1 gig file with your tests though.

Anyway, this implementation is much easier to read, less code, and runs a heck of a lot faster in some regards.

To read the names of all the entries:
1.5: 640ms
Mine: 20ms

The time in spend reading the entries and copying them out of the TAR was unchanged.

This will be very beneficial for Commons VFS.  For example, in TarFileSystem#init(), it iterates through the TAR file and copies out the name of each entry.  These patches will help that immensely.

For the record though, I have an SSD on my development machine, I don't know the benefit of a regular drive, but I imagine it will still be faster.

The full patch is "Archiver_Tar.patch".  I have left the two other patches there for my own record.

Thanks.

Not having looked at your code, yet, I want to point out that we need to be extra careful about not breaking any public APIs.  Fortunately TarBuffer is package private (unlike its grandfather in Ant's codebase).

As for big file tests: actually we do.  Large tars can become pretty small if you compress them.  Take a look at org.apache.commons.compress.archivers.tar.BigFilesIT and the run-tarit Maven profile.

Unfortunately it looks like this breaks the API - a protected field of a public class has been dropped. However the field is TarBuffer so maybe that could not have been used by 3rd party code. 

Also please don't introduce protected fields - they make code changes and thorough testing much harder. All fields should be private - or possibly package - and final where possible. Only exceptions are final constants.

Also, there are some spurious changes; some of the block start - { - markers have been moved to the next line, and there are some whitsepace changes. Please only change the minimum needed, and additional changes make it much harder to review.

[Code layout changes should be done as a separate commit]


I will make the changes suggested.  Sorry about the formatting.  Is there an Apache Eclipse formatter template available?

I don't know if there is a template available, but for patches any automatic formatting is likely to cause problems, because the existing code may not be exactly correctly styled. And some projects have a mix of styles in different files.

Please just follow the surrounding style.

Basic rules for new code that AFAIK all Commons components use:
- no tabs; use spaces for alignment
- avoid trailing spaces
- indentation is 4 spaces (sometimes 2 in XML files)

First of all, many thanks for the work you are putting into Compress.

I'm a bit reluctant about explictly adding a Buffered*Stream ourselves:

* we tell people to use Buffered*Stream as Compress wouldn't do so: http://commons.apache.org/proper/commons-compress/examples.html#Common_Notes - this means we end up layering two buffers on top of each other
* the decision to not use Buffered*Streams has been deliberate to grant more control to the user.  If I have a stream containing two subsequent TAR archives for whatever reason, will the stream be in a state that I can read the second archive cleanly after the first one is done or will (the now discarded) BufferedInputStream have read more bytes than it needed and leave the inner stream pointing ahead of the start of the second archive?

That being said, I'm sure we could remove TarBuffer the way you've done.  In the output case I don't think we actually need the explicit BufferedOutputStream at all and I haven't looked close enough at the input case to see whether it actually requires it code-wise.

I'd be interested in your benchmark code, have you tried the original code when you wrap the original stream in a Buffered*Stream from outside of Compress' code base?

Finally Re: skip - I think FindBugs detects cases where you don't use the return value of skip and it found a few places in the skip package where we implemented something similar to the skipFully method you've added.  It may be worth to look through the other packages to see whether they could benefit from the new method - not in the scope of this ticket, of course.


After reading through your patch again, I think we can use your amended skip code and remove TarBuffer without using Buffered*Streams internally.

You use the new skipFully which throws an Exception when it cannot skip the requested number of bytes, which changes the behavior of the existing skip method.  Why don't you just return the result of IOUtils#skip directly?

If we do decide a BufferedStream is needed, we could check first if one is provided, and only wrap a buffer around it if required. There's code in Commons IO (2.5; IOUtils asBufferedxxx methods) that could be borrowed:

{code}
public static BufferedInputStream asBufferedInputStream(final InputStream inputStream) {
    // reject null early on rather than waiting for IO operation to fail
    if (inputStream == null) { // not checked by BufferedInputStream
        throw new NullPointerException();
    }
    return inputStream instanceof BufferedInputStream ? (BufferedInputStream) inputStream : new BufferedInputStream(inputStream);
}
{code}

[~bodewig]: Well, TarBuffer is/was the buffering stream, so by not replacing it with another buffering stream, we are going to hurt the performance of anyone who is currently using the library without a buffered stream.  So, I like Sebb's approach and wrapping it if need-be.  So, by going with Sebb's approach, we need to decide if we want to set an explicit buffer size or just go with the default size.  As long as the buffer is larger than 512 bytes (the size of a single header), I don't see a need for us to set it.

New Patch: Archiver_Tar.2.patch

I have made many of the changes you have asked for.  There is one note and two issues:

Note:  I had to update the unit tests, because the new asBufferedInputStream throws the exception if the InputStream is NULL.  Three of the tests were creating instances of TarArchiveInputStream without an underlying InputStream because they were testing stand-alone protected methods.  I created a dummy InputStream for those to pass.

Issue 1: The old TarBuffer used to check that the streams were not System.in/System.out before attempting to close them.  I have removed that with these patches.  I saw another ticket out there about using the NoCloseInputStream method.  Perhaps that can be adopted here.

Issue 2: One of the unit tests is failing.  I'm not sure what to do about it.  It tests that if an arbitrary trailer is appended to the TAR file, that it will no be consumed by TarArchiveInput Stream.  That is not really possible with the current setup, because the BufferedInputStream may read-ahead and consume the trailer in order to buffer.

There are still a few tabs and spurious formatting changes, but it's better. Thanks.

Not sure I understand why the code does

  skip(Long.MAX_VALUE);

at one point - looks very odd.
If it's correct it needs to be documented.

The failing unit test is exactly why I warned against using BufferedInputStream internally. :-)

Personally I don't have any problem with performance only being good enough if you pass in a BufferedInputStream explicitly if there are valid use-cases we cannot enable otherwise.

OK,  I have submitted what I believe should be the final version.  It is Archiver_Tar.3.patch.

Notes: It still fails that same unit test.  I was wrong with my previous assumption as to the cause.  I think the unit test here is at fault.  The test runs an archive with random data appended onto the end of it.  I believe that the archive file has a lot of zero padding at the end.  The TarBuffer may have read that off as it was configured to read blocks of 10 * 512 chunks at one time.  I believe that a new file is required for this test.  Can you please verify and regenerate?

I have removed internal buffering and pesky tabs.

To answer your question about the skip Long.MAX_VALUE.  The skip function only lets you skip the length of the current archive.  It didn't seem necessary to me that I needed to calculate how far to jump, call jump, then be checked again.  Passing in Long.MAX_VALUE just lets the skip function handle the details in one place.

First of all I'm sorry about the "fetch me a rock" experience you've been having - and thank you for being persistent.

There still are a few formatting glitches that we can sort out easily (and you don't need to resubmit a patch).  I'd probably also make some of the new methods private that you've moved over from TarBuffer.

Personally I'm not concerned with the now removed buffer fields as they haven't been usable before.  I'll look into the failing test separately, but this has to do with COMPRES-206, I think.  To cite the GNU tar info page https://www.gnu.org/software/tar/manual/html_node/Standard.html

{quote}
When writing an archive, the last record of blocks should be written at the full size, with blocks after the zero block containing all zeros. When reading an archive, a reasonable system should properly handle an archive whose last record is shorter than the rest, or which contains garbage records after a zero block. 
{quote}

What we currently have in trunk is code that tries to read a full record and ignores the bytes after the second EOF block.  With your changes we wouldn't consume "garbage after a zero block".  The goals of "allowing streams to be concatenated" and "swallow garbage" are certainly in conflict with each other and we've settled to "swallow garbage up to a full record's size" right now.  Oh my, I feel another flag looming.


Stefan,

No problem on how things have been going.  I'm not sure if you're paid staff, but regardless, I know you have a lot on your plate.


Please let me know if you need any additional input on this ticket.  Now here comes the act of stating the obvious: I would love to see this in and released ASAP.  Sadly, I had to abandoned both Commons VFS and Commons Compress at work today.  I came across too many show stopping bugs with VFS and I was getting a NULL pointer exception with TarArchiveInputStream in COMPRESS 1.5.  I had to back-fill with Java's GZIP, ZIP, and different 3rd-party TAR library.

If you're going to follow-up with changes... consider dropping isAtEOF/setAtEOF.  They're not used anywhere in the class and some place reference the variable directly.

Follow-Up: I have included that change (and a few others) in .final.patch.


Thanks.

Sorry, my net-time is currently a bit flaky.

I've run the BigFilesIT to get an idea what amount of improvement we are talking about.  Initially the test didn't use any external buffering, so I added it and ran a few tests.  On my machine I get the following runtimes

{code}
current code, no buffering
readFileHeadersOfArchiveBiggerThan8GByte: 26.4s
readFileBiggerThan8GBytePosix: 24.0s

current code, external buffering
readFileHeadersOfArchiveBiggerThan8GByte: 23.6s
readFileBiggerThan8GBytePosix: 23.9s

"latest" patch, no buffering
readFileHeadersOfArchiveBiggerThan8GByte: 22.7s
readFileBiggerThan8GBytePosix: 21.8s

"latest" patch, external buffering
readFileHeadersOfArchiveBiggerThan8GByte: 22.6s
readFileBiggerThan8GBytePosix: 21.4s
{code}

I was surprised to see that even the tests that actually read code win by the patch - the readFileHeaders test hits skip internally a lot.  In either case I ~10% gain for cases where there is no external buffer and still ~5% with an external BufferedInputStream.  With a bit of fiddling to make the "end of archive" case what it used to be, I really think this is worth dropping TarBuffer and intend to make it work.

As for NullPointerExceptions - I'm sure they'd be worth a bug report nevertheless :-)

Stefan,

Like I said, the test for reading just the file headers is amazingly different on my SSD HD.  Since this patch changes the class so much, submitting a separate ticket for the NPE seems futile.

Please keep me posted when this makes it in!

With the version committed in svn revision 1511843 I get roughly the same results as with the "latest" from above.  I've added padding of the last buffer when writing as well as skipping the padding when hitting EOF.  This allows the original tests to pass.  What I overlooked is that you removed the calls to count(), added back as well.



