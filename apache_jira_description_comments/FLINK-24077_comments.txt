https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23560&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13205

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24965&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13307

Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25896&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=12573

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26033&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13012]

cc [~arvid], [~twalthr]

Haven't managed to have a failure locally, therefore I only have some guesses that might make the test deterministic:

Configure

[https://nightlies.apache.org/flink/flink-docs-release-1.12/dev/table/connectors/hbase.html#sink-buffer-flush-max-rows]

and/or

[https://nightlies.apache.org/flink/flink-docs-release-1.12/dev/table/connectors/hbase.html#sink-buffer-flush-interval]

 

Or maybe set [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/config/#table-dml-sync] to true

 

But let's see if the changes in [https://github.com/apache/flink/pull/17888] fix the issue already.

Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27093&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=12714

[~jingge] could you give a quick update on the progress of this ticket?

The flaky issue happened occasionally based on some unknown race conditions during the CI process. Since the DML(i.e. Select operation in this case) job is executed asynchronously, the tableResult collecting could be started after the first row has been arrived. It looks like that we collected only partial result(3, expected 8) sometimes while asserting. 

 

In order to make the test stable. I have done the following improvements:
 # After the Insert operation, run

{code:java}
batchEnv.sqlQuery("SELECT COUNT(h.rowkey) FROM " + TEST_TABLE_2 + " AS h"); {code}
to check if the Insert operation works before testing the SELECT operation.

2. After executing the SELECT query, call
{code:java}
// wait to finish
tableResult2.getJobClient().get().getJobExecutionResult().get(); {code}
to wait to finish.

 

I have changed the test locally to be a parametrized test and repeat 20 times for each test run. Until now, it has been checked on my local laptop more than 500 times. Zero issue has been found.

 

 

After opening the INFO log, found that a Flink MiniCluster will be started/stopped automatically in the background while using TableEnvironment for each query in the ITCase. Since the shutdown of the MiniCluster will be called asynchronously, CollectResultFetcher will got data lost sometimes based on race conditions and the unchecked RuntimeException java.lang.IllegalStateException will be thrown that we were not aware of.

The solution is to control the lifecycle of the MiniCluster manually. The MiniClusterWithClientResource could be a good fit in this case.

 

[~jingge] I agree. We should definitely add the cluster resource:

{code}
    @ClassRule
    public static MiniClusterWithClientResource miniClusterResource =
            new MiniClusterWithClientResource(
                    new MiniClusterResourceConfiguration.Builder()
                            .setNumberTaskManagers(1)
                            .setNumberSlotsPerTaskManager(1)
                            .build());
{code}

Actually all ITCases should do that. Maybe this could be another potential architectural test [~airblader]?

Writing an architectural rule that verifies that all IT cases (within o.a.f.table.*?) have a public, static member of type MiniClusterWithClientResource annotated with ClassRule would certainly be possible.

If [~jingge] would like to add such a rule I'm happy to review it; I can also take over doing that, of course.

Note that the cluster could also be non-static. We could of course, also change that through the rule.

[~twalthr] [~arvid] I agree with you. Beyond solving the race condition issue, forcing all ITCase doing it will also help saving the CI time significantly. I was aware that it will take about 4s for each start and stop of a MiniCluster. Imaging how long it will take for all test queries to wait for new MiniClusters up and down. Because of this,  @ClassRule is generally recommended unless there is a technical reason for using @Rule which will use a non-static MiniCluster. The architectural rule should cover both cases.

[~airblader] I'd like to add the architectural rule and then ask for your review, many thanks.

I've noted the [best practices|https://gejing.gitbook.io/101-building-modern-distributed-platform/101-getting-started-with-flink-development/debug-flink-itcase] found while working on this PR, hopefully it will help developers while working on ITCase in the future.

Merged 7976be0f8675a8753a5bb7e7a44dda6b4a347247..fca04c3aaf6346d61cf9fe022a7ac77ab4d66c91 into master. [~jingge], could you please also create backports to 1.14, 1.13, and if it's an easy cherry-pick also to 1.12?

Merged 4e962d9a7980dc143a2e4ace1211887314648d7d..cc78095923508299437eab0b39c78576b6a69c07 into 1.14, 2efb0df1fb2d2a7bf081ae205c78698f3244c625..9d9842fb610a95d9f31ad432e2546d8ce61b106e into 1.13, 8e3b291fb5b04b8e774d603a517f2dfadc6095ed..b6ae90384b47a6274e2f746c76aa31387c2d5ee8 into 1.12.

