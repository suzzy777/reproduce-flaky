Looks like the coordinator seems to be slow somehow between 05:16:52 and 05:18:56. Both failing queries had to do with time out waiting for the coordinator fragment not becoming ready within 2 minutes.

Log from coordinators:
{noformat}
I1228 05:16:55.374305 26714 query-state.cc:568] Executing instance. instance_id=8f46b2518734bef1:6ef2d40400000000 fragment_idx=0 per_fragment_instance_idx=0 coord_state_idx=0 #in-flight=9
I1228 05:16:55.374305 26710 coordinator.cc:368] started execution on 2 backends for query_id=8f46b2518734bef1:6ef2d40400000000
{noformat}

{noformat}
I1228 05:16:52.217275 26685 query-state.cc:568] Executing instance. instance_id=194f5b70907ac97c:84a116d600000000 fragment_idx=0 per_fragment_instance_idx=0 coord_state_idx=0 #in-flight=7
I1228 05:16:52.217211 26678 coordinator.cc:368] started execution on 3 backends for query_id=194f5b70907ac97c:84a116d600000000
{noformat}

The top output snapshots from that time period didn't suggest much other than Impalad wasn't scheduled much in them. Looking into query profiles next.


The profile didn't suggest much either. The last report time for the coordinator fragment was 05:16:55, which was when the profile was initially created. So, the coordinator itself didn't receive any update from the coordinator fragment itself for two minutes. This suggests the Prepare() didn't complete within the time frame.

{noformat}
Instance 194f5b70907ac97c:84a116d600000000 (host=impala-ec2-centos74-m5-4xlarge-ondemand-07b3.vpc.cloudera.com:22000)
Last report received time: 2018-12-28 05:16:52.214
{noformat}

FWIW, other fragment instances running on other Impalad peers were able to send report so it seems the status reporting handling was working on coordinator ({{impala-ec2-centos74-m5-4xlarge-ondemand-07b3.vpc.cloudera.com:22000}}):
{noformat}
Instance 194f5b70907ac97c:84a116d600000003 (host=impala-ec2-centos74-m5-4xlarge-ondemand-07b3.vpc.cloudera.com:22001)
Last report received time: 2018-12-28 05:18:56.204
Hdfs split stats (<volume id>:<# splits>/<split lengths>): -1:4/152.76 KB
{noformat}

The fragment instances running on {{impala-ec2-centos74-m5-4xlarge-ondemand-07b3.vpc.cloudera.com:22000}} eventually completed.
{noformat}
I1228 05:16:52.217275 26685 query-state.cc:568] Executing instance. instance_id=194f5b70907ac97c:84a116d600000000 fragment_idx=0 per_fragment_instance_idx=0 coord_state_idx=0 #in-flight=7
I1228 05:20:08.981983 26685 krpc-data-stream-mgr.cc:294] DeregisterRecvr(): fragment_instance_id=194f5b70907ac97c:84a116d600000000, node=2
I1228 05:20:08.982043 26685 query-state.cc:576] Instance completed. instance_id=194f5b70907ac97c:84a116d600000000 #in-flight=5 status=CANCELLED: Cancelled
{noformat}

{noformat}
I1228 05:16:52.217344 26686 query-state.cc:568] Executing instance. instance_id=194f5b70907ac97c:84a116d600000001 fragment_idx=1 per_fragment_instance_idx=0 coord_state_idx=0 #in-flight=8
I1228 05:20:08.992434 26686 llvm-codegen.cc:1104] For query 194f5b70907ac97c:84a116d600000000 the following functions were not finalized and have been removed from the module:
I1228 05:20:09.049794 26686 query-state.cc:576] Instance completed. instance_id=194f5b70907ac97c:84a116d600000001 #in-flight=5 status=CANCELLED: Cancelled
I1228 05:20:09.056677 26686 query-exec-mgr.cc:182] ReleaseQueryState(): deleted query_id=194f5b70907ac97c:84a116d600000000
{noformat}

With the data available so far, one could only conclude that the test failed due to some flaky scheduling issue which slowed down some query fragments running on {{impala-ec2-centos74-m5-4xlarge-ondemand-07b3.vpc.cloudera.com:22000}}. Given this hasn't happened again since the initial reporting, I don't think there is much we need to do at this point.

Longer term, fixing IMPALA-6818 will remove this class of flaky timeout issues.

Haven't happened in a while. The long term fix is IMPALA-6818

