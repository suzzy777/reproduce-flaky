Mailing list discussion is here https://www.mail-archive.com/dev@kafka.apache.org/msg55658.html


I can confirm this happening also on Apache Kafka 1.1.1, can you please update the bug report?
Is there any known workaround?

This is happening also on Apache Kafka 2.1.0

Is this a hard bug to fix? Can't we just add some validation and send back an error rather then having the client blow up? Seems like a pretty nasty defect to me, please add your vote so we can some priority on this.  --Kurt

Hey [~jaikiran],

If no one is currently working on this. I would love to give it a try.

Also, I see the same issue addressed in KAFKA-8573 for a different version. 

 

Hi [~akumar],
I'm not working on a fix for this one. I just reported it and there was the mailing list discussion about it (linked in one of my comments above). 
Feel free to take it up. The kafka-dev mailing list would be a better place to discuss this change to get inputs from the Kafka developers.

[~akumar], are you taking this ticket? Otherwise, happy to have a look. 

Investigated. Created [KIP-498: Add client-side configuration for maximum response size to protect against OOM title|https://cwiki.apache.org/confluence/display/KAFKA/KIP-498%3A+Add+client-side+configuration+for+maximum+response+size+to+protect+against+OOM] with a proposal for a fix.


belugabehr commented on pull request #8066: KAFKA-4090: Validate SSL connection in client
URL: https://github.com/apache/kafka/pull/8066
 
 
   Issue has been around for a while.  Will post update on JIRA.
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


OK.  I took a stab at this and I added a unit test to simulate the SSL response laid out in KIP-498.

I don't think the answer is to use some sort of max buffer size.  That feature may be nice as protection from a misconfigured server, but not for this use case.

As pointed out int KIP-498, the server identifies that this is a plain-text connection, but the client is unable to identify the connection as being SSL-enabled.
How is that the case?  Well, the server buffers data as it is being read in chunks (8K perhaps).  Once it reads enough data into the buffer, it runs a quick scan on the bytes to try to detect SSL v.s. plaintext.

I tried putting the logic into the client, but it was very difficult.  The client does not buffer data in chunks in the same way.  It reads 4 bytes from the stream, and when those four bytes are read, it parses the size and creates the necessary payload buffer.  It's very simple.

There's just not a lot of flexibility in the existing setup.  Once the data is consumed from the stream, you can't put it back.  That is, there is no way to read 16 bytes, check for SSL, and if it's not SSL, put the bytes back into the front of the stream (or just inspect without consuming) for normal processing.  Once those 16 bytes are consumed, they are consumed... even if the stream has two 8 byte packets one-after-the-other.  In this scenario, reading 16 bytes to parse one packet just ate a packet.

To detect SSL, Netty (Apache 2.0) requires 5 bytes.  So, I am doing the same.  I am consuming 4 bytes and not loading a buffer at that time.  If the stream produces just 1 more byte, then I check if SSL is present, and if not, I create the required buffer.  It is much like a lazy-load implementation.  The payload buffer doesn't get created unless it absolutely has to.

Unfortunately, the unit tests bit me hard, because they all assume there will be two reads: one 4-byte read to get the payload size (and create the payload buffer), then one read to get the payload itself.  So, the mocking all mimics two reads.  My code does 3 reads: 1 for the size, 1 to get a more byte to test for SSL, then 1 for the payload. For anything that is a "live" test, with a test socket actually receiving data, this is brittle but generally works because it is very unlikely that the socket won't at least have 4 bytes for the size in the first read.  However, I think some of these live test would fail if there was only 1 or 2 bytes came from the stream at a time.  There is one unit test that is counting the number of reads that take place.  I had to disable it because it assumes the payload buffer is created after only 1 read (flaky test if the data comes in slowly),... it blocks (for testing purposes) any subsequent reads, so my implementation never creates the lazy payload buffer because a second read is never triggered).






Thanks [~belugabehr] for sharing your results. Do you have a pre-PR with your changes so that I can have a look?

https://github.com/apache/kafka/pull/8066

Thanks [~belugabehr]. Shared on thread for[ KIP-498.|https://cwiki.apache.org/confluence/display/KAFKA/KIP-498%3A+Add+client-side+configuration+for+maximum+response+size+to+protect+against+OOM?src=jira]

Hey updates or thoughts on moving this forward?

Hey, 
 We ran into this issue with Samza jobs using a Kafka client with TLS enabled. When producing data, Samza uses two clients, a producer and Admin (setup as a consumer to fetch meta-data etc..). We haven't setup the consumer side properly and it was silently failing, and causing the job to use a lot of memory (and in some cases die because of OOM). 
 We tested PR [https://github.com/apache/kafka/pull/8066] and it seems to work properly. We successfully tested the TLS usecase and we had no OOM when we don't setup the consumer client properly (we had exceptions reporting the mis-config as per PR#8066).

Please let us know if we can help in anyway moving this forward ? 

[~adupriez] Any thoughts on this?

Had the same issue. Spring Boot 2.3.7 (Reactor Kafka, Kafka 2.4) with following configuration:

 
{code:java}
spring:
  kafka:
    producer:
      ssl:
        key-store-location: ...
        key-store-password: ...
{code}
 

 

I spent some time looking for the solution, and of course, adding this helped:
{code:java}
spring:
  kafka:
    producer:
      security:
        protocol: SSL
{code}
 

Definitely OOM is making debugging confusing.

