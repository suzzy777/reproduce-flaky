Once we will figure out our story with different IDF per on mapreduce job, we should use those methods in [{{SqoopWritable}}|https://github.com/apache/sqoop/blob/sqoop2/execution/mapreduce/src/main/java/org/apache/sqoop/job/io/SqoopWritable.java]. Right now we're serializing the text format (and hence doing conversion), but that is probably not necessary.

Well this exercise is about figuring that out since there are connectors that benefit from non CSV if we are going to claim that SQOOP2 is generic and not just for RDBMS like data sources.

I just dont see why this become handy in non text? , and also in case of non MR engine, do we need this? 

So I'd like to be educated about some concrete details on where these 2 methods will add value.

It's a way for the IDF to serialize (and read) data in their internal format, so that they don't have to be converted to text or objects that rest of the Sqoop framework understands. I believe that from performance perspective, those methods are the fastest option, correct? I'm not familiar with Spark too heavily, but I would assume that even there we will need to provide way how to transfer data from machine to machine?

Let me try to rephrase it again,

these apis in sqoop writable makes sense to me

{code}
 @Override
  public void write(DataOutput out) throws IOException {
    out.writeUTF(toIDF.getCSVTextData());
  }

  @Override
  public void readFields(DataInput in) throws IOException { toIDF.setCSVTextData(in.readUTF()); }

{code}


I fail to understand is why the IDF API methods are required, since be it any kind of writable in hadoop or spark will basically use its own implementation.

ALL IDF needs to provide is the 

getCSVTextData()
getObjectArrayData()
getData()

and its corr setters.

If I'm reading the {{write(DataOutput)}} and {{read(DataInput)}} method implementation correctly, then for let say Avro based IDF we would do the following: On one machine we would have Avro objects in memory, the call {{getCSVTextData()}} would serialize the objects into String that could be transferred to a different machine in the cluster and finally the text representation would be deserialized back into Avro object in the {{setCSVTextData()}} call.

I believe that the intention of having the same methods in IDF was, so that the IDF itself can do the serialization of the native format and we don't have to do any extra serialization/deserialization step. E.g that {{write(DataOutput out)}} would be implemented simply as {{toIDF.write(out)}}. That would simplify the scenario above to have Avro objects in memory, serialize them into bytes the most efficient way for Avro, transfer them across the wire and then directly deserialize them back into Avro objects. 

I'm not familiar with Spark, but I'm assuming that there have to be  a similar API that is serializing/deserializing data for wire transfer?

>>>>
E.g that write(DataOutput out) would be implemented simply as toIDF.write(out) 

Are you saying the current code in SqoopWritable will have to change for non CSV IDF? 

I would assume that we change the code for all IDF implementations including CSV IDF. I'm wondering why we have not done that already, so I'm assuming that there is some tricky edge case that needs to be handled.

I will try changing this code to see what is the tricky thing, second will this change per IDF? i guess so this qualifies to be in base class

The implementation of the {{read(DataInput}} and {{write(DataOutput)}} methods is specific to each IDF, right? E.g. I would assume that CSV IDF will serialize a text whereas Avro IDF would directly serialize the avro objects?

i just dont see the need for this API in first place in IDF, but that is separate point

the current code in CSV, deals with setting the data, and I fail to understand why this would change per IDF

{code}
  /**
   * {@inheritDoc}
   */
  @Override
  public void write(DataOutput out) throws IOException {
    out.writeUTF(this.data);
  }

  /**
   * {@inheritDoc}
   */
  @Override
  public void read(DataInput in) throws IOException {
    data = in.readUTF();
  }

{code}

Dont review yet, trying to see if QA both passes.

Testing file [SQOOP-1900.patch|https://issues.apache.org/jira/secure/attachment/12687388/SQOOP-1900.patch] against branch sqoop2 took 0:26:24.276135.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} Patch does not add/modifny any test case
{color:red}ERROR:{color} Some integration tests failed
{color:red}ERROR:{color} Failed integration test: {{org.apache.sqoop.integration.repository.derby.upgrade.Derby1_99_4UpgradeTest}}
{color:green}SUCCESS:{color} Clean was successful
{color:green}SUCCESS:{color} Patch applied correctly
{color:green}SUCCESS:{color} License check passed
{color:green}SUCCESS:{color} Patch compiled
{color:green}SUCCESS:{color} All unit tests passed

Console output is available [here|https://builds.apache.org/job/PreCommit-SQOOP-Build/785/console].

This message is automatically generated.

 [~jarcec] I have modified the code and tests do pass, the Derby upgrade test is flaky and has been failing on all patches.



I've created follow up JIRA for the flaky {{Derby1_99_4UpgradeTest}} test - SQOOP-1911.

+1

Commit ecf7f4469abafae341642c0d31715977cc61bc41 in sqoop's branch refs/heads/sqoop2 from [~jarcec]
[ https://git-wip-us.apache.org/repos/asf?p=sqoop.git;h=ecf7f44 ]

SQOOP-1900: Fix the SqoopWritable read/ write method to delegate the toIDF methods

(Veena Basavaraj via Jarek Jarcec Cecho)


Thank you for your effort [~vybs]!

SUCCESS: Integrated in Sqoop2-hadoop200 #666 (See [https://builds.apache.org/job/Sqoop2-hadoop200/666/])
SQOOP-1900: Fix the SqoopWritable read/ write method to delegate the toIDF methods (jarcec: https://git-wip-us.apache.org/repos/asf?p=sqoop.git&a=commit&h=ecf7f4469abafae341642c0d31715977cc61bc41)
* execution/mapreduce/src/main/java/org/apache/sqoop/job/io/SqoopWritable.java


FAILURE: Integrated in Sqoop2-hadoop100 #735 (See [https://builds.apache.org/job/Sqoop2-hadoop100/735/])
SQOOP-1900: Fix the SqoopWritable read/ write method to delegate the toIDF methods (jarcec: https://git-wip-us.apache.org/repos/asf?p=sqoop.git&a=commit&h=ecf7f4469abafae341642c0d31715977cc61bc41)
* execution/mapreduce/src/main/java/org/apache/sqoop/job/io/SqoopWritable.java


