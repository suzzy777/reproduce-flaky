[~showuon] this is being addressed in https://issues.apache.org/jira/browse/KAFKA-10413 , let me know if you still see any issue with this patch as well

[~ramkrish1489], thanks for your comments. However, this issue is still there after this patch. As I described, it's that the algorithm didn't consider the member count not the same as previous revocation round. We just evenly distributed the revoked C/T to all the new members. We should consider the member count before doing this. It's not related to the issue in KAFKA-10413.

Or we can say, you found 2 issues in KAFKA-10413, and I found one more issue in this ticket, which all will cause uneven distribution.

Please help review my PR after I completed it. Thank you.

[~ramkrish1489], I checked your PR in KAFKA-10413 again, and I modified my bug description now. You fixed the issue when member left, and you did a round-robin distribution to all the active members. Nice fix!

The issue I tried to fix here, is when new member joined, we'll have 2 rounds of rebalance for it: 1 for revocation, 1 for re-assignment. And the issue is at the 2nd round of re-assignment. What we did now is re-assign all the previous revoked C/T to the new members, and end the rebalance. However, if there is 1 (or more) members joined during the 1st round and 2nd round of rebalance, at the 2nd round of re-assignment, we'll assign all the revoked C/T to all new members, and end the rebalance. But the revoked C/T was intended for only (numNewMembers - 1), so it causes uneven distribution.

 

You can check the example in the bug description again, you should understand what I meant. Thank you.

Remember to enable the _testMultipleWorkersRejoining_ test once this is fixed.

[~kkonstantine], could you take a look the PR? I suddenly found this bug is blocking a V3.0 blocker flaky test. Please help! Thank you.

This issue corresponds to a corner case that does not seem to appear in practice often. The current suggestion to allow for consecutive revocations carries some risk. I have another fix in mind that I'd like to explore. In the meantime I'm punting this issue to the next release. 

Moving to the next release as we are past the 3.1 release code freeze.

Hi, any news on this issue ?
I have been testing and it looks like it solves my unbalance issue with autoscaling.


Is there a plan to merge it ?

Thanks

[~kkonstantine] , could you give some suggestion for the issue?  This issue has been pending for a long time. You mentioned you have fix in mind, do you have time to submit the PR? If no, could you share your thoughts here, so that we can move on to next step. Thank you.

[~kkonstantine] , there are some users faced this issue and would like to fix this issue soon. Do you have plan when this PR can be merged or provide some comments about what your concern about my fix is, and what's your suggestion to this issue. Please help. Thanks.



cc [~ChrisEgerton] [~rhauch] 

Hi [~showuon]! I'm taking time off right now but should be able to try to get up-to-speed on the state of incremental rebalancing logic in Connect and take a look at your PR and the issue description here sometime next week.

I'll note that the assertion that "This issue corresponds to a corner case that does not seem to appear in practice often." seems to be empirically incorrect given the level of attention that this ticket has received, and given some recent conversations on the dev mailing list around timely review of PRs from non-committer contributors, would love to see this get the attention it deserves.

We should all obviously try to maintain a high level of quality and efficacy in the Connect framework and the Kafka code base in general, but at this point it seems like we've let perfect become the enemy of good and have allowed this issue to remain unaddressed for far too long (nearly a year!) despite showing promise as an improvement to Connect. There's only so much I can do as a contributor, but I'd be happy to take the first step and give this a look of my own to try to help move things along.

Thanks for documenting the issue in detail [~showuon]. Adding here the comment I added to the PR. 

My main concern is indeed related to the proposed change to apply consecutive rebalances that will perform revocations.

The current incremental cooperative rebalancing algorithm, is using two consecutive rebalances in order to move tasks between workers. One rebalance during which revocations are happening and one during which the revoked tasks are reassigned. Although clearly this is not an atomic process (as this issue also demonstrates) I find that it's a good property to maintain and reason about.

Allowing for consecutive revocations that happen immediately when an imbalance is detected might mean that the workers overreact to external circumstances that have caused an imbalanced between the initial calculation of task assignments of the revocation rebalance and the subsequent rebalance for the assignment of revoked tasks. Such circumstances might have to do with rolling upgrades, scaling a cluster up or down or simply might be caused by temporary instability. We were first able to reproduce this issue in integration tests by the test that is currently disabled.

My main thought was that, instead of risking shuffling tasks too aggressively within a short period of time and open the door to bugs that will make workers oscillate between imbalanced task assignments continuously and in a tight loop, we could use the existing mechanism of scheduling delayed rebalances to program workers to perform a pair of rebalanced (revocation + reassignment) soon after an imbalance is detected. Regarding when an imbalance is detected, the good news is that the leader worker sending the assignment during the second rebalance of a pair of rebalances knows that it will send an imbalanced assignment (there's no code to detect right now that but can be easily added just before the assignment is sent). The idea here would be to send this assignment anyways, but also schedule a follow up rebalance that will have the opportunity to balance tasks soon with our standard pair of rebalances that works dependably as long as no new workers are added or removed between the two rebalances. We can discuss what is a good setting for the delay. One obvious possibility is to reuse the existing property. Adding another config just for that seems unwarranted. To shield ourselves from infinite such rebalances the leader should also keep track of how many such attempts have been made and stop attempting to balance out tasks after a certain number of tries. Of course every other normal rebalance should reset both this counter and possibly the delay.

I'd be interested to hear what do you think of this approach that is quite similar to what you have demonstrated already but potentially less risky in terms of changes in the assignor logic and how aggressively the leader attempts to fix an imbalance.

{quote}Allowing for consecutive revocations that happen immediately when an imbalance is detected might mean that the workers overreact to external circumstances that have caused an imbalanced between the initial calculation of task assignments of the revocation rebalance and the subsequent rebalance for the assignment of revoked tasks. Such circumstances might have to do with rolling upgrades, scaling a cluster up or down or simply might be caused by temporary instability.
{quote}
Have you identified a plausible case where this may be an issue? Load-balancing revocations are only necessary when the number of workers has increased (or when the number of connectors/tasks has decreased, although this is not addressed in the current rebalancing algorithm). At least with the case of new workers, is it really an overreaction to find connectors/tasks to allocate to them as soon as possible?

Going over the example cases provided:
 * With a rolling upgrade, the existing delayed rebalancing logic should already apply, preventing excessive revocations from taking place
 * With a cluster scale-down, no load-balancing revocations should be necessary, since the number of connectors/tasks per worker will increase, not decrease
 * With a cluster scale-up, immediate revocation will not only not be harmful, it will actually be advantageous as it will allow the new workers to begin work immediately instead of waiting for the scheduled rebalance delay to elapse. This could be crucial if there's a load burst across the cluster and an external auto-scaling process spins up new workers to try to respond as quickly as possible
 * With temporary instability, if workers fall out of the cluster, the existing delayed rebalancing logic should already apply, preventing excessive revocations from taking place. There may be another interpretation of what this scenario would look like in terms of workers leaving/joining the cluster; let me know if you had something else in mind

The only case I can think of where unconditionally delaying between revocations may be beneficial is if there's a rapid scale-up and then immediate scale-down of a cluster. If we hold off on revoking too many connectors/tasks from the pre-scale-up workers in the cluster, then we'll have to reassign fewer of them once the scale-down takes place. But unless I'm missing something, this is an unlikely edge case and should not be prioritized.
{quote}To shield ourselves from infinite such rebalances the leader should also keep track of how many such attempts have been made and stop attempting to balance out tasks after a certain number of tries. Of course every other normal rebalance should reset both this counter and possibly the delay.
{quote}
This is a great suggestion, especially since it can (and should) be implemented regardless of whether a delay is added between consecutive load-balancing revocations.

[~kkonstantine] [~ChrisEgerton] , thanks for your comments.

So, in summary, the risk of the current (consecutive revocations) approach:
 # overreact to external circumstances
 # infinite rebalance

I like the idea of (2), to protect ourselves from infinite rebalance, though this should not happen.

For (1), it's really hard to identify if the consecutive revocation is overreact, or quick fix the issue. I think we all agree that's case by case. Then, I think we can follow the current consecutive revocation solution to fix it. The main reason is:
 # the PR is ready for review, and should able to include it in v3.2.0 soon
 # Some users have suffered from this issue and have been waiting for the fix for a long time

 

In my opinion, we will avoid infinite rebalance anyway, we can improve it to use "delayed revocation" method if necessary in the future. WDYT?

 

 

 

[~showuon] [~ChrisEgerton] [~kkonstantine] Is this a regression so that it qualifies as a blocker for the 3.2.0 release?

With respect to release logistics, I think it's worth being clear. This issue, although it's an annoying bug, is not a regression and therefore should not be treated as a blocker for 3.2.0 if we want to be consistent with our release process at this point. Given that we are way into code freeze the fix should target {{trunk}} and be backported to the respective release branches when they go out of the code freeze period. 

[~showuon] regarding the fix itself, it's worth noting that the challenge is with testing rather that the code changes themselves. But irrespective to that, I don't think that being tactical and rushing the existing fix is necessarily the right thing to do in this case (and to be honest in my opinion timing is rarely the most important factor for complicated improvements). 

I believe that it's really worth trying to avoid rebalance storms (that can happen when multiple events that can cause a rebalance happen concurrently). If users want to minimize the time to recover from this type of situation they will still have the ability to do so by setting [scheduled.rebalance.max.delay.ms|https://kafka.apache.org/documentation/#connectconfigs_scheduled.rebalance.max.delay.ms] to a lower value. And if we follow this approach and it works sufficiently, we can consider lowering the default time in the future as an improvement. 

I believe the risks of rebalance storms are being overstated and without a practical example should not affect the fix that is chosen here.

I agree this is not a regression and is not a blocker for v3.2.0.

[~kkonstantine] , I see what you mean. OK, let's fix it in a delayed rebalance solution.

Speaking of that, I might be busy on daily jobs recently, so if anyone is interested in fixing it and submit a PR, it'll be great! 

 

Thank you.

I see two different issues when doing deployments with 2.6.2. In my deployments I add 5 Workers at once and then remove 5 other Workers at once (starting with 11, so it goes from 11 to 16 and then back to 11). I say at once but they don't really get started and removed at once, there maybe could be a 30 seconds different between the first one being ready and the last one being ready. Problems I see are:
 # What's mentioned in this ticket when Workers are being added. Basically, there are more workers in the second round than they were in the first round, which leads to unbalanced assignments.
 # When Workers are going away, we sometimes end up with one Worker with ALL the assignments that were in the 5 Workers that went away. The Worker that gets all this assignments is the last one that got started. When this happens, I see that in one generation that Worker had no assignments, and in the next one it has but it also shows the delay that comes from waiting on the Workers that left ({{{}scheduled.rebalance.max.delay.ms{}}}). After that delay expires, all assignments from the 5 Workers that went away go into that one Worker. My theory here by looking at the code is that it may have become the only one in {{candidateWorkersForReassignment}} at the time were it had no assignments, and then remained that way even when we started waiting on the Workers that went away even though this Worker had assignments by now. Either way, I don't really get this {{{}candidateWorkersForReassignment{}}}, because such workers would get all the assignments of the ones that went away, right? What if more Workers went away than the ones that don't have assignments? This is in fact what happens when I test removing 5 Workers and in the waiting period add 1 Worker, everything goes into that single new Worker.

 

 

[~mcabrera] , thanks for the sharing. Looks like you already investigated it and are familiar with the codes. Are you interested in submitting a PR to improve the rebalance algorithm?

[~showuon] I'm short on time right now, so I won't be able to tackle this.

No problem, [~mcabrera] !

[~showuon] , is this still open? I can take a stab at it. Let me know. Thanks!

[~sagarrao] yes, it's unassigned so anyone can feel free to take a shot at it.

I should note that I don't believe the proposal to use the scheduled rebalance delay in between successive revocation rounds is safe, though. With that change, it would become impossible in some scenarios to scale up a cluster within the rebalance delay, and given that the current default for that delay is five minutes, this may cause some issues for Connect cluster administrators. In fact, I would even argue that it would qualify as a regression since the outcome for users in this scenario would be significantly worse than it is at the moment.

I'm still not convinced that rebalance storms are a serious risk when removing the guard against successive revocation rounds, especially if we add some cycle-detection logic. However, if my colleagues still believe it's necessary to take extra precautions against storms, one possibility is that we can compromise and use exponential backoff between successive revocation rounds. This would be more complex to implement and require some potentially-tricky state tracking in the assignor class, but if done effectively, would allow us to start off by having little to no delay between successive revocation rounds, but, in the event that something goes wrong, eventually work our way up to waiting the full scheduled rebalance delay in between rounds.

Thanks [~ChrisEgerton] . I would take a look. I had another idea as well which is probably more extravagant. I was thinking we let the assignor do whatever it does and we let it stabilise. How about we run some checks afterwards to see if the assignments are balanced or skewed and then trigger a rebalance later on. IIRC, kafka streams does something similar by leveraging probably the Consumer#enforceRebalance. I can dig deep into it if needed. We can take it even a step further by even providing users the ability to define the idea of balanced assignments. Case in point could be running it on cloud v/s running it on prem. This would also alleviate some of the concerns raised in this ticket about rebalance storms(I know you think it's not a risk) or premature successive rebalances etc.

WDYT? Does it make sense?

[~sagarrao] it seems like there are two ideas at play here:
 # Triggering possibly-delayed rebalances independently of the assignor (i.e., without sending out an assignment with a scheduled rebalance delay)
 # Pluggable (i.e., user-supplied) definitions of a "balanced" assignment

With regards to item 1, I toyed with this idea too, but I don't really see a practical difference between this and using the scheduled rebalance delay mechanism, as long as we have cycle detection. What would be the advantage here?

With regards to item 2, there's definitely room for improvement of our notion of "balance" within a cluster given that the assumption right now is that all tasks and connectors have homogeneous resource utilization. This is obviously not the case, but nobody's put in the (substantial) work yet to try to add more granularity to our rebalance logic, and the existing logic has served us decently well up to this point. Ultimately though, it would require a KIP and is probably out of scope for this ticket since there are alternative solutions that are more lightweight and suitable for backport.

[~ChrisEgerton] You summed it up correctly. Regarding #1, you are right. The real benefit would be seen only when we combine it with idea #2 i.e pluggable notion of balances. It could be load based or equal distribution of connectors/tasks etc. Yeah it might even need a KIP which is out of scope for this. Let me look at the code for this one.

[~ChrisEgerton] , I was looking at this and this comment of yours caught my eye:

`I should note that I don't believe the proposal to use the scheduled rebalance delay in between successive revocation rounds is safe, though`.  I actually couldn't follow this comment. Today the protocol doesn't allow 2 successive rebalances happening by the same leader and Luke has been saying all along to remove it. I think that's what his PR does as well.

I think to be on the safer side, I would implement the exponential back off logic starting from 0 -> scheduled rebalance delay. But we won't still do it while a delayed rebalance is active. It could be counter based as well i.e we will tolerate w workers to be added before triggering a rebalance. Complexity wise it seems the same to me but how do the 2 compare?

 

Also, you talked about cycle detection. What does that refer to?

Sorry for the delay. My thoughts are below

 

> `I should note that I don't believe the proposal to use the scheduled rebalance delay in between successive revocation rounds is safe, though`.  I actually couldn't follow this comment. Today the protocol doesn't allow 2 successive rebalances happening by the same leader and Luke has been saying all along to remove it. I think that's what his PR does as well.

 

Yes, my PR is to allow successive rebalances "without delay". But the code author, Konstantine, suggested we should have "delay" between each rebalance (revocation), to avoid rebalance storm. But Chris doesn't agree that there would cause rebalance storm, and also, the delay might even slow down the normal scale up/down pace. That's why, Chris suggested, with some compromise, we can have a exponential backoff between successive revocation rounds. That is, with current "scheduled delay" method, each rebalance needs 5 mins to trigger the rebalance. But with "exponential backoff" way, it'll speed up the rebalance, but also keep the concept of "delay" rebalance.

 

> I think to be on the safer side, I would implement the exponential back off logic starting from 0 -> scheduled rebalance delay.

 

Sounds good

 

> But we won't still do it while a delayed rebalance is active. It could be counter based as well i.e we will tolerate w workers to be added before triggering a rebalance. Complexity wise it seems the same to me but how do the 2 compare?

 

Yes, I agree complexity wise it seems the same. Again, this way is to keep the original concept of "rebalance delay", and also speed up the rebalance pace.

 

[~sagarrao] , I saw you created a KIP for next generation Connect rabalance protocol [here|https://cwiki.apache.org/confluence/display/KAFKA/%5BDRAFT%5DIntegrating+Kafka+Connect+With+New+Consumer+Rebalance+Protocol]. I have a quick scan, and looks like it only integrates the concept of KIP-848 into Kafka Connect, and keep Connect client assignor. I'm wondering that if we can move the client assignor in Connect into group coordinator like consumer does? I don't think Connect needs to have a custom client assignor like Stream does, because Connect has no complicated "active"/"standBy" tasks like Stream does. If we can move the algorithm onto group coordinator, I think it'll make our life easier. WDYT?

 

However, I think it still needs a lot of discussion for the new KIP. Before that, we can still have a acceptable fix for this bug. Thank you.

Thanks [~showuon] for your comments. 

Regarding the new draft KIP thank you for giving a pass through it. I am working on a new version based on initial suggestions/feedback. As you pointed out, it keeps the assignment logic at the client side - similar to streams and the current rebalancing protocol. Yeah it could very well be moved to the group coordinator layer. 

Having said that, I decided to keep it in the client layer because in Incremental Cooperative protocol, clients can trigger a rebalance based on the scheduled rebalance delay. As per my understanding of KIP-848, such rebalances which the clients want to trigger, can be done by setting the `reason` field. That kind of rebalance triggering is similar(yet not as complex) as the kind of rebalance that Streams triggers. Also, if we had only Eager assignor to support, then I think moving it to the GC would have made sense as the Group coordinator needed to trigger rebalances only when workers joined or left for example. Also the logic of scheduled rebalance delays is not something the GC really needs to know or keep track of, I felt keeping it at the client side would make more sense and also in line with what KIP-848 is trying to acheive. Let me know if that makes sense.

 

Also, it would be great if this feedback can be shared on the discussion thread! It would evoke further comments from more members in the community which would help in solidifying the design! Thanks!

