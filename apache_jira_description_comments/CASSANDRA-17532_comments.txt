 [3.0|https://github.com/ekaterinadimitrova2/cassandra/pull/195]. [3.0 CI run|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1501/workflows/d0128467-3ef8-446a-8b49-9bfc4b0238b5/jobs/9757] 

[3.11|https://github.com/ekaterinadimitrova2/cassandra/pull/196]  [3.11 CI run|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1513/workflows/c3b751d5-c3f4-40d4-8397-651aa5da446e]

 [4.0|https://github.com/ekaterinadimitrova2/cassandra/pull/197]. [4.0 CI run|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1512/workflows/735dec78-3e1c-45c0-9e48-a2dcd5bfdc1f]

[trunk|https://github.com/ekaterinadimitrova2/cassandra/pull/198] [trunk CI run|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1522/workflows/6f1e9d0a-fcee-41e6-b09c-e59930a41c59]

CI is still running, I will check it back later. I raised the resources for 3.11 to get rid of the OOM, there is some other timeout. I will have to run it with clean 3.11 and raised resources to verify I didn't break anything 

Submit the following runs to verify the 1-time failures in 3.11 and trunk. 

Trunk:

Pre-this patch the failing test in a loop:

[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1528/workflows/aef7a1e8-c476-44a1-ad93-0de9f1f9cc75]

After this patch the failing test in a loop:

[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1529/workflows/fe7c5ef9-1efd-47e7-9eba-b1090a1ef16e]

3.11:

With the patch:

[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1530/workflows/e11bdae8-f48a-4608-8b93-dda1daeca43a]

Before the patch:

[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1531/workflows/e097f4e6-f01f-4d89-bb0f-11a562aed01f]

 

I will check the results later.

It seems those are just flaky tests that failed with the same frequency and same errors with and without my patch.

Submitting for review

gave feedback that we look to be applying the setting in every test rather than only the tests needed; once this is resolved with review again

We do because, otherwise we try to set properties from the InstanceConfig to older versions and get ConfigurationException. Tested. [~ifesdjeen], is this expected behavior or some bug we found? My understanding is that flag was set exactly to guard this.
At the same time it is super confusing as I would expect people to try to set old config to newer version, not newer config to old version. 

Example from 3.0:
 MixedModeRangeTombstoneTest
The moment I remove the flag from the test, I get:

{code:java}
Caused by: org.apache.cassandra.exceptions.ConfigurationException: Invalid yaml. Please remove properties [concurrent_materialized_view_writes, hints_directory] from your cassandra.yaml
{code}



This patch just moves the below from UpgradeCluster to every test

{code}
set(Constants.KEY_DTEST_API_CONFIG_CHECK, false)
{code}

The problem we face is that InstanceConfig on trunk gets configs added that make sense on trunk but not older versions; we do not have an ability to downgrade configs (if using renamed config, switch to old, or if no old exists drop), so this causes all upgrade tests to fail... we added the flag to UpgradeCluster due to a lack of versioned config support, so without actually adding a ability to deal with versioned configs, we don't get a benefit from this patch.

I feel this patch doesn't solve the intent, making our tests safer by checking; as the tests still don't check they are just more verbose.

I am +0, I am not a fan but if others want this I will not stand in the way


{code:java}
The problem we face is that InstanceConfig on trunk gets configs added that make sense on trunk but not older versions;
{code}

This problem is with all versions, not only trunk. We have new config added to every major version which doesn't exist in older versions.
{code:java}
I feel this patch doesn't solve the intent, making our tests safer by checking; as the tests still don't check they are just more verbose.
{code}

Valid point, it was never its intention to solve this limitation, same as the flag when it was introduced. The reason why I suggested to flip the flag was to make it verbose to people so they know this flag exists and they check their config intentionally before setting in a new test until that limitation is solved.

Otherwise, currently we never check the config by default, most of the people don't know about the flag and don't check it also manually carefully as they would expect Cassandra to complain as usual. I documented this behavior on the config page, but I am not sure how many people will read it at all. So at least until this is solved people can get an exception for new config and intentionally check what they set on their own instead of silently config being ignored and tests passing. Considering a new test is added once in a few months, it is not a big burden until the limitation is worked out, probably after the release. 



Opened and linked to this ticket CASSANDRA-17548, to be clear we don't solve here the limitation of the framework and that this should be done in CASSANDRA-17548. This ticket has the idea of not silently ignoring that checks are not happening until the other one is taken care of. 

