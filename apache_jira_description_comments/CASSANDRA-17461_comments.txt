Gave it a go hoping increased timeouts would help but only do for some cases, not for the whole class. Need to look further in...

This one reproduces easily in CircleCI. 

My bisecting shows it is failing after CEP-14 was committed.

[200 runs no failures|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1709/workflows/0da6056e-bb8d-4865-b24b-601bd28eadb3] before CEP-14, [200 runs and 5 failures|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1707/workflows/d6950287-f00e-428b-976e-1f19dfac0345/jobs/11936] after. It was changed in CEP-14.

[~benedict] , [~bdeggleston] , can anyone of you take a look, please? 

I don't think the test was materially changed, just refactored to share the cluster and to use the new Verbs. AFAICT it is behaviourally identical. But Paxos was changed, and this perhaps affected timings. I think perhaps we impose request timeouts more accurately now, and this could impact flakiness.

The relevant timeout to change would not be the test timeout though, but the contention and request timeouts, as these affect how quickly these request timeouts would fire which might be relevant in flaky VM environments.

Not sure when I'll get a chance to look more closely myself.

Wasn't that test ignored before CEP-14? I think it has been marked as {{@Ignored}} since its introduction on CASSANDRA-12126, which would explain why the pre-CEP-14 run passes so quickly. CEP-14 modified the test and removed the {{@Ignored}} annotation for the very first time, and it has been flaky since then.

Indeed, if the ignore is removed in 4.0, it also fails the same way as can be seen [here|https://app.circleci.com/pipelines/github/driftx/cassandra/547/workflows/3c58bb72-0fa2-4015-9919-6cba2b1b9957/jobs/6518].  Should we unblock beta from this ticket and move it to rc instead?

Noticing that a lot of our 4.1 failures in butler appear to be linked to this ticket. If we were {{@Ignored}} since the test was initially introduced, shouldn't we be skeptical of its correctness, add back the annotation, create a follow up ticket to either fix or remove the test for 4.2, and close this out?

bq. The relevant timeout to change would not be the test timeout though, but the contention and request timeouts, as these affect how quickly these request timeouts would fire which might be relevant in flaky VM environments.

Would it be worth making a quick pass at these timeouts before ignoring again?

Those are actually the timeouts that were changed in Bereng's PR above.

Tried checking out the SHA for CASSANDRA-12126 to see if the test passed this morning and it wouldn't build; didn't feel like investing more cycles than to confirm the test in question was definitely {{@Ignored}} on introduction. [~slebresne] (who's OOO for a few weeks but leaving here for posterity) - any chance you recall that test and why it was flagged as ignored on addition?

I'm going to beat the drum again here - I say we add the {{@Ignored}} annotation back and create another ticket to get to the bottom of this that's 4.2 since this isn't a regression and we can keep making progress towards 4.1. Anyone against? Happy to do the grunt work myself early next week to tidy this up.

Sorry for missing this discussion. These tests when added were known to fail because they demonstrated real bugs with Paxos v1, so we marked them \@Ignore to avoid polluting everyone's test results. Paxos v2 fixes these bugs, so these tests are now valuable.

So then, would you like to make them pass?

If you're willing to wait for me to get round to it, sure.

Otherwise try bumping the message timeouts, rather than test timeouts? Presumably the environment has gotten flakier.


Are [these|https://github.com/apache/cassandra/pull/1603/commits/2be278db2fe51ddf6ef534f9b44835046a898c00] the correct ones to adjust?

Probably. Unfortunately the links to outcomes for each test appear to be broken, but looking at the [log file|https://circleci.com/api/v1.1/project/github/adelapena/cassandra/14107/output/103/5?file=true&allocation-id=6238ab6a00c61f3c0b452b00-5-build%2F3ED5FD94], I cannot tell what log output is for what test case.

[Here|https://app.circleci.com/pipelines/github/adelapena/cassandra/1968/workflows/1917904b-8a49-4bf6-9b39-282ac1af7424/jobs/19590] is a new repeated CI run only for the new {{{}CASTest.testConflictingWritesWithStaleRingInformation{}}}. It shows a 5% flakiness. The CircleCI config file has been generated with:
{code:java}
.circleci/generate.sh -m \
  -e REPEATED_UTEST_TARGET=test-jvm-dtest-some \
  -e REPEATED_UTEST_COUNT=200 \
  -e REPEATED_UTEST_CLASS=org.apache.cassandra.distributed.test.CASTest \
  -e REPEATED_UTEST_METHODS=testConflictingWritesWithStaleRingInformation
{code}
The standard output, logs, and Junit report for each iteration can be found on [the artifacts tab|https://app.circleci.com/pipelines/github/adelapena/cassandra/1968/workflows/1917904b-8a49-4bf6-9b39-282ac1af7424/jobs/19590/artifacts]. The failed runs contain "fails" on the path, while the successful runs contain "passes". For example, these are the files for the 2nd iteration of the 6th runner:
 * [stdout/fails/2/test-jvm-dtest-some-org.apache.cassandra.distributed.test.CASTest.txt|https://output.circle-artifacts.com/output/job/2fd9786b-db08-4079-909b-8090f30ef78a/artifacts/6/stdout/fails/2/test-jvm-dtest-some-org.apache.cassandra.distributed.test.CASTest.txt]
 * [junitxml/fails/2/TEST-org.apache.cassandra.distributed.test.CASTest-testConflictingWritesWithStaleRingInformation.xml|https://output.circle-artifacts.com/output/job/2fd9786b-db08-4079-909b-8090f30ef78a/artifacts/6/junitxml/fails/2/TEST-org.apache.cassandra.distributed.test.CASTest-testConflictingWritesWithStaleRingInformation.xml]
 * [logs/fails/2/org.apache.cassandra.distributed.test.CASTest/<main>/<main>/system.log|https://output.circle-artifacts.com/output/job/2fd9786b-db08-4079-909b-8090f30ef78a/artifacts/6/logs/fails/2/org.apache.cassandra.distributed.test.CASTest/%3Cmain%3E/%3Cmain%3E/system.log]
 * [logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-e838e8c8-e117-4520-9396-ab1b145f2a2b/node1/system.log|https://output.circle-artifacts.com/output/job/2fd9786b-db08-4079-909b-8090f30ef78a/artifacts/6/logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-e838e8c8-e117-4520-9396-ab1b145f2a2b/node1/system.log]
 * [logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-e838e8c8-e117-4520-9396-ab1b145f2a2b/node2/system.log|https://output.circle-artifacts.com/output/job/2fd9786b-db08-4079-909b-8090f30ef78a/artifacts/6/logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-e838e8c8-e117-4520-9396-ab1b145f2a2b/node2/system.log]
 * [logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-e838e8c8-e117-4520-9396-ab1b145f2a2b/node3/system.log|https://output.circle-artifacts.com/output/job/2fd9786b-db08-4079-909b-8090f30ef78a/artifacts/6/logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-e838e8c8-e117-4520-9396-ab1b145f2a2b/node3/system.log]
 * [logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-fa769a52-7814-4842-8760-7c158e8282e0/node1/system.log|https://output.circle-artifacts.com/output/job/2fd9786b-db08-4079-909b-8090f30ef78a/artifacts/6/logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-fa769a52-7814-4842-8760-7c158e8282e0/node1/system.log]
 * [logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-fa769a52-7814-4842-8760-7c158e8282e0/node2/system.log|https://output.circle-artifacts.com/output/job/2fd9786b-db08-4079-909b-8090f30ef78a/artifacts/6/logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-fa769a52-7814-4842-8760-7c158e8282e0/node2/system.log]
 * [logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-fa769a52-7814-4842-8760-7c158e8282e0/node3/system.log|https://output.circle-artifacts.com/output/job/2fd9786b-db08-4079-909b-8090f30ef78a/artifacts/6/logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-fa769a52-7814-4842-8760-7c158e8282e0/node3/system.log]
 * [logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-fa769a52-7814-4842-8760-7c158e8282e0/node4/system.log|https://output.circle-artifacts.com/output/job/2fd9786b-db08-4079-909b-8090f30ef78a/artifacts/6/logs/fails/2/org.apache.cassandra.distributed.test.CASTest/cluster-fa769a52-7814-4842-8760-7c158e8282e0/node4/system.log]

So the first one of these failures is nothing to do with this test:

{noformat}
[junit-timeout] ------------- ---------------- ---------------
[junit-timeout] Testcase: org.apache.cassandra.distributed.test.CASTest:	Caused an ERROR
[junit-timeout] Uncaught exceptions were thrown during test
[junit-timeout] org.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test
[junit-timeout] 	at org.apache.cassandra.distributed.impl.AbstractCluster.checkAndResetUncaughtExceptions(AbstractCluster.java:1058)
[junit-timeout] 	at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1044)
[junit-timeout] 	at org.apache.cassandra.distributed.test.CASTest.afterClass(CASTest.java:93)
[junit-timeout] 	Suppressed: java.lang.RuntimeException: java.lang.IllegalStateException: HintsService is shut down and can't accept new hints
[junit-timeout] 		at org.apache.cassandra.service.StorageProxy$HintRunnable.run(StorageProxy.java:2585)
[junit-timeout] 		at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:81)
[junit-timeout] 		at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:47)
[junit-timeout] 		at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:57)
[junit-timeout] 		at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:120)
[junit-timeout] 		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[junit-timeout] 		at java.lang.Thread.run(Thread.java:748)
[junit-timeout] 	Caused by: java.lang.IllegalStateException: HintsService is shut down and can't accept new hints
[junit-timeout] 		at org.apache.cassandra.hints.HintsService.write(HintsService.java:165)
[junit-timeout] 		at org.apache.cassandra.service.StorageProxy$7.runMayThrow(StorageProxy.java:2664)
[junit-timeout] 		at org.apache.cassandra.service.StorageProxy$HintRunnable.run(StorageProxy.java:2581)
[junit-timeout] 
[junit-timeout] 
{noformat}

-Hmm, is there an issue with gossip? Have the defaults for gossip been changed in in-jvm dtests? There shouldn't be gossip running, but there seem to be background changes in gossip state for the nodes (that span messages being sent).-
{noformat}
[junit-timeout] INFO  [MutationStage-1] <main> 2022-07-29 12:19:44,849 CASTestBase.java:124 - Dropping PAXOS2_PREPARE_REQ from 1 to 3
[junit-timeout] INFO  [node1_GossipStage:1] node1 2022-07-29 12:19:44,896 Gossiper.java:1409 - Node /127.0.0.4:7012 is now part of the cluster
[junit-timeout] DEBUG [node1_GossipStage:1] node1 2022-07-29 12:19:44,897 StorageService.java:2887 - Node /127.0.0.4:7012 state NORMAL, token [9223372036854775801]
[junit-timeout] DEBUG [node1_GossipStage:1] node1 2022-07-29 12:19:44,901 StorageService.java:2797 - New node /127.0.0.4:7012 at token 9223372036854775801
[junit-timeout] DEBUG [node1_GossipStage:1] node1 2022-07-29 12:19:44,914 Gossiper.java:1353 - removing expire time for endpoint : /127.0.0.4:7012
[junit-timeout] INFO  [node1_GossipStage:1] node1 2022-07-29 12:19:44,914 Gossiper.java:1354 - InetAddress /127.0.0.4:7012 is now UP
[junit-timeout] INFO  [MutationStage-1] <main> 2022-07-29 12:19:44,915 CASTestBase.java:124 - Dropping PAXOS2_PREPARE_REQ from 1 to 3
[junit-timeout] INFO  [ReadStage-1] <main> 2022-07-29 12:19:45,536 CASTestBase.java:124 - Dropping READ_REQ from 1 to 3 {noformat}
Of course, this is _probably_ the application of the new ring state obtained via PaxosPrepare.

[Here|https://app.circleci.com/pipelines/github/adelapena/cassandra/1969/workflows/4b9858d8-900c-4aa3-818b-2d430234f9e9/jobs/19592] is a new repeated run for [the commit|https://github.com/ekaterinadimitrova2/cassandra/commit/d2923275e360a1ee9db498e748c269f701bb3a8b] when the test was introduced, or not ignored anymore, by CASSANDRA-17164.

I'm repeating this run because [the artifacts|https://app.circleci.com/pipelines/github/adelapena/cassandra/1969/workflows/4b9858d8-900c-4aa3-818b-2d430234f9e9/jobs/19592/artifacts] (logs, etc.) are retained for one month, so we have missed the artifacts of [Ekaterina's run on 8th Jun|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1707/workflows/d6950287-f00e-428b-976e-1f19dfac0345/jobs/11936/tests]. This run hits {{CasWriteTimeoutException}} four times, and there are no issues with shutdown.

Would it be possible to run this with {{TRACE}} logging enabled for {{org.apache.cassandra.service.paxos}}?

[~leesangboo] I see that you transitioned this ticket to In Progress - are you working on this? Can set you as assignee if so.

{quote}Would it be possible to run this with {{TRACE}} logging enabled for {{{}org.apache.cassandra.service.paxos{}}}? 
{quote}
Everything is possible as long as there is someone with a bit of time :D 

[This|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=17461-repro] is a run with TRACE logging enabled for org.apache.cassandra.service.paxos. I ran it for the commit when the test was introduced, or not ignored anymore, by CASSANDRA-17164. All logs are under the artifacts tab. 

Thanks Ekaterina, that makes clear what the problem is. The issue is that when node1 receives an update to its ring information about node4, it first contacts node4 to confirm it is alive before marking it so, and this is racing with the submission of the second round of Paxos Prepares so that the outgoing message to node4 is being dropped before it hits the wire.

I have introduced some additional logic to ensure that node4 is marked alive before the second round of prepares is submitted

https://github.com/belliottsmith/cassandra/tree/17461-4.1

[~benedict] do you have a CI run for the patch? I have just started some repeated runs for [j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1989/workflows/a6c7d6fa-08d0-41be-8ec0-049ab71c9dd7] and [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1989/workflows/994d3066-f24c-460e-b251-8491d24a9890] with the aforementioned CircleCI config file:
{code:java}
.circleci/generate.sh -m \
  -e REPEATED_UTEST_TARGET=test-jvm-dtest-some \
  -e REPEATED_UTEST_COUNT=500 \
  -e REPEATED_UTEST_CLASS=org.apache.cassandra.distributed.test.CASTest \
  -e REPEATED_UTEST_METHODS=testConflictingWritesWithStaleRingInformation
{code}

It seems that the patched test is still failing with both the {{CasWriteTimeoutException}} and the {{{}ShutdownException{}}}.

The {{ShutdownException}} is a problem with node shutdown, so I don't think it is relevant to this ticket. I have not attempted to address these issues (I actually see at least two distinct shutdown problems)

These seem to account for most of the failures now, so perhaps there is some progress?

Unfortunately I have no idea how to run your repeated tests in CircleCI, and cannot reproduce locally, so have been guided so far entirely by log output. Perhaps you can explain to me how to use your tool, so I can run again with TRACE output to see what might be happening in this timeout case, if it is indeed different?


{quote}Unfortunately I have no idea how to run your repeated tests in CircleCI, and cannot reproduce locally, so have been guided so far entirely by log output. Perhaps you can explain to me how to use your tool, so I can run again with TRACE output to see what might be happening in this timeout case, if it is indeed different?
{quote}
In order to run the test in a loop, the job we pushed, you need to cherry-pick [this commit|https://github.com/adelapena/cassandra/commit/b026210655e8759e5f47d8eb072c0ae954ee3f52] which was created by running this command:
{code:java}
.circleci/generate.sh -m \
-e REPEATED_UTEST_TARGET=test-jvm-dtest-some \
-e REPEATED_UTEST_COUNT=500 \
-e REPEATED_UTEST_CLASS=org.apache.cassandra.distributed.test.CASTest \
-e REPEATED_UTEST_METHODS=testConflictingWritesWithStaleRingInformation{code}
Then when you push your branch and generate the workflows in CircleCI (as every other patch), you go to either
 
_java_11_separate_tests_ or _java8_separate_tests_ workflow - depends on which JDK you want to use at this point, and press first either _start_j8_build_ or respectively _start_j11_build_
Then you press and approve the following job - _start_j11_repeated_utest_ or again respectively _start_j8_repeated_utest_
 
This will run the test 500 times
 
Also, I would like to mention there is a [readme|https://github.com/adelapena/cassandra/blob/trunk/.circleci/readme.md] in the CircleCI in-tree folder, also more info and examples how to run tests from the different suites in a loop  can be found in [config-2_1.yml|https://github.com/adelapena/cassandra/blob/trunk/.circleci/config-2_1.yml#L47-L99]
 
These jobs are really useful and saved us a lot of time with flaky tests. I would strongly recommend anyone to spend 15 minutes to read through things as I am sure those are really super useful. Please let me know if you have any questions or concerns. I will be happy to help
 

For TRACE, you'll need to add something like [this commit|https://github.com/ekaterinadimitrova2/cassandra/commit/2a09ac31d2219d427a23e5baa6e6708c954eb875].

I have been able to reproduce the CAS timeout locally too, by just running the test in a shell loop:
{code:java}
for i in {1..500}
do
  ant test-jvm-dtest-some -Dtest.name="org.apache.cassandra.distributed.test.CASTest" -Dtest.methods="testConflictingWritesWithStaleRingInformation" 
done
{code}
It can take a while to hit a failure, though. The CircleCI job does basically the same but using multiple parallel runners.
 

^I wasn't able to repro locally with the {{RepeatableRunner}} #justfyi

Ok, I was on the right track but I had intercepted the problem at the wrong point. I have pushed a new commit to the same branch, and the latest test run is only failing due to the hints service shutdown problem.

(Which isn't really a problem, and might be something we can just choose to ignore, though probably shutdown should avoid a sequence of events that permits a hint to be submitted to a shutdown hint service, I think that's out of the scope of this ticket)

Indeed that seems to fix the CAS timeouts in {{{}testConflictingWritesWithStaleRingInformation{}}}, leaving only the shutdown errors (CI for [j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1998/workflows/16da8bc2-ef5a-4ceb-b4fa-6fba61f30bed] and [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1998/workflows/c515b1e9-484f-4d4e-b2fa-a13721c16718]).

If we repeatedly run the entire {{CASTest}} class, other tests still hit CAS timeouts (CI for [j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1999/workflows/491c56ef-a6f9-44a5-803f-2ad475279147] and [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1999/workflows/577321bb-26f9-42ec-ba3a-6dca1f111968]). However, in this case the shutdown errors don't seem to reproduce anymore.

The CircleCI config file for running the entire test class has been generated with:
{code:java}
.circleci/generate.sh -m \
  -e REPEATED_UTEST_TARGET=test-jvm-dtest-some \
  -e REPEATED_UTEST_COUNT=500 \
  -e REPEATED_UTEST_CLASS=org.apache.cassandra.distributed.test.CASTest
{code}
 

I took a shot a simplifying the logic in waiting for node4 to be marked up [here|https://github.com/driftx/cassandra/commit/038e1c0e8e4fab1ef2d3c03c0c8907da08c42f1b] which also is left with the [shutdown errors|https://app.circleci.com/pipelines/github/driftx/cassandra/590/workflows/041b21e3-530c-4a8b-9194-b55bdf37f83e/jobs/6922].

Unfortunately that's no longer testing anything...

We sort of need the ownership discrepancy to be discovered by the Paxos operation, not to be applied before it starts.

I think the problem is the paxos operation doesn't cause discovery, if we just wait for node4 without adding it ourselves, it will never come up.  Marking it up yourself during the paxos operation won't actually happen in practice, I don't think.

I'm not sure what the correct solution is but I think there's a better way to accomplish it than a custom FD and inspecting the stack.

It _does_ cause discovery, but we have a weird behaviour (that is almost certainly wrong in most cases) where we mark a node as _down_ after any major state change and then ping it via echo before marking it as alive. We are racing with that latter part.

You can clearly see in the TRACE logs that the Paxos operation detects the divergent ring, propagates this to {{Gossiper}}, and correctly waits for pending ranges to be recomputed and gossip state applied. Unfortunately, we don't have an easy way to make it also wait for {{realMarkAlive}}. We could modify the actual behaviour of major state changes, but while I think that might be more correct, I don't want to touch it just for this test and don't have the time for the proper duty of care needed for such changes.

bq. We sort of need the ownership discrepancy to be discovered by the Paxos operation

Can you help me understand why this is the case?  At this point in time, in node1's view, node4 has been decommissioned/assassinated and is in the LEFT state, but node4's actual state is unchanged from NORMAL so any gossip exchange between node1 and node4 will cause the state change back to the correct state of NORMAL.  This doesn't have to be from a Paxos operation though (and in the real world, may not get so lucky in timing to be from one) and it looks like the other 'stale ring' tests are explicitly adding node4 back, only this one does not.

bq. in node1's view, node4 has been decommissioned/assassinated and is in the LEFT state

No, node1's view has node4 as never having joined the ring, as we invoke {{unsafeAnnulEndpoint}} after updating the token information via the LEFT state. This is just a convenient way to simulate node4 joining without node1 receiving the gossip state.

bq. any gossip exchange between node1 and node4 will cause the state change back to the correct state of NORMAL ... and in the real world, may not get so lucky in timing to be from one

You have it backwards. Ordinary operations _depend_ on gossip disseminating this promptly for correctness. We are simulating this having not happened yet, leading to an inconsistent view of the token ring and permitting operations to reach consensus without overlapping quorums, due to token ownership disagreements. Paxos operations (which must be linearizable) are now able to detect this inconsistency themselves and enforce it, by updating the gossip state directly.

Note that the {{Timeout}} is also a _valid outcome_ for this test, just a bad one for validating everything else is working correctly. The reason the {{Timeout}} occurs is that we explicitly drop messages to one of the _correct_ owners to ensure we can only succeed by contacting the _new_ owner, and if this race occurs we drop this message too, leaving only one correct owner that can respond.

bq. it looks like the other 'stale ring' tests are explicitly adding node4 back, only this one does not.

Yes, they are testing other things.



bq. other tests still hit CAS timeouts

Some of these at least are probably similar situations to this test. I will try to take a look at these, but probably not until the end of the coming week or so.

In the remaining timeout failures I noticed that these appeared to also be racing with node4 being marked up, so I created a [branch|https://github.com/driftx/cassandra/tree/CASSANDRA-17461-4.1] that waits for it after it has been added back to the ring, but now the same tests [fail|https://app.circleci.com/pipelines/github/driftx/cassandra/605/workflows/006a6d26-0a2d-4c5b-9ed2-ac04c99227b0/jobs/6990] in a different way that I haven't figured out yet, but there's TRACE logging included on that test run.

Thanks. I ran a CASTest loop and found the following tests to fail sometimes:

[testSucccessfulWriteDuringRangeMovementFollowedByRead|https://app.circleci.com/pipelines/github/belliottsmith/cassandra/316/workflows/4f40397c-5caf-4eeb-8e7d-608b30796f77/jobs/7110]
[testSuccessfulWriteDuringRangeMovementFollowedByConflicting|https://app.circleci.com/pipelines/github/belliottsmith/cassandra/317/workflows/27c83972-4006-46be-9c78-91adae6b93c8/jobs/7111]
[testSuccessfulWriteBeforeRangeMovement|https://app.circleci.com/pipelines/github/belliottsmith/cassandra/318/workflows/de219103-e7a2-4767-b28b-b774167afbae/jobs/7109]
[testIncompleteWriteFollowedBySuccessfulWriteWithStaleRingDuringRangeMovementFollowedByRead|https://app.circleci.com/pipelines/github/belliottsmith/cassandra/319/workflows/9ee97d36-d2f8-4d90-a64f-0f873344239a/jobs/7108]

I have run them in a loop with TRACE logging, though they mostly _didn't_ fail when being run on their own, even with 500 iterations. Only {{testSuccessfulWriteBeforeRangeMovement}} reliably fails, and testSuccessfulWriteDuringRangeMovementFollowedByConflicting failed once. The other two did not fail at all, ignoring the CASTest-level shutdown failures. 

I will find the time at some point to dig further into all of these outputs.

Since the race isn't really a bug, just something that can happen in this situation, I've reworked the affected tests to tolerate the race. Instead of requiring the query to succeed, it will retry a few times on timeouts, but also assert the expected ring state before and after the query. The ring change tests no longer fail in circle, even after 600 runs.

https://github.com/bdeggleston/cassandra/tree/C17461

I feel like this weakens the tests, but I think it _should_ still catch a regression _in principle_. Have we confirmed these new versions fail with v1, as they should?

{quote}I feel like this weakens the tests{quote}

How so? We're interested in whether the paxos operation updates the ring, and doesn't complete operations with stale topology, which this still tests. The tests all fail with v1, as expected. 

Well, since it should fix it for the first operation it leaves some room for some consistent breakage to occur that reliably breaks the first operation (rather than infrequently).

BUT, quite evidently we don't have the time to agonise over these tests and the alternative is probably annotating them to be ignored, and it's only a slight weakening. So +1, let's ship 4.1

This is the only ticket blocking the release of 4.1, is there anything left to be done here?

