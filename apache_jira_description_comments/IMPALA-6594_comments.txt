FWIW, I noticed that {{test_runtime_filter}} has also started failing intermittently with KRPC enabled. It was passing quite consistently before.

{noformat}
Stacktrace
query_test/test_runtime_filters.py:168: in test_row_filters
    test_file_vars={'$RUNTIME_FILTER_WAIT_TIME_MS' : str(WAIT_TIME_MS)})
common/impala_test_suite.py:397: in run_test_case
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:612: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:160: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:173: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:341: in __execute_query
    self.wait_for_completion(handle)
beeswax/impala_beeswax.py:361: in wait_for_completion
    raise ImpalaBeeswaxException("Query aborted:" + error_log, None)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    Query aborted:Memory limit exceeded: ParquetColumnReader::ReadDataPage() failed to allocate 65519 bytes for decompressed data.
E   HDFS_SCAN_NODE (id=0) could not allocate 63.98 KB without exceeding limit.
E   Error occurred on backend ec2-m2-4xlarge-centos-6-4-1637.vpc.cloudera.com:22001 by fragment ca4c41a3292707df:5279786f00000002
E   Memory left in process limit: 16.98 GB
E   Memory left in query limit: 12.42 KB
E   Query(ca4c41a3292707df:5279786f00000000): Limit=200.00 MB Reservation=160.00 MB ReservationLimit=160.00 MB OtherMemory=39.99 MB Total=199.99 MB Peak=199.99 MB
E     Fragment ca4c41a3292707df:5279786f00000008: Reservation=134.00 MB OtherMemory=30.22 MB Total=164.22 MB Peak=164.22 MB
E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB
E       AGGREGATION_NODE (id=3): Total=4.00 KB Peak=4.00 KB
E         Exprs: Total=4.00 KB Peak=4.00 KB
E       HASH_JOIN_NODE (id=2): Reservation=132.00 MB OtherMemory=198.25 KB Total=132.19 MB Peak=132.19 MB
E         Exprs: Total=25.12 KB Peak=25.12 KB
E         Hash Join Builder (join_node_id=2): Total=157.12 KB Peak=157.12 KB
E           Hash Join Builder (join_node_id=2) Exprs: Total=149.12 KB Peak=149.12 KB
E       EXCHANGE_NODE (id=4): Reservation=14.91 MB OtherMemory=67.64 KB Total=14.97 MB Peak=14.97 MB
E         KrpcDataStreamRecvr: Total=67.64 KB Peak=67.64 KB
E       EXCHANGE_NODE (id=5): Reservation=15.03 MB OtherMemory=0 Total=15.03 MB Peak=15.10 MB
E         KrpcDataStreamRecvr: Total=0 Peak=68.75 KB
E       KrpcDataStreamSender (dst_id=6): Total=16.00 KB Peak=16.00 KB
E     Fragment ca4c41a3292707df:5279786f00000002: Reservation=26.00 MB OtherMemory=9.76 MB Total=35.76 MB Peak=35.76 MB
E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB
E       HDFS_SCAN_NODE (id=0): Reservation=24.00 MB OtherMemory=9.13 MB Total=33.13 MB Peak=33.13 MB
E         Exprs: Total=260.00 KB Peak=260.00 KB
E       KrpcDataStreamSender (dst_id=4): Total=458.57 KB Peak=458.57 KB
E         KrpcDataStreamSender (dst_id=4) Exprs: Total=256.00 KB Peak=256.00 KB
E     Fragment ca4c41a3292707df:5279786f00000005: Reservation=0 OtherMemory=0 Total=0 Peak=29.53 MB
E       HDFS_SCAN_NODE (id=1): Reservation=0 OtherMemory=0 Total=0 Peak=29.26 MB
E       KrpcDataStreamSender (dst_id=5): Total=0 Peak=266.57 KB
E   
E   Memory limit exceeded: ParquetColumnReader::ReadDataPage() failed to allocate 65519 bytes for decompressed data.
E   HDFS_SCAN_NODE (id=0) could not allocate 63.98 KB without exceeding limit.
E   Error occurred on backend ec2-m2-4xlarge-centos-6-4-1637.vpc.cloudera.com:22001 by fragment ca4c41a3292707df:5279786f00000002
E   Memory left in process limit: 16.98 GB
E   Memory left in query limit: 12.42 KB
E   Query(ca4c41a3292707df:5279786f00000000): Limit=200.00 MB Reservation=160.00 MB ReservationLimit=160.00 MB OtherMemory=39.99 MB Total=199.99 MB Peak=199.99 MB
E     Fragment ca4c41a3292707df:5279786f00000008: Reservation=134.00 MB OtherMemory=30.22 MB Total=164.22 MB Peak=164.22 MB
E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB
E       AGGREGATION_NODE (id=3): Total=4.00 KB Peak=4.00 KB
E         Exprs: Total=4.00 KB Peak=4.00 KB
E       HASH_JOIN_NODE (id=2): Reservation=132.00 MB OtherMemory=198.25 KB Total=132.19 MB Peak=132.19 MB
E         Exprs: Total=25.12 KB Peak=25.12 KB
E         Hash Join Builder (join_node_id=2): Total=157.12 KB Peak=157.12 KB
E           Hash Join Builder (join_node_id=2) Exprs: Total=149.12 KB Peak=149.12 KB
E       EXCHANGE_NODE (id=4): Reservation=14.91 MB OtherMemory=67.64 KB Total=14.97 MB Peak=14.97 MB
E         KrpcDataStreamRecvr: Total=67.64 KB Peak=67.64 KB
E       EXCHANGE_NODE (id=5): Reservation=15.03 MB OtherMemory=0 Total=15.03 MB Peak=15.10 MB
E         KrpcDataStreamRecvr: Total=0 Peak=68.75 KB
E       KrpcDataStreamSender (dst_id=6): Total=16.00 KB Peak=16.00 KB
E     Fragment ca4c41a3292707df:5279786f00000002: Reservation=26.00 MB OtherMemory=9.76 MB Total=35.76 MB Peak=35.76 MB
E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB
E       HDFS_SCAN_NODE (id=0): Reservation=24.00 MB OtherMemory=9.13 MB Total=33.13 MB Peak=33.13 MB
E         Exprs: Total=260.00 KB Peak=260.00 KB
E       KrpcDataStreamSender (dst_id=4): Total=458.57 KB Peak=458.57 KB
E         KrpcDataStreamSender (dst_id=4) Exprs: Total=256.00 KB Peak=256.00 KB
E     Fragment ca4c41a3292707df:5279786f00000005: Reservation=0 OtherMemory=0 Total=0 Peak=29.53 MB
E       HDFS_SCAN_NODE (id=1): Reservation=0 OtherMemory=0 Total=0 Peak=29.26 MB
E       KrpcDataStreamSender (dst_id=5): Total=0 Peak=266.57 KB
{noformat}

Commit 1e126dacc0333c57d7f69ec89bffde2f5f018bdc in impala's branch refs/heads/2.x from [~tarmstrong@cloudera.com]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=1e126da ]

IMPALA-6594: fix tests on local fs

Skip most mem_usage_scaling tests, which are tuned for the memory
requirements on 3 daemons. Also skip test_sort_reservation_usage,
which similarly is tuned for 3 daemons.

Fix the other test to wrap the HDFS path correctly.

Testing:
Ran the modified tests by hand against the minicluster to confirm
they still ran as expected. Running full set of local tests.

Change-Id: I76086ed695bf78e3e0f2745c1964dac8330d6c19
Reviewed-on: http://gerrit.cloudera.org:8080/9463
Reviewed-by: Dan Hecht <dhecht@cloudera.com>
Tested-by: Impala Public Jenkins


Commit 66d222ec9df20e71c5e6ec816af17472adc6d808 in impala's branch refs/heads/master from [~tarmstrong@cloudera.com]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=66d222e ]

IMPALA-6594: fix tests on local fs

Skip most mem_usage_scaling tests, which are tuned for the memory
requirements on 3 daemons. Also skip test_sort_reservation_usage,
which similarly is tuned for 3 daemons.

Fix the other test to wrap the HDFS path correctly.

Testing:
Ran the modified tests by hand against the minicluster to confirm
they still ran as expected. Running full set of local tests.

Change-Id: I76086ed695bf78e3e0f2745c1964dac8330d6c19
Reviewed-on: http://gerrit.cloudera.org:8080/9463
Reviewed-by: Dan Hecht <dhecht@cloudera.com>
Tested-by: Impala Public Jenkins


Fixed the localfs ones, blocked by IMPALA-6595 for the NLJ one

Fixed by revert. Will re-run the tests on the new version of the batch.

