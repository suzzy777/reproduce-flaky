 
 - phi_convict_threshold is actually time but I don't think we should touch it considering its nature.
 - memtable_cleanup_threshold is marked deprecated in cassandra.yaml, so should be just marked also internally in Config as Deprecated I guess
 - native_transport_max_concurrent_requests_in_bytes_per_ip and native_transport_max_concurrent_requests_in_bytes - both have assigned same value if they are set <=0 so easy transfer to the new framework IMO
 - These ones should be transferred too - native_transport_receive_queue_capacity_in_bytes, user_defined_function_warn_timeout, user_defined_function_fail_timeout, validation_preview_purge_head_start_in_sec
 - Unfortunately block_for_peers_timeout_in_secs is a problematic one. I will have to think about it more because of the -1 used there. We can't just migrate it to "0 means disabled" as 0 already has another meaning. More in the java doc in Config class
 - _repair_request_timeout_in_ms_ - is a new one. Checking with the author. 

CC [~maedhroz] , [~dcapwell] and [~mck] 
Just FYI 

So the patch can be found [here|https://github.com/ekaterinadimitrova2/cassandra/pull/194], respective properties were added in the ccm dictionary  [here|https://github.com/ekaterinadimitrova2/ccm/pull/2] in order to satisfy the backward compatibility. 

I tried to split into different commits different changes and a few fixes I did, I had to revert one down the way, please, ignore it.

I left PR commits not squashed so that reviewers can check also individual commits, to be maybe easier and shorter. Let me know if you prefer everything squashed.
 * As I mentioned already, I didn't touch phi_convict_threshold and meltable_cleanup_threshold.
 * I pinged the author about _repair_request_timeout_in_ms,_ it seems that one is not needed so he will open a ticket to remove it actually.  __ 
 * While working on _block_for_peers_timeout_in_secs_ I realized I have a bug in MILLIS_CUSTOM_DURATION, -1 should mean null, not 0 which would change the meaning. This led me also to a few more checks to be added in the DatabaseDescriptor. While we don't advertise that negative numbers except -1 can be used, we weren't really prohibiting it so I am throwing an exception now in the setter. Please check if you think this is valid way to handle this and I am not breaking anything. I added one more unit test to cover this special case. The idea for the null usage came from [~dcapwell], also one  test added in the YAMLConfigurationLoaderTest is his. 
 * I ended up not transferring block_for_peers_timeout_in_secs as I noticed that it can handle even negative numbers which are not rejecter internally by TimeUnit so I am hesitant to touch this advanced parameter. I will probably discuss it on the mailing list which I plan to hit after this patch to confirm with people all special cases we have to validate things before the release. 
 * While, currently it is not a problem with the current config, we shouldn't be assigning 0ms in case a property is null but keep it null. This led me to a change in the MILLIS_DOUBLE_DURATION in regards to the NaN handling for backward compatibility. While on that I removed the handling of 0 which was supposed to be accepted without unit too. I remember we were discussing that 0 with a unit might be weird so I can return it back if we think it is better to be there. This led to broken Paxos tests where 0 without a unit was used. I fixed those to have a unit but if we prefer really the old way, I will revert the change.
 * I am not 100% sure whether there might be some issue with the new Converter NEGATIVE_SECONDS_DURATION for validation_preview_purge_head_start_in_sec. I didn't break any tests but that is not a guarantee as we know... Any check/advice/suggestion/confirmation will be highly appreciated.
 *  native_transport_receive_queue_capacity_in_bytes - as I left a comment also in the code (in order not to forget to mention it during review :) ), this one is not guarded and mentioned whether it can be negative, etc anywhere. I definitely have to confirm it with the author/community before doing anything. 

 [Here|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1487/workflows/ae03b64c-1300-4ed8-a324-4b1ccd176d47] is a preliminary J8 run, I will run also J11 when we are done with the review. I was running them down the road. So the only failures that are not already presented on trunk and which don't have tickets are the Paxos tests failures and the usage of 0 I mentioned. I pushed commit to fix them after the CI run.  

 

[~dcapwell], [~maedhroz] , I think this is ready for review and discussions around the corner cases, thank you! 

 

Finished first pass, overall LGTM, most comments are small

Thank you [~dcapwell], addressing them now and added your commit to the PR already! 
Did you also have the chance to look at my questions in the previous comment?

CCM patch LGTM, assuming the name suggestions from [~maedhroz] is dealt with (either no change, or change gets reflected to CCM as well)

+1 on the trunk patch (given we've reached agreement on the native transport reserve queue stuff on Slack) and the CCM patch (which will probably need a couple naming tweaks, as mentioned above)

+1, assuming tests are passing

Thanks, if there are no concerns on the mailing list, I will continue with the plan we agreed on there.

I will also run last CI with ccm retagged in my branch, to ensure there is no new funny surprise to deal with later :) 

cool, thanks for all the hard work!

Rebased and squashed in this new  [branch|https://github.com/apache/cassandra/commit/6a8851180b10546036b0771b21ec701096b2baf0](left the old one for reference around the review discussions, will update CHANGES.txt, commit message, authorship, etc at the end), following changes for your consideration done:
 * I didn't add an entry that we got rid of the option to set 0 without unit for Duration and DataStorage as it wasn't advertised before but leaving here for the record. This led to change in MILLIS_DOUBLE_DURATION to handle NaN which was a valid value for commitlog_sync_group_window_in_ms
 * As part of the patch we took care of a bug with permissions_update_interval, credentials_update_interval, roles_update_interval - former -1 should point to null and not 0 which was and it is a valid value. Also, their setter in DatabaseDescriptors match the startup behavior and throw IllegalArgumentException, instead of ConfigurationException from DurationSpec.
 * setNativeTransportReceiveQueueCapacityInBytes - called only by tests, so I didn't do anything about Configuration exception
 * I added to the Setters for roles_update_interval, credentials_update_interval, permissions_update_interval IllegalArgumentEception instead of ConfigurationException until 
 * native_transport_max_request_data_in_flight_per_ip - changed to default in case of -1, rejects anything less than -1. Considered as bug. Communicated on the mailing list. Added comment in Config. Is JavaDoc better? Exception will be thrown as IllegalArgumentException, the setter is called by ClientResourceLimits#setEndpointLimit, called by StorageService#setNativeTransportMaxConcurrentRequestsInBytesPerIp. Reflected the change also in BYTES_CUSTOM_DATASTORAGE Converter
 * The setters setUserDefinedFunctionFailTimeout and setUserDefinedFunctionWarnTimeout are called only by test functions so I didn't take care explicitly of the ConfigurationException.
 * validation_preview_purge_head_start - the Converter created for it NEGATIVE_SECONDS_DURATION sets to 0 anything negative as the only usage of this parameter is in getValidationPreviewPurgeHeadStartInSec where up to now we were doing
{code:java}
 int seconds = conf.validation_preview_purge_head_start_in_sec;  return Math.max(seconds, 0);{code}

 * Commit from David to make Converters type safe and fixed a few cases where converters used the wrong type
 * Tests for all branches of all Converters for the convert part added. I guess for the deconvert part for the Virtual Table we have the methods tested and they don't have branches, except MILLIS_CUSTOM_DURATION. I added also testSetConfigDisabled().

CCM commits squashed, typo fixed, new tag [cassandra-test-17431|https://github.com/ekaterinadimitrova2/ccm/tags] created in my branch for testing.

[J8 |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1508/workflows/37edd685-8049-4af7-8099-b47a67aab7c0]all test failures are known and have tickets. I will push J11 on final confirmation.

[Upgrade tests|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1508/workflows/bb62e69c-0dc0-4e55-b847-f2b405f87646/jobs/9815/tests] restarted and finished as expected. It seems there was git issue that [~dcapwell] hit also with another ticket and the original ccm. 

The summary and last squashed version were confirmed in Slack and got final +1 from [~maedhroz]  and [~dcapwell] .

Rebased, squashed, updated CHANGES.txt and NEWS.txt on this branch - https://github.com/ekaterinadimitrova2/cassandra/tree/17431-rebased

Running final CI:

[j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1551/workflows/52b83325-4839-43eb-8fb1-89aa1a0e0da9] , [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1551/workflows/46f39f4b-607c-4fc6-bee8-5911cce81bed] 

CI still running, on satisfying results(no new failures introduced by this patch), I will commit. 

Something nasty was going on after the rebase and changing few times Circle config... (Circle config has changed last week so I guess during rebase things got really messy without a warning for me....)

Cleaned circle config and raised to mid resources again plus rebased the dtest repo as I use it to test the ccm retagging...

My luck these days with CI is -100, hopefully this time it will work out:

J8 - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1554/workflows/7421c982-2892-4669-b283-fda105e3a1fc]

J11 - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1554/workflows/69b84b10-fff1-40e8-9658-369b23e6146a]

J8 with vnodes failing like crazy due to unavailable hosts 

[Restarted|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1554/workflows/7421c982-2892-4669-b283-fda105e3a1fc/jobs/10217/tests]... now failing claiming with fail to start, but when looking into nodes logs they have started.... Have to dig in more, this is crazy...

Cool, it seems midres config got messy after a patch last week. Seems it is not me. But it's already 10 and I see issues with at least three suites now....

I will test with high resources as those were tested that they work and follow up on config tomorrow.... [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1556/workflows/95e9f3d7-ad33-47d1-a866-1ca00a93820c] , [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1556/workflows/6173b784-6ee6-4d3b-bcb2-b8ac3c165fca] 

Still not finished but I noticed testCDCIndexFileWriteOnSync failing more regularly with this patch. I will have to investigate it tomorrow morning. 

So I reran in a loop the test on trunk with the patch:

130 times out of 4000 J8 and 89 times out of 4000 J11 on J8 - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1557/workflows/a1508c6d-7d81-4f05-b6bf-e763ca73d4d0]

149 on J11 - 80 containers failing - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1557/workflows/d7d7b147-79a2-4711-8394-a1747c1db140]

and without the patch:

109 times out of 4000 J8 and 105 times out of 4000 J11 on J8 - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1558/workflows/6b89f85b-083e-4ed6-9493-68e39118f56d]

J11 - Circle shows only failed containers - 86 and not number of failed tests but considering the amount of failed containers I don't expect different results. - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1558/workflows/acc8c0b8-7387-4470-902e-c5ffd33a7052/jobs/10308/parallel-runs/0?filterBy=FAILED]

No new errors - only the two types we've seen before, the null pointer and the assertion error.

I think my suspicion was wrong. Now running in the loop 1 upgrade test which I haven't seen before. The rest are known issues which have tickets already. 

Without the patch - https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1561/workflows/bdfaad5a-c46c-4606-9f8b-1c6cdd7c2886

With the patch - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1560/workflows/251996d8-b57c-4c35-b293-e1fd0d158a1c]

Not reproducible... to be on the safe side pushed 4000 times with the patch here - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1562/workflows/847459c5-16aa-43d8-bbdd-92e7165131d0]

I will check it back tomorrow morning how it ended up and commit if there are no new surprises

Pushed to trunk 4000 times too as it failed 5 times with my patch....

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1563/workflows/61bda0b7-f699-4897-877f-c7d523a03127

4 out of 4000.... so I just hit an extremely rarely flaky test. But the error seems legit so probably worth to check it, also it might be failing more with other resources. I will open a ticket after I close this one. Starting commit. 

CCM patch  [committed|https://github.com/riptano/ccm/commit/4609ac46beccd3ab86463a345ed59e998d664d04] and CCM [retagged|https://github.com/riptano/ccm/tags]

trunk patch [committed|https://github.com/apache/cassandra/commit/dac738d2eba8629d4f482d7cbfd855d2c5b9df47] second so that the CI acknowledges the CCM changes:

To https://github.com/apache/cassandra.git

   03ef67c9d5..dac738d2eb  trunk -> trunk

*I had a mistake in the 3 setters({_}setPermissionsUpdateInterval{_}, _setCredentialsUpdateInterval_ and _setRolesUpdateInterval_), was ignoring to throw the exception on negative values (which we do currently on trunk). I didn't rerun the CI as this just mimics the current trunk. Please let me know if you have any concerns.* 

 

Ticket for the flaky test was opened CASSANDRA-17565

