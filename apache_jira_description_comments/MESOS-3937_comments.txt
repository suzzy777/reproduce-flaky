I have this test passed in my ubuntu14.04 VM (clang-3.6 + docker 1.7.1).

Is your ubuntu running on vagrant?

I have that test also failing (100%) on a vmware fusion box; exact same OS, compiler and docker configuration as tested by Bernd -- but not the same image / machine.

So the slave sends the run task message to the executor;

{noformat}
I1117 15:30:16.962846 72626 slave.cpp:1793] Sending queued task '1' to executor 'e1' of framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000 at executor(1)@192.168.178.39:57979
{noformat}

But the executor chokes on that message as evident in the executor's sandbox stderr file;

{noformat}
$ cat /tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/slaves/0858c91e-303d-4b36-b940-4f96102ffc9a-S0/frameworks/0858c91e-303d-4b36-b940-4f96102ffc9a-0000/executors/e1/runs/latest/stderr
WARNING: Your kernel does not support swap limit capabilities. Limitation discarded.
E1117 23:30:16.965016 00007 messenger.go:203] Failed to unmarshal message &{slave(1)@192.168.178.39:38758 mesos.internal.RunTaskMessage framework:<user:"root" name:"default" id:<value:"0858c91e-303d-4b36-b940-4f96102ffc9a-0000" > hostname:"ubuntu" principal:"test-principal" > pid:"scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758" task:<name:"" task_id:<value:"1" > slave_id:<value:"0858c91e-303d-4b36-b940-4f96102ffc9a-S0" > resources:<name:"cpus" type:SCALAR scalar:<value:2 > role:"*" > resources:<name:"mem" type:SCALAR scalar:<value:1024 > role:"*" > resources:<name:"disk" type:SCALAR scalar:<value:1024 > role:"*" > resources:<name:"ports" type:RANGES ranges:<range:<begin:31000 end:32000 > > role:"*" > executor:<executor_id:<value:"e1" > framework_id:<value:"0858c91e-303d-4b36-b940-4f96102ffc9a-0000" > command:<value:"/bin/test-executor" > 11:"\b\x01\x1a\x17\n\x15tnachen/test-executor" > >  [18 84 10 4 114 111 111 116 18 7 100 101 102 97 117 108 116 26 43 10 41 48 56 53 56 99 57 49 101 45 51 48 51 100 45 52 98 51 54 45 98 57 52 48 45 52 102 57 54 49 48 50 102 102 99 57 97 45 48 48 48 48 58 6 117 98 117 110 116 117 66 14 116 101 115 116 45 112 114 105 110 99 105 112 97 108 26 67 115 99 104 101 100 117 108 101 114 45 102 57 52 51 52 100 52 52 45 98 49 57 53 45 52 54 49 54 45 98 49 48 53 45 99 52 98 50 99 53 56 100 53 99 100 57 64 49 57 50 46 49 54 56 46 49 55 56 46 51 57 58 51 56 55 53 56 34 251 1 10 0 18 3 10 1 49 26 41 10 39 48 56 53 56 99 57 49 101 45 51 48 51 100 45 52 98 51 54 45 98 57 52 48 45 52 102 57 54 49 48 50 102 102 99 57 97 45 83 48 34 22 10 4 99 112 117 115 16 0 26 9 9 0 0 0 0 0 0 0 64 50 1 42 34 21 10 3 109 101 109 16 0 26 9 9 0 0 0 0 0 0 144 64 50 1 42 34 22 10 4 100 105 115 107 16 0 26 9 9 0 0 0 0 0 0 144 64 50 1 42 34 24 10 5 112 111 114 116 115 16 1 34 10 10 8 8 152 242 1 16 128 250 1 50 1 42 42 102 10 4 10 2 101 49 58 20 26 18 47 98 105 110 47 116 101 115 116 45 101 120 101 99 117 116 111 114 66 43 10 41 48 56 53 56 99 57 49 101 45 51 48 51 100 45 52 98 51 54 45 98 57 52 48 45 52 102 57 54 49 48 50 102 102 99 57 97 45 48 48 48 48 90 27 8 1 26 23 10 21 116 110 97 99 104 101 110 47 116 101 115 116 45 101 120 101 99 117 116 111 114]}: proto: required field "{Unknown}" not set
E1117 23:30:23.956910 00007 slave_health_checker.go:76] Failed to request the health path: Get http://192.168.178.39:38758/slave%281%29/health: dial tcp 192.168.178.39:38758: connection refused
E1117 23:30:24.956811 00007 slave_health_checker.go:76] Failed to request the health path: Get http://192.168.178.39:38758/slave%281%29/health: dial tcp 192.168.178.39:38758: connection refused
E1117 23:30:25.956580 00007 slave_health_checker.go:76] Failed to request the health path: Get http://192.168.178.39:38758/slave%281%29/health: dial tcp 192.168.178.39:38758: connection refused
E1117 23:30:26.956360 00007 slave_health_checker.go:76] Failed to request the health path: Get http://192.168.178.39:38758/slave%281%29/health: dial tcp 192.168.178.39:38758: connection refused
E1117 23:30:27.956601 00007 slave_health_checker.go:76] Failed to request the health path: Get http://192.168.178.39:38758/slave%281%29/health: dial tcp 192.168.178.39:38758: connection refused
E1117 23:30:28.956637 00007 slave_health_checker.go:76] Failed to request the health path: Get http://192.168.178.39:38758/slave%281%29/health: dial tcp 192.168.178.39:38758: connection refused
E1117 23:30:29.956780 00007 slave_health_checker.go:76] Failed to request the health path: Get http://192.168.178.39:38758/slave%281%29/health: dial tcp 192.168.178.39:38758: connection refused
E1117 23:30:30.956872 00007 slave_health_checker.go:76] Failed to request the health path: Get http://192.168.178.39:38758/slave%281%29/health: dial tcp 192.168.178.39:38758: connection refused
E1117 23:30:31.956916 00007 slave_health_checker.go:76] Failed to request the health path: Get http://192.168.178.39:38758/slave%281%29/health: dial tcp 192.168.178.39:38758: connection refused
E1117 23:30:32.956914 00007 slave_health_checker.go:76] Failed to request the health path: Get http://192.168.178.39:38758/slave%281%29/health: dial tcp 192.168.178.39:38758: connection refused
{noformat}


My GLOG_v=2 logging up until about 5 seconds after the RunTaskMessage was sent;

{noformat}
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Launch_Executor
I1117 15:30:15.670204 72611 process.cpp:2426] Spawned process files@192.168.178.39:38758
I1117 15:30:15.670322 72632 process.cpp:2436] Resuming files@192.168.178.39:38758 at 2015-11-17 23:30:15.670303232+00:00
I1117 15:30:15.670706 72632 process.cpp:2436] Resuming help@192.168.178.39:38758 at 2015-11-17 23:30:15.670699008+00:00
I1117 15:30:15.674748 72611 process.cpp:2426] Spawned process (1)@192.168.178.39:38758
I1117 15:30:15.674780 72626 process.cpp:2436] Resuming metrics@192.168.178.39:38758 at 2015-11-17 23:30:15.674764032+00:00
I1117 15:30:15.674832 72631 process.cpp:2436] Resuming (1)@192.168.178.39:38758 at 2015-11-17 23:30:15.674824960+00:00
I1117 15:30:15.679572 72611 leveldb.cpp:176] Opened db in 4.648323ms
I1117 15:30:15.680995 72611 leveldb.cpp:183] Compacted db in 1.336978ms
I1117 15:30:15.681115 72611 leveldb.cpp:198] Created db iterator in 57880ns
I1117 15:30:15.681188 72611 leveldb.cpp:204] Seeked to beginning of db in 5511ns
I1117 15:30:15.681221 72611 leveldb.cpp:273] Iterated through 0 keys in the db in 3864ns
I1117 15:30:15.681396 72611 replica.cpp:780] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1117 15:30:15.682481 72611 process.cpp:2426] Spawned process log-replica(1)@192.168.178.39:38758
I1117 15:30:15.682551 72628 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.682531072+00:00
I1117 15:30:15.682709 72611 process.cpp:2426] Spawned process (2)@192.168.178.39:38758
I1117 15:30:15.682780 72631 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.682768128+00:00
I1117 15:30:15.682833 72630 process.cpp:2436] Resuming log(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.682811904+00:00
I1117 15:30:15.682806 72611 process.cpp:2426] Spawned process log(1)@192.168.178.39:38758
I1117 15:30:15.683143 72628 process.cpp:2436] Resuming log-reader(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.683121920+00:00
I1117 15:30:15.683228 72611 process.cpp:2426] Spawned process log-reader(1)@192.168.178.39:38758
I1117 15:30:15.683290 72630 process.cpp:2426] Spawned process log-recover(1)@192.168.178.39:38758
I1117 15:30:15.683325 72611 process.cpp:2426] Spawned process log-writer(1)@192.168.178.39:38758
I1117 15:30:15.683405 72629 process.cpp:2436] Resuming log-recover(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.683397120+00:00
I1117 15:30:15.683430 72629 recover.cpp:449] Starting replica recovery
I1117 15:30:15.683691 72611 process.cpp:2426] Spawned process (3)@192.168.178.39:38758
I1117 15:30:15.683801 72625 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.683789056+00:00
I1117 15:30:15.683758 72627 process.cpp:2436] Resuming metrics@192.168.178.39:38758 at 2015-11-17 23:30:15.683720960+00:00
I1117 15:30:15.683751 72630 process.cpp:2436] Resuming (3)@192.168.178.39:38758 at 2015-11-17 23:30:15.683745024+00:00
I1117 15:30:15.683727 72631 process.cpp:2436] Resuming log-writer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.683717888+00:00
I1117 15:30:15.683367 72628 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.683360000+00:00
I1117 15:30:15.684130 72629 recover.cpp:475] Replica is in EMPTY status
I1117 15:30:15.684165 72628 process.cpp:2436] Resuming metrics@192.168.178.39:38758 at 2015-11-17 23:30:15.684158976+00:00
I1117 15:30:15.684175 72611 process.cpp:2426] Spawned process registrar(1)@192.168.178.39:38758
I1117 15:30:15.684252 72628 process.cpp:2436] Resuming registrar(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.684248064+00:00
I1117 15:30:15.684265 72611 process.cpp:2426] Spawned process standalone-master-detector(1)@192.168.178.39:38758
I1117 15:30:15.684330 72629 process.cpp:2426] Spawned process log-recover-protocol(1)@192.168.178.39:38758
I1117 15:30:15.684331 72611 process.cpp:2426] Spawned process authorizer(1)@192.168.178.39:38758
I1117 15:30:15.684448 72625 process.cpp:2436] Resuming standalone-master-detector(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.684439808+00:00
I1117 15:30:15.684495 72625 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.684491008+00:00
I1117 15:30:15.684548 72625 process.cpp:2436] Resuming log-recover-protocol(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.684545024+00:00
I1117 15:30:15.684571 72629 process.cpp:2436] Resuming authorizer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.684561920+00:00
I1117 15:30:15.684607 72625 recover.cpp:131] Starting to wait for enough quorum of replicas before running recovery protocol, expected quroum size: 1
I1117 15:30:15.684880 72626 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.684869888+00:00
I1117 15:30:15.684576 72628 process.cpp:2436] Resuming help@192.168.178.39:38758 at 2015-11-17 23:30:15.684564224+00:00
I1117 15:30:15.685303 72625 process.cpp:2426] Spawned process __latch__(2)@192.168.178.39:38758
I1117 15:30:15.685345 72629 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.685337088+00:00
I1117 15:30:15.685371 72630 process.cpp:2436] Resuming __latch__(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.685369088+00:00
I1117 15:30:15.685382 72631 process.cpp:2436] Resuming metrics@192.168.178.39:38758 at 2015-11-17 23:30:15.685380864+00:00
I1117 15:30:15.685770 72625 recover.cpp:145] Broadcasting recover request to all replicas
I1117 15:30:15.686189 72631 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.686182912+00:00
I1117 15:30:15.686419 72631 process.cpp:2426] Spawned process (4)@192.168.178.39:38758
I1117 15:30:15.686426 72630 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.686418944+00:00
I1117 15:30:15.687130 72626 process.cpp:2436] Resuming log-recover-protocol(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.687115008+00:00
I1117 15:30:15.686458 72632 process.cpp:2436] Resuming (4)@192.168.178.39:38758 at 2015-11-17 23:30:15.686445824+00:00
I1117 15:30:15.687175 72626 recover.cpp:154] Broadcast request completed
I1117 15:30:15.687371 72632 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.687362816+00:00
I1117 15:30:15.687443 72632 replica.cpp:676] Replica in EMPTY status received a broadcasted recover request from (4)@192.168.178.39:38758
I1117 15:30:15.687505 72632 process.cpp:2436] Resuming (4)@192.168.178.39:38758 at 2015-11-17 23:30:15.687500032+00:00
I1117 15:30:15.687666 72632 process.cpp:2541] Cleaning up (4)@192.168.178.39:38758
I1117 15:30:15.687731 72630 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.687725056+00:00
I1117 15:30:15.687973 72626 recover.cpp:195] Received a recover response from a replica in EMPTY status
I1117 15:30:15.688179 72632 process.cpp:2436] Resuming standalone-master-detector(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.688171008+00:00
I1117 15:30:15.688292 72632 process.cpp:2436] Resuming __latch__(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.688288000+00:00
I1117 15:30:15.688315 72632 process.cpp:2541] Cleaning up __latch__(2)@192.168.178.39:38758
I1117 15:30:15.688340 72611 process.cpp:2426] Spawned process master@192.168.178.39:38758
I1117 15:30:15.688374 72632 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.688370944+00:00
I1117 15:30:15.688411 72627 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.688402944+00:00
I1117 15:30:15.688556 72632 master.cpp:367] Master 0858c91e-303d-4b36-b940-4f96102ffc9a (ubuntu) started on 192.168.178.39:38758
I1117 15:30:15.688604 72626 process.cpp:2541] Cleaning up log-recover-protocol(1)@192.168.178.39:38758
I1117 15:30:15.688675 72611 process.cpp:2426] Spawned process __latch__(3)@192.168.178.39:38758
I1117 15:30:15.688700 72625 process.cpp:2436] Resuming __latch__(3)@192.168.178.39:38758 at 2015-11-17 23:30:15.688691968+00:00
I1117 15:30:15.688787 72611 process.cpp:2426] Spawned process __waiter__(2)@192.168.178.39:38758
I1117 15:30:15.688912 72611 process.cpp:2734] Donating thread to __waiter__(2)@192.168.178.39:38758 while waiting
I1117 15:30:15.688966 72611 process.cpp:2436] Resuming __waiter__(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.688960000+00:00
I1117 15:30:15.688647 72628 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.688643072+00:00
I1117 15:30:15.688599 72632 master.cpp:369] Flags at startup: --acls="" --allocation_interval="1secs" --allocator="HierarchicalDRF" --authenticate="true" --authenticate_slaves="true" --authenticators="crammd5" --authorizers="local" --credentials="/tmp/LiMg7A/credentials" --framework_sorter="drf" --help="false" --hostname_lookup="true" --initialize_driver_logging="true" --log_auto_initialize="true" --logbufsecs="0" --logging_level="INFO" --max_slave_ping_timeouts="5" --quiet="false" --recovery_slave_removal_limit="100%" --registry="replicated_log" --registry_fetch_timeout="1mins" --registry_store_timeout="25secs" --registry_strict="true" --root_submissions="true" --slave_ping_timeout="15secs" --slave_reregister_timeout="10mins" --user_sorter="drf" --version="false" --webui_dir="/usr/local/share/mesos/webui" --work_dir="/tmp/LiMg7A/master" --zk_session_timeout="10secs"
I1117 15:30:15.689491 72632 master.cpp:414] Master only allowing authenticated frameworks to register
I1117 15:30:15.689510 72632 master.cpp:419] Master only allowing authenticated slaves to register
I1117 15:30:15.689533 72632 credentials.hpp:37] Loading credentials for authentication from '/tmp/LiMg7A/credentials'
I1117 15:30:15.688585 72629 process.cpp:2436] Resuming log-recover(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.688580864+00:00
I1117 15:30:15.689949 72629 recover.cpp:566] Updating replica status to STARTING
I1117 15:30:15.690279 72628 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.690268928+00:00
I1117 15:30:15.691146 72632 master.cpp:458] Using default 'crammd5' authenticator
I1117 15:30:15.691460 72628 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 1.11893ms
I1117 15:30:15.691499 72632 authenticator.cpp:520] Initializing server SASL
I1117 15:30:15.691505 72628 replica.cpp:323] Persisted replica status to STARTING
I1117 15:30:15.691678 72628 process.cpp:2436] Resuming log-recover(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.691660032+00:00
I1117 15:30:15.691817 72628 recover.cpp:475] Replica is in STARTING status
I1117 15:30:15.691958 72628 process.cpp:2426] Spawned process log-recover-protocol(2)@192.168.178.39:38758
I1117 15:30:15.691985 72630 process.cpp:2436] Resuming log-recover-protocol(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.691982080+00:00
I1117 15:30:15.691988 72627 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.691979776+00:00
I1117 15:30:15.692114 72630 recover.cpp:131] Starting to wait for enough quorum of replicas before running recovery protocol, expected quroum size: 1
I1117 15:30:15.692215 72627 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.692209920+00:00
I1117 15:30:15.692323 72630 process.cpp:2426] Spawned process __latch__(4)@192.168.178.39:38758
I1117 15:30:15.692327 72627 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.692321024+00:00
I1117 15:30:15.692451 72630 recover.cpp:145] Broadcasting recover request to all replicas
I1117 15:30:15.692589 72625 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.692580864+00:00
I1117 15:30:15.692708 72625 process.cpp:2426] Spawned process (5)@192.168.178.39:38758
I1117 15:30:15.692808 72630 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.692796928+00:00
I1117 15:30:15.692893 72625 process.cpp:2436] Resuming (5)@192.168.178.39:38758 at 2015-11-17 23:30:15.692887040+00:00
I1117 15:30:15.692910 72630 process.cpp:2436] Resuming log-recover-protocol(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.692904960+00:00
I1117 15:30:15.692945 72630 recover.cpp:154] Broadcast request completed
I1117 15:30:15.693001 72625 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.692996864+00:00
I1117 15:30:15.693047 72625 replica.cpp:676] Replica in STARTING status received a broadcasted recover request from (5)@192.168.178.39:38758
I1117 15:30:15.693099 72625 process.cpp:2436] Resuming (5)@192.168.178.39:38758 at 2015-11-17 23:30:15.693095936+00:00
I1117 15:30:15.693133 72632 auxprop.cpp:73] Initialized in-memory auxiliary property plugin
I1117 15:30:15.693217 72627 process.cpp:2436] Resuming log-recover-protocol(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.693200128+00:00
I1117 15:30:15.693249 72626 process.cpp:2436] Resuming crammd5_authenticator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.693242112+00:00
I1117 15:30:15.693271 72627 recover.cpp:195] Received a recover response from a replica in STARTING status
I1117 15:30:15.693294 72625 process.cpp:2541] Cleaning up (5)@192.168.178.39:38758
I1117 15:30:15.692350 72629 process.cpp:2436] Resuming __latch__(4)@192.168.178.39:38758 at 2015-11-17 23:30:15.692343040+00:00
I1117 15:30:15.693410 72629 process.cpp:2541] Cleaning up __latch__(4)@192.168.178.39:38758
I1117 15:30:15.693449 72626 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.693441024+00:00
I1117 15:30:15.693223 72632 process.cpp:2426] Spawned process crammd5_authenticator(1)@192.168.178.39:38758
I1117 15:30:15.693549 72632 master.cpp:495] Authorization enabled
I1117 15:30:15.693677 72629 process.cpp:2436] Resuming log-recover(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.693671936+00:00
I1117 15:30:15.693698 72627 process.cpp:2541] Cleaning up log-recover-protocol(2)@192.168.178.39:38758
I1117 15:30:15.693711 72629 recover.cpp:566] Updating replica status to VOTING
I1117 15:30:15.693779 72627 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.693775872+00:00
I1117 15:30:15.693805 72631 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.693797120+00:00
I1117 15:30:15.694061 72627 process.cpp:2436] Resuming (1)@192.168.178.39:38758 at 2015-11-17 23:30:15.694052864+00:00
I1117 15:30:15.694070 72632 process.cpp:2426] Spawned process whitelist(1)@192.168.178.39:38758
I1117 15:30:15.694113 72630 process.cpp:2436] Resuming whitelist(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.694104832+00:00
I1117 15:30:15.694151 72630 whitelist_watcher.cpp:79] No whitelist given
I1117 15:30:15.694381 72627 hierarchical.cpp:151] Initialized hierarchical allocator process
I1117 15:30:15.694584 72631 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 629218ns
I1117 15:30:15.694608 72631 replica.cpp:323] Persisted replica status to VOTING
I1117 15:30:15.694676 72631 process.cpp:2436] Resuming log-recover(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.694670848+00:00
I1117 15:30:15.694782 72631 recover.cpp:580] Successfully joined the Paxos group
I1117 15:30:15.694921 72631 recover.cpp:464] Recover process terminated
I1117 15:30:15.694965 72631 process.cpp:2541] Cleaning up log-recover(1)@192.168.178.39:38758
I1117 15:30:15.695022 72631 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.695019008+00:00
I1117 15:30:15.694941 72630 process.cpp:2436] Resuming log(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.694937088+00:00
I1117 15:30:15.695121 72630 log.cpp:358] Log recovery completed
I1117 15:30:15.695344 72625 process.cpp:2436] Resuming log-reader(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.695336192+00:00
I1117 15:30:15.695375 72630 process.cpp:2436] Resuming log-writer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.695369984+00:00
I1117 15:30:15.697638 72626 process.cpp:2436] Resuming help@192.168.178.39:38758 at 2015-11-17 23:30:15.697626112+00:00
I1117 15:30:15.698554 72625 process.cpp:2436] Resuming standalone-master-detector(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.698543104+00:00
I1117 15:30:15.698976 72628 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.698961920+00:00
I1117 15:30:15.699066 72628 master.cpp:1606] The newly elected leader is master@192.168.178.39:38758 with id 0858c91e-303d-4b36-b940-4f96102ffc9a
I1117 15:30:15.699090 72628 master.cpp:1619] Elected as the leading master!
I1117 15:30:15.699120 72628 master.cpp:1379] Recovering from registrar
I1117 15:30:15.699373 72630 process.cpp:2436] Resuming registrar(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.699356928+00:00
I1117 15:30:15.699471 72630 registrar.cpp:309] Recovering registrar
I1117 15:30:15.699645 72628 process.cpp:2436] Resuming standalone-master-detector(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.699634944+00:00
I1117 15:30:15.699895 72627 process.cpp:2436] Resuming (3)@192.168.178.39:38758 at 2015-11-17 23:30:15.699887104+00:00
I1117 15:30:15.700049 72627 log.cpp:242] Starting the writer
I1117 15:30:15.700170 72630 process.cpp:2426] Spawned process __latch__(5)@192.168.178.39:38758
I1117 15:30:15.700201 72628 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.700193792+00:00
I1117 15:30:15.700307 72631 process.cpp:2436] Resuming log-writer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.700299008+00:00
I1117 15:30:15.700247 72629 process.cpp:2436] Resuming __latch__(5)@192.168.178.39:38758 at 2015-11-17 23:30:15.700238848+00:00
I1117 15:30:15.701014 72631 process.cpp:2426] Spawned process log-coordinator(1)@192.168.178.39:38758
I1117 15:30:15.701097 72631 log.cpp:661] Attempting to start the writer
I1117 15:30:15.701071 72626 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.701056000+00:00
I1117 15:30:15.702273 72626 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.702265856+00:00
I1117 15:30:15.702461 72626 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.702455040+00:00
I1117 15:30:15.702864 72626 process.cpp:2426] Spawned process log-implicit-promise(1)@192.168.178.39:38758
I1117 15:30:15.702922 72625 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.702914048+00:00
I1117 15:30:15.702972 72628 process.cpp:2436] Resuming log-implicit-promise(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.702961920+00:00
I1117 15:30:15.703387 72628 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.703377920+00:00
I1117 15:30:15.703631 72628 process.cpp:2436] Resuming log-implicit-promise(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.703624960+00:00
I1117 15:30:15.704012 72625 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.704004096+00:00
I1117 15:30:15.704210 72631 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.704200960+00:00
I1117 15:30:15.704237 72625 process.cpp:2426] Spawned process (6)@192.168.178.39:38758
I1117 15:30:15.704267 72628 process.cpp:2436] Resuming (6)@192.168.178.39:38758 at 2015-11-17 23:30:15.704260096+00:00
I1117 15:30:15.704357 72631 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.704352000+00:00
I1117 15:30:15.704496 72631 replica.cpp:496] Replica received implicit promise request from (6)@192.168.178.39:38758 with proposal 1
I1117 15:30:15.704766 72625 process.cpp:2436] Resuming log-implicit-promise(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.704758784+00:00
I1117 15:30:15.705354 72631 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 821803ns
I1117 15:30:15.705401 72631 replica.cpp:345] Persisted promised to 1
I1117 15:30:15.705487 72631 process.cpp:2436] Resuming (6)@192.168.178.39:38758 at 2015-11-17 23:30:15.705480960+00:00
I1117 15:30:15.705664 72631 process.cpp:2541] Cleaning up (6)@192.168.178.39:38758
I1117 15:30:15.705678 72628 process.cpp:2436] Resuming log-implicit-promise(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.705668864+00:00
I1117 15:30:15.705853 72631 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.705847040+00:00
I1117 15:30:15.706079 72632 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.706054912+00:00
I1117 15:30:15.706172 72628 process.cpp:2541] Cleaning up log-implicit-promise(1)@192.168.178.39:38758
I1117 15:30:15.706470 72628 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.706462976+00:00
I1117 15:30:15.706665 72625 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.706653184+00:00
I1117 15:30:15.707015 72625 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.707010048+00:00
I1117 15:30:15.707108 72625 coordinator.cpp:240] Coordinator attempting to fill missing positions
I1117 15:30:15.707375 72625 process.cpp:2426] Spawned process log-bulk-catch-up(1)@192.168.178.39:38758
I1117 15:30:15.707476 72630 process.cpp:2436] Resuming log-bulk-catch-up(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.707463936+00:00
I1117 15:30:15.707430 72626 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.707424000+00:00
I1117 15:30:15.707782 72628 process.cpp:2436] Resuming log-catch-up(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.707773952+00:00
I1117 15:30:15.707805 72630 process.cpp:2426] Spawned process log-catch-up(1)@192.168.178.39:38758
I1117 15:30:15.708026 72628 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.708019968+00:00
I1117 15:30:15.708183 72628 process.cpp:2436] Resuming log-catch-up(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.708175872+00:00
I1117 15:30:15.708459 72628 process.cpp:2426] Spawned process log-fill(1)@192.168.178.39:38758
I1117 15:30:15.708474 72630 process.cpp:2436] Resuming log-fill(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.708462080+00:00
I1117 15:30:15.708740 72630 process.cpp:2426] Spawned process log-explicit-promise(1)@192.168.178.39:38758
I1117 15:30:15.708739 72628 process.cpp:2436] Resuming log-explicit-promise(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.708729856+00:00
I1117 15:30:15.709018 72628 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.709008896+00:00
I1117 15:30:15.709213 72628 process.cpp:2436] Resuming log-explicit-promise(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.709206016+00:00
I1117 15:30:15.709491 72629 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.709483008+00:00
I1117 15:30:15.709529 72632 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.709518848+00:00
I1117 15:30:15.709627 72629 process.cpp:2426] Spawned process (7)@192.168.178.39:38758
I1117 15:30:15.709688 72628 process.cpp:2436] Resuming (7)@192.168.178.39:38758 at 2015-11-17 23:30:15.709680128+00:00
I1117 15:30:15.709796 72629 process.cpp:2436] Resuming log-explicit-promise(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.709788160+00:00
I1117 15:30:15.710068 72628 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.710059008+00:00
I1117 15:30:15.710155 72628 replica.cpp:391] Replica received explicit promise request from (7)@192.168.178.39:38758 for position 0 with proposal 2
I1117 15:30:15.710979 72628 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 757342ns
I1117 15:30:15.711026 72628 replica.cpp:715] Persisted action at 0
I1117 15:30:15.711268 72628 process.cpp:2436] Resuming (7)@192.168.178.39:38758 at 2015-11-17 23:30:15.711260928+00:00
I1117 15:30:15.711421 72628 process.cpp:2541] Cleaning up (7)@192.168.178.39:38758
I1117 15:30:15.711432 72630 process.cpp:2436] Resuming log-explicit-promise(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.711422976+00:00
I1117 15:30:15.711616 72628 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.711610112+00:00
I1117 15:30:15.711655 72626 process.cpp:2436] Resuming log-fill(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.711652096+00:00
I1117 15:30:15.711654 72630 process.cpp:2541] Cleaning up log-explicit-promise(1)@192.168.178.39:38758
I1117 15:30:15.711971 72630 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.711963904+00:00
I1117 15:30:15.712049 72626 process.cpp:2426] Spawned process log-write(1)@192.168.178.39:38758
I1117 15:30:15.712204 72630 process.cpp:2436] Resuming log-write(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.712197120+00:00
I1117 15:30:15.712381 72626 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.712375040+00:00
I1117 15:30:15.712551 72626 process.cpp:2436] Resuming log-write(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.712544000+00:00
I1117 15:30:15.712906 72630 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.712898048+00:00
I1117 15:30:15.713049 72630 process.cpp:2426] Spawned process (8)@192.168.178.39:38758
I1117 15:30:15.713080 72629 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.713067008+00:00
I1117 15:30:15.713084 72626 process.cpp:2436] Resuming (8)@192.168.178.39:38758 at 2015-11-17 23:30:15.713076992+00:00
I1117 15:30:15.713443 72626 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.713437184+00:00
I1117 15:30:15.713492 72627 process.cpp:2436] Resuming log-write(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.713483008+00:00
I1117 15:30:15.713541 72626 replica.cpp:540] Replica received write request for position 0 from (8)@192.168.178.39:38758
I1117 15:30:15.713690 72626 leveldb.cpp:438] Reading position from leveldb took 68197ns
I1117 15:30:15.714576 72626 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 591435ns
I1117 15:30:15.714622 72626 replica.cpp:715] Persisted action at 0
I1117 15:30:15.714784 72626 process.cpp:2436] Resuming (8)@192.168.178.39:38758 at 2015-11-17 23:30:15.714777088+00:00
I1117 15:30:15.715137 72626 process.cpp:2541] Cleaning up (8)@192.168.178.39:38758
I1117 15:30:15.715142 72630 process.cpp:2436] Resuming log-write(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.715134976+00:00
I1117 15:30:15.715476 72629 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.715468800+00:00
I1117 15:30:15.715584 72629 process.cpp:2436] Resuming log-fill(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.715576832+00:00
I1117 15:30:15.715601 72630 process.cpp:2541] Cleaning up log-write(1)@192.168.178.39:38758
I1117 15:30:15.715673 72632 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.715662848+00:00
I1117 15:30:15.715826 72626 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.715815936+00:00
I1117 15:30:15.716171 72630 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.716162816+00:00
I1117 15:30:15.716284 72630 replica.cpp:694] Replica received learned notice for position 0 from @0.0.0.0:0
I1117 15:30:15.716310 72629 process.cpp:2436] Resuming log-fill(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.716300032+00:00
I1117 15:30:15.716487 72629 process.cpp:2541] Cleaning up log-fill(1)@192.168.178.39:38758
I1117 15:30:15.716506 72626 process.cpp:2436] Resuming log-catch-up(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.716495872+00:00
I1117 15:30:15.716656 72629 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.716649984+00:00
I1117 15:30:15.716928 72630 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 609457ns
I1117 15:30:15.716979 72630 replica.cpp:715] Persisted action at 0
I1117 15:30:15.717051 72630 replica.cpp:700] Replica learned NOP action at position 0
I1117 15:30:15.717211 72630 process.cpp:2436] Resuming log-catch-up(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.717201920+00:00
I1117 15:30:15.717450 72627 process.cpp:2436] Resuming log-bulk-catch-up(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.717442048+00:00
I1117 15:30:15.717491 72630 process.cpp:2541] Cleaning up log-catch-up(1)@192.168.178.39:38758
I1117 15:30:15.717602 72630 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.717596160+00:00
I1117 15:30:15.717654 72626 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.717645056+00:00
I1117 15:30:15.717731 72627 process.cpp:2541] Cleaning up log-bulk-catch-up(1)@192.168.178.39:38758
I1117 15:30:15.717881 72627 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.717856000+00:00
I1117 15:30:15.718168 72630 process.cpp:2436] Resuming log-writer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.718161152+00:00
I1117 15:30:15.718230 72630 log.cpp:677] Writer started with ending position 0
I1117 15:30:15.718467 72627 process.cpp:2436] Resuming (3)@192.168.178.39:38758 at 2015-11-17 23:30:15.718460160+00:00
I1117 15:30:15.718549 72627 log.cpp:263] Writer got elected at position
I1117 15:30:15.718787 72630 process.cpp:2436] Resuming log-reader(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.718780160+00:00
I1117 15:30:15.719290 72627 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.719281920+00:00
I1117 15:30:15.719640 72627 process.cpp:2436] Resuming (3)@192.168.178.39:38758 at 2015-11-17 23:30:15.719633152+00:00
I1117 15:30:15.720077 72628 process.cpp:2436] Resuming log-reader(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.720068096+00:00
I1117 15:30:15.720634 72625 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.720624896+00:00
I1117 15:30:15.720695 72625 replica.cpp:225] Starting read from '0' to '0'
I1117 15:30:15.720785 72625 leveldb.cpp:438] Reading position from leveldb took 41707ns
I1117 15:30:15.721279 72631 process.cpp:2436] Resuming log-reader(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.721261056+00:00
I1117 15:30:15.721715 72625 process.cpp:2436] Resuming (3)@192.168.178.39:38758 at 2015-11-17 23:30:15.721699072+00:00
I1117 15:30:15.721825 72625 log.cpp:305] Applying operations (0 entries)
I1117 15:30:15.723986 72631 process.cpp:2436] Resuming __latch__(5)@192.168.178.39:38758 at 2015-11-17 23:30:15.723965952+00:00
I1117 15:30:15.724088 72631 process.cpp:2541] Cleaning up __latch__(5)@192.168.178.39:38758
I1117 15:30:15.724133 72627 process.cpp:2436] Resuming registrar(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.724119040+00:00
I1117 15:30:15.724248 72631 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.724240128+00:00
I1117 15:30:15.724256 72627 registrar.cpp:342] Successfully fetched the registry (0B) in 24.717312ms
I1117 15:30:15.724591 72627 registrar.cpp:441] Applied 1 operations in 86452ns; attempting to update the 'registry'
I1117 15:30:15.725996 72625 process.cpp:2436] Resuming (3)@192.168.178.39:38758 at 2015-11-17 23:30:15.725985024+00:00
I1117 15:30:15.726333 72627 process.cpp:2426] Spawned process __latch__(6)@192.168.178.39:38758
I1117 15:30:15.726862 72627 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.726854144+00:00
I1117 15:30:15.727048 72627 process.cpp:2436] Resuming __latch__(6)@192.168.178.39:38758 at 2015-11-17 23:30:15.727041024+00:00
I1117 15:30:15.727144 72628 process.cpp:2436] Resuming log-writer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.727132928+00:00
I1117 15:30:15.727874 72628 log.cpp:685] Attempting to append 165 bytes to the log
I1117 15:30:15.728123 72631 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.728101120+00:00
I1117 15:30:15.728207 72631 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 1
I1117 15:30:15.728425 72631 process.cpp:2426] Spawned process log-write(2)@192.168.178.39:38758
I1117 15:30:15.728461 72629 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.728449024+00:00
I1117 15:30:15.728507 72630 process.cpp:2436] Resuming log-write(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.728493824+00:00
I1117 15:30:15.728605 72630 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.728598016+00:00
I1117 15:30:15.728737 72630 process.cpp:2436] Resuming log-write(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.728729856+00:00
I1117 15:30:15.728914 72630 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.728905984+00:00
I1117 15:30:15.729073 72630 process.cpp:2426] Spawned process (9)@192.168.178.39:38758
I1117 15:30:15.729117 72629 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.729097984+00:00
I1117 15:30:15.729120 72631 process.cpp:2436] Resuming (9)@192.168.178.39:38758 at 2015-11-17 23:30:15.729104128+00:00
I1117 15:30:15.729212 72630 process.cpp:2436] Resuming log-write(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.729206016+00:00
I1117 15:30:15.729274 72631 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.729257984+00:00
I1117 15:30:15.729351 72631 replica.cpp:540] Replica received write request for position 1 from (9)@192.168.178.39:38758
I1117 15:30:15.730206 72631 leveldb.cpp:343] Persisting action (184 bytes) to leveldb took 790807ns
I1117 15:30:15.730250 72631 replica.cpp:715] Persisted action at 1
I1117 15:30:15.730348 72631 process.cpp:2436] Resuming (9)@192.168.178.39:38758 at 2015-11-17 23:30:15.730333184+00:00
I1117 15:30:15.730496 72631 process.cpp:2541] Cleaning up (9)@192.168.178.39:38758
I1117 15:30:15.730578 72629 process.cpp:2436] Resuming log-write(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.730566144+00:00
I1117 15:30:15.730630 72631 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.730624000+00:00
I1117 15:30:15.730968 72630 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.730959872+00:00
I1117 15:30:15.731014 72629 process.cpp:2541] Cleaning up log-write(2)@192.168.178.39:38758
I1117 15:30:15.731128 72629 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.731122944+00:00
I1117 15:30:15.731225 72629 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.731222016+00:00
I1117 15:30:15.731395 72628 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.731386880+00:00
I1117 15:30:15.731473 72628 replica.cpp:694] Replica received learned notice for position 1 from @0.0.0.0:0
I1117 15:30:15.731518 72630 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.731506944+00:00
I1117 15:30:15.731701 72628 leveldb.cpp:343] Persisting action (186 bytes) to leveldb took 206571ns
I1117 15:30:15.731758 72628 replica.cpp:715] Persisted action at 1
I1117 15:30:15.731787 72628 replica.cpp:700] Replica learned APPEND action at position 1
I1117 15:30:15.731989 72629 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.731966976+00:00
I1117 15:30:15.732372 72628 process.cpp:2436] Resuming (3)@192.168.178.39:38758 at 2015-11-17 23:30:15.732364800+00:00
I1117 15:30:15.733110 72626 process.cpp:2436] Resuming __latch__(6)@192.168.178.39:38758 at 2015-11-17 23:30:15.733097984+00:00
I1117 15:30:15.733185 72626 process.cpp:2541] Cleaning up __latch__(6)@192.168.178.39:38758
I1117 15:30:15.733273 72631 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.733263104+00:00
I1117 15:30:15.733469 72630 process.cpp:2436] Resuming registrar(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.733459968+00:00
I1117 15:30:15.733701 72630 registrar.cpp:486] Successfully updated the 'registry' in 8.950784ms
I1117 15:30:15.733885 72630 registrar.cpp:372] Successfully recovered registrar
I1117 15:30:15.734002 72629 process.cpp:2436] Resuming log-writer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.733995008+00:00
I1117 15:30:15.734098 72629 log.cpp:704] Attempting to truncate the log to 1
I1117 15:30:15.734225 72628 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.734209024+00:00
I1117 15:30:15.734325 72628 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 2
I1117 15:30:15.734514 72628 process.cpp:2426] Spawned process log-write(3)@192.168.178.39:38758
I1117 15:30:15.734601 72627 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.734571008+00:00
I1117 15:30:15.734710 72627 process.cpp:2436] Resuming log-write(3)@192.168.178.39:38758 at 2015-11-17 23:30:15.734703104+00:00
I1117 15:30:15.734840 72627 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.734817024+00:00
I1117 15:30:15.734968 72627 process.cpp:2436] Resuming log-write(3)@192.168.178.39:38758 at 2015-11-17 23:30:15.734962944+00:00
I1117 15:30:15.735123 72627 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.735115008+00:00
I1117 15:30:15.735296 72627 process.cpp:2426] Spawned process (10)@192.168.178.39:38758
I1117 15:30:15.735496 72627 process.cpp:2436] Resuming log-write(3)@192.168.178.39:38758 at 2015-11-17 23:30:15.735490048+00:00
I1117 15:30:15.735355 72629 process.cpp:2436] Resuming (10)@192.168.178.39:38758 at 2015-11-17 23:30:15.735347968+00:00
I1117 15:30:15.735662 72629 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.735655168+00:00
I1117 15:30:15.735342 72626 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.735320064+00:00
I1117 15:30:15.735761 72629 replica.cpp:540] Replica received write request for position 2 from (10)@192.168.178.39:38758
I1117 15:30:15.734546 72630 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.734537984+00:00
I1117 15:30:15.736315 72630 master.cpp:1416] Recovered 0 slaves from the Registry (127B) ; allowing 10mins for slaves to re-register
I1117 15:30:15.736351 72632 process.cpp:2436] Resuming __latch__(3)@192.168.178.39:38758 at 2015-11-17 23:30:15.736342016+00:00
I1117 15:30:15.736441 72632 process.cpp:2541] Cleaning up __latch__(3)@192.168.178.39:38758
I1117 15:30:15.736618 72629 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 760533ns
I1117 15:30:15.736682 72625 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.736672000+00:00
I1117 15:30:15.736697 72629 replica.cpp:715] Persisted action at 2
I1117 15:30:15.736800 72625 process.cpp:2436] Resuming __waiter__(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.736792064+00:00
I1117 15:30:15.736855 72625 process.cpp:2541] Cleaning up __waiter__(2)@192.168.178.39:38758
I1117 15:30:15.736855 72629 process.cpp:2436] Resuming (10)@192.168.178.39:38758 at 2015-11-17 23:30:15.736848128+00:00
I1117 15:30:15.736948 72611 clock.cpp:312] Clock paused at 0x2d79f50
I1117 15:30:15.737103 72629 process.cpp:2541] Cleaning up (10)@192.168.178.39:38758
I1117 15:30:15.737164 72629 process.cpp:2436] Resuming log-write(3)@192.168.178.39:38758 at 2015-11-17 23:30:15.736940032+00:00
I1117 15:30:15.737280 72629 process.cpp:2541] Cleaning up log-write(3)@192.168.178.39:38758
I1117 15:30:15.737330 72629 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.736940032+00:00
I1117 15:30:15.737387 72629 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.736940032+00:00
I1117 15:30:15.737542 72629 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.736940032+00:00
I1117 15:30:15.737658 72629 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.736940032+00:00
I1117 15:30:15.737702 72629 replica.cpp:694] Replica received learned notice for position 2 from @0.0.0.0:0
I1117 15:30:15.737901 72632 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.736940032+00:00
I1117 15:30:15.738533 72629 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 812178ns
I1117 15:30:15.738585 72629 leveldb.cpp:401] Deleting ~1 keys from leveldb took 25495ns
I1117 15:30:15.738602 72629 replica.cpp:715] Persisted action at 2
I1117 15:30:15.738622 72629 replica.cpp:700] Replica learned TRUNCATE action at position 2
I1117 15:30:15.738742 72629 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.736940032+00:00
I1117 15:30:15.738986 72629 process.cpp:2436] Resuming (3)@192.168.178.39:38758 at 2015-11-17 23:30:15.736940032+00:00
I1117 15:30:15.747231 72611 clock.cpp:342] Clock resumed at 0x2d79d20
I1117 15:30:15.748857 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.748845056+00:00
I1117 15:30:15.750026 72611 process.cpp:2426] Spawned process fetcher(1)@192.168.178.39:38758
I1117 15:30:15.750094 72629 process.cpp:2436] Resuming fetcher(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.750078976+00:00
I1117 15:30:15.750283 72625 process.cpp:2436] Resuming (11)@192.168.178.39:38758 at 2015-11-17 23:30:15.750272000+00:00
I1117 15:30:15.750300 72611 process.cpp:2426] Spawned process (11)@192.168.178.39:38758
I1117 15:30:15.751544 72611 process.cpp:2426] Spawned process standalone-master-detector(2)@192.168.178.39:38758
I1117 15:30:15.751641 72628 process.cpp:2436] Resuming standalone-master-detector(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.751621888+00:00
I1117 15:30:15.751705 72611 process.cpp:2426] Spawned process (12)@192.168.178.39:38758
I1117 15:30:15.751799 72628 process.cpp:2436] Resuming (12)@192.168.178.39:38758 at 2015-11-17 23:30:15.751791104+00:00
I1117 15:30:15.752094 72629 process.cpp:2436] Resuming (13)@192.168.178.39:38758 at 2015-11-17 23:30:15.752082944+00:00
I1117 15:30:15.752054 72611 process.cpp:2426] Spawned process (13)@192.168.178.39:38758
I1117 15:30:15.752892 72629 process.cpp:2436] Resuming metrics@192.168.178.39:38758 at 2015-11-17 23:30:15.752881920+00:00
I1117 15:30:15.753365 72611 process.cpp:2426] Spawned process __limiter__(2)@192.168.178.39:38758
I1117 15:30:15.753401 72626 process.cpp:2436] Resuming __limiter__(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.753384960+00:00
I1117 15:30:15.753566 72611 process.cpp:2426] Spawned process monitor@192.168.178.39:38758
I1117 15:30:15.753639 72629 process.cpp:2436] Resuming monitor@192.168.178.39:38758 at 2015-11-17 23:30:15.753631232+00:00
I1117 15:30:15.753758 72611 process.cpp:2426] Spawned process slave(1)@192.168.178.39:38758
I1117 15:30:15.753844 72626 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.753832960+00:00
I1117 15:30:15.753924 72626 slave.cpp:191] Slave started on 1)@192.168.178.39:38758
I1117 15:30:15.754139 72629 process.cpp:2436] Resuming help@192.168.178.39:38758 at 2015-11-17 23:30:15.754120192+00:00
I1117 15:30:15.753947 72626 slave.cpp:192] Flags at startup: --appc_store_dir="/tmp/mesos/store/appc" --authenticatee="crammd5" --cgroups_cpu_enable_pids_and_tids_count="false" --cgroups_enable_cfs="false" --cgroups_hierarchy="/sys/fs/cgroup" --cgroups_limit_swap="false" --cgroups_root="mesos" --container_disk_watch_interval="15secs" --containerizers="mesos" --credential="/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/credential" --default_role="*" --disk_watch_interval="1mins" --docker="docker" --docker_auth_server="auth.docker.io" --docker_auth_server_port="443" --docker_kill_orphans="true" --docker_local_archives_dir="/tmp/mesos/images/docker" --docker_puller="local" --docker_puller_timeout="60" --docker_registry="registry-1.docker.io" --docker_registry_port="443" --docker_remove_delay="6hrs" --docker_socket="/var/run/docker.sock" --docker_stop_timeout="0ns" --docker_store_dir="/tmp/mesos/store/docker" --enforce_container_disk_quota="false" --executor_registration_timeout="1mins" --executor_shutdown_grace_period="5secs" --fetcher_cache_dir="/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/fetch" --fetcher_cache_size="2GB" --frameworks_home="" --gc_delay="1weeks" --gc_disk_headroom="0.1" --hadoop_home="" --help="false" --hostname_lookup="true" --image_provisioner_backend="copy" --initialize_driver_logging="true" --isolation="posix/cpu,posix/mem" --launcher_dir="/home/till/scratchpad/mesos/build/src" --logbufsecs="0" --logging_level="INFO" --oversubscribed_resources_interval="15secs" --perf_duration="10secs" --perf_interval="1mins" --qos_correction_interval_min="0ns" --quiet="false" --recover="reconnect" --recovery_timeout="15mins" --registration_backoff_factor="10ms" --resources="cpus:2;mem:1024;disk:1024;ports:[31000-32000]" --revocable_cpu_low_priority="true" --sandbox_directory="/mnt/mesos/sandbox" --strict="true" --switch_user="true" --systemd_runtime_directory="/run/systemd/system" --version="false" --work_dir="/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf"
I1117 15:30:15.754312 72626 credentials.hpp:85] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/credential'
I1117 15:30:15.754617 72626 slave.cpp:322] Slave using credential for: test-principal
I1117 15:30:15.754741 72626 process.cpp:2426] Spawned process (14)@192.168.178.39:38758
I1117 15:30:15.754870 72626 process.cpp:2426] Spawned process (15)@192.168.178.39:38758
I1117 15:30:15.754976 72626 resources.cpp:474] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I1117 15:30:15.754878 72627 process.cpp:2436] Resuming (15)@192.168.178.39:38758 at 2015-11-17 23:30:15.754865920+00:00
I1117 15:30:15.755746 72611 process.cpp:2426] Spawned process version@192.168.178.39:38758
I1117 15:30:15.755813 72629 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.755796992+00:00
I1117 15:30:15.755857 72632 process.cpp:2436] Resuming __latch__(7)@192.168.178.39:38758 at 2015-11-17 23:30:15.755848960+00:00
I1117 15:30:15.755853 72630 process.cpp:2436] Resuming version@192.168.178.39:38758 at 2015-11-17 23:30:15.755836928+00:00
I1117 15:30:15.755821 72611 process.cpp:2426] Spawned process __latch__(7)@192.168.178.39:38758
I1117 15:30:15.756019 72629 process.cpp:2436] Resuming help@192.168.178.39:38758 at 2015-11-17 23:30:15.756011008+00:00
I1117 15:30:15.756093 72626 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1117 15:30:15.756193 72626 slave.cpp:400] Slave attributes: [  ]
I1117 15:30:15.756227 72626 slave.cpp:405] Slave hostname: ubuntu
I1117 15:30:15.756263 72626 slave.cpp:410] Slave checkpoint: true
I1117 15:30:15.756284 72611 pid.cpp:93] Attempting to parse 'master@192.168.178.39:38758' into a PID
I1117 15:30:15.754770 72625 process.cpp:2436] Resuming (14)@192.168.178.39:38758 at 2015-11-17 23:30:15.754764032+00:00
I1117 15:30:15.756469 72625 process.cpp:2436] Resuming (13)@192.168.178.39:38758 at 2015-11-17 23:30:15.756460800+00:00
I1117 15:30:15.756512 72611 process.cpp:2426] Spawned process standalone-master-detector(3)@192.168.178.39:38758
I1117 15:30:15.757171 72611 sched.cpp:166] Version: 0.26.0
I1117 15:30:15.757254 72632 process.cpp:2436] Resuming metrics@192.168.178.39:38758 at 2015-11-17 23:30:15.757245184+00:00
I1117 15:30:15.757488 72611 process.cpp:2426] Spawned process scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758
I1117 15:30:15.757555 72631 process.cpp:2436] Resuming scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758 at 2015-11-17 23:30:15.757543936+00:00
I1117 15:30:15.757624 72611 process.cpp:2426] Spawned process __latch__(8)@192.168.178.39:38758
I1117 15:30:15.757716 72611 process.cpp:2426] Spawned process __waiter__(3)@192.168.178.39:38758
I1117 15:30:15.757799 72611 process.cpp:2734] Donating thread to __waiter__(3)@192.168.178.39:38758 while waiting
I1117 15:30:15.757836 72611 process.cpp:2436] Resuming __waiter__(3)@192.168.178.39:38758 at 2015-11-17 23:30:15.757831168+00:00
I1117 15:30:15.757846 72629 process.cpp:2436] Resuming __latch__(8)@192.168.178.39:38758 at 2015-11-17 23:30:15.757837056+00:00
I1117 15:30:15.757659 72630 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.757651968+00:00
I1117 15:30:15.758010 72629 process.cpp:2436] Resuming help@192.168.178.39:38758 at 2015-11-17 23:30:15.758001920+00:00
I1117 15:30:15.756546 72625 process.cpp:2436] Resuming standalone-master-detector(3)@192.168.178.39:38758 at 2015-11-17 23:30:15.756540928+00:00
I1117 15:30:15.758456 72625 process.cpp:2436] Resuming scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758 at 2015-11-17 23:30:15.758449152+00:00
I1117 15:30:15.758512 72625 sched.cpp:264] New master detected at master@192.168.178.39:38758
I1117 15:30:15.758528 72625 pid.cpp:93] Attempting to parse 'master@192.168.178.39:38758' into a PID
I1117 15:30:15.758668 72625 sched.cpp:320] Authenticating with master master@192.168.178.39:38758
I1117 15:30:15.758752 72625 sched.cpp:327] Using default CRAM-MD5 authenticatee
I1117 15:30:15.758798 72625 pid.cpp:93] Attempting to parse 'master@192.168.178.39:38758' into a PID
I1117 15:30:15.758967 72625 process.cpp:2426] Spawned process crammd5_authenticatee(1)@192.168.178.39:38758
I1117 15:30:15.759006 72630 process.cpp:2436] Resuming crammd5_authenticatee(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.758999040+00:00
I1117 15:30:15.759260 72630 authenticatee.cpp:99] Initializing client SASL
I1117 15:30:15.759240 72625 process.cpp:2436] Resuming standalone-master-detector(3)@192.168.178.39:38758 at 2015-11-17 23:30:15.759229952+00:00
I1117 15:30:15.759472 72625 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.759454976+00:00
I1117 15:30:15.759531 72626 process.cpp:2426] Spawned process __async_executor__(1)@192.168.178.39:38758
I1117 15:30:15.759575 72631 process.cpp:2436] Resuming __async_executor__(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.759567872+00:00
I1117 15:30:15.759588 72630 authenticatee.cpp:123] Creating new client SASL connection
I1117 15:30:15.759894 72627 process.cpp:2436] Resuming __async_executor__(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.759887104+00:00
I1117 15:30:15.759984 72627 state.cpp:54] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/meta'
I1117 15:30:15.760203 72627 process.cpp:2541] Cleaning up __async_executor__(1)@192.168.178.39:38758
I1117 15:30:15.760254 72630 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.760241920+00:00
I1117 15:30:15.760313 72627 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.760307968+00:00
I1117 15:30:15.760363 72630 pid.cpp:93] Attempting to parse 'scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758' into a PID
I1117 15:30:15.760501 72630 master.cpp:5150] Authenticating scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758
I1117 15:30:15.760679 72627 process.cpp:2436] Resuming crammd5_authenticator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.760673024+00:00
I1117 15:30:15.760741 72627 authenticator.cpp:415] Starting authentication session for crammd5_authenticatee(1)@192.168.178.39:38758
I1117 15:30:15.760809 72627 process.cpp:2426] Spawned process crammd5_authenticator_session(1)@192.168.178.39:38758
I1117 15:30:15.760831 72626 process.cpp:2436] Resuming (13)@192.168.178.39:38758 at 2015-11-17 23:30:15.760824064+00:00
I1117 15:30:15.760902 72626 status_update_manager.cpp:202] Recovering status update manager
I1117 15:30:15.760972 72630 process.cpp:2436] Resuming crammd5_authenticator_session(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.760966912+00:00
I1117 15:30:15.761025 72626 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.761019904+00:00
I1117 15:30:15.761152 72630 authenticator.cpp:100] Creating new server SASL connection
I1117 15:30:15.761231 72626 process.cpp:2436] Resuming (11)@192.168.178.39:38758 at 2015-11-17 23:30:15.761224960+00:00
I1117 15:30:15.761298 72626 docker.cpp:536] Recovering Docker containers
I1117 15:30:15.761379 72626 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.761373952+00:00
I1117 15:30:15.761411 72629 process.cpp:2436] Resuming crammd5_authenticatee(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.761403136+00:00
I1117 15:30:15.761509 72629 authenticatee.cpp:214] Received SASL authentication mechanisms: CRAM-MD5
I1117 15:30:15.761539 72629 authenticatee.cpp:240] Attempting to authenticate with mechanism 'CRAM-MD5'
I1117 15:30:15.761557 72626 slave.cpp:4230] Finished recovery
I1117 15:30:15.761683 72629 process.cpp:2436] Resuming crammd5_authenticator_session(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.761677056+00:00
I1117 15:30:15.761749 72629 authenticator.cpp:205] Received SASL authentication start
I1117 15:30:15.761787 72629 authenticator.cpp:327] Authentication requires more steps
I1117 15:30:15.761848 72629 process.cpp:2436] Resuming crammd5_authenticatee(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.761842944+00:00
I1117 15:30:15.761905 72629 authenticatee.cpp:260] Received SASL authentication step
I1117 15:30:15.761977 72629 process.cpp:2436] Resuming crammd5_authenticator_session(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.761966848+00:00
I1117 15:30:15.762044 72629 authenticator.cpp:233] Received SASL authentication step
I1117 15:30:15.762087 72629 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ubuntu' server FQDN: 'ubuntu' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I1117 15:30:15.762106 72629 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1117 15:30:15.762197 72629 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1117 15:30:15.762200 72631 process.cpp:2436] Resuming standalone-master-detector(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.762192896+00:00
I1117 15:30:15.762220 72629 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ubuntu' server FQDN: 'ubuntu' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I1117 15:30:15.762240 72629 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1117 15:30:15.762248 72629 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1117 15:30:15.762259 72629 authenticator.cpp:319] Authentication success
I1117 15:30:15.762207 72626 slave.cpp:4387] Querying resource estimator for oversubscribable resources
I1117 15:30:15.762356 72631 process.cpp:2436] Resuming crammd5_authenticatee(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.762351872+00:00
I1117 15:30:15.762405 72631 authenticatee.cpp:300] Authentication success
I1117 15:30:15.762429 72629 process.cpp:2436] Resuming crammd5_authenticator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.762424064+00:00
I1117 15:30:15.762476 72631 process.cpp:2436] Resuming (14)@192.168.178.39:38758 at 2015-11-17 23:30:15.762473216+00:00
I1117 15:30:15.762480 72629 authenticator.cpp:433] Authentication session cleanup for crammd5_authenticatee(1)@192.168.178.39:38758
I1117 15:30:15.762554 72629 process.cpp:2734] Donating thread to crammd5_authenticator_session(1)@192.168.178.39:38758 while waiting
I1117 15:30:15.762573 72629 process.cpp:2436] Resuming crammd5_authenticator_session(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.762571008+00:00
I1117 15:30:15.762591 72629 process.cpp:2541] Cleaning up crammd5_authenticator_session(1)@192.168.178.39:38758
I1117 15:30:15.762598 72626 pid.cpp:93] Attempting to parse 'master@192.168.178.39:38758' into a PID
I1117 15:30:15.762634 72626 slave.cpp:729] New master detected at master@192.168.178.39:38758
I1117 15:30:15.762420 72627 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.762417920+00:00
I1117 15:30:15.763048 72627 master.cpp:5180] Successfully authenticated principal 'test-principal' at scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758
I1117 15:30:15.763072 72629 process.cpp:2436] Resuming scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758 at 2015-11-17 23:30:15.763065088+00:00
I1117 15:30:15.763123 72626 slave.cpp:792] Authenticating with master master@192.168.178.39:38758
I1117 15:30:15.763142 72629 process.cpp:2734] Donating thread to crammd5_authenticatee(1)@192.168.178.39:38758 while waiting
I1117 15:30:15.763164 72629 process.cpp:2436] Resuming crammd5_authenticatee(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.763161856+00:00
I1117 15:30:15.763147 72626 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1117 15:30:15.763183 72629 process.cpp:2541] Cleaning up crammd5_authenticatee(1)@192.168.178.39:38758
I1117 15:30:15.763173 72627 process.cpp:2436] Resuming (13)@192.168.178.39:38758 at 2015-11-17 23:30:15.763165952+00:00
I1117 15:30:15.763248 72629 sched.cpp:409] Successfully authenticated with master master@192.168.178.39:38758
I1117 15:30:15.763272 72629 sched.cpp:716] Sending SUBSCRIBE call to master@192.168.178.39:38758
I1117 15:30:15.763249 72626 process.cpp:2426] Spawned process crammd5_authenticatee(2)@192.168.178.39:38758
I1117 15:30:15.763303 72629 pid.cpp:93] Attempting to parse 'master@192.168.178.39:38758' into a PID
I1117 15:30:15.763362 72626 slave.cpp:765] Detecting new master
I1117 15:30:15.763382 72629 sched.cpp:749] Will retry registration in 310.403731ms if necessary
I1117 15:30:15.763394 72631 process.cpp:2436] Resuming crammd5_authenticatee(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.763387904+00:00
I1117 15:30:15.763448 72629 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.763446016+00:00
I1117 15:30:15.763461 72626 slave.cpp:4401] Received oversubscribable resources  from the resource estimator
I1117 15:30:15.763500 72631 authenticatee.cpp:123] Creating new client SASL connection
I1117 15:30:15.763517 72626 process.cpp:2436] Resuming standalone-master-detector(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.763515904+00:00
I1117 15:30:15.763241 72627 status_update_manager.cpp:176] Pausing sending status updates
I1117 15:30:15.763577 72629 master.cpp:2176] Received SUBSCRIBE call for framework 'default' at scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758
I1117 15:30:15.764020 72629 master.cpp:1645] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1117 15:30:15.764181 72626 process.cpp:2436] Resuming authorizer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.764172032+00:00
I1117 15:30:15.764216 72629 pid.cpp:93] Attempting to parse 'slave(1)@192.168.178.39:38758' into a PID
I1117 15:30:15.764247 72629 master.cpp:5150] Authenticating slave(1)@192.168.178.39:38758
I1117 15:30:15.764339 72627 process.cpp:2436] Resuming crammd5_authenticator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.764332032+00:00
I1117 15:30:15.764379 72627 authenticator.cpp:415] Starting authentication session for crammd5_authenticatee(2)@192.168.178.39:38758
I1117 15:30:15.764408 72626 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.764403200+00:00
I1117 15:30:15.764430 72627 process.cpp:2426] Spawned process crammd5_authenticator_session(2)@192.168.178.39:38758
I1117 15:30:15.764473 72626 master.cpp:2247] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1117 15:30:15.764600 72627 process.cpp:2436] Resuming crammd5_authenticator_session(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.764591872+00:00
I1117 15:30:15.764765 72627 authenticator.cpp:100] Creating new server SASL connection
I1117 15:30:15.765228 72627 process.cpp:2436] Resuming crammd5_authenticatee(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.765217024+00:00
I1117 15:30:15.765336 72627 authenticatee.cpp:214] Received SASL authentication mechanisms: CRAM-MD5
I1117 15:30:15.765456 72627 authenticatee.cpp:240] Attempting to authenticate with mechanism 'CRAM-MD5'
I1117 15:30:15.765373 72632 process.cpp:2436] Resuming metrics@192.168.178.39:38758 at 2015-11-17 23:30:15.765364992+00:00
I1117 15:30:15.765547 72627 process.cpp:2436] Resuming crammd5_authenticator_session(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.765533952+00:00
I1117 15:30:15.765607 72627 authenticator.cpp:205] Received SASL authentication start
I1117 15:30:15.765229 72629 process.cpp:2436] Resuming (1)@192.168.178.39:38758 at 2015-11-17 23:30:15.765221120+00:00
I1117 15:30:15.765681 72627 authenticator.cpp:327] Authentication requires more steps
I1117 15:30:15.765765 72625 process.cpp:2436] Resuming crammd5_authenticatee(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.765755904+00:00
I1117 15:30:15.765712 72631 process.cpp:2436] Resuming scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758 at 2015-11-17 23:30:15.765707008+00:00
I1117 15:30:15.765892 72625 authenticatee.cpp:260] Received SASL authentication step
I1117 15:30:15.765985 72625 process.cpp:2436] Resuming crammd5_authenticator_session(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.765978112+00:00
I1117 15:30:15.766011 72631 pid.cpp:93] Attempting to parse 'master@192.168.178.39:38758' into a PID
I1117 15:30:15.766039 72631 sched.cpp:643] Framework registered with 0858c91e-303d-4b36-b940-4f96102ffc9a-0000
I1117 15:30:15.766044 72625 authenticator.cpp:233] Received SASL authentication step
I1117 15:30:15.766077 72625 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ubuntu' server FQDN: 'ubuntu' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I1117 15:30:15.766103 72625 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1117 15:30:15.766089 72631 sched.cpp:657] Scheduler::registered took 37307ns
I1117 15:30:15.766166 72625 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1117 15:30:15.766178 72631 process.cpp:2436] Resuming __latch__(8)@192.168.178.39:38758 at 2015-11-17 23:30:15.766171904+00:00
I1117 15:30:15.766203 72631 process.cpp:2541] Cleaning up __latch__(8)@192.168.178.39:38758
I1117 15:30:15.766237 72625 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ubuntu' server FQDN: 'ubuntu' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I1117 15:30:15.766264 72625 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1117 15:30:15.766302 72625 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1117 15:30:15.766314 72631 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.766309888+00:00
I1117 15:30:15.766326 72625 authenticator.cpp:319] Authentication success
I1117 15:30:15.766360 72627 process.cpp:2436] Resuming __waiter__(3)@192.168.178.39:38758 at 2015-11-17 23:30:15.766354944+00:00
I1117 15:30:15.766501 72627 process.cpp:2541] Cleaning up __waiter__(3)@192.168.178.39:38758
I1117 15:30:15.766577 72630 process.cpp:2436] Resuming crammd5_authenticatee(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.766548992+00:00
I1117 15:30:15.766788 72630 authenticatee.cpp:300] Authentication success
I1117 15:30:15.766846 72626 process.cpp:2436] Resuming __latch__(9)@192.168.178.39:38758 at 2015-11-17 23:30:15.766833920+00:00
I1117 15:30:15.766876 72631 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.766868224+00:00
I1117 15:30:15.766753 72611 process.cpp:2426] Spawned process __latch__(9)@192.168.178.39:38758
I1117 15:30:15.766993 72631 process.cpp:2734] Donating thread to crammd5_authenticatee(2)@192.168.178.39:38758 while waiting
I1117 15:30:15.766793 72632 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.766785024+00:00
I1117 15:30:15.767038 72611 process.cpp:2426] Spawned process __waiter__(4)@192.168.178.39:38758
I1117 15:30:15.767035 72631 process.cpp:2436] Resuming crammd5_authenticatee(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.767027968+00:00
I1117 15:30:15.766597 72625 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.766590208+00:00
I1117 15:30:15.767192 72631 process.cpp:2541] Cleaning up crammd5_authenticatee(2)@192.168.178.39:38758
I1117 15:30:15.767123 72611 process.cpp:2734] Donating thread to __waiter__(4)@192.168.178.39:38758 while waiting
I1117 15:30:15.767249 72625 master.cpp:5180] Successfully authenticated principal 'test-principal' at slave(1)@192.168.178.39:38758
I1117 15:30:15.767271 72611 process.cpp:2436] Resuming __waiter__(4)@192.168.178.39:38758 at 2015-11-17 23:30:15.767262976+00:00
I1117 15:30:15.767331 72625 process.cpp:2436] Resuming crammd5_authenticator_session(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.767323904+00:00
I1117 15:30:15.767386 72631 slave.cpp:860] Successfully authenticated with master master@192.168.178.39:38758
I1117 15:30:15.765990 72629 hierarchical.cpp:195] Added framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000
I1117 15:30:15.767644 72629 hierarchical.cpp:977] No resources available to allocate!
I1117 15:30:15.767653 72631 slave.cpp:1254] Will retry registration in 16.572431ms if necessary
I1117 15:30:15.767699 72630 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.767691008+00:00
I1117 15:30:15.767732 72629 hierarchical.cpp:1070] No inverse offers to send out!
I1117 15:30:15.767776 72629 hierarchical.cpp:876] Performed allocation for 0 slaves in 240946ns
I1117 15:30:15.766619 72627 process.cpp:2436] Resuming crammd5_authenticator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.766615040+00:00
I1117 15:30:15.767835 72627 authenticator.cpp:433] Authentication session cleanup for crammd5_authenticatee(2)@192.168.178.39:38758
I1117 15:30:15.768030 72630 master.cpp:3859] Registering slave at slave(1)@192.168.178.39:38758 (ubuntu) with id 0858c91e-303d-4b36-b940-4f96102ffc9a-S0
I1117 15:30:15.768506 72632 process.cpp:2436] Resuming crammd5_authenticator_session(2)@192.168.178.39:38758 at 2015-11-17 23:30:15.768495104+00:00
I1117 15:30:15.768578 72632 process.cpp:2541] Cleaning up crammd5_authenticator_session(2)@192.168.178.39:38758
I1117 15:30:15.768669 72632 process.cpp:2436] Resuming registrar(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.768662016+00:00
I1117 15:30:15.769227 72632 registrar.cpp:441] Applied 1 operations in 45222ns; attempting to update the 'registry'
I1117 15:30:15.770483 72630 process.cpp:2436] Resuming (3)@192.168.178.39:38758 at 2015-11-17 23:30:15.770464000+00:00
I1117 15:30:15.770572 72632 process.cpp:2426] Spawned process __latch__(10)@192.168.178.39:38758
I1117 15:30:15.770611 72628 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.770601984+00:00
I1117 15:30:15.770695 72628 process.cpp:2436] Resuming __latch__(10)@192.168.178.39:38758 at 2015-11-17 23:30:15.770692096+00:00
I1117 15:30:15.770977 72632 process.cpp:2436] Resuming log-writer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.770952960+00:00
I1117 15:30:15.771055 72632 log.cpp:685] Attempting to append 329 bytes to the log
I1117 15:30:15.771204 72630 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.771186176+00:00
I1117 15:30:15.771263 72630 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 3
I1117 15:30:15.771409 72630 process.cpp:2426] Spawned process log-write(4)@192.168.178.39:38758
I1117 15:30:15.771447 72632 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.771439104+00:00
I1117 15:30:15.771553 72632 process.cpp:2436] Resuming log-write(4)@192.168.178.39:38758 at 2015-11-17 23:30:15.771547136+00:00
I1117 15:30:15.771646 72632 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.771640064+00:00
I1117 15:30:15.771767 72632 process.cpp:2436] Resuming log-write(4)@192.168.178.39:38758 at 2015-11-17 23:30:15.771758080+00:00
I1117 15:30:15.771921 72632 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.771915008+00:00
I1117 15:30:15.772074 72632 process.cpp:2426] Spawned process (16)@192.168.178.39:38758
I1117 15:30:15.772140 72630 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.772120064+00:00
I1117 15:30:15.772176 72632 process.cpp:2436] Resuming log-write(4)@192.168.178.39:38758 at 2015-11-17 23:30:15.772171008+00:00
I1117 15:30:15.772166 72628 process.cpp:2436] Resuming (16)@192.168.178.39:38758 at 2015-11-17 23:30:15.772156928+00:00
I1117 15:30:15.772439 72628 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.772433920+00:00
I1117 15:30:15.772526 72628 replica.cpp:540] Replica received write request for position 3 from (16)@192.168.178.39:38758
I1117 15:30:15.772923 72628 leveldb.cpp:343] Persisting action (348 bytes) to leveldb took 365055ns
I1117 15:30:15.772974 72628 replica.cpp:715] Persisted action at 3
I1117 15:30:15.773036 72628 process.cpp:2436] Resuming (16)@192.168.178.39:38758 at 2015-11-17 23:30:15.773030912+00:00
I1117 15:30:15.773169 72628 process.cpp:2541] Cleaning up (16)@192.168.178.39:38758
I1117 15:30:15.773313 72628 process.cpp:2436] Resuming log-write(4)@192.168.178.39:38758 at 2015-11-17 23:30:15.773308160+00:00
I1117 15:30:15.773610 72628 process.cpp:2541] Cleaning up log-write(4)@192.168.178.39:38758
I1117 15:30:15.773795 72628 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.773789952+00:00
I1117 15:30:15.774204 72628 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.774198016+00:00
I1117 15:30:15.774399 72630 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.774384896+00:00
I1117 15:30:15.774440 72632 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.774429184+00:00
I1117 15:30:15.774583 72630 replica.cpp:694] Replica received learned notice for position 3 from @0.0.0.0:0
I1117 15:30:15.773694 72627 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.773677056+00:00
I1117 15:30:15.775367 72630 leveldb.cpp:343] Persisting action (350 bytes) to leveldb took 715335ns
I1117 15:30:15.775411 72630 replica.cpp:715] Persisted action at 3
I1117 15:30:15.775434 72630 replica.cpp:700] Replica learned APPEND action at position 3
I1117 15:30:15.775593 72625 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.775582976+00:00
I1117 15:30:15.775959 72630 process.cpp:2436] Resuming (3)@192.168.178.39:38758 at 2015-11-17 23:30:15.775933952+00:00
I1117 15:30:15.776504 72628 process.cpp:2436] Resuming __latch__(10)@192.168.178.39:38758 at 2015-11-17 23:30:15.776493056+00:00
I1117 15:30:15.776567 72628 process.cpp:2541] Cleaning up __latch__(10)@192.168.178.39:38758
I1117 15:30:15.776780 72632 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.776765184+00:00
I1117 15:30:15.776607 72631 process.cpp:2436] Resuming registrar(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.776599040+00:00
I1117 15:30:15.776988 72630 process.cpp:2436] Resuming log-writer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.776971008+00:00
I1117 15:30:15.777029 72631 registrar.cpp:486] Successfully updated the 'registry' in 7.652096ms
I1117 15:30:15.777050 72630 log.cpp:704] Attempting to truncate the log to 3
I1117 15:30:15.777217 72630 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.777210880+00:00
I1117 15:30:15.777297 72630 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 4
I1117 15:30:15.777411 72630 process.cpp:2426] Spawned process log-write(5)@192.168.178.39:38758
I1117 15:30:15.777436 72628 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.777431040+00:00
I1117 15:30:15.777497 72627 process.cpp:2436] Resuming log-write(5)@192.168.178.39:38758 at 2015-11-17 23:30:15.777488896+00:00
I1117 15:30:15.777791 72627 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.777785088+00:00
I1117 15:30:15.777932 72627 process.cpp:2436] Resuming log-write(5)@192.168.178.39:38758 at 2015-11-17 23:30:15.777925120+00:00
I1117 15:30:15.777410 72631 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.777401856+00:00
I1117 15:30:15.778211 72632 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.778196224+00:00
I1117 15:30:15.778364 72632 process.cpp:2426] Spawned process (17)@192.168.178.39:38758
I1117 15:30:15.778487 72632 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.778481920+00:00
I1117 15:30:15.778589 72632 process.cpp:2436] Resuming (17)@192.168.178.39:38758 at 2015-11-17 23:30:15.778582016+00:00
I1117 15:30:15.778667 72632 process.cpp:2436] Resuming log-write(5)@192.168.178.39:38758 at 2015-11-17 23:30:15.778662912+00:00
I1117 15:30:15.778767 72632 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.778759936+00:00
I1117 15:30:15.778842 72632 replica.cpp:540] Replica received write request for position 4 from (17)@192.168.178.39:38758
I1117 15:30:15.779325 72631 process.cpp:2426] Spawned process slave-observer(1)@192.168.178.39:38758
I1117 15:30:15.779520 72632 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 642920ns
I1117 15:30:15.779543 72632 replica.cpp:715] Persisted action at 4
I1117 15:30:15.779597 72632 process.cpp:2436] Resuming (17)@192.168.178.39:38758 at 2015-11-17 23:30:15.779592960+00:00
I1117 15:30:15.779716 72632 process.cpp:2541] Cleaning up (17)@192.168.178.39:38758
I1117 15:30:15.779381 72627 process.cpp:2436] Resuming slave-observer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.779372032+00:00
I1117 15:30:15.779829 72632 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.779822080+00:00
I1117 15:30:15.779744 72625 process.cpp:2436] Resuming log-write(5)@192.168.178.39:38758 at 2015-11-17 23:30:15.779737088+00:00
I1117 15:30:15.779979 72627 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.779972096+00:00
I1117 15:30:15.780164 72627 slave.cpp:3169] Received ping from slave-observer(1)@192.168.178.39:38758
I1117 15:30:15.780181 72625 process.cpp:2541] Cleaning up log-write(5)@192.168.178.39:38758
I1117 15:30:15.780249 72625 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.780244224+00:00
I1117 15:30:15.780333 72627 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.780325888+00:00
I1117 15:30:15.780481 72627 process.cpp:2436] Resuming (2)@192.168.178.39:38758 at 2015-11-17 23:30:15.780473856+00:00
I1117 15:30:15.780395 72628 process.cpp:2436] Resuming slave-observer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.780379904+00:00
I1117 15:30:15.780781 72625 process.cpp:2436] Resuming log-replica(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.780774144+00:00
I1117 15:30:15.780864 72627 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.780857088+00:00
I1117 15:30:15.780874 72625 replica.cpp:694] Replica received learned notice for position 4 from @0.0.0.0:0
I1117 15:30:15.781124 72632 process.cpp:2436] Resuming (1)@192.168.178.39:38758 at 2015-11-17 23:30:15.781102848+00:00
I1117 15:30:15.781168 72629 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.781160960+00:00
I1117 15:30:15.781159 72631 master.cpp:3927] Registered slave 0858c91e-303d-4b36-b940-4f96102ffc9a-S0 at slave(1)@192.168.178.39:38758 (ubuntu) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1117 15:30:15.781278 72629 slave.cpp:904] Registered with master master@192.168.178.39:38758; given slave ID 0858c91e-303d-4b36-b940-4f96102ffc9a-S0
I1117 15:30:15.781308 72629 fetcher.cpp:79] Clearing fetcher cache
I1117 15:30:15.781348 72632 hierarchical.cpp:344] Added slave 0858c91e-303d-4b36-b940-4f96102ffc9a-S0 (ubuntu) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1117 15:30:15.781429 72631 process.cpp:2436] Resuming (13)@192.168.178.39:38758 at 2015-11-17 23:30:15.781420032+00:00
I1117 15:30:15.781471 72631 status_update_manager.cpp:183] Resuming sending status updates
I1117 15:30:15.781560 72629 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/meta/slaves/0858c91e-303d-4b36-b940-4f96102ffc9a-S0/slave.info'
I1117 15:30:15.781600 72625 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 677566ns
I1117 15:30:15.781646 72625 leveldb.cpp:401] Deleting ~2 keys from leveldb took 25667ns
I1117 15:30:15.781661 72625 replica.cpp:715] Persisted action at 4
I1117 15:30:15.781680 72625 replica.cpp:700] Replica learned TRUNCATE action at position 4
I1117 15:30:15.781869 72630 process.cpp:2436] Resuming log-coordinator(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.781853952+00:00
I1117 15:30:15.781882 72629 slave.cpp:963] Forwarding total oversubscribed resources
I1117 15:30:15.781947 72629 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.781944064+00:00
I1117 15:30:15.782079 72629 master.cpp:4269] Received update of slave 0858c91e-303d-4b36-b940-4f96102ffc9a-S0 at slave(1)@192.168.178.39:38758 (ubuntu) with total oversubscribed resources
I1117 15:30:15.782102 72632 hierarchical.cpp:957] Allocating cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0858c91e-303d-4b36-b940-4f96102ffc9a-S0 to framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000
I1117 15:30:15.782130 72630 process.cpp:2436] Resuming (3)@192.168.178.39:38758 at 2015-11-17 23:30:15.782124032+00:00
I1117 15:30:15.782557 72632 hierarchical.cpp:1070] No inverse offers to send out!
I1117 15:30:15.782675 72632 hierarchical.cpp:892] Performed allocation for slave 0858c91e-303d-4b36-b940-4f96102ffc9a-S0 in 1.233311ms
I1117 15:30:15.782968 72632 hierarchical.cpp:400] Slave 0858c91e-303d-4b36-b940-4f96102ffc9a-S0 (ubuntu) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I1117 15:30:15.783316 72632 hierarchical.cpp:977] No resources available to allocate!
I1117 15:30:15.783375 72632 hierarchical.cpp:1070] No inverse offers to send out!
I1117 15:30:15.783396 72632 hierarchical.cpp:892] Performed allocation for slave 0858c91e-303d-4b36-b940-4f96102ffc9a-S0 in 373992ns
I1117 15:30:15.782618 72625 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.782608896+00:00
I1117 15:30:15.783892 72625 master.cpp:4979] Sending 1 offers to framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000 (default) at scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758
I1117 15:30:15.784214 72627 process.cpp:2436] Resuming scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758 at 2015-11-17 23:30:15.784201984+00:00
I1117 15:30:15.784445 72627 pid.cpp:93] Attempting to parse 'master@192.168.178.39:38758' into a PID
I1117 15:30:15.784502 72627 sched.cpp:789] Received 1 offers
I1117 15:30:15.784520 72627 pid.cpp:93] Attempting to parse 'slave(1)@192.168.178.39:38758' into a PID
I1117 15:30:15.784800 72627 sched.cpp:813] Scheduler::resourceOffers took 112395ns
I1117 15:30:15.784842 72625 process.cpp:2436] Resuming __latch__(9)@192.168.178.39:38758 at 2015-11-17 23:30:15.784833792+00:00
I1117 15:30:15.784881 72625 process.cpp:2541] Cleaning up __latch__(9)@192.168.178.39:38758
I1117 15:30:15.785029 72627 process.cpp:2436] Resuming __waiter__(4)@192.168.178.39:38758 at 2015-11-17 23:30:15.785021184+00:00
I1117 15:30:15.785037 72625 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.785032960+00:00
I1117 15:30:15.785226 72628 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.785219072+00:00
I1117 15:30:15.785337 72627 process.cpp:2541] Cleaning up __waiter__(4)@192.168.178.39:38758
I1117 15:30:15.785955 72632 process.cpp:2436] Resuming scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758 at 2015-11-17 23:30:15.785945856+00:00
I1117 15:30:15.786002 72611 process.cpp:2426] Spawned process __latch__(11)@192.168.178.39:38758
I1117 15:30:15.786049 72611 process.cpp:2426] Spawned process __waiter__(5)@192.168.178.39:38758
I1117 15:30:15.786085 72611 process.cpp:2734] Donating thread to __waiter__(5)@192.168.178.39:38758 while waiting
I1117 15:30:15.786116 72611 process.cpp:2436] Resuming __waiter__(5)@192.168.178.39:38758 at 2015-11-17 23:30:15.786109952+00:00
I1117 15:30:15.786240 72630 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.786232064+00:00
I1117 15:30:15.786317 72629 process.cpp:2436] Resuming __latch__(11)@192.168.178.39:38758 at 2015-11-17 23:30:15.786309888+00:00
I1117 15:30:15.786460 72632 pid.cpp:93] Attempting to parse 'master@192.168.178.39:38758' into a PID
I1117 15:30:15.786794 72630 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.786788096+00:00
I1117 15:30:15.787587 72630 master.cpp:2915] Processing ACCEPT call for offers: [ 0858c91e-303d-4b36-b940-4f96102ffc9a-O0 ] on slave 0858c91e-303d-4b36-b940-4f96102ffc9a-S0 at slave(1)@192.168.178.39:38758 (ubuntu) for framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000 (default) at scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758
I1117 15:30:15.787660 72630 master.cpp:2711] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
I1117 15:30:15.787824 72632 process.cpp:2436] Resuming authorizer(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.787812864+00:00
I1117 15:30:15.788250 72630 process.cpp:2426] Spawned process (18)@192.168.178.39:38758
I1117 15:30:15.788276 72629 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.788265984+00:00
I1117 15:30:15.788369 72629 process.cpp:2436] Resuming (18)@192.168.178.39:38758 at 2015-11-17 23:30:15.788364032+00:00
I1117 15:30:15.788748 72629 process.cpp:2541] Cleaning up (18)@192.168.178.39:38758
I1117 15:30:15.788751 72630 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:15.788739072+00:00
I1117 15:30:15.788820 72627 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.788813056+00:00
W1117 15:30:15.789516 72630 validation.cpp:422] Executor e1 for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W1117 15:30:15.789574 72630 validation.cpp:434] Executor e1 for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I1117 15:30:15.789929 72630 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0858c91e-303d-4b36-b940-4f96102ffc9a-S0 (ubuntu)
I1117 15:30:15.790050 72630 master.cpp:3245] Launching task 1 of framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000 (default) at scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0858c91e-303d-4b36-b940-4f96102ffc9a-S0 at slave(1)@192.168.178.39:38758 (ubuntu)
I1117 15:30:15.790287 72628 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.790277888+00:00
I1117 15:30:15.790452 72628 pid.cpp:93] Attempting to parse 'scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758' into a PID
I1117 15:30:15.790493 72628 slave.cpp:1294] Got assigned task 1 for framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000
I1117 15:30:15.791092 72628 slave.cpp:1410] Launching task 1 for framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000
I1117 15:30:15.792387 72628 paths.cpp:436] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/slaves/0858c91e-303d-4b36-b940-4f96102ffc9a-S0/frameworks/0858c91e-303d-4b36-b940-4f96102ffc9a-0000/executors/e1/runs/57938577-a370-4d25-970a-456735e837af' to user 'root'
I1117 15:30:15.795748 72628 slave.cpp:4999] Launching executor e1 of framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000 with resources  in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/slaves/0858c91e-303d-4b36-b940-4f96102ffc9a-S0/frameworks/0858c91e-303d-4b36-b940-4f96102ffc9a-0000/executors/e1/runs/57938577-a370-4d25-970a-456735e837af'
I1117 15:30:15.796106 72629 process.cpp:2436] Resuming files@192.168.178.39:38758 at 2015-11-17 23:30:15.796065024+00:00
I1117 15:30:15.796293 72626 process.cpp:2436] Resuming __latch__(11)@192.168.178.39:38758 at 2015-11-17 23:30:15.796272128+00:00
I1117 15:30:15.796452 72626 process.cpp:2541] Cleaning up __latch__(11)@192.168.178.39:38758
I1117 15:30:15.796586 72626 process.cpp:2436] Resuming (11)@192.168.178.39:38758 at 2015-11-17 23:30:15.796581888+00:00
I1117 15:30:15.796628 72629 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.796622080+00:00
I1117 15:30:15.796725 72628 slave.cpp:1628] Queuing task '1' for executor 'e1' of framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000
I1117 15:30:15.797873 72630 process.cpp:2436] Resuming __waiter__(5)@192.168.178.39:38758 at 2015-11-17 23:30:15.797456128+00:00
I1117 15:30:15.798015 72630 process.cpp:2541] Cleaning up __waiter__(5)@192.168.178.39:38758
I1117 15:30:15.798308 72611 process.cpp:2426] Spawned process __latch__(12)@192.168.178.39:38758
I1117 15:30:15.798382 72632 process.cpp:2436] Resuming __latch__(12)@192.168.178.39:38758 at 2015-11-17 23:30:15.798362112+00:00
I1117 15:30:15.798440 72611 process.cpp:2426] Spawned process __waiter__(6)@192.168.178.39:38758
I1117 15:30:15.798339 72625 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:15.798321920+00:00
I1117 15:30:15.798471 72631 process.cpp:2436] Resuming __waiter__(6)@192.168.178.39:38758 at 2015-11-17 23:30:15.798438912+00:00
I1117 15:30:15.798486 72628 slave.cpp:682] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/slaves/0858c91e-303d-4b36-b940-4f96102ffc9a-S0/frameworks/0858c91e-303d-4b36-b940-4f96102ffc9a-0000/executors/e1/runs/57938577-a370-4d25-970a-456735e837af'
I1117 15:30:15.801340 72626 docker.cpp:767] Starting container '57938577-a370-4d25-970a-456735e837af' for executor 'e1' and framework '0858c91e-303d-4b36-b940-4f96102ffc9a-0000'
I1117 15:30:15.802234 72626 docker.cpp:1010] Running docker -H unix:///var/run/docker.sock inspect tnachen/test-executor:latest
I1117 15:30:15.803982 72627 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.803824896+00:00
I1117 15:30:15.850070 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.850029056+00:00
I1117 15:30:15.851832 72632 process.cpp:2436] Resuming (11)@192.168.178.39:38758 at 2015-11-17 23:30:15.851800064+00:00
I1117 15:30:15.852000 72632 docker.cpp:361] Docker pull tnachen/test-executor completed
I1117 15:30:15.852596 72632 docker.cpp:564] Running docker -H unix:///var/run/docker.sock run -c 2048 -m 1073741824 -e LIBPROCESS_PORT=0 -e MESOS_CHECKPOINT=0 -e MESOS_DIRECTORY=/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/slaves/0858c91e-303d-4b36-b940-4f96102ffc9a-S0/frameworks/0858c91e-303d-4b36-b940-4f96102ffc9a-0000/executors/e1/runs/57938577-a370-4d25-970a-456735e837af -e MESOS_EXECUTOR_ID=e1 -e MESOS_FRAMEWORK_ID=0858c91e-303d-4b36-b940-4f96102ffc9a-0000 -e MESOS_SLAVE_ID=0858c91e-303d-4b36-b940-4f96102ffc9a-S0 -e MESOS_SLAVE_PID=slave(1)@192.168.178.39:38758 -e MESOS_SANDBOX=/mnt/mesos/sandbox -e MESOS_CONTAINER_NAME=mesos-0858c91e-303d-4b36-b940-4f96102ffc9a-S0.57938577-a370-4d25-970a-456735e837af -v /tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/slaves/0858c91e-303d-4b36-b940-4f96102ffc9a-S0/frameworks/0858c91e-303d-4b36-b940-4f96102ffc9a-0000/executors/e1/runs/57938577-a370-4d25-970a-456735e837af:/mnt/mesos/sandbox --net host --entrypoint /bin/sh --name mesos-0858c91e-303d-4b36-b940-4f96102ffc9a-S0.57938577-a370-4d25-970a-456735e837af tnachen/test-executor -c /bin/test-executor
I1117 15:30:15.854413 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.854358016+00:00
I1117 15:30:15.854701 72632 docker.cpp:723] Running docker -H unix:///var/run/docker.sock inspect mesos-0858c91e-303d-4b36-b940-4f96102ffc9a-S0.57938577-a370-4d25-970a-456735e837af
I1117 15:30:15.855816 72631 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.855742976+00:00
I1117 15:30:15.952574 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:15.952549888+00:00
I1117 15:30:15.953876 72629 docker.cpp:825] Retrying inspect since container not yet started. cmd: 'docker -H unix:///var/run/docker.sock inspect mesos-0858c91e-303d-4b36-b940-4f96102ffc9a-S0.57938577-a370-4d25-970a-456735e837af', interval: 1secs
I1117 15:30:16.055780 72627 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.055736064+00:00
I1117 15:30:16.075542 72626 process.cpp:2436] Resuming scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758 at 2015-11-17 23:30:16.075489024+00:00
I1117 15:30:16.157073 72626 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.157044992+00:00
I1117 15:30:16.257977 72626 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.257953792+00:00
I1117 15:30:16.359395 72628 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.359340032+00:00
I1117 15:30:16.461192 72628 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.461168896+00:00
I1117 15:30:16.562608 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.562572032+00:00
I1117 15:30:16.663671 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.663645952+00:00
I1117 15:30:16.695559 72627 process.cpp:2436] Resuming (1)@192.168.178.39:38758 at 2015-11-17 23:30:16.695532032+00:00
I1117 15:30:16.696324 72627 hierarchical.cpp:977] No resources available to allocate!
I1117 15:30:16.696403 72627 hierarchical.cpp:1070] No inverse offers to send out!
I1117 15:30:16.696441 72627 hierarchical.cpp:876] Performed allocation for 1 slaves in 689346ns
I1117 15:30:16.766157 72632 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.766114048+00:00
I1117 15:30:16.867794 72628 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.867769856+00:00
I1117 15:30:16.954242 72633 docker.cpp:723] Running docker -H unix:///var/run/docker.sock inspect mesos-0858c91e-303d-4b36-b940-4f96102ffc9a-S0.57938577-a370-4d25-970a-456735e837af
I1117 15:30:16.956143 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.956080896+00:00
I1117 15:30:16.958892 72629 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.958868992+00:00
I1117 15:30:16.959024 72629 process.cpp:3075] Handling HTTP event for process 'slave(1)' with path: '/slave%281%29/health'
I1117 15:30:16.959380 72629 process.cpp:2426] Spawned process __http__(1)@192.168.178.39:38758
I1117 15:30:16.959480 72630 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:16.959460864+00:00
I1117 15:30:16.959512 72626 process.cpp:2436] Resuming __http__(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.959501056+00:00
I1117 15:30:16.960325 72633 pid.cpp:93] Attempting to parse 'executor(1)@192.168.178.39:57979' into a PID
I1117 15:30:16.960567 72633 process.cpp:576] Parsed message name 'mesos.internal.RegisterExecutorMessage' for slave(1)@192.168.178.39:38758 from executor(1)@192.168.178.39:57979
I1117 15:30:16.960763 72632 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.960732928+00:00
I1117 15:30:16.960795 72633 process.cpp:2426] Spawned process __http__(2)@192.168.178.39:38758
I1117 15:30:16.960845 72633 process.cpp:2228] Accepted libprocess message to /slave%281%29/mesos.internal.RegisterExecutorMessage
I1117 15:30:16.960947 72632 slave.cpp:2405] Got registration for executor 'e1' of framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000 from executor(1)@192.168.178.39:57979
I1117 15:30:16.961333 72626 process.cpp:2436] Resuming __gc__@192.168.178.39:38758 at 2015-11-17 23:30:16.961319936+00:00
I1117 15:30:16.961486 72626 process.cpp:2436] Resuming __http__(2)@192.168.178.39:38758 at 2015-11-17 23:30:16.961469952+00:00
I1117 15:30:16.962164 72626 process.cpp:2436] Resuming (11)@192.168.178.39:38758 at 2015-11-17 23:30:16.962151168+00:00
I1117 15:30:16.962345 72626 docker.cpp:1012] Ignoring updating container '57938577-a370-4d25-970a-456735e837af' with resources passed to update is identical to existing resources
I1117 15:30:16.962537 72626 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.962523904+00:00
I1117 15:30:16.962846 72626 slave.cpp:1793] Sending queued task '1' to executor 'e1' of framework 0858c91e-303d-4b36-b940-4f96102ffc9a-0000 at executor(1)@192.168.178.39:57979
I1117 15:30:16.969185 72632 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:16.969150976+00:00
I1117 15:30:17.070058 72631 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.070013952+00:00
I1117 15:30:17.072867 72631 process.cpp:2436] Resuming (11)@192.168.178.39:38758 at 2015-11-17 23:30:17.072828928+00:00
I1117 15:30:17.073933 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.073921792+00:00
I1117 15:30:17.074542 72631 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.074532864+00:00
I1117 15:30:17.074908 72630 process.cpp:2436] Resuming (11)@192.168.178.39:38758 at 2015-11-17 23:30:17.074891008+00:00
I1117 15:30:17.173893 72631 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.173847808+00:00
I1117 15:30:17.275199 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.275170048+00:00
I1117 15:30:17.376538 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.376508928+00:00
I1117 15:30:17.477582 72630 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.477559040+00:00
I1117 15:30:17.578727 72632 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.578695936+00:00
I1117 15:30:17.679847 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.679824128+00:00
I1117 15:30:17.697546 72625 process.cpp:2436] Resuming (1)@192.168.178.39:38758 at 2015-11-17 23:30:17.697523968+00:00
I1117 15:30:17.698194 72625 hierarchical.cpp:977] No resources available to allocate!
I1117 15:30:17.698308 72625 hierarchical.cpp:1070] No inverse offers to send out!
I1117 15:30:17.698350 72625 hierarchical.cpp:876] Performed allocation for 1 slaves in 657357ns
I1117 15:30:17.780556 72630 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.780531968+00:00
I1117 15:30:17.881693 72628 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.881652224+00:00
I1117 15:30:17.957307 72627 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.957259776+00:00
I1117 15:30:17.957464 72627 process.cpp:3075] Handling HTTP event for process 'slave(1)' with path: '/slave%281%29/health'
I1117 15:30:17.957859 72630 process.cpp:2436] Resuming __http__(2)@192.168.178.39:38758 at 2015-11-17 23:30:17.957815808+00:00
I1117 15:30:17.983731 72627 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:17.983688192+00:00
I1117 15:30:18.084936 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:18.084912896+00:00
I1117 15:30:18.185655 72632 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:18.185627904+00:00
I1117 15:30:18.286857 72631 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:18.286829056+00:00
I1117 15:30:18.388072 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:18.388043008+00:00
I1117 15:30:18.490115 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:18.490092032+00:00
I1117 15:30:18.591320 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:18.591296000+00:00
I1117 15:30:18.692472 72632 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:18.692445952+00:00
I1117 15:30:18.699288 72631 process.cpp:2436] Resuming (1)@192.168.178.39:38758 at 2015-11-17 23:30:18.699262976+00:00
I1117 15:30:18.700294 72631 hierarchical.cpp:977] No resources available to allocate!
I1117 15:30:18.700372 72631 hierarchical.cpp:1070] No inverse offers to send out!
I1117 15:30:18.700413 72631 hierarchical.cpp:876] Performed allocation for 1 slaves in 703497ns
I1117 15:30:18.794410 72628 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:18.794385920+00:00
I1117 15:30:18.895831 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:18.895808000+00:00
I1117 15:30:18.956307 72632 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:18.956280832+00:00
I1117 15:30:18.956768 72632 process.cpp:3075] Handling HTTP event for process 'slave(1)' with path: '/slave%281%29/health'
I1117 15:30:18.957206 72632 process.cpp:2436] Resuming __http__(2)@192.168.178.39:38758 at 2015-11-17 23:30:18.957190144+00:00
I1117 15:30:18.997792 72632 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:18.997767936+00:00
I1117 15:30:19.099045 72628 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:19.099021824+00:00
I1117 15:30:19.200525 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:19.200502016+00:00
I1117 15:30:19.302006 72630 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:19.301979904+00:00
I1117 15:30:19.403499 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:19.403474944+00:00
I1117 15:30:19.505327 72626 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:19.505295104+00:00
I1117 15:30:19.606549 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:19.606525952+00:00
I1117 15:30:19.701881 72630 process.cpp:2436] Resuming (1)@192.168.178.39:38758 at 2015-11-17 23:30:19.701858048+00:00
I1117 15:30:19.702740 72630 hierarchical.cpp:977] No resources available to allocate!
I1117 15:30:19.702819 72630 hierarchical.cpp:1070] No inverse offers to send out!
I1117 15:30:19.702859 72630 hierarchical.cpp:876] Performed allocation for 1 slaves in 562956ns
I1117 15:30:19.708428 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:19.708406016+00:00
I1117 15:30:19.809787 72626 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:19.809761024+00:00
I1117 15:30:19.911202 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:19.911179008+00:00
I1117 15:30:19.955925 72630 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:19.955907072+00:00
I1117 15:30:19.956079 72630 process.cpp:3075] Handling HTTP event for process 'slave(1)' with path: '/slave%281%29/health'
I1117 15:30:19.956293 72630 process.cpp:2436] Resuming __http__(2)@192.168.178.39:38758 at 2015-11-17 23:30:19.956283136+00:00
I1117 15:30:20.012666 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.012638976+00:00
I1117 15:30:20.114159 72626 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.114128128+00:00
I1117 15:30:20.216086 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.216062976+00:00
I1117 15:30:20.317594 72630 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.317565952+00:00
I1117 15:30:20.419173 72626 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.419147008+00:00
I1117 15:30:20.520795 72628 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.520768768+00:00
I1117 15:30:20.544272 72633 process.cpp:2393] Dropping event for process __waiter__(1)@192.168.178.39:38758
I1117 15:30:20.622635 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.622610944+00:00
I1117 15:30:20.703586 72630 process.cpp:2436] Resuming (1)@192.168.178.39:38758 at 2015-11-17 23:30:20.703563008+00:00
I1117 15:30:20.704612 72630 hierarchical.cpp:977] No resources available to allocate!
I1117 15:30:20.704694 72630 hierarchical.cpp:1070] No inverse offers to send out!
I1117 15:30:20.704732 72630 hierarchical.cpp:876] Performed allocation for 1 slaves in 567602ns
I1117 15:30:20.723690 72626 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.723664896+00:00
I1117 15:30:20.759687 72628 process.cpp:2436] Resuming scheduler-f9434d44-b195-4616-b105-c4b2c58d5cd9@192.168.178.39:38758 at 2015-11-17 23:30:20.759664128+00:00
I1117 15:30:20.762142 72631 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:20.762114048+00:00
I1117 15:30:20.763861 72629 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.763834880+00:00
I1117 15:30:20.765234 72627 process.cpp:2436] Resuming master@192.168.178.39:38758 at 2015-11-17 23:30:20.765214976+00:00
I1117 15:30:20.825881 72630 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.825851904+00:00
I1117 15:30:20.927386 72632 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.927352064+00:00
I1117 15:30:20.957113 72626 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:20.957075968+00:00
I1117 15:30:20.957260 72626 process.cpp:3075] Handling HTTP event for process 'slave(1)' with path: '/slave%281%29/health'
I1117 15:30:20.957746 72630 process.cpp:2436] Resuming __http__(2)@192.168.178.39:38758 at 2015-11-17 23:30:20.957724160+00:00
I1117 15:30:21.029512 72627 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:21.029489152+00:00
I1117 15:30:21.131006 72632 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:21.130978048+00:00
I1117 15:30:21.232717 72626 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:21.232685056+00:00
I1117 15:30:21.334974 72630 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:21.334944000+00:00
I1117 15:30:21.436460 72628 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:21.436433152+00:00
I1117 15:30:21.538205 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:21.538182912+00:00
I1117 15:30:21.639508 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:21.639484928+00:00
I1117 15:30:21.705814 72626 process.cpp:2436] Resuming (1)@192.168.178.39:38758 at 2015-11-17 23:30:21.705789952+00:00
I1117 15:30:21.706847 72626 hierarchical.cpp:977] No resources available to allocate!
I1117 15:30:21.706935 72626 hierarchical.cpp:1070] No inverse offers to send out!
I1117 15:30:21.706977 72626 hierarchical.cpp:876] Performed allocation for 1 slaves in 688173ns
I1117 15:30:21.744493 72630 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:21.744448000+00:00
I1117 15:30:21.846101 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:21.846075904+00:00
I1117 15:30:21.947655 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:21.947632128+00:00
I1117 15:30:21.957109 72626 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:21.957084928+00:00
I1117 15:30:21.957715 72626 process.cpp:3075] Handling HTTP event for process 'slave(1)' with path: '/slave%281%29/health'
I1117 15:30:21.958012 72626 process.cpp:2436] Resuming __http__(2)@192.168.178.39:38758 at 2015-11-17 23:30:21.958000896+00:00
I1117 15:30:22.050115 72630 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:22.050089984+00:00
I1117 15:30:22.151379 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:22.151355136+00:00
I1117 15:30:22.252935 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:22.252909824+00:00
I1117 15:30:22.354441 72626 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:22.354414080+00:00
I1117 15:30:22.456123 72630 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:22.456096768+00:00
I1117 15:30:22.557734 72629 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:22.557709056+00:00
I1117 15:30:22.659188 72625 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:22.659166208+00:00
I1117 15:30:22.707701 72626 process.cpp:2436] Resuming (1)@192.168.178.39:38758 at 2015-11-17 23:30:22.707671040+00:00
I1117 15:30:22.708572 72626 hierarchical.cpp:977] No resources available to allocate!
I1117 15:30:22.708869 72626 hierarchical.cpp:1070] No inverse offers to send out!
I1117 15:30:22.709060 72626 hierarchical.cpp:876] Performed allocation for 1 slaves in 797704ns
I1117 15:30:22.761339 72631 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:22.761317888+00:00
I1117 15:30:22.862426 72627 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:22.862401024+00:00
I1117 15:30:22.957314 72632 process.cpp:2436] Resuming slave(1)@192.168.178.39:38758 at 2015-11-17 23:30:22.957287168+00:00
I1117 15:30:22.957927 72632 process.cpp:3075] Handling HTTP event for process 'slave(1)' with path: '/slave%281%29/health'
I1117 15:30:22.958273 72632 process.cpp:2436] Resuming __http__(2)@192.168.178.39:38758 at 2015-11-17 23:30:22.958260992+00:00
I1117 15:30:22.963749 72628 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:22.963723776+00:00
I1117 15:30:23.066083 72626 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:23.066057984+00:00
I1117 15:30:23.167398 72627 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:23.167373056+00:00
I1117 15:30:23.269269 72632 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:23.269239040+00:00
I1117 15:30:23.370478 72628 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:23.370455040+00:00
I1117 15:30:23.472092 72626 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:23.472066048+00:00
I1117 15:30:23.573709 72627 process.cpp:2436] Resuming reaper(1)@192.168.178.39:38758 at 2015-11-17 23:30:23.573684992+00:00
^C
{noformat}


Here is an inspect dump of that container:

{noformat}
$ sudo docker -H unix:///var/run/docker.sock inspect mesos-0858c91e-303d-4b36-b940-4f96102ffc9a-S0.57938577-a370-4d25-970a-456735e837af
[{
    "AppArmorProfile": "",
    "Args": [
        "-c",
        "/bin/test-executor"
    ],
    "Config": {
        "AttachStderr": true,
        "AttachStdin": false,
        "AttachStdout": true,
        "Cmd": [
            "-c",
            "/bin/test-executor"
        ],
        "CpuShares": 2048,
        "Cpuset": "",
        "Domainname": "",
        "Entrypoint": [
            "/bin/sh"
        ],
        "Env": [
            "LIBPROCESS_PORT=0",
            "MESOS_CHECKPOINT=0",
            "MESOS_DIRECTORY=/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/slaves/0858c91e-303d-4b36-b940-4f96102ffc9a-S0/frameworks/0858c91e-303d-4b36-b940-4f96102ffc9a-0000/executors/e1/runs/57938577-a370-4d25-970a-456735e837af",
            "MESOS_EXECUTOR_ID=e1",
            "MESOS_FRAMEWORK_ID=0858c91e-303d-4b36-b940-4f96102ffc9a-0000",
            "MESOS_SLAVE_ID=0858c91e-303d-4b36-b940-4f96102ffc9a-S0",
            "MESOS_SLAVE_PID=slave(1)@192.168.178.39:38758",
            "MESOS_SANDBOX=/mnt/mesos/sandbox",
            "MESOS_CONTAINER_NAME=mesos-0858c91e-303d-4b36-b940-4f96102ffc9a-S0.57938577-a370-4d25-970a-456735e837af",
            "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
        ],
        "ExposedPorts": null,
        "Hostname": "ubuntu",
        "Image": "tnachen/test-executor",
        "Labels": {},
        "MacAddress": "",
        "Memory": 1073741824,
        "MemorySwap": -1,
        "NetworkDisabled": false,
        "OnBuild": null,
        "OpenStdin": false,
        "PortSpecs": null,
        "StdinOnce": false,
        "Tty": false,
        "User": "",
        "Volumes": null,
        "WorkingDir": ""
    },
    "Created": "2015-11-17T23:30:15.867632119Z",
    "Driver": "aufs",
    "ExecDriver": "native-0.2",
    "ExecIDs": null,
    "HostConfig": {
        "Binds": [
            "/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/slaves/0858c91e-303d-4b36-b940-4f96102ffc9a-S0/frameworks/0858c91e-303d-4b36-b940-4f96102ffc9a-0000/executors/e1/runs/57938577-a370-4d25-970a-456735e837af:/mnt/mesos/sandbox"
        ],
        "CapAdd": null,
        "CapDrop": null,
        "CgroupParent": "",
        "ContainerIDFile": "",
        "CpuShares": 2048,
        "CpusetCpus": "",
        "Devices": [],
        "Dns": null,
        "DnsSearch": null,
        "ExtraHosts": null,
        "IpcMode": "",
        "Links": null,
        "LogConfig": {
            "Config": null,
            "Type": "json-file"
        },
        "LxcConf": [],
        "Memory": 1073741824,
        "MemorySwap": -1,
        "NetworkMode": "host",
        "PidMode": "",
        "PortBindings": {},
        "Privileged": false,
        "PublishAllPorts": false,
        "ReadonlyRootfs": false,
        "RestartPolicy": {
            "MaximumRetryCount": 0,
            "Name": "no"
        },
        "SecurityOpt": null,
        "Ulimits": null,
        "VolumesFrom": null
    },
    "HostnamePath": "/var/lib/docker/containers/d40eb2c390119b23b5a4f94a7afd0a47d881a9e99cf894ec06184262ebc5098b/hostname",
    "HostsPath": "/var/lib/docker/containers/d40eb2c390119b23b5a4f94a7afd0a47d881a9e99cf894ec06184262ebc5098b/hosts",
    "Id": "d40eb2c390119b23b5a4f94a7afd0a47d881a9e99cf894ec06184262ebc5098b",
    "Image": "58575e508df74b9faa07a10e3c80c75764e0669e1b89820519a5d449d34a551d",
    "LogPath": "/var/lib/docker/containers/d40eb2c390119b23b5a4f94a7afd0a47d881a9e99cf894ec06184262ebc5098b/d40eb2c390119b23b5a4f94a7afd0a47d881a9e99cf894ec06184262ebc5098b-json.log",
    "MountLabel": "",
    "Name": "/mesos-0858c91e-303d-4b36-b940-4f96102ffc9a-S0.57938577-a370-4d25-970a-456735e837af",
    "NetworkSettings": {
        "Bridge": "",
        "Gateway": "",
        "GlobalIPv6Address": "",
        "GlobalIPv6PrefixLen": 0,
        "IPAddress": "",
        "IPPrefixLen": 0,
        "IPv6Gateway": "",
        "LinkLocalIPv6Address": "",
        "LinkLocalIPv6PrefixLen": 0,
        "MacAddress": "",
        "PortMapping": null,
        "Ports": null
    },
    "Path": "/bin/sh",
    "ProcessLabel": "",
    "ResolvConfPath": "/var/lib/docker/containers/d40eb2c390119b23b5a4f94a7afd0a47d881a9e99cf894ec06184262ebc5098b/resolv.conf",
    "RestartCount": 0,
    "State": {
        "Dead": false,
        "Error": "",
        "ExitCode": 0,
        "FinishedAt": "2015-11-17T23:30:32.978540465Z",
        "OOMKilled": false,
        "Paused": false,
        "Pid": 0,
        "Restarting": false,
        "Running": false,
        "StartedAt": "2015-11-17T23:30:15.948355842Z"
    },
    "Volumes": {
        "/mnt/mesos/sandbox": "/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/slaves/0858c91e-303d-4b36-b940-4f96102ffc9a-S0/frameworks/0858c91e-303d-4b36-b940-4f96102ffc9a-0000/executors/e1/runs/57938577-a370-4d25-970a-456735e837af"
    },
    "VolumesRW": {
        "/mnt/mesos/sandbox": true
    }
}
]
{noformat}

The issue is with the version ubuntu you are using for it. If vagrant, then download the version which is compatible with Docker - "phusion/ubuntu-14.04-amd64". The trusty/ubuntu fails into this issue. It is a known bug with Docker - https://github.com/docker/docker/issues/4250
..........
$ cat /tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Dl9Yqf/slaves/0858c91e-303d-4b36-b940-4f96102ffc9a-S0/frameworks/0858c91e-303d-4b36-b940-4f96102ffc9a-0000/executors/e1/runs/latest/stderr
WARNING: Your kernel does not support swap limit capabilities. Limitation discarded."
..........
In short, OS is not supporting "-m" parameter with docker cli. 

Are you saying that the lack of memory limitation on the container side would fail this test?

Is the way Kernel was compiled ... possible not with .. CONFIG_MEMCG_SWAP_ENABLED .. ubuntu had that issue with 12.04 and I guess it carried forward with 14.04 trusty .. phusion seems to have solved the issue ... I ran into same issue on my local compile with vagrant and moving to kernel with this enabled, solved it ... please try this .. it might fix at your end too ... btw, there is suggestions on internet by making changes in grud file to solve this, they did not work for me ... but would be worth trying ...

add to:
/etc/default/grub:

GRUB_CMDLINE_LINUX_DEFAULT="cgroup_enable=memory swapaccount=1"

then sudo grub-update && sudo reboot


Vagrantfile:

{noformat}
:# -*- mode: ruby -*-" >
# vi: set ft=ruby :
Vagrant.configure(2) do |config|
  config.vm.box = "ubuntu/trusty64"
  config.vm.provider "virtualbox" do |vb|
    vb.memory = 16384
    vb.cpus = 8
  end
  config.vm.provision "shell", inline: <<-SHELL
    sudo apt-get update
    sudo apt-get install -y openjdk-7-jdk
    sudo apt-get install -y autoconf libtool
    sudo apt-get -y install build-essential python-dev python-boto libcurl4-nss-dev libsasl2-dev maven libapr1-dev libsvn-dev

    sudo apt-get install -y git

    # Docker:
    sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
    sudo rm -f /etc/apt/sources.list.d/docker.list
    sudo bash -c echo "deb https://apt.dockerproject.org/repo ubuntu-trusty main" > /etc/apt/sources.list.d/docker.list
    sudo apt-get update
    sudo apt-get purge lxc-docker*
    sudo apt-get install -y linux-image-extra-14.5.0

    sudo apt-get update
    apt-get -y install docker.io
    sudo docker run hello-world
  SHELL
end
{noformat}

The results remain the same after upgrading docker from 1.6.2 towards 1.9.

Tim had a great hint and that fixed the problem for me; the docker executor image was outdated.

A manual: {noformat}docker pull tnachen/test-executor{noformat} fixed the issue for me. 

Seems the reason for my problems was outdated proto code within the image.

Do you mean `tnachen/test-executor` repo?

Is it possible to share/explain the change done in the tnachen/test-executor? This info might be useful for creating docker related jobs.

This did nothing for me on Ubuntu. The test failure is still there.

You should try with docker friendly vagrant ubuntu image:

http://old.blog.phusion.nl/2013/11/08/docker-friendly-vagrant-boxes/

I believe 'trusty" is not. Moving to phusion I do not see this issue. 

Thanks! I'll try this. I wonder though what is needed to make Ubuntu "docker-friendly" beyond the instructions here: https://docs.docker.com/engine/installation/ubuntulinux/
 
OK, this is answered here:
https://github.com/phusion/baseimage-docker

Yes, it answers all the reasons ... another main reason was the way kernel was compiled with config parameters ..

Once you have the box up, try running docker with -c and -m parameters. If they work, the test cases ideally should also work ...

This is very helpful. Thanks again!

Can this bug be taken as documentation issue? With contribution page recommending right Ubuntu (vagrant) images.

I tried phusion/ubuntu-14.04-amd64. Now I get this:

{noformat}
./bin/mesos-tests.sh --gtest_filter="DockerContainerizerTest.ROOT_DOCKER_Launch_Executor"

Source directory: /home/vagrant/mesos
Build directory: /home/vagrant/mesos/build
-------------------------------------------------------------
We cannot run any cgroups tests that require mounting
hierarchies because you have the following hierarchies mounted:
/sys/fs/cgroup/blkio, /sys/fs/cgroup/cpu, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpuset, /sys/fs/cgroup/devices, /sys/fs/cgroup/freezer, /sys/fs/cgroup/hugetlb, /sys/fs/cgroup/memory, /sys/fs/cgroup/perf_event, /sys/fs/cgroup/systemd
We'll disable the CgroupsNoHierarchyTest test fixture for now.
...
{noformat}

You need to run mesos-tests as root since it's a ROOT_DOCKER test.

Sorry, forgot to write the sudo in the text above, but when I ran the test I typed it.

Ran the test on Ubuntu 14.04/vagrant-ubuntu-trusty-64 3.13.0-68-generic/mesos 0.25/ . Had similar results:


{code}
I1120 22:14:31.211333 16846 master.cpp:3248] Launching task 1 of framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000 (default) at scheduler-feee8846-7034-4fa4-8376-c42ca90fd83d@10.0.2.15:39850 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave a713297c-bce5-4207-ae08-b8b5d472f49b-S0 at slave(1)@10.0.2.15:39850 (vagrant-ubuntu-trusty-64)
I1120 22:14:31.211766 16851 slave.cpp:1270] Got assigned task 1 for framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000
I1120 22:14:31.212445 16851 slave.cpp:1386] Launching task 1 for framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000
I1120 22:14:31.217005 16851 slave.cpp:4852] Launching executor e1 of framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000 with resources  in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_F9qB7N/slaves/a713297c-bce5-4207-ae08-b8b5d472f49b-S0/frameworks/a713297c-bce5-4207-ae08-b8b5d472f49b-0000/executors/e1/runs/cd453018-9d0b-49b7-a5a7-5e841b3255d9'
I1120 22:14:31.219130 16851 slave.cpp:1604] Queuing task '1' for executor e1 of framework 'a713297c-bce5-4207-ae08-b8b5d472f49b-0000
I1120 22:14:31.220677 16849 docker.cpp:766] Starting container 'cd453018-9d0b-49b7-a5a7-5e841b3255d9' for executor 'e1' and framework 'a713297c-bce5-4207-ae08-b8b5d472f49b-0000'
E1120 22:14:31.810605 16849 slave.cpp:3342] Container 'cd453018-9d0b-49b7-a5a7-5e841b3255d9' for executor 'e1' of framework 'a713297c-bce5-4207-ae08-b8b5d472f49b-0000' failed to start: Container exited on error: exited with status 255
I1120 22:14:31.811228 16853 slave.cpp:3433] Executor 'e1' of framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000 has terminated with unknown status
I1120 22:14:31.812888 16853 slave.cpp:2717] Handling status update TASK_FAILED (UUID: 1c1f7bb8-4f9a-4757-b1ca-1c689e2d0fcb) for task 1 of framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000 from @0.0.0.0:0
W1120 22:14:31.813551 16847 docker.cpp:1002] Ignoring updating unknown container: cd453018-9d0b-49b7-a5a7-5e841b3255d9
I1120 22:14:31.813796 16853 master.cpp:4508] Executor e1 of framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000 on slave a713297c-bce5-4207-ae08-b8b5d472f49b-S0 at slave(1)@10.0.2.15:39850 (vagrant-ubuntu-trusty-64): terminated with signal Unknown signal 127
I1120 22:14:31.813849 16853 master.cpp:6178] Removing executor 'e1' with resources  of framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000 on slave a713297c-bce5-4207-ae08-b8b5d472f49b-S0 at slave(1)@10.0.2.15:39850 (vagrant-ubuntu-trusty-64)
I1120 22:14:31.814128 16847 status_update_manager.cpp:322] Received status update TASK_FAILED (UUID: 1c1f7bb8-4f9a-4757-b1ca-1c689e2d0fcb) for task 1 of framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000
I1120 22:14:31.814841 16852 slave.cpp:3016] Forwarding the update TASK_FAILED (UUID: 1c1f7bb8-4f9a-4757-b1ca-1c689e2d0fcb) for task 1 of framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000 to master@10.0.2.15:39850
I1120 22:14:31.815209 16852 master.cpp:4415] Status update TASK_FAILED (UUID: 1c1f7bb8-4f9a-4757-b1ca-1c689e2d0fcb) for task 1 of framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000 from slave a713297c-bce5-4207-ae08-b8b5d472f49b-S0 at slave(1)@10.0.2.15:39850 (vagrant-ubuntu-trusty-64)
I1120 22:14:31.815276 16852 master.cpp:4454] Forwarding status update TASK_FAILED (UUID: 1c1f7bb8-4f9a-4757-b1ca-1c689e2d0fcb) for task 1 of framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000
I1120 22:14:31.815479 16852 master.cpp:6081] Updating the latest state of task 1 of framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000 to TASK_FAILED
I1120 22:14:31.816187 16848 hierarchical.hpp:1103] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave a713297c-bce5-4207-ae08-b8b5d472f49b-S0 from framework a713297c-bce5-4207-ae08-b8b5d472f49b-0000
../../src/tests/containerizer/docker_containerizer_tests.cpp:254: Failure
Value of: statusRunning.get().state()
  Actual: TASK_FAILED
Expected: TASK_RUNNING
I1120 22:14:31.816408 16852 master

{code}

stderr for the run shows:

{code}
Warning: '-c' is deprecated, it will be replaced by '--cpu-shares' soon. See usage.
WARNING: Your kernel does not support swap limit capabilities, memory limited without swap.
F1120 22:18:10.410233 00005 node.go:18] Couldn't determine hostname: exit status 1
goroutine 1 [running]:
github.com/golang/glog.stacks(0xc20803b400, 0x0, 0x0, 0x0)
	/home/tnachen/go/src/github.com/golang/glog/glog.go:726 +0xcd
github.com/golang/glog.(*loggingT).output(0xa95fa0, 0xc200000003, 0xc208072000)
	/home/tnachen/go/src/github.com/golang/glog/glog.go:677 +0x24c
github.com/golang/glog.(*loggingT).printf(0xa95fa0, 0xc200000003, 0x8c8890, 0x1f, 0xc20807fc70, 0x1, 0x1)
	/home/tnachen/go/src/github.com/golang/glog/glog.go:635 +0x19c
github.com/golang/glog.Fatalf(0x8c8890, 0x1f, 0xc20807fc70, 0x1, 0x1)
	/home/tnachen/go/src/github.com/golang/glog/glog.go:1033 +0x64
github.com/mesos/mesos-go/mesosutil.GetHostname(0x0, 0x0, 0x0, 0x0)
	/home/tnachen/go/src/github.com/mesos/mesos-go/mesosutil/node.go:18 +0x227
github.com/mesos/mesos-go/executor.NewMesosExecutorDriver(0x7f357a2ae270, 0xc20800ac78, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
	/home/tnachen/go/src/github.com/mesos/mesos-go/executor/executor.go:81 +0x1de
main.main()
	/home/tnachen/go/src/github.com/mesos/mesos-go/examples/example_executor.go:108 +0x198

{code}

Kernel configuration as mentioned by [~vaibhavkhanduja]:

{code}
vagrant@vagrant-ubuntu-trusty-64:~/mesos-0.25.0/build$ grep MEMCG_SWAP_ENABLED /boot/config-3.13.0-68-generic 
# CONFIG_MEMCG_SWAP_ENABLED is not set
{code}

Note: sudo docker pull tnachen/test-executor was not helpful.




I just ran tests on vagrant:

"phusion/ubuntu-14.04-amd64"

.........
[==========] Running 20 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 20 tests from DockerContainerizerTest
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Launch_Executor
[       OK ] DockerContainerizerTest.ROOT_DOCKER_Launch_Executor (13819 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Launch
[       OK ] DockerContainerizerTest.ROOT_DOCKER_Launch (15737 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Kill
[       OK ] DockerContainerizerTest.ROOT_DOCKER_Kill (15830 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Usage
[       OK ] DockerContainerizerTest.ROOT_DOCKER_Usage (4169 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Update
[       OK ] DockerContainerizerTest.ROOT_DOCKER_Update (4199 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Recover
137
[       OK ] DockerContainerizerTest.ROOT_DOCKER_Recover (3061 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_SkipRecoverNonDocker
[       OK ] DockerContainerizerTest.ROOT_DOCKER_SkipRecoverNonDocker (473 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Logs
[       OK ] DockerContainerizerTest.ROOT_DOCKER_Logs (3721 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Default_CMD
[       OK ] DockerContainerizerTest.ROOT_DOCKER_Default_CMD (3676 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Override
[       OK ] DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Override (3766 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Args
[       OK ] DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Args (4268 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_SlaveRecoveryTaskContainer
[       OK ] DockerContainerizerTest.ROOT_DOCKER_SlaveRecoveryTaskContainer (7644 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_NC_PortMapping
[       OK ] DockerContainerizerTest.ROOT_DOCKER_NC_PortMapping (5445 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_LaunchSandboxWithColon
[       OK ] DockerContainerizerTest.ROOT_DOCKER_LaunchSandboxWithColon (5213 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_DestroyWhileFetching
[       OK ] DockerContainerizerTest.ROOT_DOCKER_DestroyWhileFetching (305 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_DestroyWhilePulling
[       OK ] DockerContainerizerTest.ROOT_DOCKER_DestroyWhilePulling (312 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_ExecutorCleanupWhenLaunchFailed
[       OK ] DockerContainerizerTest.ROOT_DOCKER_ExecutorCleanupWhenLaunchFailed (2278 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_FetchFailure
[       OK ] DockerContainerizerTest.ROOT_DOCKER_FetchFailure (305 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_DockerPullFailure
[       OK ] DockerContainerizerTest.ROOT_DOCKER_DockerPullFailure (305 ms)
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard
[       OK ] DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard (721 ms)
[----------] 20 tests from DockerContainerizerTest (95257 ms total)

[----------] Global test environment tear-down
[==========] 20 tests from 1 test case ran. (95477 ms total)
[  PASSED  ] 20 tests.

[~vaibhavkhanduja] What does your setup look like? Do you use the same vagrant file as I do? (see above) What needs to be changed?

[~bernd-mesos]
My environment seems to be same as your :) ... My vagrant file points to "config.vm.box = "phusion/ubuntu-14.04-amd64"". The vagrant provisioned using virtual box, with 4096 memory and 4 processors. I run this on my Mac book pro, with source checked out from github. The source code is checked out on my mac, mounted it under /vagrant. All build happens under /vagrant.
HTH

Could not reproduce this in physical machine. Going to try vagrant. Maybe this is a flaky test case?

This may very well be a config mistake. But then I'd like to know and document it. :-)

Haha, could reproduce through Vagrantfile.

Yes, change /etc/hosts to 
{noformat}
127.0.0.1 localhost vagrant-ubuntu-trusty-64
{noformat}
could pass the test. :-)

Also, [~tnachen] If you could open source the executor code would be nice. I could not found this in your github. :-)
{noformat}
/home/tnachen/go/src/github.com/mesos/mesos-go/examples/example_executor.go
{noformat}

I can't repro this with phusion/ubuntu-14.04-amd64 vagrant image?
example_executor.go is open source in mesos-go repo in mesos/mesos-go

LoL, could not found it now. https://github.com/mesos/mesos-go/search?utf8=%E2%9C%93&q=example_executor 
But I think it is because "hostname -f" command failed which node.go depends on it. Update /etc/hosts, it become ok.

My take is that to close this ticket we need to make sure we have viable instructions in the docs / on the web page.

So I read the thread and, honestly, it looks like we're making all this song and dance to make a test pass? who cares?
The question, with a failing test, is always the same:
{quote}
Is the test buggy, or are we uncovering a genuine issue in the code?
{quote}

It seems to me that this tests does not identify an issue in the code; at best, it has highlighted a combination of Ubuntu / Kernel / Docker versions/configurations that *may* cause an Executor launched inside a Docker container to fail (and, even there, I'm not so sure).

Also, please let's remind ourselves that tests are useful so that, when introducing code changes; refactorings; or new features, we can be assured that we haven't broken something that was working before: I'm not even sure this test achieves that?
(this may be a harsh statement borne out of my ignorance - please, correct me if I'm wrong on this one).

Here is my suggestion as to how to solve this issue:

- short-term: we disable this test and remove it as a {{0.26}} blocker (it doesn't seem to me that the failure highlights a regression in the code - again, correct me if I'm wrong);
- short-term: document the issue and possible workarounds for folks who may need to run Docker executors on Ubuntu;
- medium-term: if possible at all, let's find ways to identify in the test the conditions under which it's supposed to pass and, if they are met on the given platform the test is run - if not, a warning is emitted, but no failure (or something similar);
- long-run: decide whether to keep the test (modified, possibly) and / or discard it.

What do people think?

>> short-term: document the issue and possible workarounds for folks who may need to run Docker executors on Ubuntu;

I would like to take this one on and propose an RR - stay tuned....

When using the above image, the following is true for me:

The test breaks as described by Bernd.

{noformat}
$ hostname -f
vagrant-ubuntu-trusty-64
{noformat}

{noformat}
$ ifconfig
docker0   Link encap:Ethernet  HWaddr 56:84:7a:fe:97:99
          inet addr:172.17.42.1  Bcast:0.0.0.0  Mask:255.255.0.0
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)

eth0      Link encap:Ethernet  HWaddr 08:00:27:70:2a:9d
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fe70:2a9d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:246548 errors:0 dropped:0 overruns:0 frame:0
          TX packets:65399 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:298078841 (298.0 MB)  TX bytes:6093076 (6.0 MB)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:48338 errors:0 dropped:0 overruns:0 frame:0
          TX packets:48338 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:5936578 (5.9 MB)  TX bytes:5936578 (5.9 MB)
{noformat}

{noformat}
$ ping vagrant-ubuntu-trusty-64
PING vagrant-ubuntu-trusty-64 (10.0.2.15) 56(84) bytes of data.
64 bytes from vagrant-ubuntu-trusty-64 (10.0.2.15): icmp_seq=1 ttl=64 time=0.026 ms
{noformat}

So apparently, the hostname resolves towards a valid, non loopback IP (the one used by eth0).

{noformat}
$ cat /etc/hosts
127.0.0.1 localhost

# The following lines are desirable for IPv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts
{noformat}

Why would I need to add this hostname to {{/etc/hosts}} - despite the fact that it fixes this test - but why?

I also tried a different image first -- most other images seem to not have this issue as they use a different approach for binding the hostname towards an IP.

{noformat}
$ hostname -f
vagrant.vm
{noformat}

{noformat}
$ cat /etc/hosts
127.0.0.1	localhost
127.0.1.1	vagrant.vm	vagrant

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
{noformat}

The above is produced by a {{bento/ubuntu-14.04}} base image.

Boils down to this line being triggered within the docker image of this test:
https://github.com/mesos/mesos-go/blob/068d5470506e3780189fe607af40892814197c5e/mesosutil/node.go#L18

https://reviews.apache.org/r/40748/

Turns out that this simply is a technical limitation of Docker's host-networking. Docker will simply use {{/etc/hosts}} of the host system and if that does not contain the hostname, will rely on the DNS. As for the above Vagrant image, there is no DNS resolving the hostname, things go haywire.

For fixing the problem in the above Vagrantfile, all that is needed is adding something like {{config.vm.hostname = "minion"}} which will create an entry in {{/etc/hosts}} without further manual intervention.

{noformat}
commit 8b893e56ce8f7c73a444c6f76f49d7b6a5d54a3d
Author: Till Toenshoff <toenshoff@me.com>
Date:   Thu Nov 26 14:16:31 2015 +0100

    Updated documentation to point out the need of a resolvable hostname.

    Review: https://reviews.apache.org/r/40748
{noformat}


Thanks for dig out. And the vagrant image "ubuntu/trusty64" a bit strange, in /etc/nsswitch.conf, host resolve order is 
{noformat}
hosts:          files myhostname dns
{noformat}

While in general, both docker image and the iso I download form ubuntu mirror, "/etc/nsswitch.conf", host reolve only have
{noformat}
hosts:          files dns
{noformat}

If remove "myhostname" in "ubuntu/trusty64", it also could not resolve "vagrant-ubuntu-trusty-64.localdomain" and "hostname -f" failed.

Reopening this ticket for suggesting the already mentioned "medium-term" solution as also hinted by the last comment on https://reviews.apache.org/r/40748/.

We might want to try to identify the lack of a resolvable hostname before attempting to run the above tests. Doing so within the test environment analysis allows us to exclude this/these test/s while also making the user aware of this configuration problem.

FYI had the same error for DockerContainerizerTest.ROOT_DOCKER_Kill on Debian 8.

After looking into this, I'm confident that this is due to a docker bug, or some configuration issue from our side.
Docker uses the {{/etc/hosts}} file (among others) of the docker host in the container (using a mount) and changes it to include the hostname of the docker container. The Mesos containerizer uses the same hostname in the container as the host. For reasons I still have to investigate, the hostname in the container isn't changed by docker and is the same as the one of the host. Therefore, if the {{/etc/hosts}} of the docker host doesn't contain it's hostname, hostname resolution in the container won't work.
It seems to be a flag that the containerizer uses in {{docker run}}, because something like {{docker run --rm -t tnachen/test-executor -h foo cat /etc/hosts}} shows that docker's rewriting of {{/etc/hosts}} does indeed work.

Seems to be related to https://github.com/docker/docker/issues/17190#issuecomment-149438025
The overwriting of {{/etc/hosts}} by Docker is inherently racy. Seems like our test setup triggers this behavior somehow or {{/etc/hosts}} isn't overwritten. Hostname resolution inside the container will fail if the hosts hostname isn't in the container's {{/etc/hosts}} if it couldn't be overwritten.

Do you logs for this? I don't think that the issues are related, because  {{DockerContainerizerTest.ROOT_DOCKER_Kill}} doesn't launch a test executor and it fails because it can't resolve the containers hostname.

Docker _solved_ this problem by embedding a DNS, starting with version 1.10. See https://docs.docker.com/engine/userguide/networking/configure-dns/. IIRC the Docker containerizer doesn't make use of this.

