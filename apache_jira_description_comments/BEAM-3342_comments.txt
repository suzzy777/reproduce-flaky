I started with a simple pipeline that writes to Cloud Bigtable via the google.cloud bigtable package, which works locally with google.cloud installed, but doesn't work when I use a dataflow runner.  Here's what I get:

==
  message:  "Not processing workitem 2633526545277283048 since a deferred exception was found: Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/dataflow_worker/batchworker.py", line 706, in run
    self._load_main_session(self.local_staging_directory)
  File "/usr/local/lib/python2.7/dist-packages/dataflow_worker/batchworker.py", line 446, in _load_main_session
    pickler.load_session(session_file)
  File "/usr/local/lib/python2.7/dist-packages/apache_beam/internal/pickler.py", line 247, in load_session
    return dill.load_session(file_path)
  File "/usr/local/lib/python2.7/dist-packages/dill/dill.py", line 363, in load_session
    module = unpickler.load()
  File "/usr/lib/python2.7/pickle.py", line 858, in load
    dispatch[key](self)
  File "/usr/lib/python2.7/pickle.py", line 1133, in load_reduce
    value = func(*args)
  File "/usr/local/lib/python2.7/dist-packages/dill/dill.py", line 767, in _import_module
    return getattr(__import__(module, None, None, [obj]), obj)
AttributeError: 'module' object has no attribute 'bigtable'
==

Can I use the standard google.cloud bigtable client?  If so, how, and why don't BigQuery and Storage use the google.cloud clients?

cc: [~chamikara]

Great to hear that you are adding a Python connector.

Is this the correct version of the client: https://pypi.python.org/pypi/google-cloud-bigtable/0.28.1
If so probably you just need to add a dependency to https://github.com/apache/beam/blob/master/sdks/python/setup.py#L123.

For BigQuery and GCS we use client libraries generated by apitools (not exactly sure why we did that but possibly due to stable clients not being available at that time).

It turns out that we have quite a bit of work to do on the core Cloud Bigtable python client in order to make an effective Beam connector.  It could be a while before the client is ready.  

What issues are you facing?

I did a quick glance at the Datastore connector and it seems like everything it does is already supported by Google's BigTable client.

I ask because my workloads would be much easier to write in Python than Java.

The Cloud Bigtable client is just about ready with full functionality.  It did indeed take longer that we were expected.  Once we do that, there's a likelihood that a Python write connector will significantly underperform compared to Java, since the Python client only performs synchronous operations, where the Java has a high throughput asynchronous writer.

Also, in terms of reading from Cloud Bigtable, any connector needs full support for a BoundedSource, or something like it.  We could not figure out how to make BoundedSource work in Python.

Glad to hear we're getting close!

It looks like the Java implementation of `BoundedSource`, [https://github.com/apache/beam/blob/release-2.6.0/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigtable/BigtableIO.java#L786] , is just based on sampled row keys which is also available in the Python client under `table.sample_row_keys()`. Both Python and Java clients talk to the same API, don't they?

I'm happy to help out if you need a hand. I have some relevant experience in such things.

Note that I closed the duplicate of this issue but wanted to capture the design doc proposal form that JIRA: [https://docs.google.com/document/d/1iXeQvIAsGjp9orleDy0o5ExU-eMqWesgvtt231UoaPg/edit?usp=sharing]

Bigtable writes have been merged: https://github.com/apache/beam/pull/7367

We worked on reads offline, so hopefully that will land this quarter.

Is there an ETA on reads?

This issue is assigned but has not received an update in 30 days so it has been labeled "stale-assigned". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.

This issue was marked "stale-assigned" and has not received a public comment in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.

This issue is P2 but has been unassigned without any comment for 60 days so it has been labeled "stale-P2". If this issue is still affecting you, we care! Please comment and remove the label. Otherwise, in 14 days the issue will be moved to P3.

Please see https://beam.apache.org/contribute/jira-priorities/ for a detailed explanation of what these priorities mean.


This issue was marked "stale-P2" and has not received a public comment in 14 days. It is now automatically moved to P3. If you are still affected by it, you can comment and move it back to P2.

I would definitely like to see the bigtable reader implemented if it can be. Had to write a dataflow job in Java, which is really not my strong suit to do half the work then I can do the rest in Python, simply due to the bigtable reader I wrote being a simple dumb ParDo.

[~sduskis] are you still working on the BigTable reader? Would love to be able to use BigTable for both source and destination using Python.

This Jira ticket has a pull request attached to it, but is still open. Did the pull request resolve the issue? If so, could you please mark it resolved? This will help the project have a clear view of its open issues.

This issue has been migrated to https://github.com/apache/beam/issues/18722

