While we're at it, we should probably consolidate the Maven builds so that we don't have separate "with-YARN" and "pre-YARN" builds; the "Spark-Master-SBT" job tests the same four Hadoop versions, but it does it as a single paramerized Jenkins job, not two parameterized jobs.

*Edit*: actually, we need separate jobs for now since the YARN versions require a different Maven flag, whereas the SBT build takes care of this by using a AMPLAB_JENKINS_BUILD_PROFILE environment variable to set the right flags in run-tests-jenkins.

I've created Jenkins builds for this: https://amplab.cs.berkeley.edu/jenkins/view/Spark/

It looks like nearly all of these new builds are failing for various reasons, so I could use some help fixing them.

One issue is that several of the PySpark tests are failing with

{code}

OK (skipped=1)
Traceback (most recent call last):
  File "pyspark/mllib/_common.py", line 20, in <module>
    import numpy
  File "/usr/lib64/python2.6/site-packages/numpy/__init__.py", line 170, in <module>
    from . import add_newdocs
  File "/usr/lib64/python2.6/site-packages/numpy/add_newdocs.py", line 13, in <module>
    from numpy.lib import add_newdoc
  File "/usr/lib64/python2.6/site-packages/numpy/lib/__init__.py", line 8, in <module>
    from .type_check import *
  File "/usr/lib64/python2.6/site-packages/numpy/lib/type_check.py", line 11, in <module>
    import numpy.core.numeric as _nx
  File "/usr/lib64/python2.6/site-packages/numpy/core/__init__.py", line 46, in <module>
    from numpy.testing import Tester
  File "/usr/lib64/python2.6/site-packages/numpy/testing/__init__.py", line 13, in <module>
    from .utils import *
  File "/usr/lib64/python2.6/site-packages/numpy/testing/utils.py", line 15, in <module>
    from tempfile import mkdtemp
  File "/usr/lib64/python2.6/tempfile.py", line 34, in <module>
    from random import Random as _Random
  File "/home/jenkins/workspace/Spark-1.1-SBT/AMPLAB_JENKINS_BUILD_PROFILE/hadoop1.0/label/centos/python/pyspark/mllib/random.py", line 23, in <module>
    from pyspark.rdd import RDD
  File "/home/jenkins/workspace/Spark-1.1-SBT/AMPLAB_JENKINS_BUILD_PROFILE/hadoop1.0/label/centos/python/pyspark/__init__.py", line 63, in <module>
    from pyspark.context import SparkContext
  File "/home/jenkins/workspace/Spark-1.1-SBT/AMPLAB_JENKINS_BUILD_PROFILE/hadoop1.0/label/centos/python/pyspark/context.py", line 22, in <module>
    from tempfile import NamedTemporaryFile
ImportError: cannot import name NamedTemporaryFile
{code}

Some of the other failures might just be due to flaky tests exposed by higher Jenkins loads; let's see if they persist after rebuilds.

Hmm, it looks like the Python issue is an occurrence of SPARK-3910.  This _used_ to work, so I'm not sure why it's failing now.

I think a number of these are just due to running many tests at the same time. Lots failed with something like:

{code}
=====================================
HiveThriftServer2Suite failure output
=====================================
...
stderr> 15/01/07 13:06:53 INFO Utils: Successfully started service 'HTTP file server' on port 38796.
stderr> 15/01/07 13:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
stderr> 15/01/07 13:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
stderr> 15/01/07 13:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
stderr> 15/01/07 13:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
stderr> 15/01/07 13:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
stderr> 15/01/07 13:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
stderr> 15/01/07 13:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
stderr> 15/01/07 13:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
stderr> 15/01/07 13:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.
stderr> 15/01/07 13:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.
stderr> 15/01/07 13:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.
stderr> 15/01/07 13:06:54 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.
stderr> 15/01/07 13:06:54 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.
stderr> 15/01/07 13:06:54 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.
stderr> 15/01/07 13:06:54 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.
stderr> 15/01/07 13:06:54 WARN Utils: Service 'SparkUI' could not bind on port 4055. Attempting port 4056.
stderr> 15/01/07 13:06:54 ERROR SparkUI: Failed to bind SparkUI
stderr> java.net.BindException: Address already in use: Service 'SparkUI' failed after 16 retries!
{code}

Two of the Spark 1.2 builds passed subsequently after hitting this error.

I would try just restarting these jobs but I don't think I can. Most might not really be failing.

It doesn't look like the "disable web UI in tests" patches were backported to {{branch-1.0}}, so that's probably a large part of the problem.

In the case of the ThriftServer2 tests, I guess this is another case where our trick to disable the UI in tests did not apply to a process that was launched through shell scripts, so we may need to apply a fix that's similar to what I did for the SparkSubmitSuite tests.

It looks like there's some additional sources of flakiness in {{branch-1.2}}, though.  For example, "org.apache.spark.metrics.InputOutputMetricsSuite.input metrics when reading text file with multiple splits" has failed on-and-off for a few builds:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.2-SBT/5/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/testReport/junit/org.apache.spark.metrics/InputOutputMetricsSuite/input_metrics_when_reading_text_file_with_multiple_splits/

Ah, looks like Andrew has a PR to backport the webUI disabling fixes: https://github.com/apache/spark/pull/3959

I fixed the {{branch-1.1}} PySpark issue in https://github.com/apache/spark/pull/4011 and now have to fix a MiMa issue.

I'm going to resolve this as fixed, since the scope of this JIRA was to create the maintenance SBT builds and to get them into a serviceable state.  The current problems with some of those builds are outside the original scope of this JIRA and will be addressed separately.

