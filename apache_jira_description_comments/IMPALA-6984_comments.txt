Commit 705a5438027ecad591a6c8196d654b7a180fafc3 in impala's branch refs/heads/master from [~dhecht]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=705a543 ]

IMPALA-7101: Fix race between Fetch and Close RPCs that can lead to hang

If we hit EOS, we'll wait for all the backends to report status (to try
to get a complete profile). But if the query is closed after this point,
then we can get stuck waiting since once the query is closed,
ImpalaServer won't know about this coordinator and so it will stop
forwarding on the ReportStatus RPCs.

The real fix for this is IMPALA-6984, but in the mean time, add another
special case for this JIRA (see the other TODO IMPALA-6984 in
coordinator.cc).

The cancellation test only finds this race once in a while (several
hours) indirectly in a COMPUTE STATS query because the
ChildQueryExecutor will do a CloseOperation() while the execution thread
is inside Fetch(). To make this more reproducible, modify the
cancellation test to allow the close and fetch rpcs to execute
concurrently (don't join the test's fetch thread until after
close). This makes the race reproducible in a few iterations and a few
minutes.

Testing:
- Loop test_cancellation.py

Change-Id: I7c147550f86d81b818ecbdd34cf2919ced7ff8c5
Reviewed-on: http://gerrit.cloudera.org:8080/10601
Reviewed-by: Tim Armstrong <tarmstrong@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Commit 705a5438027ecad591a6c8196d654b7a180fafc3 in impala's branch refs/heads/master from [~dhecht]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=705a543 ]

IMPALA-7101: Fix race between Fetch and Close RPCs that can lead to hang

If we hit EOS, we'll wait for all the backends to report status (to try
to get a complete profile). But if the query is closed after this point,
then we can get stuck waiting since once the query is closed,
ImpalaServer won't know about this coordinator and so it will stop
forwarding on the ReportStatus RPCs.

The real fix for this is IMPALA-6984, but in the mean time, add another
special case for this JIRA (see the other TODO IMPALA-6984 in
coordinator.cc).

The cancellation test only finds this race once in a while (several
hours) indirectly in a COMPUTE STATS query because the
ChildQueryExecutor will do a CloseOperation() while the execution thread
is inside Fetch(). To make this more reproducible, modify the
cancellation test to allow the close and fetch rpcs to execute
concurrently (don't join the test's fetch thread until after
close). This makes the race reproducible in a few iterations and a few
minutes.

Testing:
- Loop test_cancellation.py

Change-Id: I7c147550f86d81b818ecbdd34cf2919ced7ff8c5
Reviewed-on: http://gerrit.cloudera.org:8080/10601
Reviewed-by: Tim Armstrong <tarmstrong@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Commit 04a6e805fb517967f60565e683db39fc7a17aa2c in impala's branch refs/heads/2.x from [~dhecht]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=04a6e80 ]

IMPALA-7101: Fix race between Fetch and Close RPCs that can lead to hang

If we hit EOS, we'll wait for all the backends to report status (to try
to get a complete profile). But if the query is closed after this point,
then we can get stuck waiting since once the query is closed,
ImpalaServer won't know about this coordinator and so it will stop
forwarding on the ReportStatus RPCs.

The real fix for this is IMPALA-6984, but in the mean time, add another
special case for this JIRA (see the other TODO IMPALA-6984 in
coordinator.cc).

The cancellation test only finds this race once in a while (several
hours) indirectly in a COMPUTE STATS query because the
ChildQueryExecutor will do a CloseOperation() while the execution thread
is inside Fetch(). To make this more reproducible, modify the
cancellation test to allow the close and fetch rpcs to execute
concurrently (don't join the test's fetch thread until after
close). This makes the race reproducible in a few iterations and a few
minutes.

Testing:
- Loop test_cancellation.py

Change-Id: I7c147550f86d81b818ecbdd34cf2919ced7ff8c5
Reviewed-on: http://gerrit.cloudera.org:8080/10601
Reviewed-by: Tim Armstrong <tarmstrong@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>
Reviewed-on: http://gerrit.cloudera.org:8080/10619
Tested-by: Tim Armstrong <tarmstrong@cloudera.com>


Commit 04a6e805fb517967f60565e683db39fc7a17aa2c in impala's branch refs/heads/2.x from [~dhecht]
[ https://git-wip-us.apache.org/repos/asf?p=impala.git;h=04a6e80 ]

IMPALA-7101: Fix race between Fetch and Close RPCs that can lead to hang

If we hit EOS, we'll wait for all the backends to report status (to try
to get a complete profile). But if the query is closed after this point,
then we can get stuck waiting since once the query is closed,
ImpalaServer won't know about this coordinator and so it will stop
forwarding on the ReportStatus RPCs.

The real fix for this is IMPALA-6984, but in the mean time, add another
special case for this JIRA (see the other TODO IMPALA-6984 in
coordinator.cc).

The cancellation test only finds this race once in a while (several
hours) indirectly in a COMPUTE STATS query because the
ChildQueryExecutor will do a CloseOperation() while the execution thread
is inside Fetch(). To make this more reproducible, modify the
cancellation test to allow the close and fetch rpcs to execute
concurrently (don't join the test's fetch thread until after
close). This makes the race reproducible in a few iterations and a few
minutes.

Testing:
- Loop test_cancellation.py

Change-Id: I7c147550f86d81b818ecbdd34cf2919ced7ff8c5
Reviewed-on: http://gerrit.cloudera.org:8080/10601
Reviewed-by: Tim Armstrong <tarmstrong@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>
Reviewed-on: http://gerrit.cloudera.org:8080/10619
Tested-by: Tim Armstrong <tarmstrong@cloudera.com>


Now that IMPALA-4063 is fixed, we should have a guarantee that the a final profile will be sent even when the fragment instances are cancelled. This is guaranteed as cancellation won't have any effect until all fragment instances on a backend have finished calling {{FIS:Prepare()}}Â  so the query state thread will definitely send one report upon completion of all fragment instances.

However, the logic in {{Coordinator::BackendState::Done()}} is preventing the final profile from being applied as {{status_}} is already set to {{CANCELLED}}. It may need to be fixed up in order for the coordinator to cleanly cancel the fragment instances on EOS.
{noformat}
inline bool Coordinator::BackendState::IsDone() const {
  return num_remaining_instances_ == 0 || !status_.ok();
}
{noformat}

[~stakiar] are you planning to work on this still? We think we might be seeing this in some multithreading benchmarks we're doing.

I'm interested in picking this up - I think I have an idea about how to implement it.


No plans to work on this in the near-term, feel free to pick it up.

Commit 506a303c6d48fd5d0653cdfd58b3ae419328ff31 in impala's branch refs/heads/master from Tim Armstrong
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=506a303 ]

IMPALA-6984: coordinator cancels backends on EOS

Before this patch, when the coordinator returned the last row,
it waited for backends to finish of their own accord, which
could happen indirectly as exchanges got closed.

The idea of this change is to send out cancellation RPCs to
expedite cancellation, then wait for the final exec status
reports to come in. Those reports will be included in the
final profile because the backend is *not* marked as done
when sending out the cancellation RPCs.

The bulk of this change is modifying the cancellation code
path to allow sending the cancel RPCs but *not* consider
the backend done until it gets back the final status report.
The old "fire and forget" mode of cancellation is still used
for explicit cancellation and errors.

Testing:
Ran exhaustive tests.

Ran cancellation tests under TSAN, checked for errors.

Manually inspected logs of some queries with limit,
saw that it sent cancellation then waited for backends
as expected.

Added a functional perf test that goes from ~5s down to < ~1s
on my system.

Change-Id: I966eceaafdc18a019708b780aee4ee9d70fd3a47
Reviewed-on: http://gerrit.cloudera.org:8080/15840
Reviewed-by: Thomas Tauber-Marshall <tmarshall@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Commit 95ee26354dc0ce61e5844430d1eaf553fd13d154 in impala's branch refs/heads/master from Sahil Takiar
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=95ee263 ]

IMPALA-9755: Flaky test: test_global_exchange_counters

De-flake TestObservability.test_global_exchange_counters in
test_observability.py.

IMPALA-6984 added a feature to send a Cancel RPC to running fragments
when the coordinator fragment fetches all rows defined by a limit. This
causes fragments to terminate early (which is a good thing). However,
test_global_exchange_counters expects each fragment to produce some
rows, which is why it recently became flaky.

This patch modifies test_global_exchange_counters so that it allows for
some fragments to produce 0 rows.

Testing:
* Ran test_observability.py locally
* Looped 8 concurrent streams of test_global_exchange_counters for an
  hour, no failures (previously I was able to reproduce the test issue
  within 5 minutes)

Change-Id: Icb3a1b5ccb5695eb71343e96cc830f12d5c72f1e
Reviewed-on: http://gerrit.cloudera.org:8080/15960
Reviewed-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Commit c413f9b558d51de877f497590baf14139ad5cf99 in impala's branch refs/heads/master from Joe McDonnell
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=c413f9b ]

IMPALA-10047: Revert core piece of IMPALA-6984

Performance testing on TPC-DS found a peformance regression
on short queries due to delayed exec status reports. Further
testing traced this back to IMPALA-6984's behavior of
cancelling backends on EOS. The coordinator log show that
CancelBackends() call intermittently taking 10 seconds due
to timing out in the RPC layer.

As a temporary workaround, this reverts the core part of
IMPALA-6984 that added that CancelBackends() call for EOS.
It leaves the rest of IMPALA-6984 intact, as other code has built
on top of it.

Testing:
 - Core job
 - Performance tests

Change-Id: Ibf00a56e91f0376eaaa552e3bb4763501bfb49e8
(cherry picked from commit b91f3c0e064d592f3cdf2a2e089ca6546133ba55)
Reviewed-on: http://gerrit.cloudera.org:8080/16288
Reviewed-by: Joe McDonnell <joemcdonnell@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Commit c413f9b558d51de877f497590baf14139ad5cf99 in impala's branch refs/heads/master from Joe McDonnell
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=c413f9b ]

IMPALA-10047: Revert core piece of IMPALA-6984

Performance testing on TPC-DS found a peformance regression
on short queries due to delayed exec status reports. Further
testing traced this back to IMPALA-6984's behavior of
cancelling backends on EOS. The coordinator log show that
CancelBackends() call intermittently taking 10 seconds due
to timing out in the RPC layer.

As a temporary workaround, this reverts the core part of
IMPALA-6984 that added that CancelBackends() call for EOS.
It leaves the rest of IMPALA-6984 intact, as other code has built
on top of it.

Testing:
 - Core job
 - Performance tests

Change-Id: Ibf00a56e91f0376eaaa552e3bb4763501bfb49e8
(cherry picked from commit b91f3c0e064d592f3cdf2a2e089ca6546133ba55)
Reviewed-on: http://gerrit.cloudera.org:8080/16288
Reviewed-by: Joe McDonnell <joemcdonnell@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Commit c413f9b558d51de877f497590baf14139ad5cf99 in impala's branch refs/heads/master from Joe McDonnell
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=c413f9b ]

IMPALA-10047: Revert core piece of IMPALA-6984

Performance testing on TPC-DS found a peformance regression
on short queries due to delayed exec status reports. Further
testing traced this back to IMPALA-6984's behavior of
cancelling backends on EOS. The coordinator log show that
CancelBackends() call intermittently taking 10 seconds due
to timing out in the RPC layer.

As a temporary workaround, this reverts the core part of
IMPALA-6984 that added that CancelBackends() call for EOS.
It leaves the rest of IMPALA-6984 intact, as other code has built
on top of it.

Testing:
 - Core job
 - Performance tests

Change-Id: Ibf00a56e91f0376eaaa552e3bb4763501bfb49e8
(cherry picked from commit b91f3c0e064d592f3cdf2a2e089ca6546133ba55)
Reviewed-on: http://gerrit.cloudera.org:8080/16288
Reviewed-by: Joe McDonnell <joemcdonnell@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Commit c413f9b558d51de877f497590baf14139ad5cf99 in impala's branch refs/heads/master from Joe McDonnell
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=c413f9b ]

IMPALA-10047: Revert core piece of IMPALA-6984

Performance testing on TPC-DS found a peformance regression
on short queries due to delayed exec status reports. Further
testing traced this back to IMPALA-6984's behavior of
cancelling backends on EOS. The coordinator log show that
CancelBackends() call intermittently taking 10 seconds due
to timing out in the RPC layer.

As a temporary workaround, this reverts the core part of
IMPALA-6984 that added that CancelBackends() call for EOS.
It leaves the rest of IMPALA-6984 intact, as other code has built
on top of it.

Testing:
 - Core job
 - Performance tests

Change-Id: Ibf00a56e91f0376eaaa552e3bb4763501bfb49e8
(cherry picked from commit b91f3c0e064d592f3cdf2a2e089ca6546133ba55)
Reviewed-on: http://gerrit.cloudera.org:8080/16288
Reviewed-by: Joe McDonnell <joemcdonnell@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Since the core part of this was reverted, reopening.

I think we need to avoid the self-RPCs at a minimum - IMPALA-5119 . We might also want to parallelise the cancel RPCs.

