One more example: https://api.travis-ci.org/jobs/181230039/log.txt?deansi=true

{code}
_binder: java.util.HashMap[String,Object] = {}
z: org.apache.zeppelin.spark.ZeppelinContext = org.apache.zeppelin.spark.ZeppelinContext@fb658a
sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@129ef7b1
sqlc: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@8c4a0f0
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@8c4a0f0
import org.apache.spark.SparkContext._
import sqlContext.implicits._
import sqlContext.sql
import org.apache.spark.sql.functions._
16/12/05 02:06:01 INFO PySparkInterpreter: File /tmp/zeppelin_pyspark-8457815456166793424.py created
16/12/05 02:06:01 INFO SparkInterpreter: Sending metainfos to Zeppelin server: {url=http://172.17.0.13:4040}
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 91.731 sec <<< FAILURE! - in org.apache.zeppelin.spark.PySparkInterpreterTest
testBasicIntp(org.apache.zeppelin.spark.PySparkInterpreterTest)  Time elapsed: 89.249 sec  <<< FAILURE!
java.lang.AssertionError: expected:<SUCCESS> but was:<ERROR>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.zeppelin.spark.PySparkInterpreterTest.testBasicIntp(PySparkInterpreterTest.java:128)
{code}



GitHub user Leemoonsoo opened a pull request:

    https://github.com/apache/zeppelin/pull/2017

    [ZEPPELIN-1746] Flaky test: PySparkInterpreterTest

    ### What is this PR for?
    PySparkInterpreter (and PySparkInterpreterMatplotlibTest, SparkInterpreterTest, SparkSqlInterpreterTest) test does not terminate SparkInterpreter after test end.
    
    In the beginning, keeping SparkInterpreter alive was designed to prevent create/destroy SparkInterpreter multiple times in various tests, because it takes some time.
    
    However, that is somehow misbehaving and getting problems like
    
    1. starting another SparkInterpreter before one SparkInterpreter terminates
    2. Does not finish PySparkInterpreter process explicitly cause py4j client somehow creates a lot of connection attempt, that consumes all capacity of systems port open, and make next SparkInterpreter start fail.
    
    This PR terminates SparkInterpreter after test class finishes, to prevent above behavior.
    
    ### What type of PR is it?
    Bug Fix
    
    ### Todos
    * [x] - Terminate SparkInterpreter after each test class
    
    ### What is the Jira issue?
    https://issues.apache.org/jira/browse/ZEPPELIN-1746
    
    ### How should this be tested?
    CI becomes green
    
    ### Questions:
    * Does the licenses files need update? no
    * Is there breaking changes for older versions? no
    * Does this needs documentation? no


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Leemoonsoo/zeppelin ZEPPELIN-1746

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/zeppelin/pull/2017.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2017
    
----
commit e6ba4e1414b6cedaddbcfbd251e721a9d8e91cc4
Author: Lee moon soo <moon@apache.org>
Date:   2017-02-13T17:48:59Z

    Try start and terminate spark context after each test class

----


Github user Leemoonsoo commented on the issue:

    https://github.com/apache/zeppelin/pull/2017
  
    Since many CI fails with this error, i'm merging it to master as a hotfix.


Github user asfgit closed the pull request at:

    https://github.com/apache/zeppelin/pull/2017


