It shows the similar ERROR message and stacktrace also when exit from spark shell.

{noformat}
scala> :quit
16/03/07 13:06:31 INFO SparkUI: Stopped Spark web UI at http://192.168.33.129:4040
16/03/07 13:06:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/03/07 13:06:31 INFO MemoryStore: MemoryStore cleared
16/03/07 13:06:31 INFO BlockManager: BlockManager stopped
16/03/07 13:06:31 INFO BlockManagerMaster: BlockManagerMaster stopped
16/03/07 13:06:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/03/07 13:06:31 INFO SparkContext: Successfully stopped SparkContext
[WARN] Task failed
java.lang.NoClassDefFoundError: Could not initialize class scala.tools.fusesource_embedded.jansi.internal.Kernel32
        at scala.tools.fusesource_embedded.jansi.internal.WindowsSupport.setConsoleMode(WindowsSupport.java:60)
        at scala.tools.jline_embedded.WindowsTerminal.setConsoleMode(WindowsTerminal.java:208)
        at scala.tools.jline_embedded.WindowsTerminal.restore(WindowsTerminal.java:95)
        at scala.tools.jline_embedded.TerminalSupport$1.run(TerminalSupport.java:52)
        at scala.tools.jline_embedded.internal.ShutdownHooks.runTasks(ShutdownHooks.java:66)
        at scala.tools.jline_embedded.internal.ShutdownHooks.access$000(ShutdownHooks.java:22)
        at scala.tools.jline_embedded.internal.ShutdownHooks$1.run(ShutdownHooks.java:47)

16/03/07 13:06:31 INFO ShutdownHookManager: Shutdown hook called
16/03/07 13:06:31 INFO ShutdownHookManager: Deleting directory C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940\repl-f753505f-69cf-4593-bb21-f5aa2683bcca
16/03/07 13:06:31 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940\repl-f753505f-69cf-4593-bb21-f5aa2683bcca
java.io.IOException: Failed to delete: C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940\repl-f753505f-69cf-4593-bb21-f5aa2683bcca
        at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:935)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:64)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:61)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:61)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:217)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1788)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:189)
        at scala.util.Try$.apply(Try.scala:192)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:179)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
16/03/07 13:06:31 INFO ShutdownHookManager: Deleting directory C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940
16/03/07 13:06:31 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940
java.io.IOException: Failed to delete: C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940
        at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:935)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:64)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:61)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:61)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:217)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1788)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:189)
        at scala.util.Try$.apply(Try.scala:192)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:179)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
{noformat}

I'm having the same issue. It's worse than that though..

Pasting code in the shell does random things like the following:

{code}
scala> (1 to 10).foreach(x => {
     |     println(x)
     | })
<console>:3: error: ';' expected but ')' found.
j)
 ^
{code}

Note how there is a magic *_j_* inserted in the code evaluated by the shell that wasn't present in pasted data!!

Something is clearly funky with jline here. It didn't init and who knows what that means about how lines are parsed in the shell. I don't know of any resolution; looks like a jline + Windows thing.

Googling around, it might be a conflict between jline2 and jline 0.9x

Scala 2.10.x was compiling a local copy of jline and as of 2.11.x, is using jline 2.12.1

Spark dependency tree shows: 
{code}
[INFO] +- org.apache.curator:curator-recipes:jar:2.4.0:compile
[INFO] |  +- org.apache.curator:curator-framework:jar:2.4.0:compile
[INFO] |  |  \- org.apache.curator:curator-client:jar:2.4.0:compile
[INFO] |  \- org.apache.zookeeper:zookeeper:jar:3.4.5:compile
[INFO] |     \- jline:jline:jar:0.9.94:compile
{code}


I'm not sure it broke something but seems to be working on my local build using a modified apache curator dependency like this:  

{code}
      <dependency>
        <groupId>org.apache.curator</groupId>
        <artifactId>curator-recipes</artifactId>
        <version>${curator.version}</version>
        <scope>${hadoop.deps.scope}</scope>
        <exclusions>
          <exclusion>
            <groupId>org.jboss.netty</groupId>
            <artifactId>netty</artifactId>
          </exclusion>
          <exclusion>
            <groupId>jline</groupId>
            <artifactId>jline</artifactId>
          </exclusion>
      </exclusions>
      </dependency>
{code}

I only excluded jline.


I agree that's a likely explanation. Maybe the conflict only surfaces in Windows. Curator is still used by some ZK-using parts of the code like the standalone Master, but, I can't quite figure out why it would need jline. It may only be needed if, say, you're running its command line tools. This exclusion may be a good idea then.

I wonder if this has consequences for the 2.10 build -- I'd suspect less of a problem if it was using the old jline?

Looks like it was renamed to a different package.  Therefore no conflict possible.
{code}
package scala.tools.jline
{code}

Hm. The maven coordinates didn't change. I wonder if there is somehow still an issue where the new version contains code in the old namespace for backwards compatibility or whatever. That is I wonder why the exclusion above fixed it for you?

Spark coordinates for curator did not change..  It still depends on Curator 2.4.0 that later depends on ZK and jline 0.9.94.

The problem is in Scala itself. It went from a local copy of jline sources in 2.10.x: https://github.com/scala/scala/tree/2.10.x/src/jline

to a standard maven import in 2.11.x:
https://github.com/scala/scala/blob/2.11.x/build.sbt#L74

However, I'm not sure exactly how it plays out at runtime but I guess it's something in the lines of:
- Spark jar is loaded in the JVM
- Scala runtime is loaded in the JVM but jline resources are discarded because one copy is already there (this part wasn't happening in scala 2.10.x since resources were names or located somewhere else)
- Spark tries to initialize the scala REPL but that fails because it loads the wrong jline/jansi


Since I'm no expert in JVM classloading and the likes so your guess is probably better than mine..

I think your exclusion is legitimate for curator-recipes. It shouldn't affect 2.10 in any event because it embeds its own copy. For 2.11, we're going to have conflicts if the assembly somehow resolves jline 0.9.x, which can happen with the closest-first dependency rules. Feel free to make a PR.

I'd gladly do that but the current state of the master branch is somewhat unstable..  Tens of tests fail OOTB from my vanilla fork before any modifications! Is that expected?

Although a couple of the thousands of tests are flaky and spurious failures are fairly common, I don't think you'd normally get lots of failures. It may be due to something more fundamental about the env or what you're building. You can post some errors here for a look. You can try opening a [WIP] PR to see how it tests versus Jenkins too .

User 'michellemay' has created a pull request for this issue:
https://github.com/apache/spark/pull/12043

Tests passes on Jenkins which is a good thing..  However, my local run did not bode so well:

> Tests: succeeded 1402, failed 23, canceled 0, ignored 12, pending 0
Failed tests:
   org.apache.spark.sql.SQLQuerySuite
   org.apache.spark.sql.DateFunctionsSuite
   org.apache.spark.sql.jdbc.JDBCSuite
   org.apache.spark.sql.sources.TableScanSuite
   org.apache.spark.sql.ScalaReflectionRelationSuite
   org.apache.spark.sql.execution.datasources.json.JsonSuite
   org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite

I'm running on a standard AWS linux instance (r3.2xlarge), with Java 1.8 with latest master branch..  What should I do with that ?


It depends on why it fails,  from the error logs earlier. I would guess there is some common cause, and it's something to do with the env, but don't know what.

Issue resolved by pull request 12043
[https://github.com/apache/spark/pull/12043]

I noticed that when using the binary package  "spark-2.0.0-bin-without-hadoop.tgz" (i.e. with user-provided Hadoop pointed to via "export SPARK_DIST_CLASSPATH=$(hadoop classpath)") the same error still happens.

java.lang.NoClassDefFoundError: Could not initialize class scala.tools.fusesource_embedded.jansi.internal.Kernel32
        at scala.tools.fusesource_embedded.jansi.internal.WindowsSupport.getConsoleMode(WindowsSupport.java:50)

I compared the jars provided with spark-2.0.0-bin-with-hadoop-2.7 to the ones provided with spark-2.0.0-bin-without-hadoop and noticed that jline-2.12.jar is present in the "with-hadoop" but is missing from the "without-hadoop" binary package.

When I copy the jline-2.12.jar to the jars folder of "withou-hadoop", I can start bin\spark-shell  without getting this error.

Is there a reason jline-2.12.jar is not part of the "without-hadoop" package?

Possibly related to https://github.com/apache/spark/commit/4775eb414fa8285cfdc301e52dac52a2ef64c9e1 / SPARK-16770 ?

