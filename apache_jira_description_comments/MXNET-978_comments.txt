There's a lot of work to fully support this for all operators.  I think we should pick a philosophy for prioritizing that allows real applications to be built as quickly as possible. There are lots of applications when this works, like better optimization algorithms, GAN training, RL algorithms, neural architecture search, etc. Each of these require having the second derivative for every op in a network. So we should pick useful network architectures where we can get the 2nd derivative for the entire network.

In order to get something working as quickly as possible, that implies to me starting with the simplest useful network architectures, and then moving towards progressively more complex architectures ordered by how useful/important they are. This makes me think the order should be approximately:
 * Fully-connected feedforward networks (multi-layer perceptron MLP)
 * CNN. Starting with AlexNet (simple) and then adding ResNet (common) and similar
 * RNN. Start with simple stacked RNN, then add LSTM, GRU, encoder-decoder, attention, transformer, etc
 * Everything else

Something like that for order of architecture types. But I do think it makes sense to start with MLP since that's the easiest way to get an end-to-end example working, and does cover some interesting real-world use cases. Also MLP requires a pretty short list of ops. I think it's basically:
 * Fully-connected layer (vector-matrix product)
 * Softmax output (most common output)
 * ReLU activation (most common, also trivial 2nd derivative)
 * Dropout (not required for MLP, but very commonly used)
 * Batch-Norm (not required for MLP, but quite useful in my experience to keep the optimization well conditioned)
 * Anything else?

 

here is the list of operators who do now have an FGradient assigned to them with there status

[x] = finished

[-] = in progress 

[] BatchNorm_v1
 [] IdentityAttachKLSparseReg
 [] LeakyReLU
 [] Pad
 [] SliceChannel
 [] SwapAxis
 [] _NoGradient
 [] _arange
 [] _backward_Activation
 [] _backward_BatchNorm
 [] _backward_CachedOp
 [] _backward_Concat
 [] _backward_Convolution
 [] _backward_Custom
 [] _backward_CustomFunction
 [] _backward_Deconvolution
 [] _backward_Dropout
 [] _backward_Embedding
 [] _backward_FullyConnected
 [] _backward_LRN
 [] _backward_LayerNorm
 [] _backward_Pooling
 [] _backward_ROIAlign
 [] _backward_SoftmaxActivation
 [] _backward_SparseEmbedding
 [] _backward_UpSampling
 [] _backward_abs
 [] _backward_add
 [] _backward_arccos
 [] _backward_arccosh
 [] _backward_arcsin
 [] _backward_arcsinh
 [] _backward_arctan
 [] _backward_arctanh
 [] _backward_batch_dot
 [] _backward_broadcast_add
 [] _backward_broadcast_div
 [] _backward_broadcast_hypot
 [] _backward_broadcast_maximum
 [] _backward_broadcast_minimum
 [] _backward_broadcast_mod
 [] _backward_broadcast_mul
 [] _backward_broadcast_power
 [] _backward_broadcast_sub
 [] _backward_cast
 [] _backward_cbrt
 [] _backward_clip
 [] _backward_cond
 [] _backward_contrib_AdaptiveAvgPooling2D
 [] _backward_contrib_BilinearResize2D
 [] _backward_contrib_bipartite_matching
 [] _backward_contrib_box_iou
 [] _backward_contrib_box_nms
 [] _backward_copy
 [-] _backward_cos
 [] _backward_cosh
 [] _backward_degrees
 [] _backward_diag
 [] _backward_div
 [] _backward_div_scalar
 [] _backward_dot
 [] _backward_expm1
 [] _backward_foreach
 [] _backward_gamma
 [] _backward_gammaln
 [] _backward_hard_sigmoid
 [] _backward_hypot
 [] _backward_hypot_scalar
 [] _backward_linalg_gelqf
 [] _backward_linalg_gemm
 [] _backward_linalg_gemm2
 [] _backward_linalg_potrf
 [] _backward_linalg_potri
 [] _backward_linalg_sumlogdiag
 [] _backward_linalg_syevd
 [] _backward_linalg_syrk
 [] _backward_linalg_trmm
 [] _backward_linalg_trsm
 [] _backward_linear_reg_out
 [] _backward_log
 [] _backward_log10
 [] _backward_log1p
 [] _backward_log2
 [] _backward_log_softmax
 [] _backward_logistic_reg_out
 [] _backward_mae_reg_out
 [] _backward_max
 [] _backward_maximum
 [] _backward_maximum_scalar
 [] _backward_mean
 [] _backward_min
 [] _backward_minimum
 [] _backward_minimum_scalar
 [] _backward_mod
 [] _backward_mod_scalar
[-] _backward_mul
 [] _backward_mul_scalar
 [] _backward_nanprod
 [] _backward_nansum
 [] _backward_norm
 [] _backward_pick
 [] _backward_power
 [] _backward_power_scalar
 [] _backward_prod
 [] _backward_radians
 [] _backward_rcbrt
 [] _backward_rdiv_scalar
 [] _backward_reciprocal
 [] _backward_relu
 [] _backward_repeat
 [] _backward_reverse
 [] _backward_rmod_scalar
 [] _backward_rpower_scalar
 [] _backward_rsqrt
 [] _backward_sample_multinomial
 [] _backward_sigmoid
 [] _backward_sign
[-]] _backward_sin
 [] _backward_sinh
 [] _backward_slice
 [] _backward_slice_axis
 [] _backward_slice_like
 [] _backward_smooth_l1
 [] _backward_softmax
 [] _backward_softmax_cross_entropy
 [] _backward_softmin
 [] _backward_softsign
 [] _backward_sparse_retain
 [] _backward_sqrt
 [] _backward_square
 [] _backward_square_sum
 [] _backward_squeeze
 [] _backward_stack
 [] _backward_sub
 [] _backward_sum
 [] _backward_take
 [] _backward_tan
 [] _backward_tanh
 [] _backward_tile
 [] _backward_topk
 [] _backward_where
 [] _backward_while_loop
 [] _broadcast_backward
 [] _contrib_CTCLoss
 [] _contrib_SyncBatchNorm
 [] _contrib_backward_quadratic
 [] _contrib_CTCLoss
 [] _contrib_dequantize
 [] _contrib_quantize
 [] _contrib_quantized_conv
 [] _contrib_quantized_flatten
 [] _contrib_quantized_fully_connected
 [] _contrib_quantized_pooling
 [] _contrib_requantize
 [] _slice_assign
 [] _slice_assign_scalar
 [] _cvcopyMakeBorder
 [] _cvimdecode
 [] _cvimread
 [] _cvimresize
 [] _eye
 [] _full
 [] _grad_add
 [] _histogram
 [] _ones
 [] _random_exponential
 [] _random_gamma
 [] _random_generalized_negative_binomial
 [] _random_negative_binomial
 [] _random_normal
 [] _random_poisson
 [] _random_uniform
 [] _sample_unique_zipfian
 [] _scatter_set_nd
 [] _shuffle
 [] _slice_assign
 [] _slice_assign_scalar
 [] _sparse_adagrad_update
 [] adam_update
 [] ftrl_update
 [] log10
 [] sgd_mom_update
 [] sgd_update
 [] _zeros
 [] _zeros_without_dtype
 [] adam_update
 [] argmax_channel
 [] batch_take
 [] ftml_update
 [] ftrl_update
 [] khatri_rao
 [] log10
 [] mp_sgd_mom_update
 [] mp_sgd_update
 [] _random_normal
 [] Pad
 [] _random_exponential
 [] _random_gamma
 [] _random_generalized_negative_binomial
 [] _random_negative_binomial
 [] _random_normal
 [] _random_poisson
 [] _random_uniform
 [] rmsprop_update
 [] rmspropalex_update
 [] sgd_mom_update
 [] sgd_update
 [] _shuffle
 [] signsgd_update
 [] signum_update
 [] SliceChannel
 [] SwapAxis
 [] _random_uniform

[~leopd] I have created the stories and tasks based on the priority you listed in the scrum board. We will start picking up tasks from this list and work on:

https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=278&projectKey=MXNET&view=planning.nodetail&selectedIssue=MXNET-1135&epics=visible&selectedEpic=MXNET-978

[~Calhoun] I have assigned 3 tasks (cos, sin, mul) as indicated in your comment to you. Feel free to add and assign other tasks. Thanks!

Due to my other committment, I will start working on this Epic from November.

