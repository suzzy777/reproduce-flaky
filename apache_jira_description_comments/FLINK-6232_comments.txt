Hi [~Yuhong_kyo], thanks for opening the JIRA.
I think this looks good.
Fabian

Hi [~fhueske], I have made a preliminary implementation of this JIRA.
But i can not use rowtime/proctime attribute, i should wait for FLINK-5884 merging into.
Should i push my code for reviewing first and made a few iterations on this JIRA?


GitHub user hongyuhong opened a pull request:

    https://github.com/apache/flink/pull/3715

    [FLINK-6232][Table&SQL]Support proctime inner equi-join between two s…

    …treams in sql api
    
    Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.
    If your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).
    In addition to going through the list, please provide a meaningful description of your changes.
    
    - [ ] General
      - The pull request references the related JIRA issue ("[FLINK-XXX] Jira title text")
      - The pull request addresses only one issue
      - Each commit in the PR has a meaningful commit message (including the JIRA id)
    
    - [ ] Documentation
      - Documentation has been added for new functionality
      - Old documentation affected by the pull request has been updated
      - JavaDoc for public methods has been added
    
    - [ ] Tests & Build
      - Functionality added by the pull request is covered by tests
      - `mvn clean verify` has been executed successfully locally or a Travis build has passed


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/hongyuhong/flink flink-6232

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/3715.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3715
    
----
commit fe412d6fec8366e5d690cf5088fdcd466e2bb212
Author: hongyuhong 00223286 <hongyuhong@huawei.com>
Date:   2017-04-13T03:32:29Z

    [FLINK-6232][Table&SQL]Support proctime inner equi-join between two streams in sql api

----


Hi [~fhueske] i have implement it use normal timestamp attribute but not like a.proctime/a.rowtime, cause it's not supported, I would appreciate if you can give some advice.

Thanks very much.

Hi [~fhueske], I have merge the flink-5884 time indicators and update the pr.

Thanks very much.
Yuhong

Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115850396
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -0,0 +1,180 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamJoin(
    +   cluster: RelOptCluster,
    +   traitSet: RelTraitSet,
    +   leftNode: RelNode,
    +   rightNode: RelNode,
    +   joinNode: FlinkLogicalJoin,
    +   leftSchema: RowSchema,
    +   schema: RowSchema,
    +   ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +  with CommonJoin
    +  with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinNode,
    +      leftSchema,
    +      schema,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +
    +    s"${joinTypeToString(joinNode.getJoinType)}" +
    +      s"(condition: (${joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString)}), " +
    +      s"select: (${joinSelectionToString(schema.logicalType)}))"
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    super.explainTerms(pw)
    +      .item("condition", joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString))
    +      .item("select", joinSelectionToString(schema.logicalType))
    +      .item("joinType", joinTypeToString(joinNode.getJoinType))
    +  }
    +
    +  override def translateToPlan(tableEnv: StreamTableEnvironment): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    // get the equality keys and other condition
    +    val (leftKeys, rightKeys, otherCondition) =
    +      JoinUtil.analyzeJoinCondition(joinNode, getExpressionString)
    --- End diff --
    
    This can be done by 
    ```
    val joinInfo = JoinInfo.of(leftNode, rightNode, condition)
    val leftKeys: Array[Int] = joinInfo.leftKeys.toIntArray
    val rightKeys: Array[Int] = joinInfo.rightKeys.toIntArray
    val otherCondition = joinInfo.getRemaining(cluster.getRexBuilder)
    ``` 
    So we do not need a special method for this. The type checks are not required, because Calcite will make sure during validation that only compatible types are compared. So we can be sure that types are valid.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115845838
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamJoinRule.scala ---
    @@ -0,0 +1,69 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.rules.datastream
    +
    +import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
    +import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamJoin
    +
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.flink.table.plan.nodes.FlinkConventions
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +
    +class DataStreamJoinRule
    +  extends ConverterRule(
    +      classOf[FlinkLogicalJoin],
    +      FlinkConventions.LOGICAL,
    +      FlinkConventions.DATASTREAM,
    +      "DataStreamJoinRule") {
    +
    +  override def matches(call: RelOptRuleCall): Boolean = {
    +    val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
    +
    +    val joinInfo = join.analyzeCondition
    +
    +    // joins require an equi-condition or a conjunctive predicate with at least one equi-condition
    +    // and disable outer joins with non-equality predicates
    +    !joinInfo.pairs().isEmpty && (joinInfo.isEqui || join.getJoinType == JoinRelType.INNER)
    +  }
    +
    +  override def convert(rel: RelNode): RelNode = {
    +
    +    val join: FlinkLogicalJoin = rel.asInstanceOf[FlinkLogicalJoin]
    +    val traitSet: RelTraitSet = rel.getTraitSet.replace(FlinkConventions.DATASTREAM)
    +    val convLeft: RelNode = RelOptRule.convert(join.getInput(0), FlinkConventions.DATASTREAM)
    +    val convRight: RelNode = RelOptRule.convert(join.getInput(1), FlinkConventions.DATASTREAM)
    +
    +    new DataStreamJoin(
    +      rel.getCluster,
    +      traitSet,
    +      convLeft,
    +      convRight,
    +      join,
    --- End diff --
    
    please pass join condition and join type instead of the `LogicalFlinkJoin`. This is a plan node of the logical plan that should not be part of a node in the physical plan.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115796448
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamJoinRule.scala ---
    @@ -0,0 +1,69 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.rules.datastream
    +
    +import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
    +import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamJoin
    +
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.flink.table.plan.nodes.FlinkConventions
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +
    +class DataStreamJoinRule
    +  extends ConverterRule(
    +      classOf[FlinkLogicalJoin],
    +      FlinkConventions.LOGICAL,
    +      FlinkConventions.DATASTREAM,
    +      "DataStreamJoinRule") {
    +
    +  override def matches(call: RelOptRuleCall): Boolean = {
    +    val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
    +
    +    val joinInfo = join.analyzeCondition
    +
    +    // joins require an equi-condition or a conjunctive predicate with at least one equi-condition
    +    // and disable outer joins with non-equality predicates
    +    !joinInfo.pairs().isEmpty && (joinInfo.isEqui || join.getJoinType == JoinRelType.INNER)
    --- End diff --
    
    Is this condition correct? Are outer joins (incl. LEFT, RIGHT, FULL OUTER) supported if the join is an equality join (all conjunctive predicates are equality predicates)?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115851247
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,488 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.FilterFunction
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.runtime.FilterRunner
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze join condition to get equi-conditon and other condition
    +    * @param  joinNode   logicaljoin node
    +    * @param  expression the function to generate condition string
    +    */
    +  private[flink] def analyzeJoinCondition(
    --- End diff --
    
    I don't think we need this method. We can analyze the condition with Calcite's `JoinInfo`. Calcite's validation checks before optimization and translation that the types of the conditions are OK.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115873251
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the rightstream elments
    --- End diff --
    
    -> `loop the other input's elements`?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115874222
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -0,0 +1,180 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamJoin(
    +   cluster: RelOptCluster,
    +   traitSet: RelTraitSet,
    +   leftNode: RelNode,
    +   rightNode: RelNode,
    +   joinNode: FlinkLogicalJoin,
    +   leftSchema: RowSchema,
    +   schema: RowSchema,
    +   ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +  with CommonJoin
    +  with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinNode,
    +      leftSchema,
    +      schema,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +
    +    s"${joinTypeToString(joinNode.getJoinType)}" +
    +      s"(condition: (${joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString)}), " +
    +      s"select: (${joinSelectionToString(schema.logicalType)}))"
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    super.explainTerms(pw)
    +      .item("condition", joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString))
    +      .item("select", joinSelectionToString(schema.logicalType))
    +      .item("joinType", joinTypeToString(joinNode.getJoinType))
    +  }
    +
    +  override def translateToPlan(tableEnv: StreamTableEnvironment): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    // get the equality keys and other condition
    +    val (leftKeys, rightKeys, otherCondition) =
    +      JoinUtil.analyzeJoinCondition(joinNode, getExpressionString)
    +
    +    if (left.isInstanceOf[StreamTableSourceScan]
    +        || right.isInstanceOf[StreamTableSourceScan]) {
    +      throw new TableException(
    +        "Join between stream and table is not supported yet.")
    +    }
    +    // analyze time boundary and time predicate type(proctime/rowtime)
    +    val (timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime) =
    +      JoinUtil.analyzeTimeBoundary(
    +        otherCondition,
    +        leftSchema.logicalType.getFieldCount,
    +        leftSchema.physicalType.getFieldCount,
    +        schema.logicalType,
    +        joinNode.getCluster.getRexBuilder,
    +        config)
    +
    +    val leftDataStream = left.asInstanceOf[DataStreamRel].translateToPlan(tableEnv)
    +    val rightDataStream = right.asInstanceOf[DataStreamRel].translateToPlan(tableEnv)
    +
    +    // generate other condition filter function
    +    val filterFunction =
    +      JoinUtil.generateFilterFunction(
    +        config,
    +        joinNode.getJoinType,
    +        CRowTypeInfo(schema.physicalTypeInfo).rowType,
    +        conditionWithoutTime,
    +        ruleDescription)
    +
    +    joinNode.getJoinType match {
    +      case JoinRelType.INNER =>
    +        timeType match {
    +          case _ if FlinkTypeFactory.isProctimeIndicatorType(timeType) =>
    +            // Proctime JoinCoProcessFunction
    +            createProcTimeInnerJoinFunction(
    +              leftStreamWindowSize,
    +              rightStreamWindowSize,
    +              leftDataStream,
    +              rightDataStream,
    +              filterFunction,
    +              leftKeys,
    +              rightKeys
    +            )
    +          case _ if FlinkTypeFactory.isRowtimeIndicatorType(timeType) =>
    +            // RowTime JoinCoProcessFunction
    +            throw new TableException(
    +              "RowTime inner join between stream and stream is not supported yet.")
    +        }
    +      case JoinRelType.FULL =>
    +        throw new TableException(
    +          "Full join between stream and stream is not supported yet.")
    +      case JoinRelType.LEFT =>
    +        throw new TableException(
    +          "Left join between stream and stream is not supported yet.")
    +      case JoinRelType.RIGHT =>
    +        throw new TableException(
    +          "Right join between stream and stream is not supported yet.")
    +    }
    +  }
    +
    +  def createProcTimeInnerJoinFunction(
    +      leftStreamWindowSize: Long,
    +      rightStreamWindowSize: Long,
    +      leftDataStream: DataStream[CRow],
    +      rightDataStream: DataStream[CRow],
    +      filterFunction: RichFilterFunction[Row],
    +      leftKeys: Array[Int],
    +      rightKeys: Array[Int]): DataStream[CRow] = {
    +
    +    val returnTypeInfo = CRowTypeInfo(schema.physicalTypeInfo)
    +
    +    val procInnerJoinFunc = new ProcTimeInnerJoin(
    +      leftStreamWindowSize,
    +      rightStreamWindowSize,
    +      leftDataStream.getType,
    +      rightDataStream.getType,
    +      filterFunction)
    +
    +    if (!leftKeys.isEmpty) {
    +      leftDataStream.connect(rightDataStream)
    +        .keyBy(leftKeys, rightKeys)
    +        .process(procInnerJoinFunc)
    +        .returns(returnTypeInfo)
    +    } else {
    +      leftDataStream.connect(rightDataStream)
    --- End diff --
    
    If we support this case, we should also have a test for it.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115847866
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -0,0 +1,180 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamJoin(
    +   cluster: RelOptCluster,
    +   traitSet: RelTraitSet,
    +   leftNode: RelNode,
    +   rightNode: RelNode,
    +   joinNode: FlinkLogicalJoin,
    +   leftSchema: RowSchema,
    --- End diff --
    
    why don't we need the `rightSchema`?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115844805
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamJoinRule.scala ---
    @@ -0,0 +1,69 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.rules.datastream
    +
    +import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
    +import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamJoin
    +
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.flink.table.plan.nodes.FlinkConventions
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +
    +class DataStreamJoinRule
    +  extends ConverterRule(
    +      classOf[FlinkLogicalJoin],
    +      FlinkConventions.LOGICAL,
    +      FlinkConventions.DATASTREAM,
    +      "DataStreamJoinRule") {
    +
    +  override def matches(call: RelOptRuleCall): Boolean = {
    +    val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
    +
    +    val joinInfo = join.analyzeCondition
    +
    +    // joins require an equi-condition or a conjunctive predicate with at least one equi-condition
    +    // and disable outer joins with non-equality predicates
    +    !joinInfo.pairs().isEmpty && (joinInfo.isEqui || join.getJoinType == JoinRelType.INNER)
    --- End diff --
    
    I assume not, because the join type is not passed on to the `DataStreamJoin`.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115852788
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -0,0 +1,180 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamJoin(
    +   cluster: RelOptCluster,
    +   traitSet: RelTraitSet,
    +   leftNode: RelNode,
    +   rightNode: RelNode,
    +   joinNode: FlinkLogicalJoin,
    +   leftSchema: RowSchema,
    +   schema: RowSchema,
    +   ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +  with CommonJoin
    +  with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinNode,
    +      leftSchema,
    +      schema,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +
    +    s"${joinTypeToString(joinNode.getJoinType)}" +
    +      s"(condition: (${joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString)}), " +
    +      s"select: (${joinSelectionToString(schema.logicalType)}))"
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    super.explainTerms(pw)
    +      .item("condition", joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString))
    +      .item("select", joinSelectionToString(schema.logicalType))
    +      .item("joinType", joinTypeToString(joinNode.getJoinType))
    +  }
    +
    +  override def translateToPlan(tableEnv: StreamTableEnvironment): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    // get the equality keys and other condition
    +    val (leftKeys, rightKeys, otherCondition) =
    +      JoinUtil.analyzeJoinCondition(joinNode, getExpressionString)
    +
    +    if (left.isInstanceOf[StreamTableSourceScan]
    +        || right.isInstanceOf[StreamTableSourceScan]) {
    +      throw new TableException(
    +        "Join between stream and table is not supported yet.")
    +    }
    +    // analyze time boundary and time predicate type(proctime/rowtime)
    +    val (timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime) =
    +      JoinUtil.analyzeTimeBoundary(
    +        otherCondition,
    +        leftSchema.logicalType.getFieldCount,
    +        leftSchema.physicalType.getFieldCount,
    +        schema.logicalType,
    +        joinNode.getCluster.getRexBuilder,
    +        config)
    +
    +    val leftDataStream = left.asInstanceOf[DataStreamRel].translateToPlan(tableEnv)
    +    val rightDataStream = right.asInstanceOf[DataStreamRel].translateToPlan(tableEnv)
    +
    +    // generate other condition filter function
    +    val filterFunction =
    +      JoinUtil.generateFilterFunction(
    +        config,
    +        joinNode.getJoinType,
    +        CRowTypeInfo(schema.physicalTypeInfo).rowType,
    --- End diff --
    
    we can just pass `schema.physicalTypeInfo`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115865585
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    --- End diff --
    
    rename to `isLeft`?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115847719
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -0,0 +1,180 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamJoin(
    +   cluster: RelOptCluster,
    +   traitSet: RelTraitSet,
    +   leftNode: RelNode,
    +   rightNode: RelNode,
    +   joinNode: FlinkLogicalJoin,
    --- End diff --
    
    please do not include a `FlinkLogicalJoin` node but the condition and the join type.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115872865
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    --- End diff --
    
    what are the timer states used for? I only see one `value()` call to get the value which checks `== 0` if a timer is registered. Can we make this a boolean state then? 
    What would happen if we would not have this state? Would there be more timers (timers are unique by time).


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115871238
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the rightstream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isPositive) {
    +            compositeOutput(value, oppoRowList.get(i))
    +          } else {
    +            compositeOutput(oppoRowList.get(i), value)
    +          }
    +
    +          if (filterFunc.filter(outputC.row)) {
    +            out.collect(outputC)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size() - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      listToRemove.remove(i)
    +      i -= 1
    +    }
    +  }
    +
    +  /**
    +    * set output fields according left and right stream row
    +    */
    +  private def compositeOutput(
    +     leftRow: Row,
    +     rightRow: Row): Unit = {
    +
    +    var i = 0
    +    while (i < element1Type.getArity) {
    +      outputC.row.setField(i, leftRow.getField(i))
    +      i += 1
    +    }
    +
    +    i = 0
    +    while (i < element2Type.getArity) {
    +      outputC.row.setField(i + element1Type.getArity, rightRow.getField(i))
    +      i += 1
    +    }
    +
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +    curTime: Long,
    +    winSize: Long,
    +    rowMapState: MapState[Long, JList[Row]],
    +    timerState: ValueState[Long],
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val expiredList = new util.ArrayList[Long]
    --- End diff --
    
    make this a member variable which is cleared and reused.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115875501
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the rightstream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isPositive) {
    +            compositeOutput(value, oppoRowList.get(i))
    +          } else {
    +            compositeOutput(oppoRowList.get(i), value)
    +          }
    +
    +          if (filterFunc.filter(outputC.row)) {
    +            out.collect(outputC)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size() - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      listToRemove.remove(i)
    +      i -= 1
    +    }
    +  }
    +
    +  /**
    +    * set output fields according left and right stream row
    +    */
    +  private def compositeOutput(
    +     leftRow: Row,
    +     rightRow: Row): Unit = {
    +
    +    var i = 0
    +    while (i < element1Type.getArity) {
    +      outputC.row.setField(i, leftRow.getField(i))
    +      i += 1
    +    }
    +
    +    i = 0
    +    while (i < element2Type.getArity) {
    +      outputC.row.setField(i + element1Type.getArity, rightRow.getField(i))
    +      i += 1
    +    }
    +
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +    curTime: Long,
    +    winSize: Long,
    +    rowMapState: MapState[Long, JList[Row]],
    +    timerState: ValueState[Long],
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val expiredList = new util.ArrayList[Long]
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // loop the timestamps to find out expired records, when meet one record
    +    // after the expried timestamp, break the loop. If the keys is ordered, thus
    +    // can reduce loop num, if the keys is unordered, also can expire at least one
    +    // element every time the timer trigger
    +    while (keyIter.hasNext && nextTimer == 0) {
    +      val curTime = keyIter.next
    +      if (curTime < expiredTime) {
    +        expiredList.add(curTime)
    +      } else {
    +        nextTimer = curTime
    --- End diff --
    
    This should be the place to break the loop as mentioned in the comment, no?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115864221
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -0,0 +1,180 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamJoin(
    +   cluster: RelOptCluster,
    +   traitSet: RelTraitSet,
    +   leftNode: RelNode,
    +   rightNode: RelNode,
    +   joinNode: FlinkLogicalJoin,
    +   leftSchema: RowSchema,
    +   schema: RowSchema,
    +   ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +  with CommonJoin
    +  with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinNode,
    +      leftSchema,
    +      schema,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +
    +    s"${joinTypeToString(joinNode.getJoinType)}" +
    +      s"(condition: (${joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString)}), " +
    +      s"select: (${joinSelectionToString(schema.logicalType)}))"
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    super.explainTerms(pw)
    +      .item("condition", joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString))
    +      .item("select", joinSelectionToString(schema.logicalType))
    +      .item("joinType", joinTypeToString(joinNode.getJoinType))
    +  }
    +
    +  override def translateToPlan(tableEnv: StreamTableEnvironment): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    // get the equality keys and other condition
    +    val (leftKeys, rightKeys, otherCondition) =
    +      JoinUtil.analyzeJoinCondition(joinNode, getExpressionString)
    +
    +    if (left.isInstanceOf[StreamTableSourceScan]
    +        || right.isInstanceOf[StreamTableSourceScan]) {
    +      throw new TableException(
    +        "Join between stream and table is not supported yet.")
    +    }
    +    // analyze time boundary and time predicate type(proctime/rowtime)
    +    val (timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime) =
    +      JoinUtil.analyzeTimeBoundary(
    +        otherCondition,
    +        leftSchema.logicalType.getFieldCount,
    +        leftSchema.physicalType.getFieldCount,
    +        schema.logicalType,
    +        joinNode.getCluster.getRexBuilder,
    +        config)
    +
    +    val leftDataStream = left.asInstanceOf[DataStreamRel].translateToPlan(tableEnv)
    +    val rightDataStream = right.asInstanceOf[DataStreamRel].translateToPlan(tableEnv)
    +
    +    // generate other condition filter function
    +    val filterFunction =
    --- End diff --
    
    I think we do not wrap the generated code in a wrapping `FilterFunction`. Instead we can pass it directly to the `CoProcessFunction` and compile it there in the `open()` method.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115864982
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    --- End diff --
    
    I think we can use `TypeInformation[Row]` here and don't need the `CRow` wrapper.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115852606
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -0,0 +1,180 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamJoin(
    +   cluster: RelOptCluster,
    +   traitSet: RelTraitSet,
    +   leftNode: RelNode,
    +   rightNode: RelNode,
    +   joinNode: FlinkLogicalJoin,
    +   leftSchema: RowSchema,
    +   schema: RowSchema,
    +   ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +  with CommonJoin
    +  with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinNode,
    +      leftSchema,
    +      schema,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +
    +    s"${joinTypeToString(joinNode.getJoinType)}" +
    +      s"(condition: (${joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString)}), " +
    +      s"select: (${joinSelectionToString(schema.logicalType)}))"
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    super.explainTerms(pw)
    +      .item("condition", joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString))
    +      .item("select", joinSelectionToString(schema.logicalType))
    +      .item("joinType", joinTypeToString(joinNode.getJoinType))
    +  }
    +
    +  override def translateToPlan(tableEnv: StreamTableEnvironment): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    // get the equality keys and other condition
    +    val (leftKeys, rightKeys, otherCondition) =
    +      JoinUtil.analyzeJoinCondition(joinNode, getExpressionString)
    +
    +    if (left.isInstanceOf[StreamTableSourceScan]
    --- End diff --
    
    I don't think we need this restriction. A `StreamTableSourceScan` produces a stream not a table.
    So by this we forbid regular stream-stream joins.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115864005
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/FilterRunner.scala ---
    @@ -0,0 +1,54 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime
    +
    +import org.apache.flink.api.common.functions.util.FunctionUtils
    +import org.apache.flink.api.common.functions.{FilterFunction, RichFilterFunction}
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.table.codegen.Compiler
    +import org.slf4j.LoggerFactory
    +
    +class FilterRunner[IN] (
    --- End diff --
    
    I don't think we need this additional wrapper. We could just pass the generated code of the `FilterFunction` to the `CoProcessFunction` and compile it in its open() method.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115868422
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    --- End diff --
    
    use a `JoinFunction` instead of a `FilterFunction`. Compile the code here instead of using a wrapper.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115855446
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -0,0 +1,180 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamJoin(
    +   cluster: RelOptCluster,
    +   traitSet: RelTraitSet,
    +   leftNode: RelNode,
    +   rightNode: RelNode,
    +   joinNode: FlinkLogicalJoin,
    +   leftSchema: RowSchema,
    +   schema: RowSchema,
    +   ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +  with CommonJoin
    +  with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinNode,
    +      leftSchema,
    +      schema,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +
    +    s"${joinTypeToString(joinNode.getJoinType)}" +
    +      s"(condition: (${joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString)}), " +
    +      s"select: (${joinSelectionToString(schema.logicalType)}))"
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    super.explainTerms(pw)
    +      .item("condition", joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString))
    +      .item("select", joinSelectionToString(schema.logicalType))
    +      .item("joinType", joinTypeToString(joinNode.getJoinType))
    +  }
    +
    +  override def translateToPlan(tableEnv: StreamTableEnvironment): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    // get the equality keys and other condition
    +    val (leftKeys, rightKeys, otherCondition) =
    +      JoinUtil.analyzeJoinCondition(joinNode, getExpressionString)
    +
    +    if (left.isInstanceOf[StreamTableSourceScan]
    +        || right.isInstanceOf[StreamTableSourceScan]) {
    +      throw new TableException(
    +        "Join between stream and table is not supported yet.")
    +    }
    +    // analyze time boundary and time predicate type(proctime/rowtime)
    +    val (timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime) =
    +      JoinUtil.analyzeTimeBoundary(
    +        otherCondition,
    +        leftSchema.logicalType.getFieldCount,
    +        leftSchema.physicalType.getFieldCount,
    +        schema.logicalType,
    +        joinNode.getCluster.getRexBuilder,
    +        config)
    +
    +    val leftDataStream = left.asInstanceOf[DataStreamRel].translateToPlan(tableEnv)
    +    val rightDataStream = right.asInstanceOf[DataStreamRel].translateToPlan(tableEnv)
    +
    +    // generate other condition filter function
    +    val filterFunction =
    +      JoinUtil.generateFilterFunction(
    +        config,
    +        joinNode.getJoinType,
    +        CRowTypeInfo(schema.physicalTypeInfo).rowType,
    +        conditionWithoutTime,
    +        ruleDescription)
    +
    +    joinNode.getJoinType match {
    +      case JoinRelType.INNER =>
    +        timeType match {
    +          case _ if FlinkTypeFactory.isProctimeIndicatorType(timeType) =>
    +            // Proctime JoinCoProcessFunction
    +            createProcTimeInnerJoinFunction(
    +              leftStreamWindowSize,
    +              rightStreamWindowSize,
    +              leftDataStream,
    +              rightDataStream,
    +              filterFunction,
    +              leftKeys,
    +              rightKeys
    +            )
    +          case _ if FlinkTypeFactory.isRowtimeIndicatorType(timeType) =>
    +            // RowTime JoinCoProcessFunction
    +            throw new TableException(
    +              "RowTime inner join between stream and stream is not supported yet.")
    +        }
    +      case JoinRelType.FULL =>
    +        throw new TableException(
    +          "Full join between stream and stream is not supported yet.")
    +      case JoinRelType.LEFT =>
    +        throw new TableException(
    +          "Left join between stream and stream is not supported yet.")
    +      case JoinRelType.RIGHT =>
    +        throw new TableException(
    +          "Right join between stream and stream is not supported yet.")
    +    }
    +  }
    +
    +  def createProcTimeInnerJoinFunction(
    +      leftStreamWindowSize: Long,
    +      rightStreamWindowSize: Long,
    +      leftDataStream: DataStream[CRow],
    +      rightDataStream: DataStream[CRow],
    +      filterFunction: RichFilterFunction[Row],
    +      leftKeys: Array[Int],
    +      rightKeys: Array[Int]): DataStream[CRow] = {
    +
    +    val returnTypeInfo = CRowTypeInfo(schema.physicalTypeInfo)
    +
    +    val procInnerJoinFunc = new ProcTimeInnerJoin(
    +      leftStreamWindowSize,
    +      rightStreamWindowSize,
    +      leftDataStream.getType,
    +      rightDataStream.getType,
    +      filterFunction)
    +
    +    if (!leftKeys.isEmpty) {
    +      leftDataStream.connect(rightDataStream)
    +        .keyBy(leftKeys, rightKeys)
    +        .process(procInnerJoinFunc)
    +        .returns(returnTypeInfo)
    +    } else {
    +      leftDataStream.connect(rightDataStream)
    --- End diff --
    
    this case should never happen because `DataStreamJoinRule` requires `!joinInfo.pairs().isEmpty`. 
    I think we can remove the condition in the rule and also support non-keyed joins.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115868097
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,488 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.FilterFunction
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.runtime.FilterRunner
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze join condition to get equi-conditon and other condition
    +    * @param  joinNode   logicaljoin node
    +    * @param  expression the function to generate condition string
    +    */
    +  private[flink] def analyzeJoinCondition(
    +    joinNode: FlinkLogicalJoin,
    +    expression: (RexNode, List[String], Option[List[RexNode]]) => String) = {
    +
    +    val joinInfo = joinNode.analyzeCondition()
    +    val keyPairs = joinInfo.pairs.toList
    +    val otherCondition =
    +      if(joinInfo.isEqui) null
    +      else joinInfo.getRemaining(joinNode.getCluster.getRexBuilder)
    +
    +    val leftKeys = ArrayBuffer.empty[Int]
    +    val rightKeys = ArrayBuffer.empty[Int]
    +    if (!keyPairs.isEmpty) {
    +      val leftFields = joinNode.getLeft.getRowType.getFieldList
    +      val rightFields = joinNode.getRight.getRowType.getFieldList
    +
    +      keyPairs.foreach(pair => {
    +        val leftKeyType = leftFields.get(pair.source).getType.getSqlTypeName
    +        val rightKeyType = rightFields.get(pair.target).getType.getSqlTypeName
    +
    +        // check if keys are compatible
    +        if (leftKeyType == rightKeyType) {
    +          // add key pair
    +          leftKeys.append(pair.source)
    +          rightKeys.append(pair.target)
    +        } else {
    +          throw TableException(
    +            "Equality join predicate on incompatible types.\n" +
    +              s"\tLeft: ${joinNode.getLeft.toString},\n" +
    +              s"\tRight: ${joinNode.getRight.toString},\n" +
    +              s"\tCondition: (${expression(joinNode.getCondition,
    +                joinNode.getRowType.getFieldNames.toList, None)})"
    +          )
    +        }
    +      })
    +    }
    +    (leftKeys.toArray, rightKeys.toArray, otherCondition)
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    --- End diff --
    
    I did not go through the details of this method yet but apparently it analyzes and decomposes the time based condition. It would be very good to have a couple of unit tests which only check this method and that it produces the correct result for different input values.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115866207
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the rightstream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isPositive) {
    +            compositeOutput(value, oppoRowList.get(i))
    +          } else {
    +            compositeOutput(oppoRowList.get(i), value)
    +          }
    +
    +          if (filterFunc.filter(outputC.row)) {
    +            out.collect(outputC)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size() - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      listToRemove.remove(i)
    +      i -= 1
    +    }
    +  }
    +
    +  /**
    +    * set output fields according left and right stream row
    +    */
    +  private def compositeOutput(
    --- End diff --
    
    Can be removed if we code-gen a `JoinFunction` instead of a `FilterFunction`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115874289
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/CodeGenerator.scala ---
    @@ -698,6 +698,12 @@ class CodeGenerator(
               s"void join(Object _in1, Object _in2, org.apache.flink.util.Collector $collectorTerm)",
               List(s"$inputTypeTerm1 $input1Term = ($inputTypeTerm1) _in1;",
                    s"$inputTypeTerm2 $input2Term = ($inputTypeTerm2) _in2;"))
    +      } else if (clazz == classOf[FilterFunction[_]]) {
    --- End diff --
    
    Not needed if we use `JoinFunction` instead of `FilterFunction`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115866096
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the rightstream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    --- End diff --
    
    We can use a `JoinFunction` to apply the filter condition and compose the output row. Have a look at the `DataSetJoin` which is doing the same. This will be more efficient, because we do not need loops and the condition is applied before any work is done to assemble the output row.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115868597
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    --- End diff --
    
    indent


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115873581
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the rightstream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isPositive) {
    +            compositeOutput(value, oppoRowList.get(i))
    +          } else {
    +            compositeOutput(oppoRowList.get(i), value)
    +          }
    +
    +          if (filterFunc.filter(outputC.row)) {
    +            out.collect(outputC)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size() - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      listToRemove.remove(i)
    --- End diff --
    
    `remove` on an `ArrayList` is inefficient, because it has to shift (i.e., copy) all elements.
    We should rather go over the list and call `clear()` when we are done.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115874115
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/FilterRunner.scala ---
    @@ -0,0 +1,54 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime
    +
    +import org.apache.flink.api.common.functions.util.FunctionUtils
    +import org.apache.flink.api.common.functions.{FilterFunction, RichFilterFunction}
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.table.codegen.Compiler
    +import org.slf4j.LoggerFactory
    +
    +class FilterRunner[IN] (
    --- End diff --
    
    Actually, we could use a `JoinFunction` instead of a `FilterFunction` as well.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115873645
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the rightstream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isPositive) {
    +            compositeOutput(value, oppoRowList.get(i))
    +          } else {
    +            compositeOutput(oppoRowList.get(i), value)
    +          }
    +
    +          if (filterFunc.filter(outputC.row)) {
    +            out.collect(outputC)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size() - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      listToRemove.remove(i)
    +      i -= 1
    +    }
    +  }
    +
    +  /**
    +    * set output fields according left and right stream row
    +    */
    +  private def compositeOutput(
    +     leftRow: Row,
    +     rightRow: Row): Unit = {
    +
    +    var i = 0
    +    while (i < element1Type.getArity) {
    +      outputC.row.setField(i, leftRow.getField(i))
    +      i += 1
    +    }
    +
    +    i = 0
    +    while (i < element2Type.getArity) {
    +      outputC.row.setField(i + element1Type.getArity, rightRow.getField(i))
    +      i += 1
    +    }
    +
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +    curTime: Long,
    +    winSize: Long,
    +    rowMapState: MapState[Long, JList[Row]],
    +    timerState: ValueState[Long],
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val expiredList = new util.ArrayList[Long]
    --- End diff --
    
    We could use `listToRemove` here and call `clear()` when we are done.


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115919570
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the rightstream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isPositive) {
    +            compositeOutput(value, oppoRowList.get(i))
    +          } else {
    +            compositeOutput(oppoRowList.get(i), value)
    +          }
    +
    +          if (filterFunc.filter(outputC.row)) {
    +            out.collect(outputC)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size() - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      listToRemove.remove(i)
    +      i -= 1
    +    }
    +  }
    +
    +  /**
    +    * set output fields according left and right stream row
    +    */
    +  private def compositeOutput(
    +     leftRow: Row,
    +     rightRow: Row): Unit = {
    +
    +    var i = 0
    +    while (i < element1Type.getArity) {
    +      outputC.row.setField(i, leftRow.getField(i))
    +      i += 1
    +    }
    +
    +    i = 0
    +    while (i < element2Type.getArity) {
    +      outputC.row.setField(i + element1Type.getArity, rightRow.getField(i))
    +      i += 1
    +    }
    +
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +    curTime: Long,
    +    winSize: Long,
    +    rowMapState: MapState[Long, JList[Row]],
    +    timerState: ValueState[Long],
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val expiredList = new util.ArrayList[Long]
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // loop the timestamps to find out expired records, when meet one record
    +    // after the expried timestamp, break the loop. If the keys is ordered, thus
    +    // can reduce loop num, if the keys is unordered, also can expire at least one
    +    // element every time the timer trigger
    +    while (keyIter.hasNext && nextTimer == 0) {
    +      val curTime = keyIter.next
    +      if (curTime < expiredTime) {
    +        expiredList.add(curTime)
    +      } else {
    +        nextTimer = curTime
    --- End diff --
    
    Yes, when it found an record after the expired time, i will not continue to loop, it will register timer for this record to delete itself until next timer trigger, thus can reduce useless loop operation.


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115919578
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the rightstream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isPositive) {
    +            compositeOutput(value, oppoRowList.get(i))
    +          } else {
    +            compositeOutput(oppoRowList.get(i), value)
    +          }
    +
    +          if (filterFunc.filter(outputC.row)) {
    +            out.collect(outputC)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size() - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      listToRemove.remove(i)
    --- End diff --
    
    I always remove the last element to avoid the shift operation, cause if use clear(), it will loop the elements once again.


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r115919604
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    --- End diff --
    
    The timer state is to guarantee that there will only exist one timer for each key.
    e.g suppose the window size is 10 ms.
    when the first element[a1, time(1)] come, it will register one timer at 12ms for it to make sure a1 will be deleted at 12ms,
    when the second element[a2, time(2)] come, it found that there exist one timer, so it won't register a timer again.
    When the first timer trigger, it will delete a1, and it found still exist records, i will register another timer for the next record.


Github user hongyuhong commented on the issue:

    https://github.com/apache/flink/pull/3715
  
    Hi @fhueske , Thanks very much for the review! I have update the pr use JoinFunction instead of FilterFunction, and i will add more test later.
    
    Thanks very much.
    Yuhong 


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116261121
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -37,14 +39,18 @@ import org.apache.flink.types.Row
       * Flink RelNode which matches along with JoinOperator and its related operations.
       */
     class DataStreamJoin(
    -   cluster: RelOptCluster,
    -   traitSet: RelTraitSet,
    -   leftNode: RelNode,
    -   rightNode: RelNode,
    -   joinNode: FlinkLogicalJoin,
    -   leftSchema: RowSchema,
    -   schema: RowSchema,
    -   ruleDescription: String)
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinInfo: JoinInfo,
    --- End diff --
    
    `JoinInfo` can be computed from `leftNode`, `rightNode`, and `condition`. No need to pass it as a parameter, IMO.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116260594
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/CodeGenerator.scala ---
    @@ -698,14 +698,7 @@ class CodeGenerator(
               s"void join(Object _in1, Object _in2, org.apache.flink.util.Collector $collectorTerm)",
               List(s"$inputTypeTerm1 $input1Term = ($inputTypeTerm1) _in1;",
                    s"$inputTypeTerm2 $input2Term = ($inputTypeTerm2) _in2;"))
    -      } else if (clazz == classOf[FilterFunction[_]]) {
    -        val baseClass = classOf[RichFilterFunction[_]]
    -        val inputTypeTerm = boxedTypeTermForTypeInfo(input1)
    -        (baseClass,
    -          s"boolean filter(Object _in1)",
    -          List(s"$inputTypeTerm $input1Term = ($inputTypeTerm) _in1;"))
    -      }
    -      else {
    +      } else {
    --- End diff --
    
    please keep the original formatting as 
    ```
    }
    else {
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116269180
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -159,9 +172,9 @@ class DataStreamJoin(
         val procInnerJoinFunc = new ProcTimeInnerJoin(
           leftStreamWindowSize,
           rightStreamWindowSize,
    -      leftDataStream.getType,
    -      rightDataStream.getType,
    -      filterFunction)
    +      leftDataStream.getType.asInstanceOf[CRowTypeInfo].rowType,
    +      rightDataStream.getType.asInstanceOf[CRowTypeInfo].rowType,
    --- End diff --
    
    can be `rightSchema.physicalTypeInfo`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116273281
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -298,16 +289,17 @@ class ProcTimeInnerJoin(
         while (keyIter.hasNext && nextTimer == 0) {
           val curTime = keyIter.next
           if (curTime < expiredTime) {
    -        expiredList.add(curTime)
    +        listToRemove.add(curTime)
           } else {
             nextTimer = curTime
           }
         }
     
    -    var i = 0
    -    while (i < expiredList.size) {
    -      rowMapState.remove(expiredList.get(i))
    -      i += 1
    +    var i = listToRemove.size() - 1
    +    while (i >= 0) {
    +      rowMapState.remove(listToRemove.get(i))
    +      listToRemove.remove(i)
    --- End diff --
    
    I think it's even more efficient to traverse the `listToRemove` without modifying it and calling `listToRemove.clear()` afterwards.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116270545
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -199,31 +157,38 @@ object JoinUtil {
         val generator = new CodeGenerator(
           config,
           nullCheck,
    -      returnType)
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
     
         // if other condition is null, the filterfunc always return true
         val body = if (otherCondition == null) {
    --- End diff --
    
    will `otherCondition` be `null` or `Literal(True)`?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116272631
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the rightstream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isPositive) {
    +            compositeOutput(value, oppoRowList.get(i))
    +          } else {
    +            compositeOutput(oppoRowList.get(i), value)
    +          }
    +
    +          if (filterFunc.filter(outputC.row)) {
    +            out.collect(outputC)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size() - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      listToRemove.remove(i)
    +      i -= 1
    +    }
    +  }
    +
    +  /**
    +    * set output fields according left and right stream row
    +    */
    +  private def compositeOutput(
    +     leftRow: Row,
    +     rightRow: Row): Unit = {
    +
    +    var i = 0
    +    while (i < element1Type.getArity) {
    +      outputC.row.setField(i, leftRow.getField(i))
    +      i += 1
    +    }
    +
    +    i = 0
    +    while (i < element2Type.getArity) {
    +      outputC.row.setField(i + element1Type.getArity, rightRow.getField(i))
    +      i += 1
    +    }
    +
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +    curTime: Long,
    +    winSize: Long,
    +    rowMapState: MapState[Long, JList[Row]],
    +    timerState: ValueState[Long],
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val expiredList = new util.ArrayList[Long]
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // loop the timestamps to find out expired records, when meet one record
    +    // after the expried timestamp, break the loop. If the keys is ordered, thus
    +    // can reduce loop num, if the keys is unordered, also can expire at least one
    +    // element every time the timer trigger
    +    while (keyIter.hasNext && nextTimer == 0) {
    +      val curTime = keyIter.next
    +      if (curTime < expiredTime) {
    +        expiredList.add(curTime)
    +      } else {
    +        nextTimer = curTime
    --- End diff --
    
    Ah, sorry. I was expecting a `break` (which is not available in Scala) and did not recognize the additional `nextTimer == 0` condition of the `while` loop.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116269111
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -159,9 +172,9 @@ class DataStreamJoin(
         val procInnerJoinFunc = new ProcTimeInnerJoin(
           leftStreamWindowSize,
           rightStreamWindowSize,
    -      leftDataStream.getType,
    -      rightDataStream.getType,
    -      filterFunction)
    +      leftDataStream.getType.asInstanceOf[CRowTypeInfo].rowType,
    --- End diff --
    
    can be `leftSchema.physicalTypeInfo`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116272041
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -44,12 +48,18 @@ import org.apache.flink.util.Collector
     class ProcTimeInnerJoin(
       private val leftStreamWindowSize: Long,
       private val rightStreamWindowSize: Long,
    -  private val element1Type: TypeInformation[CRow],
    -  private val element2Type: TypeInformation[CRow],
    -  private val filterFunc: RichFilterFunction[Row])
    -  extends CoProcessFunction[CRow, CRow, CRow] {
    +  private val element1Type: TypeInformation[Row],
    +  private val element2Type: TypeInformation[Row],
    +  private val genJoinFunc: GeneratedFunction[FlatJoinFunction[Row, Row, Row], Row])
    --- End diff --
    
    The other functions pass the `code` and `name` of the class and have `@transient` parameter for the `returnType`. Have a look at `CRowFlatMapRunner` for instance


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116268688
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -0,0 +1,180 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamJoin(
    +   cluster: RelOptCluster,
    +   traitSet: RelTraitSet,
    +   leftNode: RelNode,
    +   rightNode: RelNode,
    +   joinNode: FlinkLogicalJoin,
    +   leftSchema: RowSchema,
    +   schema: RowSchema,
    +   ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +  with CommonJoin
    +  with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinNode,
    +      leftSchema,
    +      schema,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +
    +    s"${joinTypeToString(joinNode.getJoinType)}" +
    +      s"(condition: (${joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString)}), " +
    +      s"select: (${joinSelectionToString(schema.logicalType)}))"
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    super.explainTerms(pw)
    +      .item("condition", joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString))
    +      .item("select", joinSelectionToString(schema.logicalType))
    +      .item("joinType", joinTypeToString(joinNode.getJoinType))
    +  }
    +
    +  override def translateToPlan(tableEnv: StreamTableEnvironment): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    // get the equality keys and other condition
    +    val (leftKeys, rightKeys, otherCondition) =
    +      JoinUtil.analyzeJoinCondition(joinNode, getExpressionString)
    +
    +    if (left.isInstanceOf[StreamTableSourceScan]
    --- End diff --
    
    @hongyuhong, why do we need this condition?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116261221
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -37,14 +39,18 @@ import org.apache.flink.types.Row
       * Flink RelNode which matches along with JoinOperator and its related operations.
       */
     class DataStreamJoin(
    -   cluster: RelOptCluster,
    -   traitSet: RelTraitSet,
    -   leftNode: RelNode,
    -   rightNode: RelNode,
    -   joinNode: FlinkLogicalJoin,
    -   leftSchema: RowSchema,
    -   schema: RowSchema,
    -   ruleDescription: String)
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinInfo: JoinInfo,
    +    joinType: JoinRelType,
    +    joinHint: JoinHint,
    --- End diff --
    
    Remove `JoinHint`. This is a hint for the DataSet API which is not useful here.


Github user rtudoran commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116396723
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    --- End diff --
    
    @fhueske @hongyuhong As far as i know there is an automatic deduplication of timers. So even if you would register a timer for every event that comes, as long as it is registered to trigger on the same time - the onTimer method will be executed once.
    If there is no way to register the triggering on the same time - then i am wondering whether you need a state. In case there would be a crash and the events would be restored - you would anyway had to register a timer again - which would happen also if you would only have a class variable field (e.g. a boolean).



Github user rtudoran commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116397057
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamJoin.scala ---
    @@ -0,0 +1,180 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamJoin(
    +   cluster: RelOptCluster,
    +   traitSet: RelTraitSet,
    +   leftNode: RelNode,
    +   rightNode: RelNode,
    +   joinNode: FlinkLogicalJoin,
    +   leftSchema: RowSchema,
    +   schema: RowSchema,
    +   ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +  with CommonJoin
    +  with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinNode,
    +      leftSchema,
    +      schema,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +
    +    s"${joinTypeToString(joinNode.getJoinType)}" +
    +      s"(condition: (${joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString)}), " +
    +      s"select: (${joinSelectionToString(schema.logicalType)}))"
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    super.explainTerms(pw)
    +      .item("condition", joinConditionToString(schema.logicalType,
    +        joinNode.getCondition, getExpressionString))
    +      .item("select", joinSelectionToString(schema.logicalType))
    +      .item("joinType", joinTypeToString(joinNode.getJoinType))
    +  }
    +
    +  override def translateToPlan(tableEnv: StreamTableEnvironment): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    // get the equality keys and other condition
    +    val (leftKeys, rightKeys, otherCondition) =
    +      JoinUtil.analyzeJoinCondition(joinNode, getExpressionString)
    +
    +    if (left.isInstanceOf[StreamTableSourceScan]
    --- End diff --
    
    @fhueske @hongyuhong 
    I believe the logic is that we need to differentiate in this backbone class between the cases of JOIN: Stream - Stream and Stream - Table, and then have a condition for each. I believe this case is to restrict (for now) the implementation only for the Stream-Stream.
    The question is now - will it always be the case that if we create an SQL query, that has one of the inputs a table - this will be of "StreamTableSourceScan" ...is it possible to have it of a different type?



Github user rtudoran commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116397441
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -199,31 +157,38 @@ object JoinUtil {
         val generator = new CodeGenerator(
           config,
           nullCheck,
    -      returnType)
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
     
         // if other condition is null, the filterfunc always return true
         val body = if (otherCondition == null) {
    --- End diff --
    
    @fhueske @hongyuhong - i think it is Lietral(True)...at least when i was working with inner queries i was always testing for an always true condition



Github user rtudoran commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116397679
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/FilterRunner.scala ---
    @@ -0,0 +1,54 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime
    +
    +import org.apache.flink.api.common.functions.util.FunctionUtils
    +import org.apache.flink.api.common.functions.{FilterFunction, RichFilterFunction}
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.table.codegen.Compiler
    +import org.slf4j.LoggerFactory
    +
    +class FilterRunner[IN] (
    --- End diff --
    
    @fhueske just for curiosity - is there an advantage of compiling it in the open - basically at runtime when it is deployed (if i am not mistaken) compared to making the compilation statically at compiler time of the query? 


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116416819
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    --- End diff --
    
    @rtudoran yes, there is an automatic deduplication of timers for the same timestamp. But the timerstate is not for the same timestamp, it is for the same key. I would not keep so much timers, it guarantee that there will only have one timer, after this timer finish, then i will register another. Cause we also will expire the other stream's records in processElement, if register timer for every event, sometimes it's no need. So using timerstate can reduce the timer count.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116450145
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/FilterRunner.scala ---
    @@ -0,0 +1,54 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime
    +
    +import org.apache.flink.api.common.functions.util.FunctionUtils
    +import org.apache.flink.api.common.functions.{FilterFunction, RichFilterFunction}
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.table.codegen.Compiler
    +import org.slf4j.LoggerFactory
    +
    +class FilterRunner[IN] (
    --- End diff --
    
    Yes, it won't work otherwise ;-) The reason is the following: Flink serializes user functions (i.e., the `ProcTimeInnerJoin` `CoProcessFunction`) with Java serialization. We would compile the code-gen'd function at compile time and pass the resulting object into the user function, the deserialization would fail, because the code-gen'd class is not known at the remote worked. The code-gen'd class was only added to the classloader of the client JVM but not in the worker JVM. By shipping a String and compiling in the worker, we ensure that all information is available at the worker. 
    
    We could of course also ship the generated class and load it to the classloader of the worker before deserializing the user function (and the included code-gen'd object), but Flink does not have a mechanism for this yet. We might add it later at some point because it would also be interesting for code-gen'd serializers and comparators.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116450411
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -199,31 +157,38 @@ object JoinUtil {
         val generator = new CodeGenerator(
           config,
           nullCheck,
    -      returnType)
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
     
         // if other condition is null, the filterfunc always return true
         val body = if (otherCondition == null) {
    --- End diff --
    
    Yes, I would also expect a `Literal(True)` but wasn't sure.


Github user fhueske commented on the issue:

    https://github.com/apache/flink/pull/3715
  
    Hi @hongyuhong, thanks for the update. 
    
    Please do not squash update commits into the PR. Otherwise it is not possible to review individual changes. If you squash commits, I have to check the whole PR again because I cannot see what was changed since my last review.
    
    It would be also good if you could leave a short message when you update a PR. Github does not send out notifications if the PR branch was updated.
    
    I will review your PR later.
    Thanks, Fabian


Github user hongyuhong commented on the issue:

    https://github.com/apache/flink/pull/3715
  
    Hi @fhueske, Thanks for your reminding, i'm sorry that i forgot to ping you. I thought a PR could have only one commit, so i squash them to one, i will notice that.
    
    Thanks very much.


Github user fhueske commented on the issue:

    https://github.com/apache/flink/pull/3715
  
    The initial PR should have one commit. Following changes should be put as commits on top of the initial commit. But you are right, we should update the contribution guidelines to make this more clear.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116993635
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamJoinRule.scala ---
    @@ -0,0 +1,69 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.rules.datastream
    +
    +import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
    +import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamJoin
    +import org.apache.flink.table.plan.nodes.FlinkConventions
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +
    +class DataStreamJoinRule
    +  extends ConverterRule(
    +      classOf[FlinkLogicalJoin],
    +      FlinkConventions.LOGICAL,
    +      FlinkConventions.DATASTREAM,
    +      "DataStreamJoinRule") {
    +
    +  override def matches(call: RelOptRuleCall): Boolean = {
    +    val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
    +
    +    val joinInfo = join.analyzeCondition
    --- End diff --
    
    I think it makes sense to separate different join cases on the level of plan operators. That would mean we would check here if the join has bounded time predicates (for example `left.rowtime BETWEEN right.rowtime - INTERVAL '1' MINUTE AND right.rowtime + INTERVAL '1' MINUTE`).
    
    If this is the case, we would extract the relevant time predicates and create a stream-stream join RelNode. This would move a bit of code out of the `DataStreamJoin` into the rule.
    
    IMO, the benefit is that it will be easier to add other joins because we only need to add a new rule, a new RelNode and runtime code. Hence, we would not need to touch the existing join strategy.
    
    What do you think about this @hongyuhong?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116797215
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val EQUI_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val LESS_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val GREAT_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL)
    +
    +  /**
    +    * Analyze time-conditon to divide all time-condition into great and less condition
    +    */
    +  private def analyzeTimeCondition(
    +    condition: RexNode,
    +    greatCondition: util.List[TimeSingleCondition],
    +    lessCondition: util.List[TimeSingleCondition],
    +    leftFieldCount: Int,
    +    inputType: RelDataType): Unit = {
    +    if (condition.isInstanceOf[RexCall]) {
    +      val call: RexCall = condition.asInstanceOf[RexCall]
    +      call.getKind match {
    +        case SqlKind.AND =>
    +          var i = 0
    +          while (i < call.getOperands.size) {
    +            analyzeTimeCondition(
    +              call.getOperands.get(i),
    +              greatCondition,
    +              lessCondition,
    +              leftFieldCount,
    +              inputType)
    +            i += 1
    +          }
    +        case kind if kind.belongsTo(COMPARISON) =>
    +          // analyze left expression
    +          val (isExistTimeIndicator1, timeType1, isLeftStreamAttr1) =
    +            analyzeTimeExpression(call.getOperands.get(0), leftFieldCount, inputType)
    +
    +          // make sure proctime/rowtime exist
    +          if (isExistTimeIndicator1) {
    +            // analyze right expression
    +            val (isExistTimeIndicator2, timeType2, isLeftStreamAttr2) =
    +            analyzeTimeExpression(call.getOperands.get(1), leftFieldCount, inputType)
    +            if (!isExistTimeIndicator2) {
    +              throw TableException(
    +                "Equality join time conditon should include time indicator both side."
    --- End diff --
    
    "conditon" -> "condition"


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117000947
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    --- End diff --
    
    use of `var` is discouraged in Scala. Try to use `val`.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116791707
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    --- End diff --
    
    "conditon" -> "condition"


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116861487
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    --- End diff --
    
    more inline comments would be good


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116847591
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    --- End diff --
    
    Please avoid Java class if not necessary


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116796524
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val EQUI_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val LESS_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val GREAT_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL)
    +
    +  /**
    +    * Analyze time-conditon to divide all time-condition into great and less condition
    +    */
    +  private def analyzeTimeCondition(
    +    condition: RexNode,
    +    greatCondition: util.List[TimeSingleCondition],
    +    lessCondition: util.List[TimeSingleCondition],
    +    leftFieldCount: Int,
    +    inputType: RelDataType): Unit = {
    +    if (condition.isInstanceOf[RexCall]) {
    +      val call: RexCall = condition.asInstanceOf[RexCall]
    +      call.getKind match {
    +        case SqlKind.AND =>
    +          var i = 0
    +          while (i < call.getOperands.size) {
    +            analyzeTimeCondition(
    +              call.getOperands.get(i),
    +              greatCondition,
    +              lessCondition,
    +              leftFieldCount,
    +              inputType)
    +            i += 1
    +          }
    +        case kind if kind.belongsTo(COMPARISON) =>
    --- End diff --
    
    can be expressed as 
    
    ```
    case kind @ (SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL |
                   SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116791596
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    --- End diff --
    
    Please wrap the arguments of a function as follows (if it does not fit in a single line):
    ```
    analyzeTimeCondition(
      condition, 
      greateConditions,
      lessConditions, 
      leftLogicalFieldCnt, 
      inputType)
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116847715
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    --- End diff --
    
    please clean up unused imports


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116993947
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamJoinRule.scala ---
    @@ -0,0 +1,69 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.rules.datastream
    +
    +import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
    +import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamJoin
    +import org.apache.flink.table.plan.nodes.FlinkConventions
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +
    +class DataStreamJoinRule
    +  extends ConverterRule(
    +      classOf[FlinkLogicalJoin],
    +      FlinkConventions.LOGICAL,
    +      FlinkConventions.DATASTREAM,
    +      "DataStreamJoinRule") {
    +
    +  override def matches(call: RelOptRuleCall): Boolean = {
    +    val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
    +
    +    val joinInfo = join.analyzeCondition
    +
    +    // joins require at least one equi-condition
    +    !joinInfo.pairs().isEmpty
    +  }
    +
    +  override def convert(rel: RelNode): RelNode = {
    +
    +    val join: FlinkLogicalJoin = rel.asInstanceOf[FlinkLogicalJoin]
    +    val traitSet: RelTraitSet = rel.getTraitSet.replace(FlinkConventions.DATASTREAM)
    +    val convLeft: RelNode = RelOptRule.convert(join.getInput(0), FlinkConventions.DATASTREAM)
    +    val convRight: RelNode = RelOptRule.convert(join.getInput(1), FlinkConventions.DATASTREAM)
    +    val joinInfo = join.analyzeCondition
    +
    +    new DataStreamJoin(
    --- End diff --
    
    If check and extract the time predicates in the rule, we would pass the extracted conditions and remaining condition here to the DataStreamJoin


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116791316
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    --- End diff --
    
    please use Scala lists where possible. 
    In this case (size == 1), you could also try to use `Option[TimeSingleCondition]`.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116992617
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamJoinRule.scala ---
    @@ -0,0 +1,69 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.rules.datastream
    +
    +import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
    +import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamJoin
    +import org.apache.flink.table.plan.nodes.FlinkConventions
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +
    +class DataStreamJoinRule
    +  extends ConverterRule(
    +      classOf[FlinkLogicalJoin],
    +      FlinkConventions.LOGICAL,
    +      FlinkConventions.DATASTREAM,
    +      "DataStreamJoinRule") {
    +
    +  override def matches(call: RelOptRuleCall): Boolean = {
    +    val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
    +
    +    val joinInfo = join.analyzeCondition
    +
    +    // joins require at least one equi-condition
    +    !joinInfo.pairs().isEmpty
    --- End diff --
    
    We can do the join also without equi-join condition. In this case it the join would run with parallelism 1. 


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116791975
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    --- End diff --
    
    `greaterConditions > 1 || greaterConditions == 0` can be replaced by `greaterConditions != 1`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116794639
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val EQUI_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val LESS_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val GREAT_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL)
    +
    +  /**
    +    * Analyze time-conditon to divide all time-condition into great and less condition
    +    */
    +  private def analyzeTimeCondition(
    +    condition: RexNode,
    --- End diff --
    
    please indent the arguments by two more spaces and add a new line to make the signature better distinguishable from the body:+
    
    ```
    private def analyzeTimeCondition(
        condition: RexNode,
        greatCondition: util.List[TimeSingleCondition],
        lessCondition: util.List[TimeSingleCondition],
        leftFieldCount: Int,
        inputType: RelDataType): Unit = {
    
      if (condition.isInstanceOf[RexCall]) {
      ...
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116794136
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val EQUI_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val LESS_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val GREAT_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL)
    +
    +  /**
    +    * Analyze time-conditon to divide all time-condition into great and less condition
    +    */
    +  private def analyzeTimeCondition(
    +    condition: RexNode,
    +    greatCondition: util.List[TimeSingleCondition],
    +    lessCondition: util.List[TimeSingleCondition],
    +    leftFieldCount: Int,
    +    inputType: RelDataType): Unit = {
    +    if (condition.isInstanceOf[RexCall]) {
    +      val call: RexCall = condition.asInstanceOf[RexCall]
    +      call.getKind match {
    +        case SqlKind.AND =>
    +          var i = 0
    +          while (i < call.getOperands.size) {
    +            analyzeTimeCondition(
    +              call.getOperands.get(i),
    +              greatCondition,
    +              lessCondition,
    +              leftFieldCount,
    +              inputType)
    +            i += 1
    +          }
    +        case kind if kind.belongsTo(COMPARISON) =>
    +          // analyze left expression
    +          val (isExistTimeIndicator1, timeType1, isLeftStreamAttr1) =
    +            analyzeTimeExpression(call.getOperands.get(0), leftFieldCount, inputType)
    +
    +          // make sure proctime/rowtime exist
    +          if (isExistTimeIndicator1) {
    +            // analyze right expression
    +            val (isExistTimeIndicator2, timeType2, isLeftStreamAttr2) =
    +            analyzeTimeExpression(call.getOperands.get(1), leftFieldCount, inputType)
    +            if (!isExistTimeIndicator2) {
    +              throw TableException(
    +                "Equality join time conditon should include time indicator both side."
    +              )
    +            } else if (timeType1 != timeType2) {
    +              throw TableException(
    +                "Equality join time conditon should include same time indicator each side."
    +              )
    +            } else if (isLeftStreamAttr1 == isLeftStreamAttr2) {
    +              throw TableException(
    +                "Equality join time conditon should include both two streams's time indicator."
    +              )
    +            } else {
    +              val isGreate: Boolean = kind.belongsTo(GREAT_COMPARISON)
    +              val isEqual: Boolean = kind.belongsTo(EQUI_COMPARISON)
    +              (isGreate, isLeftStreamAttr1) match {
    +                case (true, true) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    --- End diff --
    
    case classes are usually instantiated without the `new` keyword. 
    Also put each argument in a new line:
    
    ```
    val newCond: TimeSingleCondition = TimeSingleCondition(
      timeType1, 
      call.getOperands.get(0), 
      call.getOperands.get(1), 
      isEqual, 
      call)
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116792670
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val EQUI_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val LESS_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val GREAT_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL)
    +
    +  /**
    +    * Analyze time-conditon to divide all time-condition into great and less condition
    +    */
    +  private def analyzeTimeCondition(
    --- End diff --
    
    can be defined as an internal method in `analyzeTimeBoundary`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116790511
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    --- End diff --
    
    -> `leftLogicalFieldCnt`
    
    Add `rightLogicalFieldCnt`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116795204
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val EQUI_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val LESS_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val GREAT_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL)
    +
    +  /**
    +    * Analyze time-conditon to divide all time-condition into great and less condition
    +    */
    +  private def analyzeTimeCondition(
    +    condition: RexNode,
    +    greatCondition: util.List[TimeSingleCondition],
    +    lessCondition: util.List[TimeSingleCondition],
    +    leftFieldCount: Int,
    +    inputType: RelDataType): Unit = {
    +    if (condition.isInstanceOf[RexCall]) {
    --- End diff --
    
    the pattern `isInstanceOf` followed by `asInstanceOf` can be better expressed by pattern matching:
    
    ```
    condition match {
      case call: RexCall =>
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116791061
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    --- End diff --
    
    +r -> `greaterConditions`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117001241
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    --- End diff --
    
    if conditions can return results in Scala:
    
    ```
    val res = if (x > 10) "large" else "small"
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116792128
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    --- End diff --
    
    The whole condition can be changed to `lessConditions != 1 || greaterConditions != 1`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r116854249
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val EQUI_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val LESS_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val GREAT_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL)
    +
    +  /**
    +    * Analyze time-conditon to divide all time-condition into great and less condition
    +    */
    +  private def analyzeTimeCondition(
    +    condition: RexNode,
    +    greatCondition: util.List[TimeSingleCondition],
    +    lessCondition: util.List[TimeSingleCondition],
    +    leftFieldCount: Int,
    +    inputType: RelDataType): Unit = {
    +    if (condition.isInstanceOf[RexCall]) {
    +      val call: RexCall = condition.asInstanceOf[RexCall]
    +      call.getKind match {
    +        case SqlKind.AND =>
    +          var i = 0
    +          while (i < call.getOperands.size) {
    +            analyzeTimeCondition(
    +              call.getOperands.get(i),
    +              greatCondition,
    +              lessCondition,
    +              leftFieldCount,
    +              inputType)
    +            i += 1
    +          }
    +        case kind if kind.belongsTo(COMPARISON) =>
    +          // analyze left expression
    +          val (isExistTimeIndicator1, timeType1, isLeftStreamAttr1) =
    +            analyzeTimeExpression(call.getOperands.get(0), leftFieldCount, inputType)
    +
    +          // make sure proctime/rowtime exist
    +          if (isExistTimeIndicator1) {
    +            // analyze right expression
    +            val (isExistTimeIndicator2, timeType2, isLeftStreamAttr2) =
    +            analyzeTimeExpression(call.getOperands.get(1), leftFieldCount, inputType)
    +            if (!isExistTimeIndicator2) {
    +              throw TableException(
    +                "Equality join time conditon should include time indicator both side."
    +              )
    +            } else if (timeType1 != timeType2) {
    +              throw TableException(
    +                "Equality join time conditon should include same time indicator each side."
    +              )
    +            } else if (isLeftStreamAttr1 == isLeftStreamAttr2) {
    +              throw TableException(
    +                "Equality join time conditon should include both two streams's time indicator."
    +              )
    +            } else {
    +              val isGreate: Boolean = kind.belongsTo(GREAT_COMPARISON)
    +              val isEqual: Boolean = kind.belongsTo(EQUI_COMPARISON)
    +              (isGreate, isLeftStreamAttr1) match {
    +                case (true, true) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(0), call.getOperands.get(1), isEqual, call)
    +                  greatCondition.add(newCond)
    +                case (true, false) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(1), call.getOperands.get(0), isEqual, call)
    +                  lessCondition.add(newCond)
    +                case (false, true) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(0), call.getOperands.get(1), isEqual, call)
    +                  lessCondition.add(newCond)
    +                case (false, false) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(1), call.getOperands.get(0), isEqual, call)
    +                  greatCondition.add(newCond)
    +
    +              }
    +            }
    +          }
    +        case _ =>
    +      }
    +    }
    +  }
    +
    +  /**
    +    * Analyze time-expression(b.proctime + interval '1' hour) to check whether is valid
    +    * and get the time predicate type(proctime or rowtime)
    +    */
    +  private def analyzeTimeExpression(
    --- End diff --
    
    Can be implemented in a more Scala-like style similar to this:
    ```
      private def analyzeTimeExpression(
        expression: RexNode,
        leftFieldCount: Int,
        inputType: RelDataType): (Option[RelDataType], Boolean) = {
    
        expression match {
          case i: RexInputRef =>
            val idx = i.getIndex
            inputType.getFieldList.get(idx).getType match {
              case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
                // left table time indicator
                (Some(t), true)
              case t: TimeIndicatorRelDataType =>
                // right table time indicator
                (Some(t), false)
              case _ =>
                (None, false)
            }
          case c: RexCall =>
            val timeIndicators = c.operands.map(analyzeTimeExpression(_, leftFieldCount, inputType))
            // check that we have only a single time indicator
            val timeIndicator = timeIndicators.reduceLeft { (l, r) =>
              if (l._1.isEmpty) {
                r
              } else if (r._1.isEmpty) {
                l
              } else {
                throw new TableException("Only a single time indicators is allowed on each side of a condition.")
              }
            }
            timeIndicator
          case _ =>
            (None, false)
        }
      }
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117002279
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    --- End diff --
    
    As a first step, the `condition` should be converted into CNF (conjunctive normal form) for normalization. Calcite offers the `RexUtil.toCnf()` method for that.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117033149
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val EQUI_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val LESS_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val GREAT_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL)
    +
    +  /**
    +    * Analyze time-conditon to divide all time-condition into great and less condition
    +    */
    +  private def analyzeTimeCondition(
    +    condition: RexNode,
    +    greatCondition: util.List[TimeSingleCondition],
    +    lessCondition: util.List[TimeSingleCondition],
    +    leftFieldCount: Int,
    +    inputType: RelDataType): Unit = {
    +    if (condition.isInstanceOf[RexCall]) {
    +      val call: RexCall = condition.asInstanceOf[RexCall]
    +      call.getKind match {
    +        case SqlKind.AND =>
    +          var i = 0
    +          while (i < call.getOperands.size) {
    +            analyzeTimeCondition(
    +              call.getOperands.get(i),
    +              greatCondition,
    +              lessCondition,
    +              leftFieldCount,
    +              inputType)
    +            i += 1
    +          }
    +        case kind if kind.belongsTo(COMPARISON) =>
    +          // analyze left expression
    +          val (isExistTimeIndicator1, timeType1, isLeftStreamAttr1) =
    +            analyzeTimeExpression(call.getOperands.get(0), leftFieldCount, inputType)
    +
    +          // make sure proctime/rowtime exist
    +          if (isExistTimeIndicator1) {
    +            // analyze right expression
    +            val (isExistTimeIndicator2, timeType2, isLeftStreamAttr2) =
    +            analyzeTimeExpression(call.getOperands.get(1), leftFieldCount, inputType)
    +            if (!isExistTimeIndicator2) {
    +              throw TableException(
    +                "Equality join time conditon should include time indicator both side."
    +              )
    +            } else if (timeType1 != timeType2) {
    +              throw TableException(
    +                "Equality join time conditon should include same time indicator each side."
    +              )
    +            } else if (isLeftStreamAttr1 == isLeftStreamAttr2) {
    +              throw TableException(
    +                "Equality join time conditon should include both two streams's time indicator."
    +              )
    +            } else {
    +              val isGreate: Boolean = kind.belongsTo(GREAT_COMPARISON)
    +              val isEqual: Boolean = kind.belongsTo(EQUI_COMPARISON)
    +              (isGreate, isLeftStreamAttr1) match {
    +                case (true, true) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(0), call.getOperands.get(1), isEqual, call)
    +                  greatCondition.add(newCond)
    +                case (true, false) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(1), call.getOperands.get(0), isEqual, call)
    +                  lessCondition.add(newCond)
    +                case (false, true) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(0), call.getOperands.get(1), isEqual, call)
    +                  lessCondition.add(newCond)
    +                case (false, false) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(1), call.getOperands.get(0), isEqual, call)
    +                  greatCondition.add(newCond)
    +
    +              }
    +            }
    +          }
    +        case _ =>
    +      }
    +    }
    +  }
    +
    +  /**
    +    * Analyze time-expression(b.proctime + interval '1' hour) to check whether is valid
    +    * and get the time predicate type(proctime or rowtime)
    +    */
    +  private def analyzeTimeExpression(
    +     expression: RexNode,
    +     leftFieldCount: Int,
    +     inputType: RelDataType): (Boolean, RelDataType, Boolean) = {
    +
    +    var timeType: RelDataType = null
    +    var isExistTimeIndicator = false
    +    var isLeftStreamAttr = false
    +    if (expression.isInstanceOf[RexInputRef]) {
    +      val idx = expression.asInstanceOf[RexInputRef].getIndex
    +      timeType = inputType.getFieldList.get(idx).getType
    +      timeType match {
    +        case _ if FlinkTypeFactory.isProctimeIndicatorType(timeType) =>
    +          isExistTimeIndicator = true
    +        case _ if FlinkTypeFactory.isRowtimeIndicatorType(timeType) =>
    +          isExistTimeIndicator = true
    +        case _ =>
    +          isExistTimeIndicator = false
    +      }
    +
    +      if (idx < leftFieldCount) {
    +        isLeftStreamAttr = true
    +      }
    +    } else if (expression.isInstanceOf[RexCall]) {
    +      val call: RexCall = expression.asInstanceOf[RexCall]
    +      var i = 0
    +      while (i < call.getOperands.size) {
    +        val (curIsExistSysTimeAttr, curTimeType, curIsLeftStreamAttr) =
    +          analyzeTimeExpression(call.getOperands.get(i), leftFieldCount, inputType)
    +        if (isExistTimeIndicator && curIsExistSysTimeAttr) {
    +          throw TableException(
    +            s"Equality join time conditon can not include duplicate {$timeType} attribute."
    +          )
    +        }
    +        if (curIsExistSysTimeAttr) {
    +          isExistTimeIndicator = curIsExistSysTimeAttr
    +          timeType = curTimeType
    +          isLeftStreamAttr = curIsLeftStreamAttr
    +        }
    +
    +        i += 1
    +      }
    +
    +    }
    +    (isExistTimeIndicator, timeType, isLeftStreamAttr)
    +  }
    +
    +  /**
    +    * Calcute the time boundary. Replace the rowtime/proctime with zero literal.
    +    * such as:
    +    *  a.proctime - inteval '1' second > b.proctime - interval '1' second - interval '2' second
    +    *  |-----------left--------------|   |-------------------right---------------------------\
    +    * then the boundary of a is right - left:
    +    *  ((0 - 1000) - 2000) - (0 - 1000) = -2000
    +    */
    +  private def reduceTimeExpression(
    +    leftNode: RexNode,
    +    rightNode: RexNode,
    +    rexBuilder: RexBuilder,
    +    config: TableConfig): RexLiteral = {
    +
    +    val replLeft = replaceTimeIndicatorWithLiteral(leftNode, rexBuilder, true)
    +    val replRight = replaceTimeIndicatorWithLiteral(rightNode, rexBuilder,false)
    +    val literalRex = rexBuilder.makeCall(SqlStdOperatorTable.MINUS, replLeft, replRight)
    +
    +    val exprReducer = new ExpressionReducer(config)
    +    val originList = new util.ArrayList[RexNode]()
    +    originList.add(literalRex)
    +    val reduceList = new util.ArrayList[RexNode]()
    +    exprReducer.reduce(rexBuilder, originList, reduceList)
    +
    +    reduceList.get(0) match {
    +      case call: RexCall => call.getOperands.get(0).asInstanceOf[RexLiteral]
    +      case literal: RexLiteral => literal
    +      case _ =>
    +        throw TableException(
    +          s"Equality join time condition only support constant."
    +        )
    +
    +    }
    +  }
    +
    +  /**
    +    * replace the rowtime/proctime with zero literal.
    +    * Because calculation between timestamp can only be TIMESTAMP +/- INTERVAL
    +    * so such as b.proctime + interval '1' hour - a.proctime
    +    * will be translate into TIMESTAMP(0) + interval '1' hour - interval '0' second
    +    */
    +  private def replaceTimeIndicatorWithLiteral(
    --- End diff --
    
    can be implemented in a more Scala-like way as:
    
    ```
    private def replaceTimeIndicatorWithLiteral(
        expr: RexNode,
        rexBuilder: RexBuilder,
        isTimeStamp: Boolean): RexNode = {
    
        expr match {
          case c: RexCall =>
            // replace in call operands
            val newOps = c.operands.map(o => replaceTimeIndicatorWithLiteral(o, rexBuilder, isTimeStamp))
            rexBuilder.makeCall(c.getType, c.getOperator, newOps)
          case i: RexInputRef if FlinkTypeFactory.isTimeIndicatorType(i.getType) && isTimeStamp =>
            // replace with timestamp
            rexBuilder.makeZeroLiteral(expr.getType)
          case i: RexInputRef if FlinkTypeFactory.isTimeIndicatorType(i.getType) && !isTimeStamp =>
            // replace with time interval
            val sqlQualifier = new SqlIntervalQualifier(TimeUnit.SECOND, 0, null, 0, SqlParserPos.ZERO)
            rexBuilder.makeIntervalLiteral(JBigDecimal.ZERO, sqlQualifier)
          case _: RexInputRef =>
            throw new TableException(s"Time join condition may only reference time indicator fields.")
          case _ => expr
        }
      }
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117006282
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,488 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.FilterFunction
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.runtime.FilterRunner
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze join condition to get equi-conditon and other condition
    +    * @param  joinNode   logicaljoin node
    +    * @param  expression the function to generate condition string
    +    */
    +  private[flink] def analyzeJoinCondition(
    +    joinNode: FlinkLogicalJoin,
    +    expression: (RexNode, List[String], Option[List[RexNode]]) => String) = {
    +
    +    val joinInfo = joinNode.analyzeCondition()
    +    val keyPairs = joinInfo.pairs.toList
    +    val otherCondition =
    +      if(joinInfo.isEqui) null
    +      else joinInfo.getRemaining(joinNode.getCluster.getRexBuilder)
    +
    +    val leftKeys = ArrayBuffer.empty[Int]
    +    val rightKeys = ArrayBuffer.empty[Int]
    +    if (!keyPairs.isEmpty) {
    +      val leftFields = joinNode.getLeft.getRowType.getFieldList
    +      val rightFields = joinNode.getRight.getRowType.getFieldList
    +
    +      keyPairs.foreach(pair => {
    +        val leftKeyType = leftFields.get(pair.source).getType.getSqlTypeName
    +        val rightKeyType = rightFields.get(pair.target).getType.getSqlTypeName
    +
    +        // check if keys are compatible
    +        if (leftKeyType == rightKeyType) {
    +          // add key pair
    +          leftKeys.append(pair.source)
    +          rightKeys.append(pair.target)
    +        } else {
    +          throw TableException(
    +            "Equality join predicate on incompatible types.\n" +
    +              s"\tLeft: ${joinNode.getLeft.toString},\n" +
    +              s"\tRight: ${joinNode.getRight.toString},\n" +
    +              s"\tCondition: (${expression(joinNode.getCondition,
    +                joinNode.getRowType.getFieldNames.toList, None)})"
    +          )
    +        }
    +      })
    +    }
    +    (leftKeys.toArray, rightKeys.toArray, otherCondition)
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    --- End diff --
    
    I think the logic of this function is correct. However, I find it a bit hard to follow because it starts with many conditions.
    What do you think about the following approach:
    
    1. convert condition to CNF
    2. go over all conjunctive terms and split them into those referencing a time field and those that don't reference a time field (those latter terms are the remaining conditions)
    3. Check that there are only two such terms, throw an exception otherwise
    4. Check for each conjunctive term that is references time fields of both inputs exactly once. If there is one that does not, throw an exception.
    5. Extract the offset from each of the two terms (this is the complex step)
    
    This way, we keep the logic simple until the end and don't need to distinguish between greater / smaller, left input, right input, etc. from the start.
    
    What do you think?



Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117031586
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val EQUI_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val LESS_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val GREAT_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL)
    +
    +  /**
    +    * Analyze time-conditon to divide all time-condition into great and less condition
    +    */
    +  private def analyzeTimeCondition(
    +    condition: RexNode,
    +    greatCondition: util.List[TimeSingleCondition],
    +    lessCondition: util.List[TimeSingleCondition],
    +    leftFieldCount: Int,
    +    inputType: RelDataType): Unit = {
    +    if (condition.isInstanceOf[RexCall]) {
    +      val call: RexCall = condition.asInstanceOf[RexCall]
    +      call.getKind match {
    +        case SqlKind.AND =>
    +          var i = 0
    +          while (i < call.getOperands.size) {
    +            analyzeTimeCondition(
    +              call.getOperands.get(i),
    +              greatCondition,
    +              lessCondition,
    +              leftFieldCount,
    +              inputType)
    +            i += 1
    +          }
    +        case kind if kind.belongsTo(COMPARISON) =>
    +          // analyze left expression
    +          val (isExistTimeIndicator1, timeType1, isLeftStreamAttr1) =
    +            analyzeTimeExpression(call.getOperands.get(0), leftFieldCount, inputType)
    +
    +          // make sure proctime/rowtime exist
    +          if (isExistTimeIndicator1) {
    +            // analyze right expression
    +            val (isExistTimeIndicator2, timeType2, isLeftStreamAttr2) =
    +            analyzeTimeExpression(call.getOperands.get(1), leftFieldCount, inputType)
    +            if (!isExistTimeIndicator2) {
    +              throw TableException(
    +                "Equality join time conditon should include time indicator both side."
    +              )
    +            } else if (timeType1 != timeType2) {
    +              throw TableException(
    +                "Equality join time conditon should include same time indicator each side."
    +              )
    +            } else if (isLeftStreamAttr1 == isLeftStreamAttr2) {
    +              throw TableException(
    +                "Equality join time conditon should include both two streams's time indicator."
    +              )
    +            } else {
    +              val isGreate: Boolean = kind.belongsTo(GREAT_COMPARISON)
    +              val isEqual: Boolean = kind.belongsTo(EQUI_COMPARISON)
    +              (isGreate, isLeftStreamAttr1) match {
    +                case (true, true) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(0), call.getOperands.get(1), isEqual, call)
    +                  greatCondition.add(newCond)
    +                case (true, false) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(1), call.getOperands.get(0), isEqual, call)
    +                  lessCondition.add(newCond)
    +                case (false, true) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(0), call.getOperands.get(1), isEqual, call)
    +                  lessCondition.add(newCond)
    +                case (false, false) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(1), call.getOperands.get(0), isEqual, call)
    +                  greatCondition.add(newCond)
    +
    +              }
    +            }
    +          }
    +        case _ =>
    +      }
    +    }
    +  }
    +
    +  /**
    +    * Analyze time-expression(b.proctime + interval '1' hour) to check whether is valid
    +    * and get the time predicate type(proctime or rowtime)
    +    */
    +  private def analyzeTimeExpression(
    +     expression: RexNode,
    +     leftFieldCount: Int,
    +     inputType: RelDataType): (Boolean, RelDataType, Boolean) = {
    +
    +    var timeType: RelDataType = null
    +    var isExistTimeIndicator = false
    +    var isLeftStreamAttr = false
    +    if (expression.isInstanceOf[RexInputRef]) {
    +      val idx = expression.asInstanceOf[RexInputRef].getIndex
    +      timeType = inputType.getFieldList.get(idx).getType
    +      timeType match {
    +        case _ if FlinkTypeFactory.isProctimeIndicatorType(timeType) =>
    +          isExistTimeIndicator = true
    +        case _ if FlinkTypeFactory.isRowtimeIndicatorType(timeType) =>
    +          isExistTimeIndicator = true
    +        case _ =>
    +          isExistTimeIndicator = false
    +      }
    +
    +      if (idx < leftFieldCount) {
    +        isLeftStreamAttr = true
    +      }
    +    } else if (expression.isInstanceOf[RexCall]) {
    +      val call: RexCall = expression.asInstanceOf[RexCall]
    +      var i = 0
    +      while (i < call.getOperands.size) {
    +        val (curIsExistSysTimeAttr, curTimeType, curIsLeftStreamAttr) =
    +          analyzeTimeExpression(call.getOperands.get(i), leftFieldCount, inputType)
    +        if (isExistTimeIndicator && curIsExistSysTimeAttr) {
    +          throw TableException(
    +            s"Equality join time conditon can not include duplicate {$timeType} attribute."
    +          )
    +        }
    +        if (curIsExistSysTimeAttr) {
    +          isExistTimeIndicator = curIsExistSysTimeAttr
    +          timeType = curTimeType
    +          isLeftStreamAttr = curIsLeftStreamAttr
    +        }
    +
    +        i += 1
    +      }
    +
    +    }
    +    (isExistTimeIndicator, timeType, isLeftStreamAttr)
    +  }
    +
    +  /**
    +    * Calcute the time boundary. Replace the rowtime/proctime with zero literal.
    +    * such as:
    +    *  a.proctime - inteval '1' second > b.proctime - interval '1' second - interval '2' second
    +    *  |-----------left--------------|   |-------------------right---------------------------\
    +    * then the boundary of a is right - left:
    +    *  ((0 - 1000) - 2000) - (0 - 1000) = -2000
    +    */
    +  private def reduceTimeExpression(
    +    leftNode: RexNode,
    +    rightNode: RexNode,
    +    rexBuilder: RexBuilder,
    +    config: TableConfig): RexLiteral = {
    +
    +    val replLeft = replaceTimeIndicatorWithLiteral(leftNode, rexBuilder, true)
    +    val replRight = replaceTimeIndicatorWithLiteral(rightNode, rexBuilder,false)
    +    val literalRex = rexBuilder.makeCall(SqlStdOperatorTable.MINUS, replLeft, replRight)
    +
    +    val exprReducer = new ExpressionReducer(config)
    +    val originList = new util.ArrayList[RexNode]()
    +    originList.add(literalRex)
    +    val reduceList = new util.ArrayList[RexNode]()
    +    exprReducer.reduce(rexBuilder, originList, reduceList)
    +
    +    reduceList.get(0) match {
    +      case call: RexCall => call.getOperands.get(0).asInstanceOf[RexLiteral]
    +      case literal: RexLiteral => literal
    +      case _ =>
    +        throw TableException(
    +          s"Equality join time condition only support constant."
    +        )
    +
    +    }
    +  }
    +
    +  /**
    +    * replace the rowtime/proctime with zero literal.
    +    * Because calculation between timestamp can only be TIMESTAMP +/- INTERVAL
    +    * so such as b.proctime + interval '1' hour - a.proctime
    +    * will be translate into TIMESTAMP(0) + interval '1' hour - interval '0' second
    +    */
    +  private def replaceTimeIndicatorWithLiteral(
    +    expr: RexNode,
    +    rexBuilder: RexBuilder,
    +    isTimeStamp: Boolean): RexNode = {
    +    if (expr.isInstanceOf[RexCall]) {
    +      val call: RexCall = expr.asInstanceOf[RexCall]
    +      var i = 0
    +      val operands = new util.ArrayList[RexNode]
    +      while (i < call.getOperands.size) {
    +        val newRex =
    +          replaceTimeIndicatorWithLiteral(call.getOperands.get(i), rexBuilder, isTimeStamp)
    +        operands.add(newRex)
    +        i += 1
    +      }
    +      rexBuilder.makeCall(call.getType, call.getOperator, operands)
    +    } else if (expr.isInstanceOf[RexInputRef]) {
    --- End diff --
    
    Are we sure at this point that there are no other fields included in the condition?



Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117037366
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    --- End diff --
    
    This method needs some good unit tests.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117033348
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val EQUI_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val LESS_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val GREAT_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL)
    +
    +  /**
    +    * Analyze time-conditon to divide all time-condition into great and less condition
    +    */
    +  private def analyzeTimeCondition(
    +    condition: RexNode,
    +    greatCondition: util.List[TimeSingleCondition],
    +    lessCondition: util.List[TimeSingleCondition],
    +    leftFieldCount: Int,
    +    inputType: RelDataType): Unit = {
    +    if (condition.isInstanceOf[RexCall]) {
    +      val call: RexCall = condition.asInstanceOf[RexCall]
    +      call.getKind match {
    +        case SqlKind.AND =>
    +          var i = 0
    +          while (i < call.getOperands.size) {
    +            analyzeTimeCondition(
    +              call.getOperands.get(i),
    +              greatCondition,
    +              lessCondition,
    +              leftFieldCount,
    +              inputType)
    +            i += 1
    +          }
    +        case kind if kind.belongsTo(COMPARISON) =>
    +          // analyze left expression
    +          val (isExistTimeIndicator1, timeType1, isLeftStreamAttr1) =
    +            analyzeTimeExpression(call.getOperands.get(0), leftFieldCount, inputType)
    +
    +          // make sure proctime/rowtime exist
    +          if (isExistTimeIndicator1) {
    +            // analyze right expression
    +            val (isExistTimeIndicator2, timeType2, isLeftStreamAttr2) =
    +            analyzeTimeExpression(call.getOperands.get(1), leftFieldCount, inputType)
    +            if (!isExistTimeIndicator2) {
    +              throw TableException(
    +                "Equality join time conditon should include time indicator both side."
    +              )
    +            } else if (timeType1 != timeType2) {
    +              throw TableException(
    +                "Equality join time conditon should include same time indicator each side."
    +              )
    +            } else if (isLeftStreamAttr1 == isLeftStreamAttr2) {
    +              throw TableException(
    +                "Equality join time conditon should include both two streams's time indicator."
    +              )
    +            } else {
    +              val isGreate: Boolean = kind.belongsTo(GREAT_COMPARISON)
    +              val isEqual: Boolean = kind.belongsTo(EQUI_COMPARISON)
    +              (isGreate, isLeftStreamAttr1) match {
    +                case (true, true) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(0), call.getOperands.get(1), isEqual, call)
    +                  greatCondition.add(newCond)
    +                case (true, false) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(1), call.getOperands.get(0), isEqual, call)
    +                  lessCondition.add(newCond)
    +                case (false, true) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(0), call.getOperands.get(1), isEqual, call)
    +                  lessCondition.add(newCond)
    +                case (false, false) =>
    +                  val newCond: TimeSingleCondition = new TimeSingleCondition(timeType1,
    +                    call.getOperands.get(1), call.getOperands.get(0), isEqual, call)
    +                  greatCondition.add(newCond)
    +
    +              }
    +            }
    +          }
    +        case _ =>
    +      }
    +    }
    +  }
    +
    +  /**
    +    * Analyze time-expression(b.proctime + interval '1' hour) to check whether is valid
    +    * and get the time predicate type(proctime or rowtime)
    +    */
    +  private def analyzeTimeExpression(
    +     expression: RexNode,
    +     leftFieldCount: Int,
    +     inputType: RelDataType): (Boolean, RelDataType, Boolean) = {
    +
    +    var timeType: RelDataType = null
    +    var isExistTimeIndicator = false
    +    var isLeftStreamAttr = false
    +    if (expression.isInstanceOf[RexInputRef]) {
    +      val idx = expression.asInstanceOf[RexInputRef].getIndex
    +      timeType = inputType.getFieldList.get(idx).getType
    +      timeType match {
    +        case _ if FlinkTypeFactory.isProctimeIndicatorType(timeType) =>
    +          isExistTimeIndicator = true
    +        case _ if FlinkTypeFactory.isRowtimeIndicatorType(timeType) =>
    +          isExistTimeIndicator = true
    +        case _ =>
    +          isExistTimeIndicator = false
    +      }
    +
    +      if (idx < leftFieldCount) {
    +        isLeftStreamAttr = true
    +      }
    +    } else if (expression.isInstanceOf[RexCall]) {
    +      val call: RexCall = expression.asInstanceOf[RexCall]
    +      var i = 0
    +      while (i < call.getOperands.size) {
    +        val (curIsExistSysTimeAttr, curTimeType, curIsLeftStreamAttr) =
    +          analyzeTimeExpression(call.getOperands.get(i), leftFieldCount, inputType)
    +        if (isExistTimeIndicator && curIsExistSysTimeAttr) {
    +          throw TableException(
    +            s"Equality join time conditon can not include duplicate {$timeType} attribute."
    +          )
    +        }
    +        if (curIsExistSysTimeAttr) {
    +          isExistTimeIndicator = curIsExistSysTimeAttr
    +          timeType = curTimeType
    +          isLeftStreamAttr = curIsLeftStreamAttr
    +        }
    +
    +        i += 1
    +      }
    +
    +    }
    +    (isExistTimeIndicator, timeType, isLeftStreamAttr)
    +  }
    +
    +  /**
    +    * Calcute the time boundary. Replace the rowtime/proctime with zero literal.
    +    * such as:
    +    *  a.proctime - inteval '1' second > b.proctime - interval '1' second - interval '2' second
    +    *  |-----------left--------------|   |-------------------right---------------------------\
    +    * then the boundary of a is right - left:
    +    *  ((0 - 1000) - 2000) - (0 - 1000) = -2000
    +    */
    +  private def reduceTimeExpression(
    +    leftNode: RexNode,
    +    rightNode: RexNode,
    +    rexBuilder: RexBuilder,
    +    config: TableConfig): RexLiteral = {
    +
    +    val replLeft = replaceTimeIndicatorWithLiteral(leftNode, rexBuilder, true)
    +    val replRight = replaceTimeIndicatorWithLiteral(rightNode, rexBuilder,false)
    +    val literalRex = rexBuilder.makeCall(SqlStdOperatorTable.MINUS, replLeft, replRight)
    +
    +    val exprReducer = new ExpressionReducer(config)
    +    val originList = new util.ArrayList[RexNode]()
    +    originList.add(literalRex)
    +    val reduceList = new util.ArrayList[RexNode]()
    +    exprReducer.reduce(rexBuilder, originList, reduceList)
    +
    +    reduceList.get(0) match {
    +      case call: RexCall => call.getOperands.get(0).asInstanceOf[RexLiteral]
    +      case literal: RexLiteral => literal
    +      case _ =>
    +        throw TableException(
    +          s"Equality join time condition only support constant."
    +        )
    +
    +    }
    +  }
    +
    +  /**
    +    * replace the rowtime/proctime with zero literal.
    +    * Because calculation between timestamp can only be TIMESTAMP +/- INTERVAL
    +    * so such as b.proctime + interval '1' hour - a.proctime
    +    * will be translate into TIMESTAMP(0) + interval '1' hour - interval '0' second
    +    */
    +  private def replaceTimeIndicatorWithLiteral(
    --- End diff --
    
    Can also be an inline function of `reduceTimeExpression()`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117027410
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, RexNode) = {
    +    // analyze the time-conditon to get greate and less condition,
    +    // make sure left stream field in the left of the condition
    +    // e.g b.proctime > a.proctime - 1 will be translate to a.proctime - 1 < b.proctime
    +    val greateConditions = new util.ArrayList[TimeSingleCondition]()
    +    val lessConditions = new util.ArrayList[TimeSingleCondition]()
    +    analyzeTimeCondition(condition, greateConditions,
    +      lessConditions, leftLogicalFieldCnt, inputType)
    +    if (greateConditions.size != lessConditions.size
    +        || greateConditions.size > 1
    +        || greateConditions.size == 0) {
    +      throw TableException(
    +        "Equality join time conditon should have proctime or rowtime indicator."
    +      )
    +    }
    +
    +    val greatCond = greateConditions.get(0)
    +    val lessCond = lessConditions.get(0)
    +    if (greatCond.timeType != lessCond.timeType) {
    +      throw TableException(
    +        "Equality join time conditon should all use proctime or all use rowtime."
    +      )
    +    }
    +
    +    var leftStreamWindowSize: Long = 0
    +    var rightStreamWindowSize: Long = 0
    +
    +    // only a.proctime > b.proctime - interval '1' hour need to store a stream
    +    val timeLiteral: RexLiteral =
    +        reduceTimeExpression(greatCond.rightExpr, greatCond.leftExpr, rexBuilder, config)
    +    leftStreamWindowSize = timeLiteral.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (leftStreamWindowSize < 0) {
    +      leftStreamWindowSize = -leftStreamWindowSize
    +      if (!greatCond.isEqual) {
    +        leftStreamWindowSize -= 1
    +      }
    +    } else {
    +      leftStreamWindowSize = 0
    +    }
    +
    +    // only a.proctime < b.proctime + interval '1' hour need to store b stream
    +    val timeLiteral2: RexLiteral =
    +        reduceTimeExpression(lessCond.leftExpr, lessCond.rightExpr, rexBuilder, config)
    +    rightStreamWindowSize = timeLiteral2.getValue2.asInstanceOf[Long]
    +    // only need to store past records
    +    if (rightStreamWindowSize < 0) {
    +      rightStreamWindowSize = -rightStreamWindowSize
    +      if (!lessCond.isEqual) {
    +        rightStreamWindowSize -= 1
    +      }
    +    } else {
    +      rightStreamWindowSize = 0
    +    }
    +
    +    // get condition without time-condition
    +    // e.g a.price > b.price and a.proctime between b.proctime and b.proctime + interval '1' hour
    +    // will return a.price > b.price and true and true
    +    var conditionWithoutTime = removeTimeCondition(
    +      condition,
    +      greatCond.originCall,
    +      lessCond.originCall,
    +      rexBuilder,
    +      leftLogicalFieldCnt,
    +      leftPhysicalFieldCnt)
    +
    +    // reduce the expression
    +    // true and ture => true, otherwise keep the origin expression
    +    try {
    +      val exprReducer = new ExpressionReducer(config)
    +      val originList = new util.ArrayList[RexNode]()
    +      originList.add(conditionWithoutTime)
    +      val reduceList = new util.ArrayList[RexNode]()
    +      exprReducer.reduce(rexBuilder, originList, reduceList)
    +      conditionWithoutTime = reduceList.get(0)
    +    } catch {
    +      case _ : CodeGenException => // ignore
    +    }
    +
    +    (greatCond.timeType, leftStreamWindowSize, rightStreamWindowSize, conditionWithoutTime)
    +  }
    +
    +  /**
    +    * Generate other non-equi condition function
    +    * @param  config   table env config
    +    * @param  joinType  join type to determain whether input can be null
    +    * @param  leftType  left stream type
    +    * @param  rightType  right stream type
    +    * @param  returnType   return type
    +    * @param  otherCondition   non-equi condition
    +    * @param  ruleDescription  rule description
    +    */
    +  private[flink] def generateJoinFunction(
    +    config: TableConfig,
    +    joinType: JoinRelType,
    +    leftType: TypeInformation[Row],
    +    rightType: TypeInformation[Row],
    +    returnType: RowSchema,
    +    otherCondition: RexNode,
    +    ruleDescription: String) = {
    +
    +    // whether input can be null
    +    val nullCheck = joinType match {
    +      case JoinRelType.INNER => false
    +      case JoinRelType.LEFT  => true
    +      case JoinRelType.RIGHT => true
    +      case JoinRelType.FULL  => true
    +    }
    +
    +    // generate other non-equi function code
    +    val generator = new CodeGenerator(
    +      config,
    +      nullCheck,
    +      leftType,
    +      Some(rightType))
    +
    +    val conversion = generator.generateConverterResultExpression(
    +      returnType.physicalTypeInfo,
    +      returnType.physicalType.getFieldNames)
    +
    +    // if other condition is literal(true), then output the result directly
    +    val body = if (otherCondition.isAlwaysTrue) {
    +      s"""
    +         |${conversion.code}
    +         |${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |""".stripMargin
    +    }
    +    else {
    +      val condition = generator.generateExpression(otherCondition)
    +      s"""
    +         |${condition.code}
    +         |if (${condition.resultTerm}) {
    +         |  ${conversion.code}
    +         |  ${generator.collectorTerm}.collect(${conversion.resultTerm});
    +         |}
    +         |""".stripMargin
    +    }
    +
    +    val genFunction = generator.generateFunction(
    +      ruleDescription,
    +      classOf[FlatJoinFunction[Row, Row, Row]],
    +      body,
    +      returnType.physicalTypeInfo)
    +
    +    genFunction
    +  }
    +
    +  private case class TimeSingleCondition(
    +      timeType: RelDataType,
    +      leftExpr: RexNode,
    +      rightExpr: RexNode,
    +      isEqual: Boolean,
    +      originCall: RexNode)
    +
    +  val COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val EQUI_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN_OR_EQUAL,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val LESS_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.LESS_THAN,
    +    SqlKind.LESS_THAN_OR_EQUAL)
    +
    +  val GREAT_COMPARISON: util.Set[SqlKind] = EnumSet.of(
    +    SqlKind.GREATER_THAN,
    +    SqlKind.GREATER_THAN_OR_EQUAL)
    +
    +  /**
    +    * Analyze time-conditon to divide all time-condition into great and less condition
    +    */
    +  private def analyzeTimeCondition(
    +    condition: RexNode,
    +    greatCondition: util.List[TimeSingleCondition],
    --- End diff --
    
    If I understand the function correctly, it will not detect conditions like `t1.rowtime - t2.rowtime < 10.minutes`, right?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117004436
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,468 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.{FilterFunction, FlatJoinFunction}
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenException, CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    --- End diff --
    
    I think this would make the code in this class a lot simpler, because we would not need to recursively dig into the condition. We can iterate over all conjunctive conditions and check for each if it is a valid time bound condition and either remove it or not. 


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117170779
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,488 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.FilterFunction
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.runtime.FilterRunner
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze join condition to get equi-conditon and other condition
    +    * @param  joinNode   logicaljoin node
    +    * @param  expression the function to generate condition string
    +    */
    +  private[flink] def analyzeJoinCondition(
    +    joinNode: FlinkLogicalJoin,
    +    expression: (RexNode, List[String], Option[List[RexNode]]) => String) = {
    +
    +    val joinInfo = joinNode.analyzeCondition()
    +    val keyPairs = joinInfo.pairs.toList
    +    val otherCondition =
    +      if(joinInfo.isEqui) null
    +      else joinInfo.getRemaining(joinNode.getCluster.getRexBuilder)
    +
    +    val leftKeys = ArrayBuffer.empty[Int]
    +    val rightKeys = ArrayBuffer.empty[Int]
    +    if (!keyPairs.isEmpty) {
    +      val leftFields = joinNode.getLeft.getRowType.getFieldList
    +      val rightFields = joinNode.getRight.getRowType.getFieldList
    +
    +      keyPairs.foreach(pair => {
    +        val leftKeyType = leftFields.get(pair.source).getType.getSqlTypeName
    +        val rightKeyType = rightFields.get(pair.target).getType.getSqlTypeName
    +
    +        // check if keys are compatible
    +        if (leftKeyType == rightKeyType) {
    +          // add key pair
    +          leftKeys.append(pair.source)
    +          rightKeys.append(pair.target)
    +        } else {
    +          throw TableException(
    +            "Equality join predicate on incompatible types.\n" +
    +              s"\tLeft: ${joinNode.getLeft.toString},\n" +
    +              s"\tRight: ${joinNode.getRight.toString},\n" +
    +              s"\tCondition: (${expression(joinNode.getCondition,
    +                joinNode.getRowType.getFieldNames.toList, None)})"
    +          )
    +        }
    +      })
    +    }
    +    (leftKeys.toArray, rightKeys.toArray, otherCondition)
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    --- End diff --
    
    Hi @fhueske, i think this solution is more clearer exactly. I will rewrite the logic asap. But i think we still need to distinguish between greater/smaller at step 5, if the condition is 
    "a.proctime > b.proctime - interval '1' second and
    b.proctime < a.proctime + interval '1' second"
    we also need to throw an exception.
    So i want to extract the offset and mark which input the offset belong to, if they belong to the same input, i will throw an exception. 
    what do you think?


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117176330
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamJoinRule.scala ---
    @@ -0,0 +1,69 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.rules.datastream
    +
    +import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
    +import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamJoin
    +import org.apache.flink.table.plan.nodes.FlinkConventions
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +
    +class DataStreamJoinRule
    +  extends ConverterRule(
    +      classOf[FlinkLogicalJoin],
    +      FlinkConventions.LOGICAL,
    +      FlinkConventions.DATASTREAM,
    +      "DataStreamJoinRule") {
    +
    +  override def matches(call: RelOptRuleCall): Boolean = {
    +    val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
    +
    +    val joinInfo = join.analyzeCondition
    --- End diff --
    
    Hi @fhueske , you mean that we should distinguish different join cases, for example, this issue belong to stream-stream join, and there exist other case like stream-table join, window-window join(tumble), each case has different relnode and each case deal with inner/outer , rowtime/protime in their relnode, so maybe we can not call this relnode DataStreamJoin but need more detail, is that right? 
    And if each case have their own rule, then we need to check the case in matchs() function, is it?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117200048
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +
    +  override def open(config: Configuration) {
    +    outputC = new CRow(new Row(element1Type.getArity + element2Type.getArity), true)
    +    filterFunc.setRuntimeContext(getRuntimeContext)
    +    filterFunc.open(config)
    +
    +    listToRemove = new util.ArrayList[Long]()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] =
    +    new ListTypeInfo[Row](element1Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] =
    +      new ListTypeInfo[Row](element2Type.asInstanceOf[CRowTypeInfo].rowType)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +    timestamp: Long,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +    out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +    valueC: CRow,
    +    ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +    out: Collector[CRow],
    +    winSize: Long,
    +    timerState: ValueState[Long],
    +    rowMapState: MapState[Long, JList[Row]],
    +    oppoRowMapState: MapState[Long, JList[Row]],
    +    oppoWinSize: Long,
    +    isPositive: Boolean): Unit = {
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the rightstream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isPositive) {
    +            compositeOutput(value, oppoRowList.get(i))
    +          } else {
    +            compositeOutput(oppoRowList.get(i), value)
    +          }
    +
    +          if (filterFunc.filter(outputC.row)) {
    +            out.collect(outputC)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size() - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      listToRemove.remove(i)
    --- End diff --
    
    That is true.
    However, `ArrayList.clear()` is implemented as a simple for-loop without any checks, whereas `remove()` applies multiple checks and calculations (index out of bounds, computation if and how much to shift, etc.). I agree this is very low-level, but would also argue that a final `clear()` call is easier to understand.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117202009
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,488 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +import java.util.EnumSet
    +
    +import org.apache.calcite.avatica.util.TimeUnit
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.fun.SqlStdOperatorTable
    +import org.apache.calcite.sql.parser.SqlParserPos
    +import org.apache.calcite.sql.{SqlIntervalQualifier, SqlKind}
    +import org.apache.flink.api.common.functions.FilterFunction
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.runtime.FilterRunner
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +import scala.collection.mutable.ArrayBuffer
    +
    +
    +object JoinUtil {
    +
    +  /**
    +    * Analyze join condition to get equi-conditon and other condition
    +    * @param  joinNode   logicaljoin node
    +    * @param  expression the function to generate condition string
    +    */
    +  private[flink] def analyzeJoinCondition(
    +    joinNode: FlinkLogicalJoin,
    +    expression: (RexNode, List[String], Option[List[RexNode]]) => String) = {
    +
    +    val joinInfo = joinNode.analyzeCondition()
    +    val keyPairs = joinInfo.pairs.toList
    +    val otherCondition =
    +      if(joinInfo.isEqui) null
    +      else joinInfo.getRemaining(joinNode.getCluster.getRexBuilder)
    +
    +    val leftKeys = ArrayBuffer.empty[Int]
    +    val rightKeys = ArrayBuffer.empty[Int]
    +    if (!keyPairs.isEmpty) {
    +      val leftFields = joinNode.getLeft.getRowType.getFieldList
    +      val rightFields = joinNode.getRight.getRowType.getFieldList
    +
    +      keyPairs.foreach(pair => {
    +        val leftKeyType = leftFields.get(pair.source).getType.getSqlTypeName
    +        val rightKeyType = rightFields.get(pair.target).getType.getSqlTypeName
    +
    +        // check if keys are compatible
    +        if (leftKeyType == rightKeyType) {
    +          // add key pair
    +          leftKeys.append(pair.source)
    +          rightKeys.append(pair.target)
    +        } else {
    +          throw TableException(
    +            "Equality join predicate on incompatible types.\n" +
    +              s"\tLeft: ${joinNode.getLeft.toString},\n" +
    +              s"\tRight: ${joinNode.getRight.toString},\n" +
    +              s"\tCondition: (${expression(joinNode.getCondition,
    +                joinNode.getRowType.getFieldNames.toList, None)})"
    +          )
    +        }
    +      })
    +    }
    +    (leftKeys.toArray, rightKeys.toArray, otherCondition)
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return condition without time-condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftFieldCount left stream fields count
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    --- End diff --
    
    Yes, the logic of greater and smaller / left and right side, etc. needs to go into step 5. Maybe have a look at Calcite's `RexUtil` which might have some helpful methods.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117205051
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamJoinRule.scala ---
    @@ -0,0 +1,69 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.rules.datastream
    +
    +import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
    +import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamJoin
    +import org.apache.flink.table.plan.nodes.FlinkConventions
    +import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +
    +class DataStreamJoinRule
    +  extends ConverterRule(
    +      classOf[FlinkLogicalJoin],
    +      FlinkConventions.LOGICAL,
    +      FlinkConventions.DATASTREAM,
    +      "DataStreamJoinRule") {
    +
    +  override def matches(call: RelOptRuleCall): Boolean = {
    +    val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
    +
    +    val joinInfo = join.analyzeCondition
    --- End diff --
    
    Yes, that's what I was thinking of. It would separate the implementation of the different joins.


Github user hongyuhong commented on the issue:

    https://github.com/apache/flink/pull/3715
  
    Hi @fhueske , i have update a version :).
    1. As for how to specify the join case is a stream and stream join, currently i check through there exist time indicator but not exist such as tumble(proctime).
    2. I have test the analyzeTimeBoundary function in JoinTest, for the case ''a.proctime - b.proctime > 5 sec', it will report an error 'Cannot apply '-' to arguments of type '<TIMESTAMP(3)> - <TIMESTAMP(3)>'. Supported form(s): '<NUMERIC> - <NUMERIC>''. So maybe we don't need to support such situation. Another situation is 'timestampdiff(second, a.proctime, b.proctime) > 5', i seems not yet support in flink, should we add the support?
    
    Thanks very much.
    Yuhong


Github user rtudoran commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r117651897
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,323 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.RichFilterFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param filterFunc    the function of other non-equi condition include time condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +  private val leftStreamWindowSize: Long,
    +  private val rightStreamWindowSize: Long,
    +  private val element1Type: TypeInformation[CRow],
    +  private val element2Type: TypeInformation[CRow],
    +  private val filterFunc: RichFilterFunction[Row])
    +  extends CoProcessFunction[CRow, CRow, CRow] {
    +
    +  private var outputC: CRow = _
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    --- End diff --
    
    I understand and it make sense to me. As it reduces the overhead i think it is ok. @fhueske - do you have other opinion? 


Github user rtudoran commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r119357940
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamRowStreamJoin.scala ---
    @@ -0,0 +1,184 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.{JoinInfo, JoinRelType}
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamRowStreamJoin(
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinType: JoinRelType,
    +    leftSchema: RowSchema,
    +    rightSchema: RowSchema,
    +    schema: RowSchema,
    +    ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +  with CommonJoin
    +  with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamRowStreamJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinCondition,
    +      joinType,
    +      leftSchema,
    +      rightSchema,
    +      schema,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +
    +    s"${joinTypeToString(joinType)}" +
    +      s"(condition: (${joinConditionToString(schema.logicalType,
    +        joinCondition, getExpressionString)}), " +
    +      s"select: (${joinSelectionToString(schema.logicalType)}))"
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    super.explainTerms(pw)
    +      .item("condition", joinConditionToString(schema.logicalType,
    +        joinCondition, getExpressionString))
    +      .item("select", joinSelectionToString(schema.logicalType))
    +      .item("joinType", joinTypeToString(joinType))
    +  }
    +
    +  override def translateToPlan(tableEnv: StreamTableEnvironment): DataStream[CRow] = {
    --- End diff --
    
    update to the new interface 
    override def translateToPlan(
          tableEnv: StreamTableEnvironment,
          queryConfig: StreamQueryConfig): DataStream[CRow] = {


Github user rtudoran commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r119358128
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamRowStreamJoin.scala ---
    @@ -0,0 +1,184 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.{JoinInfo, JoinRelType}
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamRowStreamJoin(
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinType: JoinRelType,
    +    leftSchema: RowSchema,
    +    rightSchema: RowSchema,
    +    schema: RowSchema,
    +    ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +  with CommonJoin
    +  with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamRowStreamJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinCondition,
    +      joinType,
    +      leftSchema,
    +      rightSchema,
    +      schema,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +
    +    s"${joinTypeToString(joinType)}" +
    +      s"(condition: (${joinConditionToString(schema.logicalType,
    +        joinCondition, getExpressionString)}), " +
    +      s"select: (${joinSelectionToString(schema.logicalType)}))"
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    super.explainTerms(pw)
    +      .item("condition", joinConditionToString(schema.logicalType,
    +        joinCondition, getExpressionString))
    +      .item("select", joinSelectionToString(schema.logicalType))
    +      .item("joinType", joinTypeToString(joinType))
    +  }
    +
    +  override def translateToPlan(tableEnv: StreamTableEnvironment): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    // get the equality keys and other condition
    +    val joinInfo = JoinInfo.of(leftNode, rightNode, joinCondition)
    +    val leftKeys = joinInfo.leftKeys.toIntArray
    +    val rightKeys = joinInfo.rightKeys.toIntArray
    +    val otherCondition = joinInfo.getRemaining(cluster.getRexBuilder)
    +
    +    // analyze time boundary and time predicate type(proctime/rowtime)
    +    val (timeType, leftStreamWindowSize, rightStreamWindowSize, remainCondition) =
    +      JoinUtil.analyzeTimeBoundary(
    +        otherCondition,
    +        leftSchema.logicalType.getFieldCount,
    +        leftSchema.physicalType.getFieldCount,
    +        schema.logicalType,
    +        cluster.getRexBuilder,
    +        config)
    +
    +    val leftDataStream = left.asInstanceOf[DataStreamRel].translateToPlan(tableEnv)
    +    val rightDataStream = right.asInstanceOf[DataStreamRel].translateToPlan(tableEnv)
    --- End diff --
    
    pass the query config along the way 
    val leftDataStream = left.asInstanceOf[DataStreamRel].translateToPlan(tableEnv, queryConfig)
        val rightDataStream = right.asInstanceOf[DataStreamRel].translateToPlan(tableEnv, queryConfig)


Github user hongyuhong commented on the issue:

    https://github.com/apache/flink/pull/3715
  
    Hi @rtudoran @fhueske, i have update the pr with the latest interface, thanks for the reviewing.
    
    Thanks very much.
    Yuhong


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r122838733
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/calcite/RelTimeIndicatorConverter.scala ---
    @@ -162,8 +162,25 @@ class RelTimeIndicatorConverter(rexBuilder: RexBuilder) extends RelShuttle {
         LogicalProject.create(input, projects, fieldNames)
       }
     
    -  override def visit(join: LogicalJoin): RelNode =
    -    throw new TableException("Logical join in a stream environment is not supported yet.")
    +  override def visit(join: LogicalJoin): RelNode = {
    +    val left = join.getLeft.accept(this)
    +    val right = join.getRight.accept(this)
    +
    +    // check if input field contains time indicator type
    +    // materialize field if no time indicator is present anymore
    +    // if input field is already materialized, change to timestamp type
    +    val inputFields = left.getRowType.getFieldList.map(_.getType) ++
    +      right.getRowType.getFieldList.map(_.getType)
    +    val materializer = new RexTimeIndicatorMaterializer(
    +      rexBuilder,
    +      inputFields)
    +
    +    val condition = join.getCondition.accept(materializer)
    --- End diff --
    
    I think we do not need to materialize time indicators for join predicates. If the time indicators are used in valid time-based join predicates we do not code-gen the predicate and if they the time-based join predicate is not valid, the query will fail anyway.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123020420
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    --- End diff --
    
    "Both time attributes in a join condition must be of the same type."


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123003230
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    --- End diff --
    
    remove "other"?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123023871
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    --- End diff --
    
    "A time-based stream join requires exactly two join predicates that bound the time in both directions."


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123028155
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    --- End diff --
    
    Make the conditions here a bit more strict:
    - check that time predicates are `<`, `<=`, `>=`, or `>`
    - check that no other field is accessed
    
    This should make the following code easier because we can be sure that the predicate is OK.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r122842288
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/CommonJoin.scala ---
    @@ -0,0 +1,51 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.plan.nodes
    +
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex.RexNode
    +
    +import scala.collection.JavaConverters._
    +
    +trait CommonJoin {
    +
    +  private[flink] def joinSelectionToString(inputType: RelDataType): String = {
    +    inputType.getFieldNames.asScala.toList.mkString(", ")
    +  }
    +
    +  private[flink] def joinConditionToString(
    +    inputType: RelDataType,
    +    joinCondition: RexNode,
    +    expression: (RexNode, List[String], Option[List[RexNode]]) => String): String = {
    +
    +    val inFields = inputType.getFieldNames.asScala.toList
    +    expression(joinCondition, inFields, None)
    +  }
    +
    +  private[flink] def joinTypeToString(joinType: JoinRelType) = {
    +    joinType match {
    +      case JoinRelType.INNER => "InnerJoin"
    +      case JoinRelType.LEFT=> "LeftOuterJoin"
    +      case JoinRelType.RIGHT => "RightOuterJoin"
    +      case JoinRelType.FULL => "FullOuterJoin"
    +    }
    +  }
    +
    --- End diff --
    
    add `explainTerms` and `toString`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123029455
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    +        val timeIndicators = analyzeSingleConditionTerm(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +   * analysis if condition term has time indicator
    +   */
    +  def analyzeSingleConditionTerm(
    +      expression: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): Seq[(RelDataType, Boolean)] = {
    +
    +    expression match {
    +      case i: RexInputRef =>
    +        val idx = i.getIndex
    +        inputType.getFieldList.get(idx).getType match {
    +          case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
    +            // left table time indicator
    +            Seq((t, true))
    +          case t: TimeIndicatorRelDataType =>
    +            // right table time indicator
    +            Seq((t, false))
    +          case _ => Seq()
    +        }
    +      case c: RexCall =>
    +        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +      case _ => Seq()
    +    }
    +  }
    +
    +  /**
    +    * Extract time offset and determain which table the offset belong to
    +    */
    +  def extractTimeOffsetFromCondition(
    +      timeTerm: RexNode,
    +      isLeftExprBelongLeftTable: Boolean,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig) = {
    +
    +    val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    +    val leftLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        rexBuilder,
    +        config)
    +
    +    val rightLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +
    +    val (tmpTimeOffset: Long, isLeftTableTimeOffset: Boolean) =
    +      timeTerm.getKind match {
    +        // e.g a.proctime > b.proctime - 5 sec, we need to store stream a.
    +        // the left expr(a) belong to left table, so the offset belong to left table
    +        case kind @ (SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    +          (rightLiteral - leftLiteral, isLeftExprBelongLeftTable)
    +        // e.g a.proctime < b.proctime + 5 sec, we need to store stream b.
    +        case kind @ (SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL) =>
    +          (leftLiteral - rightLiteral, !isLeftExprBelongLeftTable)
    +        case _ => 0
    +      }
    +
    +    val timeOffset =
    +      // only preceding offset need to store records
    +      if (tmpTimeOffset < 0)
    +        // determain the boudary value
    +        if (timeTerm.getKind == SqlKind.LESS_THAN || timeTerm.getKind == SqlKind.GREATER_THAN) {
    +          -tmpTimeOffset - 1
    +        } else {
    +          -tmpTimeOffset
    +        }
    +      else 0
    +
    +    (timeOffset, isLeftTableTimeOffset)
    +  }
    +
    +  /**
    +    * Calcute the time boundary. Replace the rowtime/proctime with zero literal.
    +    * For example:
    +    *  a.proctime - inteval '1' second > b.proctime - interval '1' second - interval '2' second
    +    *  |-----------left--------------|   |-------------------right---------------------------\
    +    * then the boundary of a is right - left:
    +    *  ((0 - 1000) - 2000) - (0 - 1000) = -2000(-preceding, +following)
    +    */
    +  private def reduceTimeExpression(
    +      rexNode: RexNode,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): Long = {
    +
    +    /**
    +      * replace the rowtime/proctime with zero literal.
    +      * Because calculation between timestamp can only be TIMESTAMP +/- INTERVAL
    +      * so such as b.proctime + interval '1' hour - a.proctime
    +      * will be translate into TIMESTAMP(0) + interval '1' hour - interval '0' second
    --- End diff --
    
    This indicates that `b.proctime` is converted into a zero timestamp and `a.proctime` into a zero interval. I don't see this distinction in the code though.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123228354
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +    private val leftStreamWindowSize: Long,
    +    private val rightStreamWindowSize: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoWinSize: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isLeft) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +          } else {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +      curTime: Long,
    +      winSize: Long,
    +      rowMapState: MapState[Long, JList[Row]],
    +      timerState: ValueState[Long],
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // loop the timestamps to find out expired records, when meet one record
    +    // after the expried timestamp, break the loop. If the keys is ordered, thus
    --- End diff --
    
    expried -> expired


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r122841979
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/CommonJoin.scala ---
    @@ -0,0 +1,51 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.plan.nodes
    +
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex.RexNode
    +
    +import scala.collection.JavaConverters._
    +
    +trait CommonJoin {
    --- End diff --
    
    The `DataSetJoin` should also extend from this class. 


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123004606
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    --- End diff --
    
    can be simplified to `c.getOperands.exists(isExistTumble(_))`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r122843078
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamRowStreamJoin.scala ---
    @@ -0,0 +1,186 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.{JoinInfo, JoinRelType}
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamQueryConfig, StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamRowStreamJoin(
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinType: JoinRelType,
    +    leftSchema: RowSchema,
    +    rightSchema: RowSchema,
    +    schema: RowSchema,
    +    ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +  with CommonJoin
    +  with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamRowStreamJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinCondition,
    +      joinType,
    +      leftSchema,
    +      rightSchema,
    +      schema,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +
    +    s"${joinTypeToString(joinType)}" +
    +      s"(condition: (${joinConditionToString(schema.logicalType,
    +        joinCondition, getExpressionString)}), " +
    +      s"select: (${joinSelectionToString(schema.logicalType)}))"
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    super.explainTerms(pw)
    +      .item("condition", joinConditionToString(schema.logicalType,
    +        joinCondition, getExpressionString))
    +      .item("select", joinSelectionToString(schema.logicalType))
    +      .item("joinType", joinTypeToString(joinType))
    +  }
    +
    +  override def translateToPlan(
    +      tableEnv: StreamTableEnvironment,
    +      queryConfig: StreamQueryConfig): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    // get the equality keys and other condition
    +    val joinInfo = JoinInfo.of(leftNode, rightNode, joinCondition)
    +    val leftKeys = joinInfo.leftKeys.toIntArray
    +    val rightKeys = joinInfo.rightKeys.toIntArray
    +    val otherCondition = joinInfo.getRemaining(cluster.getRexBuilder)
    +
    +    // analyze time boundary and time predicate type(proctime/rowtime)
    +    val (timeType, leftStreamWindowSize, rightStreamWindowSize, remainCondition) =
    +      JoinUtil.analyzeTimeBoundary(
    --- End diff --
    
    I think we should move the analysis to the rule. Otherwise, we might end up with a plan that cannot be translated. It is the rule's responsibility to ensure that the translated plan can be executed.
    
    The rule can then pass the analyzed time predicate parameters (time type, bounds) to the `DataStreamRowStreamJoin`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123021563
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    --- End diff --
    
    Is this case required?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123030370
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    +        val timeIndicators = analyzeSingleConditionTerm(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +   * analysis if condition term has time indicator
    +   */
    +  def analyzeSingleConditionTerm(
    +      expression: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): Seq[(RelDataType, Boolean)] = {
    +
    +    expression match {
    +      case i: RexInputRef =>
    +        val idx = i.getIndex
    +        inputType.getFieldList.get(idx).getType match {
    +          case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
    +            // left table time indicator
    +            Seq((t, true))
    +          case t: TimeIndicatorRelDataType =>
    +            // right table time indicator
    +            Seq((t, false))
    +          case _ => Seq()
    +        }
    +      case c: RexCall =>
    +        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +      case _ => Seq()
    +    }
    +  }
    +
    +  /**
    +    * Extract time offset and determain which table the offset belong to
    +    */
    +  def extractTimeOffsetFromCondition(
    +      timeTerm: RexNode,
    +      isLeftExprBelongLeftTable: Boolean,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig) = {
    +
    +    val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    +    val leftLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        rexBuilder,
    +        config)
    +
    +    val rightLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +
    +    val (tmpTimeOffset: Long, isLeftTableTimeOffset: Boolean) =
    +      timeTerm.getKind match {
    +        // e.g a.proctime > b.proctime - 5 sec, we need to store stream a.
    +        // the left expr(a) belong to left table, so the offset belong to left table
    +        case kind @ (SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    +          (rightLiteral - leftLiteral, isLeftExprBelongLeftTable)
    +        // e.g a.proctime < b.proctime + 5 sec, we need to store stream b.
    +        case kind @ (SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL) =>
    +          (leftLiteral - rightLiteral, !isLeftExprBelongLeftTable)
    +        case _ => 0
    +      }
    +
    +    val timeOffset =
    +      // only preceding offset need to store records
    +      if (tmpTimeOffset < 0)
    +        // determain the boudary value
    +        if (timeTerm.getKind == SqlKind.LESS_THAN || timeTerm.getKind == SqlKind.GREATER_THAN) {
    +          -tmpTimeOffset - 1
    +        } else {
    +          -tmpTimeOffset
    +        }
    +      else 0
    +
    +    (timeOffset, isLeftTableTimeOffset)
    +  }
    +
    +  /**
    +    * Calcute the time boundary. Replace the rowtime/proctime with zero literal.
    +    * For example:
    +    *  a.proctime - inteval '1' second > b.proctime - interval '1' second - interval '2' second
    +    *  |-----------left--------------|   |-------------------right---------------------------\
    +    * then the boundary of a is right - left:
    +    *  ((0 - 1000) - 2000) - (0 - 1000) = -2000(-preceding, +following)
    +    */
    +  private def reduceTimeExpression(
    +      rexNode: RexNode,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): Long = {
    +
    +    /**
    +      * replace the rowtime/proctime with zero literal.
    +      * Because calculation between timestamp can only be TIMESTAMP +/- INTERVAL
    +      * so such as b.proctime + interval '1' hour - a.proctime
    +      * will be translate into TIMESTAMP(0) + interval '1' hour - interval '0' second
    +      */
    +    def replaceTimeFieldWithLiteral(expr: RexNode): RexNode = {
    +      expr match {
    +        case c: RexCall if c.getOperator == TimeMaterializationSqlFunction =>
    +          // replace with timestamp
    +          rexBuilder.makeZeroLiteral(expr.getType)
    +        case c: RexCall =>
    +          // replace in call operands
    +          val newOps = c.operands.map(replaceTimeFieldWithLiteral(_))
    +          rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +        case i: RexInputRef if FlinkTypeFactory.isTimeIndicatorType(i.getType) =>
    +          // replace with timestamp
    +          rexBuilder.makeZeroLiteral(expr.getType)
    +        case _: RexInputRef =>
    +          throw new TableException("Time join condition may only reference time indicator fields.")
    +        case _ => expr
    +      }
    +    }
    +
    +    val literalRex = replaceTimeFieldWithLiteral(rexNode)
    +
    +    val exprReducer = new ExpressionReducer(config)
    +    val originList = new util.ArrayList[RexNode]()
    --- End diff --
    
    can be simplified to `val originList = util.Collections.singletonList(literalRex)` (`Collections` is `java.util.Collections`)


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123026855
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    +        val timeIndicators = analyzeSingleConditionTerm(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +   * analysis if condition term has time indicator
    +   */
    +  def analyzeSingleConditionTerm(
    +      expression: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): Seq[(RelDataType, Boolean)] = {
    +
    +    expression match {
    +      case i: RexInputRef =>
    +        val idx = i.getIndex
    +        inputType.getFieldList.get(idx).getType match {
    +          case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
    +            // left table time indicator
    +            Seq((t, true))
    +          case t: TimeIndicatorRelDataType =>
    +            // right table time indicator
    +            Seq((t, false))
    +          case _ => Seq()
    +        }
    +      case c: RexCall =>
    +        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +      case _ => Seq()
    +    }
    +  }
    +
    +  /**
    +    * Extract time offset and determain which table the offset belong to
    +    */
    +  def extractTimeOffsetFromCondition(
    +      timeTerm: RexNode,
    +      isLeftExprBelongLeftTable: Boolean,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig) = {
    +
    +    val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    +    val leftLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        rexBuilder,
    +        config)
    +
    +    val rightLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +
    +    val (tmpTimeOffset: Long, isLeftTableTimeOffset: Boolean) =
    +      timeTerm.getKind match {
    +        // e.g a.proctime > b.proctime - 5 sec, we need to store stream a.
    +        // the left expr(a) belong to left table, so the offset belong to left table
    +        case kind @ (SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    +          (rightLiteral - leftLiteral, isLeftExprBelongLeftTable)
    +        // e.g a.proctime < b.proctime + 5 sec, we need to store stream b.
    +        case kind @ (SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL) =>
    +          (leftLiteral - rightLiteral, !isLeftExprBelongLeftTable)
    +        case _ => 0
    +      }
    +
    +    val timeOffset =
    +      // only preceding offset need to store records
    +      if (tmpTimeOffset < 0)
    +        // determain the boudary value
    +        if (timeTerm.getKind == SqlKind.LESS_THAN || timeTerm.getKind == SqlKind.GREATER_THAN) {
    +          -tmpTimeOffset - 1
    +        } else {
    +          -tmpTimeOffset
    +        }
    +      else 0
    +
    +    (timeOffset, isLeftTableTimeOffset)
    +  }
    +
    +  /**
    +    * Calcute the time boundary. Replace the rowtime/proctime with zero literal.
    +    * For example:
    +    *  a.proctime - inteval '1' second > b.proctime - interval '1' second - interval '2' second
    --- End diff --
    
    Update example? The method is used to reduce one side of a condition.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123017357
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    --- End diff --
    
    replace `_size > 0` by `_.nonEmpty`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123020909
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    --- End diff --
    
    "Time-based join conditions must reference the time attribute of both input tables."



Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123025909
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    +        val timeIndicators = analyzeSingleConditionTerm(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +   * analysis if condition term has time indicator
    +   */
    +  def analyzeSingleConditionTerm(
    +      expression: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): Seq[(RelDataType, Boolean)] = {
    +
    +    expression match {
    +      case i: RexInputRef =>
    +        val idx = i.getIndex
    +        inputType.getFieldList.get(idx).getType match {
    +          case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
    +            // left table time indicator
    +            Seq((t, true))
    +          case t: TimeIndicatorRelDataType =>
    +            // right table time indicator
    +            Seq((t, false))
    +          case _ => Seq()
    +        }
    +      case c: RexCall =>
    +        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +      case _ => Seq()
    +    }
    +  }
    +
    +  /**
    +    * Extract time offset and determain which table the offset belong to
    +    */
    +  def extractTimeOffsetFromCondition(
    +      timeTerm: RexNode,
    +      isLeftExprBelongLeftTable: Boolean,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig) = {
    +
    +    val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    +    val leftLiteral =
    --- End diff --
    
    Check the type of the `RexCall` first (<, <= ,=>, >) or that this is a binary operation (with two operands)?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123016738
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    +        val timeIndicators = analyzeSingleConditionTerm(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +   * analysis if condition term has time indicator
    +   */
    +  def analyzeSingleConditionTerm(
    --- End diff --
    
    rename to `extractTimeIndicatorAccesses`?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123222750
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamRowStreamJoin.scala ---
    @@ -0,0 +1,186 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.{JoinInfo, JoinRelType}
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamQueryConfig, StreamTableEnvironment, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{JoinUtil, ProcTimeInnerJoin}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamRowStreamJoin(
    --- End diff --
    
    Rename to `DataStreamWindowJoin`?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123031759
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    +        val timeIndicators = analyzeSingleConditionTerm(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +   * analysis if condition term has time indicator
    +   */
    +  def analyzeSingleConditionTerm(
    +      expression: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): Seq[(RelDataType, Boolean)] = {
    +
    +    expression match {
    +      case i: RexInputRef =>
    +        val idx = i.getIndex
    +        inputType.getFieldList.get(idx).getType match {
    +          case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
    +            // left table time indicator
    +            Seq((t, true))
    +          case t: TimeIndicatorRelDataType =>
    +            // right table time indicator
    +            Seq((t, false))
    +          case _ => Seq()
    +        }
    +      case c: RexCall =>
    +        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +      case _ => Seq()
    +    }
    +  }
    +
    +  /**
    +    * Extract time offset and determain which table the offset belong to
    +    */
    +  def extractTimeOffsetFromCondition(
    +      timeTerm: RexNode,
    +      isLeftExprBelongLeftTable: Boolean,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig) = {
    +
    +    val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    +    val leftLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        rexBuilder,
    +        config)
    +
    +    val rightLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +
    +    val (tmpTimeOffset: Long, isLeftTableTimeOffset: Boolean) =
    +      timeTerm.getKind match {
    +        // e.g a.proctime > b.proctime - 5 sec, we need to store stream a.
    +        // the left expr(a) belong to left table, so the offset belong to left table
    +        case kind @ (SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    +          (rightLiteral - leftLiteral, isLeftExprBelongLeftTable)
    +        // e.g a.proctime < b.proctime + 5 sec, we need to store stream b.
    +        case kind @ (SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL) =>
    +          (leftLiteral - rightLiteral, !isLeftExprBelongLeftTable)
    +        case _ => 0
    +      }
    +
    +    val timeOffset =
    +      // only preceding offset need to store records
    +      if (tmpTimeOffset < 0)
    +        // determain the boudary value
    +        if (timeTerm.getKind == SqlKind.LESS_THAN || timeTerm.getKind == SqlKind.GREATER_THAN) {
    +          -tmpTimeOffset - 1
    +        } else {
    +          -tmpTimeOffset
    +        }
    +      else 0
    +
    +    (timeOffset, isLeftTableTimeOffset)
    +  }
    +
    +  /**
    +    * Calcute the time boundary. Replace the rowtime/proctime with zero literal.
    +    * For example:
    +    *  a.proctime - inteval '1' second > b.proctime - interval '1' second - interval '2' second
    +    *  |-----------left--------------|   |-------------------right---------------------------\
    +    * then the boundary of a is right - left:
    +    *  ((0 - 1000) - 2000) - (0 - 1000) = -2000(-preceding, +following)
    +    */
    +  private def reduceTimeExpression(
    +      rexNode: RexNode,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): Long = {
    +
    +    /**
    +      * replace the rowtime/proctime with zero literal.
    +      * Because calculation between timestamp can only be TIMESTAMP +/- INTERVAL
    +      * so such as b.proctime + interval '1' hour - a.proctime
    +      * will be translate into TIMESTAMP(0) + interval '1' hour - interval '0' second
    +      */
    +    def replaceTimeFieldWithLiteral(expr: RexNode): RexNode = {
    +      expr match {
    +        case c: RexCall if c.getOperator == TimeMaterializationSqlFunction =>
    +          // replace with timestamp
    +          rexBuilder.makeZeroLiteral(expr.getType)
    +        case c: RexCall =>
    +          // replace in call operands
    +          val newOps = c.operands.map(replaceTimeFieldWithLiteral(_))
    +          rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +        case i: RexInputRef if FlinkTypeFactory.isTimeIndicatorType(i.getType) =>
    +          // replace with timestamp
    +          rexBuilder.makeZeroLiteral(expr.getType)
    +        case _: RexInputRef =>
    +          throw new TableException("Time join condition may only reference time indicator fields.")
    +        case _ => expr
    +      }
    +    }
    +
    +    val literalRex = replaceTimeFieldWithLiteral(rexNode)
    +
    +    val exprReducer = new ExpressionReducer(config)
    +    val originList = new util.ArrayList[RexNode]()
    +    originList.add(literalRex)
    +    val reduceList = new util.ArrayList[RexNode]()
    +    exprReducer.reduce(rexBuilder, originList, reduceList)
    +
    +    reduceList.get(0) match {
    +      case call: RexCall =>
    +        call.getOperands.get(0).asInstanceOf[RexLiteral].getValue2.asInstanceOf[Long]
    +      case literal: RexLiteral =>
    +        literal.getValue2.asInstanceOf[Long]
    +      case _ =>
    +        throw TableException("Equality join time condition only support constant.")
    --- End diff --
    
    "Time condition may only consist of time attributes, literals, and arithmetic operators." (is that correct?)


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123228716
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +    private val leftStreamWindowSize: Long,
    +    private val rightStreamWindowSize: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoWinSize: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isLeft) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +          } else {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +      curTime: Long,
    +      winSize: Long,
    +      rowMapState: MapState[Long, JList[Row]],
    +      timerState: ValueState[Long],
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // loop the timestamps to find out expired records, when meet one record
    +    // after the expried timestamp, break the loop. If the keys is ordered, thus
    +    // can reduce loop num, if the keys is unordered, also can expire at least one
    +    // element every time the timer trigger
    +    while (keyIter.hasNext && nextTimer == 0) {
    +      val curTime = keyIter.next
    --- End diff --
    
    rename `curTime` it is also a parameter of the function which is confusing.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123226087
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +    private val leftStreamWindowSize: Long,
    +    private val rightStreamWindowSize: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoWinSize: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    --- End diff --
    
    Does this mean that we put elements into state which is only removed if we receive an element from the other input? This would result in a memory leak if the other side never sends an element.



Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123226130
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +    private val leftStreamWindowSize: Long,
    +    private val rightStreamWindowSize: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoWinSize: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elments
    --- End diff --
    
    elments -> elements


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123189670
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/JoinITCase.scala ---
    @@ -0,0 +1,204 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.api.scala.stream.sql
    +
    +import org.apache.flink.api.scala._
    +import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
    +import org.apache.flink.table.api.scala._
    +import org.apache.flink.table.api.scala.stream.utils.{StreamITCase, StreamingWithStateTestBase}
    +import org.apache.flink.table.api.{TableEnvironment, TableException}
    +import org.apache.flink.types.Row
    +import org.junit.Assert._
    +import org.junit._
    +
    +import scala.collection.mutable
    +
    +class JoinITCase extends StreamingWithStateTestBase {
    +
    +  val data = List(
    +    (1L, 1, "Hello"),
    +    (2L, 2, "Hello"),
    +    (3L, 3, "Hello"),
    +    (4L, 4, "Hello"),
    +    (5L, 5, "Hello"),
    +    (6L, 6, "Hello"),
    +    (7L, 7, "Hello World"),
    +    (8L, 8, "Hello World"),
    +    (20L, 20, "Hello World"))
    +
    +  /**
    +    * both stream should have boundary
    +    */
    +  @Test(expected = classOf[TableException])
    +  def testJoinException0(): Unit = {
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    env.setStateBackend(getStateBackend)
    +    val tEnv = TableEnvironment.getTableEnvironment(env)
    +    StreamITCase.testResults = mutable.MutableList()
    +
    +    val t1 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +    val t2 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +
    +    tEnv.registerTable("T1", t1)
    +    tEnv.registerTable("T2", t2)
    +
    +    val sqlQuery = "SELECT t2.a, t2.c, t1.c from T1 as t1 join T2 as t2 on t1.a = t2.a"
    +
    +    val result = tEnv.sql(sqlQuery).toDataStream[Row]
    +    result.addSink(new StreamITCase.StringSink)
    +    env.execute()
    +  }
    +
    +  /**
    +    * both stream should have boundary
    +    */
    +  @Test(expected = classOf[TableException])
    +  def testJoinException1(): Unit = {
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    env.setStateBackend(getStateBackend)
    +    val tEnv = TableEnvironment.getTableEnvironment(env)
    +    StreamITCase.testResults = mutable.MutableList()
    +
    +    val t1 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +    val t2 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +
    +    tEnv.registerTable("T1", t1)
    +    tEnv.registerTable("T2", t2)
    +
    +    val sqlQuery = "SELECT t2.a, t2.c, t1.c from T1 as t1 join T2 as t2 on t1.a = t2.a " +
    +      "and t1.proctime > t2.proctime - interval '5' second"
    +
    +    val result = tEnv.sql(sqlQuery).toDataStream[Row]
    +    result.addSink(new StreamITCase.StringSink)
    +    env.execute()
    +  }
    +
    +  /**
    +    * both stream should use same time indicator
    +    */
    +  @Test(expected = classOf[TableException])
    +  def testJoinException2(): Unit = {
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    env.setStateBackend(getStateBackend)
    +    val tEnv = TableEnvironment.getTableEnvironment(env)
    +    StreamITCase.testResults = mutable.MutableList()
    +
    +    val t1 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +    val t2 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'rowtime.rowtime)
    +
    +    tEnv.registerTable("T1", t1)
    +    tEnv.registerTable("T2", t2)
    +
    +    val sqlQuery = "SELECT t2.a, t2.c, t1.c from T1 as t1 join T2 as t2 on t1.a = t2.a " +
    +      "and t1.proctime > t2.rowtime - interval '5' second "
    +
    +    val result = tEnv.sql(sqlQuery).toDataStream[Row]
    +    result.addSink(new StreamITCase.StringSink)
    +    env.execute()
    +  }
    +
    +
    +  /** test process time inner join **/
    +  @Test
    +  def testProcessTimeInnerJoin(): Unit = {
    --- End diff --
    
    We cannot test this operator with a regular ITCase. Since the join result is based on processing time, we cannot guarantee the result. Very wide bound have the problem that they don't test important edge cases (the join result becomes a Cartesian product). Also the test might fail due to GC stalls, on slower machines, etc. The only way to test this operator is the test harness.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123231612
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/JoinITCase.scala ---
    @@ -0,0 +1,204 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.api.scala.stream.sql
    +
    +import org.apache.flink.api.scala._
    +import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
    +import org.apache.flink.table.api.scala._
    +import org.apache.flink.table.api.scala.stream.utils.{StreamITCase, StreamingWithStateTestBase}
    +import org.apache.flink.table.api.{TableEnvironment, TableException}
    +import org.apache.flink.types.Row
    +import org.junit.Assert._
    +import org.junit._
    +
    +import scala.collection.mutable
    +
    +class JoinITCase extends StreamingWithStateTestBase {
    +
    +  val data = List(
    +    (1L, 1, "Hello"),
    +    (2L, 2, "Hello"),
    +    (3L, 3, "Hello"),
    +    (4L, 4, "Hello"),
    +    (5L, 5, "Hello"),
    +    (6L, 6, "Hello"),
    +    (7L, 7, "Hello World"),
    +    (8L, 8, "Hello World"),
    +    (20L, 20, "Hello World"))
    +
    +  /**
    +    * both stream should have boundary
    +    */
    +  @Test(expected = classOf[TableException])
    +  def testJoinException0(): Unit = {
    --- End diff --
    
    We should not need an ITCase for these checks. The problem is that the validation is done during translation. If we would move the correctness checks into the optimizer, we don't need to translate the program.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123209008
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +    private val leftStreamWindowSize: Long,
    +    private val rightStreamWindowSize: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +  override def open(config: Configuration) {
    --- End diff --
    
    add a new line before the method


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123020714
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    --- End diff --
    
    "Time-based join conditions must reference the time attribute of both input tables."


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123205673
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    +        val timeIndicators = analyzeSingleConditionTerm(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +   * analysis if condition term has time indicator
    +   */
    +  def analyzeSingleConditionTerm(
    +      expression: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): Seq[(RelDataType, Boolean)] = {
    +
    +    expression match {
    +      case i: RexInputRef =>
    +        val idx = i.getIndex
    +        inputType.getFieldList.get(idx).getType match {
    +          case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
    +            // left table time indicator
    +            Seq((t, true))
    +          case t: TimeIndicatorRelDataType =>
    +            // right table time indicator
    +            Seq((t, false))
    +          case _ => Seq()
    +        }
    +      case c: RexCall =>
    +        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +      case _ => Seq()
    +    }
    +  }
    +
    +  /**
    +    * Extract time offset and determain which table the offset belong to
    +    */
    +  def extractTimeOffsetFromCondition(
    +      timeTerm: RexNode,
    +      isLeftExprBelongLeftTable: Boolean,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig) = {
    +
    +    val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    +    val leftLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        rexBuilder,
    +        config)
    +
    +    val rightLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +
    +    val (tmpTimeOffset: Long, isLeftTableTimeOffset: Boolean) =
    +      timeTerm.getKind match {
    +        // e.g a.proctime > b.proctime - 5 sec, we need to store stream a.
    +        // the left expr(a) belong to left table, so the offset belong to left table
    +        case kind @ (SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    +          (rightLiteral - leftLiteral, isLeftExprBelongLeftTable)
    +        // e.g a.proctime < b.proctime + 5 sec, we need to store stream b.
    +        case kind @ (SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL) =>
    +          (leftLiteral - rightLiteral, !isLeftExprBelongLeftTable)
    +        case _ => 0
    +      }
    +
    +    val timeOffset =
    +      // only preceding offset need to store records
    +      if (tmpTimeOffset < 0)
    +        // determain the boudary value
    +        if (timeTerm.getKind == SqlKind.LESS_THAN || timeTerm.getKind == SqlKind.GREATER_THAN) {
    +          -tmpTimeOffset - 1
    +        } else {
    +          -tmpTimeOffset
    +        }
    +      else 0
    --- End diff --
    
    I think this might result in invalid bounds. For example the predicate `l.ptime > r.ptime - 10.secs AND l.ptime < r.ptime - 5.secs` would be changed to `l.ptime > r.ptime - 10.secs AND l.ptime <= r.ptime`.
    
    In principle bounds as in the example above are valid (join each event of `r` with all events from `l` that happened between 10 and 5 seconds before the right event). Changing the bound changes the semantics of the query. If we do not support this case in the initial version, we should throw an exception but not change the semantics.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123232793
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/JoinTest.scala ---
    @@ -0,0 +1,149 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.api.scala.stream.sql
    +
    +import org.apache.calcite.rel.logical.LogicalJoin
    +import org.apache.flink.api.scala._
    +import org.apache.flink.table.api.scala._
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.runtime.join.JoinUtil
    +import org.apache.flink.table.utils.TableTestUtil._
    +import org.apache.flink.table.utils.{StreamTableTestUtil, TableTestBase}
    +import org.junit.Assert._
    +import org.junit.Test
    +
    +class JoinTest extends TableTestBase {
    +  private val streamUtil: StreamTableTestUtil = streamTestUtil()
    +  streamUtil.addTable[(Int, String, Long)]("MyTable", 'a, 'b, 'c.rowtime, 'proctime.proctime)
    +  streamUtil.addTable[(Int, String, Long)]("MyTable2", 'a, 'b, 'c.rowtime, 'proctime.proctime)
    +
    +  @Test
    +  def testProcessingTimeInnerJoin() = {
    +
    +    val sqlQuery = "SELECT t1.a, t2.b " +
    +      "FROM MyTable as t1 join MyTable2 as t2 on t1.a = t2.a and " +
    +      "t1.proctime between t2.proctime - interval '1' hour and t2.proctime + interval '1' hour"
    +    val expected =
    +      unaryNode(
    +        "DataStreamCalc",
    +        binaryNode(
    +          "DataStreamRowStreamJoin",
    +          unaryNode(
    +            "DataStreamCalc",
    +            streamTableNode(0),
    +            term("select", "a", "proctime")
    +          ),
    +          unaryNode(
    +            "DataStreamCalc",
    +            streamTableNode(1),
    +            term("select", "a", "b", "proctime")
    +          ),
    +          term("condition",
    +            "AND(=(a, a0), >=(TIME_MATERIALIZATION(proctime), " +
    +              "-(TIME_MATERIALIZATION(proctime0), 3600000)), " +
    +              "<=(TIME_MATERIALIZATION(proctime), " +
    +              "DATETIME_PLUS(TIME_MATERIALIZATION(proctime0), 3600000)))"),
    +          term("select", "a, proctime, a0, b, proctime0"),
    +          term("joinType", "InnerJoin")
    +        ),
    +        term("select", "a", "b")
    +      )
    +
    +    streamUtil.verifySql(sqlQuery, expected)
    +  }
    +
    +
    +  @Test
    +  def testJoinTimeBoundary(): Unit = {
    +    verifyTimeBoundary(
    +      "t1.proctime between t2.proctime - interval '1' hour " +
    +        "and t2.proctime + interval '1' hour",
    +      3600000,
    +      3600000,
    +      "proctime")
    +
    +    verifyTimeBoundary(
    +      "t1.proctime > t2.proctime - interval '1' second and " +
    +        "t1.proctime < t2.proctime + interval '1' second",
    +      999,
    +      999,
    +      "proctime")
    +
    +    verifyTimeBoundary(
    +      "t1.c >= t2.c - interval '1' second and " +
    +        "t1.c <= t2.c + interval '1' second",
    +      1000,
    +      1000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t1.c >= t2.c and " +
    +        "t1.c <= t2.c + interval '1' second",
    +      0,
    +      1000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t1.c >= t2.c + interval '1' second and " +
    +        "t1.c <= t2.c + interval '10' second",
    +      0,
    +      10000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t2.c - interval '1' second <= t1.c and " +
    +        "t2.c + interval '10' second >= t1.c",
    +      1000,
    +      10000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t1.c - interval '2' second >= t2.c + interval '1' second -" +
    +        "interval '10' second and " +
    +        "t1.c <= t2.c + interval '10' second",
    +      7000,
    +      10000,
    +      "rowtime")
    +  }
    +
    +  def verifyTimeBoundary(
    --- End diff --
    
    I like this test, but it would also be good to check that all unsupported cases throw exceptions.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123229427
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +    private val leftStreamWindowSize: Long,
    +    private val rightStreamWindowSize: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoWinSize: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isLeft) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +          } else {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +      curTime: Long,
    +      winSize: Long,
    +      rowMapState: MapState[Long, JList[Row]],
    +      timerState: ValueState[Long],
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // loop the timestamps to find out expired records, when meet one record
    +    // after the expried timestamp, break the loop. If the keys is ordered, thus
    +    // can reduce loop num, if the keys is unordered, also can expire at least one
    +    // element every time the timer trigger
    +    while (keyIter.hasNext && nextTimer == 0) {
    +      val curTime = keyIter.next
    +      if (curTime < expiredTime) {
    +        listToRemove.add(curTime)
    +      } else {
    +        nextTimer = curTime
    +      }
    +    }
    +
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      rowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    --- End diff --
    
    if the `rowMapState` is empty, it should be completely cleared to remove all state: `rowMapState.clear()`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123238701
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/JoinHarnessTest.scala ---
    @@ -0,0 +1,200 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.harness
    +
    +import java.util.concurrent.ConcurrentLinkedQueue
    +import java.lang.{Integer => JInt}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.BasicTypeInfo._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.RowTypeInfo
    +import org.apache.flink.streaming.api.operators.co.KeyedCoProcessOperator
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord
    +import org.apache.flink.streaming.util.{KeyedTwoInputStreamOperatorTestHarness, TwoInputStreamOperatorTestHarness}
    +import org.apache.flink.table.codegen.GeneratedFunction
    +import org.apache.flink.table.runtime.harness.HarnessTestBase.{RowResultSortComparator, TupleRowKeySelector}
    +import org.apache.flink.table.runtime.join.ProcTimeInnerJoin
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.junit.Test
    +
    +
    +class JoinHarnessTest extends HarnessTestBase{
    +
    +  private val rT = new RowTypeInfo(Array[TypeInformation[_]](
    +    INT_TYPE_INFO,
    +    STRING_TYPE_INFO),
    +    Array("a", "b"))
    +
    +
    +  val funcCode: String =
    +    """
    +      |public class TestJoinFunction
    +      |          extends org.apache.flink.api.common.functions.RichFlatJoinFunction {
    +      |  transient org.apache.flink.types.Row out =
    +      |            new org.apache.flink.types.Row(4);
    +      |  public TestJoinFunction() throws Exception {}
    +      |
    +      |  @Override
    +      |  public void open(org.apache.flink.configuration.Configuration parameters)
    +      |  throws Exception {}
    +      |
    +      |  @Override
    +      |  public void join(Object _in1, Object _in2, org.apache.flink.util.Collector c)
    +      |   throws Exception {
    +      |   org.apache.flink.types.Row in1 = (org.apache.flink.types.Row) _in1;
    +      |   org.apache.flink.types.Row in2 = (org.apache.flink.types.Row) _in2;
    +      |
    +      |   out.setField(0, in1.getField(0));
    +      |   out.setField(1, in1.getField(1));
    +      |   out.setField(2, in2.getField(0));
    +      |   out.setField(3, in2.getField(1));
    +      |
    +      |   c.collect(out);
    +      |
    +      |  }
    +      |
    +      |  @Override
    +      |  public void close() throws Exception {}
    +      |}
    +    """.stripMargin
    +
    +  @Test
    +  def testProcTimeJoin() {
    --- End diff --
    
    Please add comments for the scenarios that this test covers.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123238837
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/JoinHarnessTest.scala ---
    @@ -0,0 +1,200 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.harness
    +
    +import java.util.concurrent.ConcurrentLinkedQueue
    +import java.lang.{Integer => JInt}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.BasicTypeInfo._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.RowTypeInfo
    +import org.apache.flink.streaming.api.operators.co.KeyedCoProcessOperator
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord
    +import org.apache.flink.streaming.util.{KeyedTwoInputStreamOperatorTestHarness, TwoInputStreamOperatorTestHarness}
    +import org.apache.flink.table.codegen.GeneratedFunction
    +import org.apache.flink.table.runtime.harness.HarnessTestBase.{RowResultSortComparator, TupleRowKeySelector}
    +import org.apache.flink.table.runtime.join.ProcTimeInnerJoin
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.junit.Test
    +
    +
    +class JoinHarnessTest extends HarnessTestBase{
    +
    +  private val rT = new RowTypeInfo(Array[TypeInformation[_]](
    +    INT_TYPE_INFO,
    +    STRING_TYPE_INFO),
    +    Array("a", "b"))
    +
    +
    +  val funcCode: String =
    +    """
    +      |public class TestJoinFunction
    +      |          extends org.apache.flink.api.common.functions.RichFlatJoinFunction {
    +      |  transient org.apache.flink.types.Row out =
    +      |            new org.apache.flink.types.Row(4);
    +      |  public TestJoinFunction() throws Exception {}
    +      |
    +      |  @Override
    +      |  public void open(org.apache.flink.configuration.Configuration parameters)
    +      |  throws Exception {}
    +      |
    +      |  @Override
    +      |  public void join(Object _in1, Object _in2, org.apache.flink.util.Collector c)
    +      |   throws Exception {
    +      |   org.apache.flink.types.Row in1 = (org.apache.flink.types.Row) _in1;
    +      |   org.apache.flink.types.Row in2 = (org.apache.flink.types.Row) _in2;
    +      |
    +      |   out.setField(0, in1.getField(0));
    +      |   out.setField(1, in1.getField(1));
    +      |   out.setField(2, in2.getField(0));
    +      |   out.setField(3, in2.getField(1));
    +      |
    +      |   c.collect(out);
    +      |
    +      |  }
    +      |
    +      |  @Override
    +      |  public void close() throws Exception {}
    +      |}
    +    """.stripMargin
    +
    +  @Test
    +  def testProcTimeJoin() {
    +
    +    val joinProcessFunc = new ProcTimeInnerJoin(10, 20, rT, rT, "TestJoinFunction", funcCode)
    +
    +    val operator: KeyedCoProcessOperator[Integer, CRow, CRow, CRow] =
    +      new KeyedCoProcessOperator[Integer, CRow, CRow, CRow](joinProcessFunc)
    +    val testHarness: TwoInputStreamOperatorTestHarness[CRow, CRow, CRow] =
    +      new KeyedTwoInputStreamOperatorTestHarness[Integer, CRow, CRow, CRow](
    +       operator,
    +       new TupleRowKeySelector[Integer](0),
    +       new TupleRowKeySelector[Integer](0),
    +       BasicTypeInfo.INT_TYPE_INFO,
    +       1,1,0)
    +
    +    testHarness.open()
    +
    +    testHarness.setProcessingTime(1)
    +    testHarness.processElement1(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa"), true), 1))
    +    testHarness.setProcessingTime(2)
    +    testHarness.processElement1(new StreamRecord(
    +      CRow(Row.of(2: JInt, "bbb"), true), 2))
    +    testHarness.setProcessingTime(3)
    +    testHarness.processElement1(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa2"), true), 3))
    +
    +    testHarness.processElement2(new StreamRecord(
    +      CRow(Row.of(1: JInt, "Hi1"), true), 3))
    +    testHarness.setProcessingTime(4)
    +    testHarness.processElement2(new StreamRecord(
    +      CRow(Row.of(2: JInt, "Hello1"), true), 4))
    +
    +    testHarness.setProcessingTime(12)
    +    testHarness.processElement2(new StreamRecord(
    +      CRow(Row.of(1: JInt, "Hi2"), true), 12))
    +
    +    testHarness.setProcessingTime(25)
    +    testHarness.processElement1(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa3"), true), 25))
    +    testHarness.processElement1(new StreamRecord(
    +      CRow(Row.of(2: JInt, "bbb2"), true), 25))
    +    testHarness.processElement2(new StreamRecord(
    +      CRow(Row.of(2: JInt, "Hello2"), true), 25))
    +    val result = testHarness.getOutput
    +
    +    val expectedOutput = new ConcurrentLinkedQueue[Object]()
    +
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa", 1: JInt, "Hi1"), true), 3))
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa2", 1: JInt, "Hi1"), true), 3))
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(2: JInt, "bbb", 2: JInt, "Hello1"), true), 4))
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa2", 1: JInt, "Hi2"), true), 12))
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa3", 1: JInt, "Hi2"), true), 25))
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(2: JInt, "bbb2", 2: JInt, "Hello2"), true), 25))
    +
    +    verify(expectedOutput, result, new RowResultSortComparator(6))
    +
    +    testHarness.close()
    +  }
    +
    +  @Test
    +  def testProcTimeJoin2() {
    --- End diff --
    
    Please add comments for the scenarios that this test covers.
    It should also check that state is properly cleaned up.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123225232
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    --- End diff --
    
    Does it make sense to split the implementation into two operators:
    1. both streams need to be buffered (`l.ptime > r.ptime - 10.secs AND l.ptime < r.ptime + 5.secs`)
    2. only one stream needs to be buffered (`l.ptime > r.ptime - 10.secs AND l.ptime < r.ptime - 5.secs`)


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123229245
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +    private val leftStreamWindowSize: Long,
    +    private val rightStreamWindowSize: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoWinSize: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isLeft) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +          } else {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +      curTime: Long,
    +      winSize: Long,
    +      rowMapState: MapState[Long, JList[Row]],
    +      timerState: ValueState[Long],
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // loop the timestamps to find out expired records, when meet one record
    +    // after the expried timestamp, break the loop. If the keys is ordered, thus
    +    // can reduce loop num, if the keys is unordered, also can expire at least one
    +    // element every time the timer trigger
    +    while (keyIter.hasNext && nextTimer == 0) {
    +      val curTime = keyIter.next
    +      if (curTime < expiredTime) {
    +        listToRemove.add(curTime)
    +      } else {
    +        nextTimer = curTime
    +      }
    +    }
    +
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      rowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +
    +    // if exist records which later than the expire time,
    +    // register a timer for it, otherwise update the timerState to 0
    +    // to let processElement register timer when element come
    +    if (nextTimer != 0) {
    +      ctx.timerService.registerProcessingTimeTimer(nextTimer + winSize + 1)
    +      timerState.update(nextTimer + winSize + 1)
    +    } else {
    +      timerState.update(0)
    --- End diff --
    
    Rather call `timerState.clear()` to remove all state.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123236585
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/JoinHarnessTest.scala ---
    @@ -0,0 +1,200 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.harness
    +
    +import java.util.concurrent.ConcurrentLinkedQueue
    +import java.lang.{Integer => JInt}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.BasicTypeInfo._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.RowTypeInfo
    +import org.apache.flink.streaming.api.operators.co.KeyedCoProcessOperator
    +import org.apache.flink.streaming.runtime.streamrecord.StreamRecord
    +import org.apache.flink.streaming.util.{KeyedTwoInputStreamOperatorTestHarness, TwoInputStreamOperatorTestHarness}
    +import org.apache.flink.table.codegen.GeneratedFunction
    +import org.apache.flink.table.runtime.harness.HarnessTestBase.{RowResultSortComparator, TupleRowKeySelector}
    +import org.apache.flink.table.runtime.join.ProcTimeInnerJoin
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.junit.Test
    +
    +
    +class JoinHarnessTest extends HarnessTestBase{
    +
    +  private val rT = new RowTypeInfo(Array[TypeInformation[_]](
    +    INT_TYPE_INFO,
    +    STRING_TYPE_INFO),
    +    Array("a", "b"))
    +
    +
    +  val funcCode: String =
    +    """
    +      |public class TestJoinFunction
    +      |          extends org.apache.flink.api.common.functions.RichFlatJoinFunction {
    +      |  transient org.apache.flink.types.Row out =
    +      |            new org.apache.flink.types.Row(4);
    +      |  public TestJoinFunction() throws Exception {}
    +      |
    +      |  @Override
    +      |  public void open(org.apache.flink.configuration.Configuration parameters)
    +      |  throws Exception {}
    +      |
    +      |  @Override
    +      |  public void join(Object _in1, Object _in2, org.apache.flink.util.Collector c)
    +      |   throws Exception {
    +      |   org.apache.flink.types.Row in1 = (org.apache.flink.types.Row) _in1;
    +      |   org.apache.flink.types.Row in2 = (org.apache.flink.types.Row) _in2;
    +      |
    +      |   out.setField(0, in1.getField(0));
    +      |   out.setField(1, in1.getField(1));
    +      |   out.setField(2, in2.getField(0));
    +      |   out.setField(3, in2.getField(1));
    +      |
    +      |   c.collect(out);
    +      |
    +      |  }
    +      |
    +      |  @Override
    +      |  public void close() throws Exception {}
    +      |}
    +    """.stripMargin
    +
    +  @Test
    +  def testProcTimeJoin() {
    +
    +    val joinProcessFunc = new ProcTimeInnerJoin(10, 20, rT, rT, "TestJoinFunction", funcCode)
    +
    +    val operator: KeyedCoProcessOperator[Integer, CRow, CRow, CRow] =
    +      new KeyedCoProcessOperator[Integer, CRow, CRow, CRow](joinProcessFunc)
    +    val testHarness: TwoInputStreamOperatorTestHarness[CRow, CRow, CRow] =
    +      new KeyedTwoInputStreamOperatorTestHarness[Integer, CRow, CRow, CRow](
    +       operator,
    +       new TupleRowKeySelector[Integer](0),
    +       new TupleRowKeySelector[Integer](0),
    +       BasicTypeInfo.INT_TYPE_INFO,
    +       1,1,0)
    +
    +    testHarness.open()
    +
    +    testHarness.setProcessingTime(1)
    +    testHarness.processElement1(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa"), true), 1))
    +    testHarness.setProcessingTime(2)
    +    testHarness.processElement1(new StreamRecord(
    +      CRow(Row.of(2: JInt, "bbb"), true), 2))
    +    testHarness.setProcessingTime(3)
    +    testHarness.processElement1(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa2"), true), 3))
    +
    +    testHarness.processElement2(new StreamRecord(
    +      CRow(Row.of(1: JInt, "Hi1"), true), 3))
    +    testHarness.setProcessingTime(4)
    +    testHarness.processElement2(new StreamRecord(
    +      CRow(Row.of(2: JInt, "Hello1"), true), 4))
    +
    +    testHarness.setProcessingTime(12)
    +    testHarness.processElement2(new StreamRecord(
    +      CRow(Row.of(1: JInt, "Hi2"), true), 12))
    +
    +    testHarness.setProcessingTime(25)
    +    testHarness.processElement1(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa3"), true), 25))
    +    testHarness.processElement1(new StreamRecord(
    +      CRow(Row.of(2: JInt, "bbb2"), true), 25))
    +    testHarness.processElement2(new StreamRecord(
    +      CRow(Row.of(2: JInt, "Hello2"), true), 25))
    +    val result = testHarness.getOutput
    +
    +    val expectedOutput = new ConcurrentLinkedQueue[Object]()
    +
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa", 1: JInt, "Hi1"), true), 3))
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa2", 1: JInt, "Hi1"), true), 3))
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(2: JInt, "bbb", 2: JInt, "Hello1"), true), 4))
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa2", 1: JInt, "Hi2"), true), 12))
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(1: JInt, "aaa3", 1: JInt, "Hi2"), true), 25))
    +    expectedOutput.add(new StreamRecord(
    +      CRow(Row.of(2: JInt, "bbb2", 2: JInt, "Hello2"), true), 25))
    +
    +    verify(expectedOutput, result, new RowResultSortComparator(6))
    +
    +    testHarness.close()
    --- End diff --
    
    Please advance the processing time beyond the window sizes and check that the state is completely cleaned up. 
    The TestHarness
    Have a look at some of the tests in the `OverWindowHarnessTest` which verify that state is cleaned up.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123037106
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    --- End diff --
    
    Is it necessary to recursively traverse and convert `RexInputRef` and `RexCalls` etc? Can't we simply create an `AND` condition for all remaining condition?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123030850
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    +        val timeIndicators = analyzeSingleConditionTerm(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +   * analysis if condition term has time indicator
    +   */
    +  def analyzeSingleConditionTerm(
    +      expression: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): Seq[(RelDataType, Boolean)] = {
    +
    +    expression match {
    +      case i: RexInputRef =>
    +        val idx = i.getIndex
    +        inputType.getFieldList.get(idx).getType match {
    +          case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
    +            // left table time indicator
    +            Seq((t, true))
    +          case t: TimeIndicatorRelDataType =>
    +            // right table time indicator
    +            Seq((t, false))
    +          case _ => Seq()
    +        }
    +      case c: RexCall =>
    +        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +      case _ => Seq()
    +    }
    +  }
    +
    +  /**
    +    * Extract time offset and determain which table the offset belong to
    +    */
    +  def extractTimeOffsetFromCondition(
    +      timeTerm: RexNode,
    +      isLeftExprBelongLeftTable: Boolean,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig) = {
    +
    +    val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    +    val leftLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        rexBuilder,
    +        config)
    +
    +    val rightLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +
    +    val (tmpTimeOffset: Long, isLeftTableTimeOffset: Boolean) =
    +      timeTerm.getKind match {
    +        // e.g a.proctime > b.proctime - 5 sec, we need to store stream a.
    +        // the left expr(a) belong to left table, so the offset belong to left table
    +        case kind @ (SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    +          (rightLiteral - leftLiteral, isLeftExprBelongLeftTable)
    +        // e.g a.proctime < b.proctime + 5 sec, we need to store stream b.
    +        case kind @ (SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL) =>
    +          (leftLiteral - rightLiteral, !isLeftExprBelongLeftTable)
    +        case _ => 0
    +      }
    +
    +    val timeOffset =
    +      // only preceding offset need to store records
    +      if (tmpTimeOffset < 0)
    +        // determain the boudary value
    +        if (timeTerm.getKind == SqlKind.LESS_THAN || timeTerm.getKind == SqlKind.GREATER_THAN) {
    +          -tmpTimeOffset - 1
    +        } else {
    +          -tmpTimeOffset
    +        }
    +      else 0
    +
    +    (timeOffset, isLeftTableTimeOffset)
    +  }
    +
    +  /**
    +    * Calcute the time boundary. Replace the rowtime/proctime with zero literal.
    +    * For example:
    +    *  a.proctime - inteval '1' second > b.proctime - interval '1' second - interval '2' second
    +    *  |-----------left--------------|   |-------------------right---------------------------\
    +    * then the boundary of a is right - left:
    +    *  ((0 - 1000) - 2000) - (0 - 1000) = -2000(-preceding, +following)
    +    */
    +  private def reduceTimeExpression(
    +      rexNode: RexNode,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): Long = {
    +
    +    /**
    +      * replace the rowtime/proctime with zero literal.
    +      * Because calculation between timestamp can only be TIMESTAMP +/- INTERVAL
    +      * so such as b.proctime + interval '1' hour - a.proctime
    +      * will be translate into TIMESTAMP(0) + interval '1' hour - interval '0' second
    +      */
    +    def replaceTimeFieldWithLiteral(expr: RexNode): RexNode = {
    +      expr match {
    +        case c: RexCall if c.getOperator == TimeMaterializationSqlFunction =>
    +          // replace with timestamp
    +          rexBuilder.makeZeroLiteral(expr.getType)
    +        case c: RexCall =>
    +          // replace in call operands
    +          val newOps = c.operands.map(replaceTimeFieldWithLiteral(_))
    +          rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +        case i: RexInputRef if FlinkTypeFactory.isTimeIndicatorType(i.getType) =>
    +          // replace with timestamp
    +          rexBuilder.makeZeroLiteral(expr.getType)
    +        case _: RexInputRef =>
    +          throw new TableException("Time join condition may only reference time indicator fields.")
    +        case _ => expr
    +      }
    +    }
    +
    +    val literalRex = replaceTimeFieldWithLiteral(rexNode)
    +
    +    val exprReducer = new ExpressionReducer(config)
    +    val originList = new util.ArrayList[RexNode]()
    +    originList.add(literalRex)
    +    val reduceList = new util.ArrayList[RexNode]()
    +    exprReducer.reduce(rexBuilder, originList, reduceList)
    --- End diff --
    
    Can we call the ExpressionReducer just once for each time predicate, i.e., for the left and right side at the same time? It is calling the code generator and compiling a Java class which is quite expensive. 


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123009399
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    --- End diff --
    
    The check is only approximate, i.e., the stream join operator might not be able to execute the query even if this check is passed.
    
    For example it only checks if there is at least one time indicator in the condition. However, we would need to check that there are exactly two conjunctive terms that have time indicator attributes on both sides and define bounds to both sides. Basically the complete analysis that we later do in the join. I think we can do this analysis already in the rule and pass the result of the analysis to the join.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123227488
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +    private val leftStreamWindowSize: Long,
    +    private val rightStreamWindowSize: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoWinSize: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isLeft) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +          } else {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    --- End diff --
    
    If I understand this correctly, this state cleanup is just an additional "best-effort" cleaning. 
    The timer based cleanup should be sufficient to remove all state over time, correct?
    
    The reason why I ask this is the following: We should only rely on timers to clean up state because we cannot be sure if we receive any more elements for this key. If a key becomes inactive, we need to remove all state to avoid memory leaks. This is only possible with timers. If the cleanup in `processElement` is just an additional cleanup step, this is fine and should be done. However, we should not rely on this.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123231931
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/JoinITCase.scala ---
    @@ -0,0 +1,204 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.api.scala.stream.sql
    +
    +import org.apache.flink.api.scala._
    +import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
    +import org.apache.flink.table.api.scala._
    +import org.apache.flink.table.api.scala.stream.utils.{StreamITCase, StreamingWithStateTestBase}
    +import org.apache.flink.table.api.{TableEnvironment, TableException}
    +import org.apache.flink.types.Row
    +import org.junit.Assert._
    +import org.junit._
    +
    +import scala.collection.mutable
    +
    +class JoinITCase extends StreamingWithStateTestBase {
    +
    +  val data = List(
    +    (1L, 1, "Hello"),
    +    (2L, 2, "Hello"),
    +    (3L, 3, "Hello"),
    +    (4L, 4, "Hello"),
    +    (5L, 5, "Hello"),
    +    (6L, 6, "Hello"),
    +    (7L, 7, "Hello World"),
    +    (8L, 8, "Hello World"),
    +    (20L, 20, "Hello World"))
    +
    +  /**
    +    * both stream should have boundary
    +    */
    +  @Test(expected = classOf[TableException])
    +  def testJoinException0(): Unit = {
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    env.setStateBackend(getStateBackend)
    +    val tEnv = TableEnvironment.getTableEnvironment(env)
    +    StreamITCase.testResults = mutable.MutableList()
    +
    +    val t1 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +    val t2 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +
    +    tEnv.registerTable("T1", t1)
    +    tEnv.registerTable("T2", t2)
    +
    +    val sqlQuery = "SELECT t2.a, t2.c, t1.c from T1 as t1 join T2 as t2 on t1.a = t2.a"
    +
    +    val result = tEnv.sql(sqlQuery).toDataStream[Row]
    +    result.addSink(new StreamITCase.StringSink)
    +    env.execute()
    +  }
    +
    +  /**
    +    * both stream should have boundary
    +    */
    +  @Test(expected = classOf[TableException])
    +  def testJoinException1(): Unit = {
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    env.setStateBackend(getStateBackend)
    +    val tEnv = TableEnvironment.getTableEnvironment(env)
    +    StreamITCase.testResults = mutable.MutableList()
    +
    +    val t1 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +    val t2 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +
    +    tEnv.registerTable("T1", t1)
    +    tEnv.registerTable("T2", t2)
    +
    +    val sqlQuery = "SELECT t2.a, t2.c, t1.c from T1 as t1 join T2 as t2 on t1.a = t2.a " +
    +      "and t1.proctime > t2.proctime - interval '5' second"
    +
    +    val result = tEnv.sql(sqlQuery).toDataStream[Row]
    +    result.addSink(new StreamITCase.StringSink)
    +    env.execute()
    +  }
    +
    +  /**
    +    * both stream should use same time indicator
    +    */
    +  @Test(expected = classOf[TableException])
    +  def testJoinException2(): Unit = {
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    env.setStateBackend(getStateBackend)
    +    val tEnv = TableEnvironment.getTableEnvironment(env)
    +    StreamITCase.testResults = mutable.MutableList()
    +
    +    val t1 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +    val t2 = env.fromCollection(data).toTable(tEnv, 'a, 'b, 'c, 'rowtime.rowtime)
    +
    +    tEnv.registerTable("T1", t1)
    +    tEnv.registerTable("T2", t2)
    +
    +    val sqlQuery = "SELECT t2.a, t2.c, t1.c from T1 as t1 join T2 as t2 on t1.a = t2.a " +
    +      "and t1.proctime > t2.rowtime - interval '5' second "
    +
    +    val result = tEnv.sql(sqlQuery).toDataStream[Row]
    +    result.addSink(new StreamITCase.StringSink)
    +    env.execute()
    +  }
    +
    +
    +  /** test process time inner join **/
    +  @Test
    +  def testProcessTimeInnerJoin(): Unit = {
    --- End diff --
    
    If we want to have end-to-end tests, I would not check the correctness of the result (because this can be flaky). Instead I would add a comment that correctness checks are done in the HarnessTest.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123233427
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/JoinTest.scala ---
    @@ -0,0 +1,149 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.api.scala.stream.sql
    +
    +import org.apache.calcite.rel.logical.LogicalJoin
    +import org.apache.flink.api.scala._
    +import org.apache.flink.table.api.scala._
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.runtime.join.JoinUtil
    +import org.apache.flink.table.utils.TableTestUtil._
    +import org.apache.flink.table.utils.{StreamTableTestUtil, TableTestBase}
    +import org.junit.Assert._
    +import org.junit.Test
    +
    +class JoinTest extends TableTestBase {
    +  private val streamUtil: StreamTableTestUtil = streamTestUtil()
    +  streamUtil.addTable[(Int, String, Long)]("MyTable", 'a, 'b, 'c.rowtime, 'proctime.proctime)
    +  streamUtil.addTable[(Int, String, Long)]("MyTable2", 'a, 'b, 'c.rowtime, 'proctime.proctime)
    +
    +  @Test
    +  def testProcessingTimeInnerJoin() = {
    +
    +    val sqlQuery = "SELECT t1.a, t2.b " +
    +      "FROM MyTable as t1 join MyTable2 as t2 on t1.a = t2.a and " +
    +      "t1.proctime between t2.proctime - interval '1' hour and t2.proctime + interval '1' hour"
    +    val expected =
    +      unaryNode(
    +        "DataStreamCalc",
    +        binaryNode(
    +          "DataStreamRowStreamJoin",
    +          unaryNode(
    +            "DataStreamCalc",
    +            streamTableNode(0),
    +            term("select", "a", "proctime")
    +          ),
    +          unaryNode(
    +            "DataStreamCalc",
    +            streamTableNode(1),
    +            term("select", "a", "b", "proctime")
    +          ),
    +          term("condition",
    +            "AND(=(a, a0), >=(TIME_MATERIALIZATION(proctime), " +
    +              "-(TIME_MATERIALIZATION(proctime0), 3600000)), " +
    +              "<=(TIME_MATERIALIZATION(proctime), " +
    +              "DATETIME_PLUS(TIME_MATERIALIZATION(proctime0), 3600000)))"),
    +          term("select", "a, proctime, a0, b, proctime0"),
    +          term("joinType", "InnerJoin")
    +        ),
    +        term("select", "a", "b")
    +      )
    +
    +    streamUtil.verifySql(sqlQuery, expected)
    +  }
    +
    +
    +  @Test
    +  def testJoinTimeBoundary(): Unit = {
    +    verifyTimeBoundary(
    +      "t1.proctime between t2.proctime - interval '1' hour " +
    +        "and t2.proctime + interval '1' hour",
    +      3600000,
    +      3600000,
    +      "proctime")
    +
    +    verifyTimeBoundary(
    +      "t1.proctime > t2.proctime - interval '1' second and " +
    +        "t1.proctime < t2.proctime + interval '1' second",
    +      999,
    +      999,
    +      "proctime")
    +
    +    verifyTimeBoundary(
    +      "t1.c >= t2.c - interval '1' second and " +
    +        "t1.c <= t2.c + interval '1' second",
    +      1000,
    +      1000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t1.c >= t2.c and " +
    +        "t1.c <= t2.c + interval '1' second",
    +      0,
    +      1000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t1.c >= t2.c + interval '1' second and " +
    +        "t1.c <= t2.c + interval '10' second",
    +      0,
    +      10000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t2.c - interval '1' second <= t1.c and " +
    +        "t2.c + interval '10' second >= t1.c",
    +      1000,
    +      10000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t1.c - interval '2' second >= t2.c + interval '1' second -" +
    +        "interval '10' second and " +
    +        "t1.c <= t2.c + interval '10' second",
    +      7000,
    +      10000,
    +      "rowtime")
    +  }
    +
    +  def verifyTimeBoundary(
    --- End diff --
    
    I would move this also into a dedicated test class. I don't think it needs to extend the `TableTestBase` class.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123226695
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +    private val leftStreamWindowSize: Long,
    +    private val rightStreamWindowSize: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoWinSize: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    --- End diff --
    
    If the `winSize == 0`, do we need to put it into state at all? Can't we just join with the state of the other stream?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r123035300
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    +        val timeIndicators = analyzeSingleConditionTerm(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +   * analysis if condition term has time indicator
    +   */
    +  def analyzeSingleConditionTerm(
    +      expression: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): Seq[(RelDataType, Boolean)] = {
    +
    +    expression match {
    +      case i: RexInputRef =>
    +        val idx = i.getIndex
    +        inputType.getFieldList.get(idx).getType match {
    +          case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
    +            // left table time indicator
    +            Seq((t, true))
    +          case t: TimeIndicatorRelDataType =>
    +            // right table time indicator
    +            Seq((t, false))
    +          case _ => Seq()
    +        }
    +      case c: RexCall =>
    +        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +      case _ => Seq()
    +    }
    +  }
    +
    +  /**
    +    * Extract time offset and determain which table the offset belong to
    +    */
    +  def extractTimeOffsetFromCondition(
    +      timeTerm: RexNode,
    +      isLeftExprBelongLeftTable: Boolean,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig) = {
    +
    +    val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    +    val leftLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        rexBuilder,
    +        config)
    +
    +    val rightLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +
    +    val (tmpTimeOffset: Long, isLeftTableTimeOffset: Boolean) =
    +      timeTerm.getKind match {
    --- End diff --
    
    this assumes that both time attributes are on different sides. 
    I think it could also happen that both time attributes are on the same side. So we should make sure that we either exit in this case or move one of the time attributes to the other side.


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125202807
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    --- End diff --
    
    The index of the RexInputRef is the index combine two stream. For example, stream A(id, attr1, attr2, attr3) join stream B(id, attr1), then the condition a.id > b.id, the b.id's index will be 4 but not 0, so we need to traverse and convert it to 0, thus in the code-gen join function, we can use in2.getField(0) to get the value.


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125202985
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    +        val timeIndicators = analyzeSingleConditionTerm(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +   * analysis if condition term has time indicator
    +   */
    +  def analyzeSingleConditionTerm(
    +      expression: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): Seq[(RelDataType, Boolean)] = {
    +
    +    expression match {
    +      case i: RexInputRef =>
    +        val idx = i.getIndex
    +        inputType.getFieldList.get(idx).getType match {
    +          case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
    +            // left table time indicator
    +            Seq((t, true))
    +          case t: TimeIndicatorRelDataType =>
    +            // right table time indicator
    +            Seq((t, false))
    +          case _ => Seq()
    +        }
    +      case c: RexCall =>
    +        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +      case _ => Seq()
    +    }
    +  }
    +
    +  /**
    +    * Extract time offset and determain which table the offset belong to
    +    */
    +  def extractTimeOffsetFromCondition(
    +      timeTerm: RexNode,
    +      isLeftExprBelongLeftTable: Boolean,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig) = {
    +
    +    val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    +    val leftLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        rexBuilder,
    +        config)
    +
    +    val rightLiteral =
    +      reduceTimeExpression(
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +
    +    val (tmpTimeOffset: Long, isLeftTableTimeOffset: Boolean) =
    +      timeTerm.getKind match {
    --- End diff --
    
    for the case which both in the same side like ```a.proctime - b.proctime > 5 sec```, it will report an error ```Cannot apply '-' to arguments of type '<TIMESTAMP(3)> - <TIMESTAMP(3)> '```. So i think it will not exist such situation.


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125203253
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +    private val leftStreamWindowSize: Long,
    +    private val rightStreamWindowSize: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoWinSize: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    --- End diff --
    
    The remove operation here is just an additional operation, it will expired in onTimer() anyway.
    Yes, when winSize == 0, the record will not be put into state.


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125203271
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeInnerJoin.scala ---
    @@ -0,0 +1,316 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftStreamWindowSize    the left stream window size
    +  * @param rightStreamWindowSize    the right stream window size
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeInnerJoin(
    +    private val leftStreamWindowSize: Long,
    +    private val rightStreamWindowSize: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process leftstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWindowSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      rightStreamWindowSize,
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process rightstream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWindowSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftStreamWindowSize,
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWindowSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWindowSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoWinSize: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoExpiredTime =
    +      if (oppoWinSize == 0) Long.MinValue else curProcessTime - oppoWinSize
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0 && winSize != -1) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elments
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoExpiredTime) {
    +        listToRemove.add(eleTime)
    +      } else {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isLeft) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +          } else {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    --- End diff --
    
    Yes,  this is an additional "best-effort" cleaning.


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125203460
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/JoinTest.scala ---
    @@ -0,0 +1,149 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.api.scala.stream.sql
    +
    +import org.apache.calcite.rel.logical.LogicalJoin
    +import org.apache.flink.api.scala._
    +import org.apache.flink.table.api.scala._
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.runtime.join.JoinUtil
    +import org.apache.flink.table.utils.TableTestUtil._
    +import org.apache.flink.table.utils.{StreamTableTestUtil, TableTestBase}
    +import org.junit.Assert._
    +import org.junit.Test
    +
    +class JoinTest extends TableTestBase {
    +  private val streamUtil: StreamTableTestUtil = streamTestUtil()
    +  streamUtil.addTable[(Int, String, Long)]("MyTable", 'a, 'b, 'c.rowtime, 'proctime.proctime)
    +  streamUtil.addTable[(Int, String, Long)]("MyTable2", 'a, 'b, 'c.rowtime, 'proctime.proctime)
    +
    +  @Test
    +  def testProcessingTimeInnerJoin() = {
    +
    +    val sqlQuery = "SELECT t1.a, t2.b " +
    +      "FROM MyTable as t1 join MyTable2 as t2 on t1.a = t2.a and " +
    +      "t1.proctime between t2.proctime - interval '1' hour and t2.proctime + interval '1' hour"
    +    val expected =
    +      unaryNode(
    +        "DataStreamCalc",
    +        binaryNode(
    +          "DataStreamRowStreamJoin",
    +          unaryNode(
    +            "DataStreamCalc",
    +            streamTableNode(0),
    +            term("select", "a", "proctime")
    +          ),
    +          unaryNode(
    +            "DataStreamCalc",
    +            streamTableNode(1),
    +            term("select", "a", "b", "proctime")
    +          ),
    +          term("condition",
    +            "AND(=(a, a0), >=(TIME_MATERIALIZATION(proctime), " +
    +              "-(TIME_MATERIALIZATION(proctime0), 3600000)), " +
    +              "<=(TIME_MATERIALIZATION(proctime), " +
    +              "DATETIME_PLUS(TIME_MATERIALIZATION(proctime0), 3600000)))"),
    +          term("select", "a, proctime, a0, b, proctime0"),
    +          term("joinType", "InnerJoin")
    +        ),
    +        term("select", "a", "b")
    +      )
    +
    +    streamUtil.verifySql(sqlQuery, expected)
    +  }
    +
    +
    +  @Test
    +  def testJoinTimeBoundary(): Unit = {
    +    verifyTimeBoundary(
    +      "t1.proctime between t2.proctime - interval '1' hour " +
    +        "and t2.proctime + interval '1' hour",
    +      3600000,
    +      3600000,
    +      "proctime")
    +
    +    verifyTimeBoundary(
    +      "t1.proctime > t2.proctime - interval '1' second and " +
    +        "t1.proctime < t2.proctime + interval '1' second",
    +      999,
    +      999,
    +      "proctime")
    +
    +    verifyTimeBoundary(
    +      "t1.c >= t2.c - interval '1' second and " +
    +        "t1.c <= t2.c + interval '1' second",
    +      1000,
    +      1000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t1.c >= t2.c and " +
    +        "t1.c <= t2.c + interval '1' second",
    +      0,
    +      1000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t1.c >= t2.c + interval '1' second and " +
    +        "t1.c <= t2.c + interval '10' second",
    +      0,
    +      10000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t2.c - interval '1' second <= t1.c and " +
    +        "t2.c + interval '10' second >= t1.c",
    +      1000,
    +      10000,
    +      "rowtime")
    +
    +    verifyTimeBoundary(
    +      "t1.c - interval '2' second >= t2.c + interval '1' second -" +
    +        "interval '10' second and " +
    +        "t1.c <= t2.c + interval '10' second",
    +      7000,
    +      10000,
    +      "rowtime")
    +  }
    +
    +  def verifyTimeBoundary(
    --- End diff --
    
    Yes, it's better to check unsupported case here, but i think we still need to extend TableTestBase, cause we need to use the StreamTableTestUtil to do the sql parse.


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125205215
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/JoinUtil.scala ---
    @@ -0,0 +1,385 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.calcite.sql.fun.{SqlFloorFunction, SqlStdOperatorTable}
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object JoinUtil {
    +
    +  /**
    +    * check if the join case is stream join stream
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  inputType   left and right connect stream type
    +    */
    +  private[flink] def isStreamStreamJoin(
    +      condition: RexNode,
    +      inputType: RelDataType) = {
    +
    +    def isExistTumble(expr: RexNode): Boolean = {
    +      expr match {
    +        case c: RexCall =>
    +          c.getOperator match {
    +            case _: SqlFloorFunction =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case SqlStdOperatorTable.TUMBLE =>
    +              c.getOperands.map(analyzeSingleConditionTerm(_, 0, inputType)).exists(_.size > 0)
    +            case _ =>
    +              c.getOperands.map(isExistTumble(_)).exists(_ == true)
    +          }
    +        case _ => false
    +      }
    +    }
    +
    +    val isExistTimeIndicator = analyzeSingleConditionTerm(condition, 0, inputType).size > 0
    +    val isExistTumbleExpr = isExistTumble(condition)
    +
    +    !isExistTumbleExpr && isExistTimeIndicator
    +  }
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   other condtion include time-condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("There only can and must have 2 time conditions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftTableOffset, rightTableOffset) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException("Both input need time boundary.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftTableOffset, rightTableOffset, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException("The time indicators for each input should be the same.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Both input's time indicators is needed.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "There only can and must have one time indicators for each input.")
    +        }
    +      case other =>
    --- End diff --
    
    This case is for some RexNode did not belong to RexCall such RexLiteral(true)


Github user hongyuhong commented on the issue:

    https://github.com/apache/flink/pull/3715
  
    Hi @fhueske, sorry for late reply, i have update the pr.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125421732
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamWindowJoinRule.scala ---
    @@ -20,28 +20,60 @@ package org.apache.flink.table.plan.rules.datastream
     
     import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
     import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.`type`.RelDataType
     import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.table.api.{TableConfig, TableException}
     import org.apache.flink.table.plan.nodes.FlinkConventions
    -import org.apache.flink.table.plan.nodes.datastream.DataStreamRowStreamJoin
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin
     import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
     import org.apache.flink.table.plan.schema.RowSchema
    -import org.apache.flink.table.runtime.join.JoinUtil
    +import org.apache.flink.table.runtime.join.WindowJoinUtil
     
    -class DataStreamRowStreamJoinRule
    +class DataStreamWindowJoinRule
       extends ConverterRule(
           classOf[FlinkLogicalJoin],
           FlinkConventions.LOGICAL,
           FlinkConventions.DATASTREAM,
           "DataStreamJoinRule") {
     
    +  /** Time indicator type **/
    +  private var timeType: RelDataType = _
    --- End diff --
    
    I'm not sure if we can store these values as local variables. It kind assumes that the rule is called in a certain way (first check `matches()` and immediately after that `convert()`. The rule might also be called concurrently from different threads. I'd rather run the same analysis again in `convert()`.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125427060
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamWindowJoin.scala ---
    @@ -87,44 +98,38 @@ class DataStreamRowStreamJoin(
     
         val config = tableEnv.getConfig
     
    +    val leftIsAccRetract = DataStreamRetractionRules.isAccRetract(left)
    +    val rightIsAccRetract = DataStreamRetractionRules.isAccRetract(right)
    +    if (leftIsAccRetract || rightIsAccRetract) {
    +      throw new TableException(
    +        "Retraction on stream window join is not supported yet.")
    --- End diff --
    
    ```
    Windowed stream join does not support updates.
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125454546
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -191,8 +166,10 @@ object JoinUtil {
     
       /**
        * analysis if condition term has time indicator
    --- End diff --
    
    ```
    Extracts all time indicator attributes that are accessed in an expression.
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125440019
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamWindowJoinRule.scala ---
    @@ -20,28 +20,60 @@ package org.apache.flink.table.plan.rules.datastream
     
     import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
     import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.`type`.RelDataType
     import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.table.api.{TableConfig, TableException}
     import org.apache.flink.table.plan.nodes.FlinkConventions
    -import org.apache.flink.table.plan.nodes.datastream.DataStreamRowStreamJoin
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin
     import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
     import org.apache.flink.table.plan.schema.RowSchema
    -import org.apache.flink.table.runtime.join.JoinUtil
    +import org.apache.flink.table.runtime.join.WindowJoinUtil
     
    -class DataStreamRowStreamJoinRule
    +class DataStreamWindowJoinRule
       extends ConverterRule(
           classOf[FlinkLogicalJoin],
           FlinkConventions.LOGICAL,
           FlinkConventions.DATASTREAM,
           "DataStreamJoinRule") {
     
    +  /** Time indicator type **/
    +  private var timeType: RelDataType = _
    +
    +  /** left input lower boudary **/
    +  private var leftLowerBoundary: Long = _
    +
    +  /** left input upper boudary **/
    +  private var leftUpperBoundary: Long = _
    +
    +  /** remain join condition exclude equal condition and time condition **/
    +  private var remainCondition: Option[RexNode] = _
    +
       override def matches(call: RelOptRuleCall): Boolean = {
         val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
     
         val joinInfo = join.analyzeCondition
     
    -    JoinUtil.isStreamStreamJoin(
    -      joinInfo.getRemaining(join.getCluster.getRexBuilder),
    -      join.getRowType)
    +    try {
    +      val leftRowSchema = new RowSchema(join.getLeft.getRowType)
    +
    +      val result =
    +        WindowJoinUtil.analyzeTimeBoundary(
    +          joinInfo.getRemaining(join.getCluster.getRexBuilder),
    +          leftRowSchema.logicalType.getFieldCount,
    +          leftRowSchema.physicalType.getFieldCount,
    +          join.getRowType,
    +          join.getCluster.getRexBuilder,
    +          TableConfig.DEFAULT)
    +      timeType = result._1
    +      leftLowerBoundary = result._2
    +      leftUpperBoundary = result._3
    +      remainCondition = result._4
    +      true
    --- End diff --
    
    Also check for `join.getJoinType == JoinType.INNER`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125446795
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -238,13 +247,13 @@ class ProcTimeInnerJoin(
     
         }
     
    -    // loop the the other stream elments
    +    // loop the the other stream elements
         val oppositeKeyIter = oppoRowMapState.keys().iterator()
         while (oppositeKeyIter.hasNext) {
           val eleTime = oppositeKeyIter.next()
    -      if (eleTime < oppoExpiredTime) {
    +      if (eleTime < oppoLowerTime) {
             listToRemove.add(eleTime)
    -      } else {
    +      } else if (eleTime <= oppoUpperTime){
    --- End diff --
    
    +space between `) {`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125452889
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -0,0 +1,361 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object WindowJoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   join condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    * @return  timetype, left lower boundary, right lower boundary, remain condition
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("A time-based stream join requires exactly " +
    +        "two join predicates that bound the time in both directions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftLowerBound, leftUpperBound) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException(
    +            "Time-based join conditions must reference the time attribute of both input tables.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftLowerBound, leftUpperBound, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   *
    +   * @return (Seq(timeTerms), Seq(remainTerms)),
    +   */
    +  private def splitJoinCondition(
    --- End diff --
    
    I think we can simplify this method a bit more by removing the recursive call.
    We could do a first check if the `cnfCondition` is of `SqlKind.AND` and throw an exception if this is not the case (it is in CNF and we need at least two predicates for the bounds).
    If it is an AND, we simply iterate over the operands of `AND`.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125417060
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamWindowJoin.scala ---
    @@ -44,15 +46,19 @@ class DataStreamRowStreamJoin(
         leftSchema: RowSchema,
         rightSchema: RowSchema,
         schema: RowSchema,
    +    timeType: RelDataType,
    --- End diff --
    
    Can be a boolean flag `isRowTime`


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125423693
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamWindowJoinRule.scala ---
    @@ -20,28 +20,60 @@ package org.apache.flink.table.plan.rules.datastream
     
     import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
     import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.`type`.RelDataType
     import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.table.api.{TableConfig, TableException}
     import org.apache.flink.table.plan.nodes.FlinkConventions
    -import org.apache.flink.table.plan.nodes.datastream.DataStreamRowStreamJoin
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin
     import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
     import org.apache.flink.table.plan.schema.RowSchema
    -import org.apache.flink.table.runtime.join.JoinUtil
    +import org.apache.flink.table.runtime.join.WindowJoinUtil
     
    -class DataStreamRowStreamJoinRule
    +class DataStreamWindowJoinRule
       extends ConverterRule(
           classOf[FlinkLogicalJoin],
           FlinkConventions.LOGICAL,
           FlinkConventions.DATASTREAM,
           "DataStreamJoinRule") {
     
    +  /** Time indicator type **/
    +  private var timeType: RelDataType = _
    +
    +  /** left input lower boudary **/
    +  private var leftLowerBoundary: Long = _
    +
    +  /** left input upper boudary **/
    +  private var leftUpperBoundary: Long = _
    +
    +  /** remain join condition exclude equal condition and time condition **/
    +  private var remainCondition: Option[RexNode] = _
    +
       override def matches(call: RelOptRuleCall): Boolean = {
         val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
     
         val joinInfo = join.analyzeCondition
     
    -    JoinUtil.isStreamStreamJoin(
    -      joinInfo.getRemaining(join.getCluster.getRexBuilder),
    -      join.getRowType)
    +    try {
    +      val leftRowSchema = new RowSchema(join.getLeft.getRowType)
    +
    +      val result =
    +        WindowJoinUtil.analyzeTimeBoundary(
    +          joinInfo.getRemaining(join.getCluster.getRexBuilder),
    +          leftRowSchema.logicalType.getFieldCount,
    +          leftRowSchema.physicalType.getFieldCount,
    +          join.getRowType,
    +          join.getCluster.getRexBuilder,
    +          TableConfig.DEFAULT)
    +      timeType = result._1
    --- End diff --
    
    This can be done more concisely as:
    
    ```
    (timeType, leftLowerBoundary, leftUpperBoundary, remainCondition) =
      WindowJoinUtil.analyzeTimeBoundary(...)
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125447810
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -0,0 +1,326 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftLowerBound
    +  *        the left stream lower bound, and -leftLowerBound is the right stream upper bound
    +  * @param leftUpperBound
    +  *        the left stream upper bound, and -leftUpperBound is the right stream lower bound
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeWindowInnerJoin(
    +    private val leftLowerBound: Long,
    +    private val leftUpperBound: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  private val leftStreamWinSize: Long = if (leftLowerBound < 0) -leftLowerBound else 0
    +  private val rightStreamWinSize: Long = if (leftUpperBound > 0) leftUpperBound else 0
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process left stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWinSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      -leftUpperBound,     // right stream lower
    +      -leftLowerBound,     // right stream upper
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process right stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWinSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftLowerBound,    // left stream upper
    +      leftUpperBound,    // left stream upper
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWinSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWinSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoLowerBound: Long,
    +      oppoUpperBound: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoLowerTime = curProcessTime + oppoLowerBound
    +    val oppoUpperTime = curProcessTime + oppoUpperBound
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elements
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoLowerTime) {
    +        listToRemove.add(eleTime)
    +      } else if (eleTime <= oppoUpperTime){
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isLeft) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +          } else {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    --- End diff --
    
    ```
    Removes records which are outside the join window from the state. 
    Registers a new timer if the state still holds records after the clean-up.
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125448467
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -0,0 +1,326 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftLowerBound
    +  *        the left stream lower bound, and -leftLowerBound is the right stream upper bound
    +  * @param leftUpperBound
    +  *        the left stream upper bound, and -leftUpperBound is the right stream lower bound
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeWindowInnerJoin(
    +    private val leftLowerBound: Long,
    +    private val leftUpperBound: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  private val leftStreamWinSize: Long = if (leftLowerBound < 0) -leftLowerBound else 0
    +  private val rightStreamWinSize: Long = if (leftUpperBound > 0) leftUpperBound else 0
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process left stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWinSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      -leftUpperBound,     // right stream lower
    +      -leftLowerBound,     // right stream upper
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process right stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWinSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftLowerBound,    // left stream upper
    +      leftUpperBound,    // left stream upper
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWinSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWinSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoLowerBound: Long,
    +      oppoUpperBound: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoLowerTime = curProcessTime + oppoLowerBound
    +    val oppoUpperTime = curProcessTime + oppoUpperBound
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elements
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoLowerTime) {
    +        listToRemove.add(eleTime)
    +      } else if (eleTime <= oppoUpperTime){
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isLeft) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +          } else {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +      curTime: Long,
    +      winSize: Long,
    +      rowMapState: MapState[Long, JList[Row]],
    +      timerState: ValueState[Long],
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // loop the timestamps to find out expired records, when meet one record
    --- End diff --
    
    ```
    Search for expired timestamps. If we find a non-expired timestamp, remember the timestamp and leave the loop.
    This way we find all expired timestamps if they are sorted without doing a full pass.
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125446548
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -238,13 +247,13 @@ class ProcTimeInnerJoin(
     
         }
     
    -    // loop the the other stream elments
    +    // loop the the other stream elements
    --- End diff --
    
    "the the"


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125465060
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/JoinTest.scala ---
    @@ -101,22 +120,22 @@ class JoinTest extends TableTestBase {
         verifyTimeBoundary(
    --- End diff --
    
    I'd add another case where both bounds are negative.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125443593
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -0,0 +1,326 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftLowerBound
    +  *        the left stream lower bound, and -leftLowerBound is the right stream upper bound
    +  * @param leftUpperBound
    +  *        the left stream upper bound, and -leftUpperBound is the right stream lower bound
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeWindowInnerJoin(
    +    private val leftLowerBound: Long,
    +    private val leftUpperBound: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  private val leftStreamWinSize: Long = if (leftLowerBound < 0) -leftLowerBound else 0
    +  private val rightStreamWinSize: Long = if (leftUpperBound > 0) leftUpperBound else 0
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process left stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWinSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      -leftUpperBound,     // right stream lower
    +      -leftLowerBound,     // right stream upper
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process right stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWinSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftLowerBound,    // left stream upper
    +      leftUpperBound,    // left stream upper
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWinSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWinSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoLowerBound: Long,
    +      oppoUpperBound: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    --- End diff --
    
    We can set this to `true` when initializing `cRowWrapper`. The join does not support any kinds of updates (or deletes).


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125457882
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -210,77 +187,71 @@ object JoinUtil {
               case _ => Seq()
             }
           case c: RexCall =>
    -        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +        c.operands.map(extractTimeIndicatorAccesses(_, leftFieldCount, inputType)).reduce(_++_)
           case _ => Seq()
         }
       }
     
       /**
    -    * Extract time offset and determain which table the offset belong to
    +    * Extract time offset and determain it's the lower bound of left stream or the upper bound
    +    *
    +    * @return window boundary, is left lower bound
         */
       def extractTimeOffsetFromCondition(
           timeTerm: RexNode,
           isLeftExprBelongLeftTable: Boolean,
           rexBuilder: RexBuilder,
    -      config: TableConfig) = {
    +      config: TableConfig): (Long, Boolean) = {
     
         val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    -    val leftLiteral =
    -      reduceTimeExpression(
    -        timeCall.operands.get(0),
    -        rexBuilder,
    -        config)
     
    -    val rightLiteral =
    -      reduceTimeExpression(
    -        timeCall.operands.get(1),
    -        rexBuilder,
    -        config)
    -
    -    val (tmpTimeOffset: Long, isLeftTableTimeOffset: Boolean) =
    +    val isLeftLowerBound: Boolean =
           timeTerm.getKind match {
    -        // e.g a.proctime > b.proctime - 5 sec, we need to store stream a.
    -        // the left expr(a) belong to left table, so the offset belong to left table
    +        // e.g a.proctime > b.proctime - 5 sec, then it's the lower bound of a and the value is -5
    +        // e.g b.proctime > a.proctime - 5 sec, then it's not the lower bound of a but upper bound
             case kind @ (SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    -          (rightLiteral - leftLiteral, isLeftExprBelongLeftTable)
    -        // e.g a.proctime < b.proctime + 5 sec, we need to store stream b.
    +          isLeftExprBelongLeftTable
    +        // e.g a.proctime < b.proctime + 5 sec, the the upper bound of a is 5
             case kind @ (SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL) =>
    -          (leftLiteral - rightLiteral, !isLeftExprBelongLeftTable)
    -        case _ => 0
    +          !isLeftExprBelongLeftTable
    +        case _ =>
    +          throw new TableException("Unsupport time-condition.")
           }
     
    -    val timeOffset =
    -      // only preceding offset need to store records
    -      if (tmpTimeOffset < 0)
    -        // determain the boudary value
    +    val (leftLiteral, rightLiteral) =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +    val tmpTimeOffset: Long =
    +      if (isLeftExprBelongLeftTable) rightLiteral - leftLiteral else leftLiteral - rightLiteral
    +
    +    val boundary =
    +      tmpTimeOffset.signum * (
             if (timeTerm.getKind == SqlKind.LESS_THAN || timeTerm.getKind == SqlKind.GREATER_THAN) {
    -          -tmpTimeOffset - 1
    +          tmpTimeOffset.abs - 1
             } else {
    -          -tmpTimeOffset
    -        }
    -      else 0
    +          tmpTimeOffset.abs
    +        })
     
    -    (timeOffset, isLeftTableTimeOffset)
    +    (boundary, isLeftLowerBound)
       }
     
       /**
         * Calcute the time boundary. Replace the rowtime/proctime with zero literal.
    --- End diff --
    
    ```
    Calculates the time boundary by replacing the time attribute by a zero literal and reducing the expression.
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125447106
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -0,0 +1,326 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftLowerBound
    +  *        the left stream lower bound, and -leftLowerBound is the right stream upper bound
    +  * @param leftUpperBound
    +  *        the left stream upper bound, and -leftUpperBound is the right stream lower bound
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeWindowInnerJoin(
    +    private val leftLowerBound: Long,
    +    private val leftUpperBound: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  private val leftStreamWinSize: Long = if (leftLowerBound < 0) -leftLowerBound else 0
    +  private val rightStreamWinSize: Long = if (leftUpperBound > 0) leftUpperBound else 0
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process left stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWinSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      -leftUpperBound,     // right stream lower
    +      -leftLowerBound,     // right stream upper
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process right stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWinSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftLowerBound,    // left stream upper
    +      leftUpperBound,    // left stream upper
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWinSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWinSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoLowerBound: Long,
    +      oppoUpperBound: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoLowerTime = curProcessTime + oppoLowerBound
    +    val oppoUpperTime = curProcessTime + oppoUpperBound
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elements
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoLowerTime) {
    +        listToRemove.add(eleTime)
    +      } else if (eleTime <= oppoUpperTime){
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isLeft) {
    --- End diff --
    
    move condition out of the loop:
    
    ```
    if (isLeft) {
      while...
    } else {
      while...
    }
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125456634
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -162,24 +135,26 @@ object JoinUtil {
               (l._1 ++ r._1, l._2 ++ r._2)
             }
           case c: RexCall =>
    -        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        val timeIndicators = extractTimeIndicatorAccesses(c, leftFieldCount, inputType)
             timeIndicators match {
               case Seq() =>
                 (Seq(), Seq(c))
               case Seq(v1, v2) =>
                 if (v1._1 != v2._1) {
    -              throw new TableException("The time indicators for each input should be the same.")
    +              throw new TableException(
    +                "Both time attributes in a join condition must be of the same type.")
                 }
                 if (v1._2 == v2._2) {
    -              throw new TableException("Both input's time indicators is needed.")
    +              throw new TableException("Time-based join conditions " +
    +                "must reference the time attribute of both input tables.")
                 }
                 (Seq((v1._1, v1._2, c)), Seq())
    --- End diff --
    
    If we check the `RexCall` type here, we can analyze both sides of the condition independently and also handle cases where both attributes are on the same side of the condition.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125456339
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -162,24 +135,26 @@ object JoinUtil {
               (l._1 ++ r._1, l._2 ++ r._2)
             }
           case c: RexCall =>
    -        val timeIndicators = analyzeSingleConditionTerm(c, leftFieldCount, inputType)
    +        val timeIndicators = extractTimeIndicatorAccesses(c, leftFieldCount, inputType)
             timeIndicators match {
               case Seq() =>
                 (Seq(), Seq(c))
               case Seq(v1, v2) =>
                 if (v1._1 != v2._1) {
    -              throw new TableException("The time indicators for each input should be the same.")
    +              throw new TableException(
    +                "Both time attributes in a join condition must be of the same type.")
                 }
                 if (v1._2 == v2._2) {
    -              throw new TableException("Both input's time indicators is needed.")
    +              throw new TableException("Time-based join conditions " +
    +                "must reference the time attribute of both input tables.")
                 }
                 (Seq((v1._1, v1._2, c)), Seq())
    --- End diff --
    
    I think the way the `boolean` field is derived is a bit dangerous. We assume here that the `RexCall` is a comparison where the first operand is the left term and the second operand is the right term but it could also be any other call (with an arbitrary number of operands) that returns a boolean. Although we later check that the `RexCall` is a comparison, I think we should do this here.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125457951
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -210,77 +187,71 @@ object JoinUtil {
               case _ => Seq()
             }
           case c: RexCall =>
    -        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +        c.operands.map(extractTimeIndicatorAccesses(_, leftFieldCount, inputType)).reduce(_++_)
           case _ => Seq()
         }
       }
     
       /**
    -    * Extract time offset and determain which table the offset belong to
    +    * Extract time offset and determain it's the lower bound of left stream or the upper bound
    +    *
    +    * @return window boundary, is left lower bound
         */
       def extractTimeOffsetFromCondition(
           timeTerm: RexNode,
           isLeftExprBelongLeftTable: Boolean,
           rexBuilder: RexBuilder,
    -      config: TableConfig) = {
    +      config: TableConfig): (Long, Boolean) = {
     
         val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    -    val leftLiteral =
    -      reduceTimeExpression(
    -        timeCall.operands.get(0),
    -        rexBuilder,
    -        config)
     
    -    val rightLiteral =
    -      reduceTimeExpression(
    -        timeCall.operands.get(1),
    -        rexBuilder,
    -        config)
    -
    -    val (tmpTimeOffset: Long, isLeftTableTimeOffset: Boolean) =
    +    val isLeftLowerBound: Boolean =
           timeTerm.getKind match {
    -        // e.g a.proctime > b.proctime - 5 sec, we need to store stream a.
    -        // the left expr(a) belong to left table, so the offset belong to left table
    +        // e.g a.proctime > b.proctime - 5 sec, then it's the lower bound of a and the value is -5
    +        // e.g b.proctime > a.proctime - 5 sec, then it's not the lower bound of a but upper bound
             case kind @ (SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    -          (rightLiteral - leftLiteral, isLeftExprBelongLeftTable)
    -        // e.g a.proctime < b.proctime + 5 sec, we need to store stream b.
    +          isLeftExprBelongLeftTable
    +        // e.g a.proctime < b.proctime + 5 sec, the the upper bound of a is 5
             case kind @ (SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL) =>
    -          (leftLiteral - rightLiteral, !isLeftExprBelongLeftTable)
    -        case _ => 0
    +          !isLeftExprBelongLeftTable
    +        case _ =>
    +          throw new TableException("Unsupport time-condition.")
           }
     
    -    val timeOffset =
    -      // only preceding offset need to store records
    -      if (tmpTimeOffset < 0)
    -        // determain the boudary value
    +    val (leftLiteral, rightLiteral) =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +    val tmpTimeOffset: Long =
    +      if (isLeftExprBelongLeftTable) rightLiteral - leftLiteral else leftLiteral - rightLiteral
    +
    +    val boundary =
    +      tmpTimeOffset.signum * (
             if (timeTerm.getKind == SqlKind.LESS_THAN || timeTerm.getKind == SqlKind.GREATER_THAN) {
    -          -tmpTimeOffset - 1
    +          tmpTimeOffset.abs - 1
             } else {
    -          -tmpTimeOffset
    -        }
    -      else 0
    +          tmpTimeOffset.abs
    +        })
     
    -    (timeOffset, isLeftTableTimeOffset)
    +    (boundary, isLeftLowerBound)
       }
     
       /**
         * Calcute the time boundary. Replace the rowtime/proctime with zero literal.
         * For example:
    -    *  a.proctime - inteval '1' second > b.proctime - interval '1' second - interval '2' second
    -    *  |-----------left--------------|   |-------------------right---------------------------\
    -    * then the boundary of a is right - left:
    -    *  ((0 - 1000) - 2000) - (0 - 1000) = -2000(-preceding, +following)
    +    * b.proctime - interval '1' second - interval '2' second will be translate to
    --- End diff --
    
    will be translate -> will be translated


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125457556
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -210,77 +187,71 @@ object JoinUtil {
               case _ => Seq()
             }
           case c: RexCall =>
    -        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +        c.operands.map(extractTimeIndicatorAccesses(_, leftFieldCount, inputType)).reduce(_++_)
           case _ => Seq()
         }
       }
     
       /**
    -    * Extract time offset and determain which table the offset belong to
    +    * Extract time offset and determain it's the lower bound of left stream or the upper bound
    +    *
    +    * @return window boundary, is left lower bound
         */
       def extractTimeOffsetFromCondition(
           timeTerm: RexNode,
           isLeftExprBelongLeftTable: Boolean,
           rexBuilder: RexBuilder,
    -      config: TableConfig) = {
    +      config: TableConfig): (Long, Boolean) = {
     
         val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    -    val leftLiteral =
    -      reduceTimeExpression(
    -        timeCall.operands.get(0),
    -        rexBuilder,
    -        config)
     
    -    val rightLiteral =
    -      reduceTimeExpression(
    -        timeCall.operands.get(1),
    -        rexBuilder,
    -        config)
    -
    -    val (tmpTimeOffset: Long, isLeftTableTimeOffset: Boolean) =
    +    val isLeftLowerBound: Boolean =
           timeTerm.getKind match {
    -        // e.g a.proctime > b.proctime - 5 sec, we need to store stream a.
    -        // the left expr(a) belong to left table, so the offset belong to left table
    +        // e.g a.proctime > b.proctime - 5 sec, then it's the lower bound of a and the value is -5
    +        // e.g b.proctime > a.proctime - 5 sec, then it's not the lower bound of a but upper bound
             case kind @ (SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    -          (rightLiteral - leftLiteral, isLeftExprBelongLeftTable)
    -        // e.g a.proctime < b.proctime + 5 sec, we need to store stream b.
    +          isLeftExprBelongLeftTable
    +        // e.g a.proctime < b.proctime + 5 sec, the the upper bound of a is 5
             case kind @ (SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL) =>
    -          (leftLiteral - rightLiteral, !isLeftExprBelongLeftTable)
    -        case _ => 0
    +          !isLeftExprBelongLeftTable
    +        case _ =>
    +          throw new TableException("Unsupport time-condition.")
    --- End diff --
    
    Unsupport -> Unsupported


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125464505
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/JoinTest.scala ---
    @@ -67,27 +66,47 @@ class JoinTest extends TableTestBase {
         streamUtil.verifySql(sqlQuery, expected)
       }
     
    +  @Test(expected = classOf[TableException])
    --- End diff --
    
    Please add a brief comment why the query is expected to fail.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125448648
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -0,0 +1,326 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftLowerBound
    +  *        the left stream lower bound, and -leftLowerBound is the right stream upper bound
    +  * @param leftUpperBound
    +  *        the left stream upper bound, and -leftUpperBound is the right stream lower bound
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeWindowInnerJoin(
    +    private val leftLowerBound: Long,
    +    private val leftUpperBound: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  private val leftStreamWinSize: Long = if (leftLowerBound < 0) -leftLowerBound else 0
    +  private val rightStreamWinSize: Long = if (leftUpperBound > 0) leftUpperBound else 0
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process left stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWinSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      -leftUpperBound,     // right stream lower
    +      -leftLowerBound,     // right stream upper
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process right stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWinSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftLowerBound,    // left stream upper
    +      leftUpperBound,    // left stream upper
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWinSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWinSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoLowerBound: Long,
    +      oppoUpperBound: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoLowerTime = curProcessTime + oppoLowerBound
    +    val oppoUpperTime = curProcessTime + oppoUpperBound
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elements
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoLowerTime) {
    +        listToRemove.add(eleTime)
    +      } else if (eleTime <= oppoUpperTime){
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isLeft) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +          } else {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +      curTime: Long,
    +      winSize: Long,
    +      rowMapState: MapState[Long, JList[Row]],
    +      timerState: ValueState[Long],
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // loop the timestamps to find out expired records, when meet one record
    +    // after the expired timestamp, break the loop. If the keys is ordered, thus
    +    // can reduce loop num, if the keys is unordered, also can expire at least one
    +    // element every time the timer trigger
    +    while (keyIter.hasNext && nextTimer == 0) {
    +      val recordTime = keyIter.next
    +      if (recordTime < expiredTime) {
    +        listToRemove.add(recordTime)
    +      } else {
    +        nextTimer = recordTime
    +      }
    +    }
    +
    +    var i = listToRemove.size - 1
    --- End diff --
    
    Add a comment "Remove expired records from state."


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125478938
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/JoinHarnessTest.scala ---
    @@ -160,37 +181,52 @@ class JoinHarnessTest extends HarnessTestBase{
     
         testHarness.setProcessingTime(1)
         testHarness.processElement1(new StreamRecord(
    -      CRow(Row.of(1: JInt, "aaa"), true), 1))
    +      CRow(Row.of(1: JInt, "aaa1"), true), 1))
         testHarness.setProcessingTime(2)
         testHarness.processElement1(new StreamRecord(
    -      CRow(Row.of(2: JInt, "bbb"), true), 2))
    +      CRow(Row.of(2: JInt, "aaa2"), true), 2))
         testHarness.setProcessingTime(3)
         testHarness.processElement1(new StreamRecord(
    -      CRow(Row.of(1: JInt, "aaa2"), true), 3))
    +      CRow(Row.of(1: JInt, "aaa3"), true), 3))
    +    assert(testHarness.numKeyedStateEntries() == 4)
    +    assert(testHarness.numProcessingTimeTimers() == 2)
     
    +    // Do not store b elemets
    --- End diff --
    
    elemets -> elements


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125448784
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -0,0 +1,326 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftLowerBound
    +  *        the left stream lower bound, and -leftLowerBound is the right stream upper bound
    +  * @param leftUpperBound
    +  *        the left stream upper bound, and -leftUpperBound is the right stream lower bound
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeWindowInnerJoin(
    +    private val leftLowerBound: Long,
    +    private val leftUpperBound: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  private val leftStreamWinSize: Long = if (leftLowerBound < 0) -leftLowerBound else 0
    +  private val rightStreamWinSize: Long = if (leftUpperBound > 0) leftUpperBound else 0
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process left stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWinSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      -leftUpperBound,     // right stream lower
    +      -leftLowerBound,     // right stream upper
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process right stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWinSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftLowerBound,    // left stream upper
    +      leftUpperBound,    // left stream upper
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWinSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWinSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoLowerBound: Long,
    +      oppoUpperBound: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +    cRowWrapper.setChange(valueC.change)
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoLowerTime = curProcessTime + oppoLowerBound
    +    val oppoUpperTime = curProcessTime + oppoUpperBound
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the the other stream elements
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoLowerTime) {
    +        listToRemove.add(eleTime)
    +      } else if (eleTime <= oppoUpperTime){
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        while (i < oppoRowList.size) {
    +          if (isLeft) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +          } else {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +          }
    +          i += 1
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +  }
    +
    +  /**
    +    * expire records which before curTime - windowSize,
    +    * and register a timer if still exist records.
    +    * Ensure that one key only has one timer, so register another
    +    * timer until last timer trigger.
    +    */
    +  private def expireOutTimeRow(
    +      curTime: Long,
    +      winSize: Long,
    +      rowMapState: MapState[Long, JList[Row]],
    +      timerState: ValueState[Long],
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // loop the timestamps to find out expired records, when meet one record
    +    // after the expired timestamp, break the loop. If the keys is ordered, thus
    +    // can reduce loop num, if the keys is unordered, also can expire at least one
    +    // element every time the timer trigger
    +    while (keyIter.hasNext && nextTimer == 0) {
    +      val recordTime = keyIter.next
    +      if (recordTime < expiredTime) {
    +        listToRemove.add(recordTime)
    +      } else {
    +        nextTimer = recordTime
    +      }
    +    }
    +
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      rowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +
    +    // if exist records which later than the expire time,
    --- End diff --
    
    ```
    If the state has non-expired timestamps, register a new timer. 
    Otherwise clean the complete state for this input.
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125463323
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -0,0 +1,361 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object WindowJoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   join condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    * @return  timetype, left lower boundary, right lower boundary, remain condition
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("A time-based stream join requires exactly " +
    +        "two join predicates that bound the time in both directions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftLowerBound, leftUpperBound) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException(
    +            "Time-based join conditions must reference the time attribute of both input tables.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    --- End diff --
    
    Can we use `RowSchema.mapRexNode()` to compute the mapping?


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125482464
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamWindowJoin.scala ---
    @@ -87,44 +98,38 @@ class DataStreamRowStreamJoin(
     
         val config = tableEnv.getConfig
     
    +    val leftIsAccRetract = DataStreamRetractionRules.isAccRetract(left)
    --- End diff --
    
    Retractions are just one (of two) ways to encode updates in a stream. This join implementation does not support any kind of updates (which is fine). However, we have to check if the input streams produce updates or not. The `StreamTableEnvironment` has a private class `AppendOnlyValidator`. We should move `AppendOnlyValidator` and `UniqueKeyExtractor` into a class `UpdateUtils` together with the methods `isAppendOnly()` and `getUniqueKeyFields()`. `isAppendOnly()` checks if an operator produces updates or not and should be used here.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125458518
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -0,0 +1,361 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.functions.TimeMaterializationSqlFunction
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConversions._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object WindowJoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition   join condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  leftPhysicalFieldCnt left stream physical field num
    +    * @param  inputType   left and right connect stream type
    +    * @param  rexBuilder   util to build rexNode
    +    * @param  config      table environment config
    +    * @return  timetype, left lower boundary, right lower boundary, remain condition
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      leftPhysicalFieldCnt: Int,
    +      inputType: RelDataType,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (RelDataType, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) =
    +      splitJoinCondition(
    +        cnfCondition,
    +        leftLogicalFieldCnt,
    +        inputType
    +      )
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("A time-based stream join requires exactly " +
    +        "two join predicates that bound the time in both directions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +      timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftLowerBound, leftUpperBound) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException(
    +            "Time-based join conditions must reference the time attribute of both input tables.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +      remainTerms match {
    +        case Seq() => None
    +        case _ =>
    +          // turn the logical field index to physical field index
    +          def transInputRef(expr: RexNode): RexNode = {
    +            expr match {
    +              case c: RexCall =>
    +                val newOps = c.operands.map(transInputRef(_))
    +                rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +              case i: RexInputRef if i.getIndex >= leftLogicalFieldCnt =>
    +                rexBuilder.makeInputRef(
    +                  i.getType,
    +                  i.getIndex - leftLogicalFieldCnt + leftPhysicalFieldCnt)
    +              case _ => expr
    +            }
    +          }
    +
    +          Some(remainTerms.map(transInputRef(_)).reduceLeft( (l, r) => {
    +            RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +          }))
    +      }
    +
    +    (timeTerms.get(0)._1, leftLowerBound, leftUpperBound, remainCondition)
    +  }
    +
    +  /**
    +   * Split the join conditions into time condition and non-time condition
    +   *
    +   * @return (Seq(timeTerms), Seq(remainTerms)),
    +   */
    +  private def splitJoinCondition(
    +      cnfCondition: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    cnfCondition match {
    +      case c: RexCall if c.getKind == SqlKind.AND =>
    +        val timeIndicators =
    +          c.operands.map(splitJoinCondition(_, leftFieldCount, inputType))
    +        timeIndicators.reduceLeft { (l, r) =>
    +          (l._1 ++ r._1, l._2 ++ r._2)
    +        }
    +      case c: RexCall =>
    +        val timeIndicators = extractTimeIndicatorAccesses(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException(
    +                "Both time attributes in a join condition must be of the same type.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Time-based join conditions " +
    +                "must reference the time attribute of both input tables.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "Time-based join conditions must reference the time attribute of both input tables.")
    +        }
    +      case other =>
    +        val timeIndicators = extractTimeIndicatorAccesses(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +   * analysis if condition term has time indicator
    +    *
    +   * @return seq(timeType, is left input time indicator)
    +   */
    +  def extractTimeIndicatorAccesses(
    +      expression: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): Seq[(RelDataType, Boolean)] = {
    +
    +    expression match {
    +      case i: RexInputRef =>
    +        val idx = i.getIndex
    +        inputType.getFieldList.get(idx).getType match {
    +          case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
    +            // left table time indicator
    +            Seq((t, true))
    +          case t: TimeIndicatorRelDataType =>
    +            // right table time indicator
    +            Seq((t, false))
    +          case _ => Seq()
    +        }
    +      case c: RexCall =>
    +        c.operands.map(extractTimeIndicatorAccesses(_, leftFieldCount, inputType)).reduce(_++_)
    +      case _ => Seq()
    +    }
    +  }
    +
    +  /**
    +    * Extract time offset and determain it's the lower bound of left stream or the upper bound
    +    *
    +    * @return window boundary, is left lower bound
    +    */
    +  def extractTimeOffsetFromCondition(
    +      timeTerm: RexNode,
    +      isLeftExprBelongLeftTable: Boolean,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (Long, Boolean) = {
    +
    +    val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    +
    +    val isLeftLowerBound: Boolean =
    +      timeTerm.getKind match {
    +        // e.g a.proctime > b.proctime - 5 sec, then it's the lower bound of a and the value is -5
    +        // e.g b.proctime > a.proctime - 5 sec, then it's not the lower bound of a but upper bound
    +        case kind @ (SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    +          isLeftExprBelongLeftTable
    +        // e.g a.proctime < b.proctime + 5 sec, the the upper bound of a is 5
    +        case kind @ (SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL) =>
    +          !isLeftExprBelongLeftTable
    +        case _ =>
    +          throw new TableException("Unsupport time-condition.")
    +      }
    +
    +    val (leftLiteral, rightLiteral) =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +    val tmpTimeOffset: Long =
    +      if (isLeftExprBelongLeftTable) rightLiteral - leftLiteral else leftLiteral - rightLiteral
    +
    +    val boundary =
    +      tmpTimeOffset.signum * (
    +        if (timeTerm.getKind == SqlKind.LESS_THAN || timeTerm.getKind == SqlKind.GREATER_THAN) {
    +          tmpTimeOffset.abs - 1
    +        } else {
    +          tmpTimeOffset.abs
    +        })
    +
    +    (boundary, isLeftLowerBound)
    +  }
    +
    +  /**
    +    * Calcute the time boundary. Replace the rowtime/proctime with zero literal.
    +    * For example:
    +    * b.proctime - interval '1' second - interval '2' second will be translate to
    +    * 0 - 1000 - 2000
    +    */
    +  private def reduceTimeExpression(
    +      leftRexNode: RexNode,
    +      rightRexNode: RexNode,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (Long, Long) = {
    +
    +    /**
    +      * replace the rowtime/proctime with zero literal.
    +      */
    +    def replaceTimeFieldWithLiteral(expr: RexNode): RexNode = {
    +      expr match {
    +        case c: RexCall if c.getOperator == TimeMaterializationSqlFunction =>
    --- End diff --
    
    This case should not happen anymore because we removed the time materialization for join conditions.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125461422
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -210,77 +187,71 @@ object JoinUtil {
               case _ => Seq()
             }
           case c: RexCall =>
    -        c.operands.map(analyzeSingleConditionTerm(_, leftFieldCount, inputType)).reduce(_++_)
    +        c.operands.map(extractTimeIndicatorAccesses(_, leftFieldCount, inputType)).reduce(_++_)
           case _ => Seq()
         }
       }
     
       /**
    -    * Extract time offset and determain which table the offset belong to
    +    * Extract time offset and determain it's the lower bound of left stream or the upper bound
    --- End diff --
    
    ```
    Computes the absolute bound on the left operand of a comparison expression and 
    whether the bound is an upper or lower bound.
    ```


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125459202
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -300,22 +271,27 @@ object JoinUtil {
           }
         }
     
    -    val literalRex = replaceTimeFieldWithLiteral(rexNode)
    +    val literalLeftRex = replaceTimeFieldWithLiteral(leftRexNode)
    +    val literalRightRex = replaceTimeFieldWithLiteral(rightRexNode)
     
         val exprReducer = new ExpressionReducer(config)
         val originList = new util.ArrayList[RexNode]()
    -    originList.add(literalRex)
    +    originList.add(literalLeftRex)
    +    originList.add(literalRightRex)
         val reduceList = new util.ArrayList[RexNode]()
         exprReducer.reduce(rexBuilder, originList, reduceList)
     
    -    reduceList.get(0) match {
    +    val literals = reduceList.map(f => f match {
           case call: RexCall =>
    --- End diff --
    
    Which case is this? 
    Isn't a blind cast of the first operand to `RexLiteral` dangerous and could result in a `ClassCastException`?


GitHub user hongyuhong opened a pull request:

    https://github.com/apache/flink/pull/4266

    [FLINK-6232][Table&Sql] support proctime inner windowed stream join

    Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.
    If your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).
    In addition to going through the list, please provide a meaningful description of your changes.
    
    - [ ] General
      - The pull request references the related JIRA issue ("[FLINK-XXX] Jira title text")
      - The pull request addresses only one issue
      - Each commit in the PR has a meaningful commit message (including the JIRA id)
    
    - [ ] Documentation
      - Documentation has been added for new functionality
      - Old documentation affected by the pull request has been updated
      - JavaDoc for public methods has been added
    
    - [ ] Tests & Build
      - Functionality added by the pull request is covered by tests
      - `mvn clean verify` has been executed successfully locally or a Travis build has passed


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/hongyuhong/flink flink-6232-re

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/4266.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4266
    
----
commit e2baf558e100eb7bb0d6b9927cb166b892f78d8f
Author: hongyuhong <hongyuhong@huawei.com>
Date:   2017-07-06T03:24:04Z

    [FLINK-6232][Table&Sql] support proctime inner windowed stream join

----


Github user wuchong commented on the issue:

    https://github.com/apache/flink/pull/4266
  
    Hi @hongyuhong , is this the same PR with #3715 ?  In order to rebase/remove merge commit, please do not create a new PR, otherwise committers may review an out-date PR or lose the review context. 
    
    You can force update your repo branch via `git push <your-repo-name> flink-6232 --force` and close this PR.
    
    Thanks,
    Jark  


Github user hongyuhong commented on the issue:

    https://github.com/apache/flink/pull/4266
  
    Hi @wuchong, thanks for the reminding. There still have some modify in the new commit, so i want to reserve the older commit for easy reviewing, after the reviewing finish, i will close the pr https://github.com/apache/flink/pull/3715.
    
    Thanks very much.



Github user hongyuhong commented on the issue:

    https://github.com/apache/flink/pull/3715
  
    Hi @fhueske, thanks for the reviewing, i have update the pr here for reviewing, and also create a new pr https://github.com/apache/flink/issues/4266 to merge the commit, once you finish reviewing, i will close this pr.
    
    Thanks very much.


Github user hongyuhong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r125807639
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamWindowJoinRule.scala ---
    @@ -20,28 +20,60 @@ package org.apache.flink.table.plan.rules.datastream
     
     import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
     import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.`type`.RelDataType
     import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.table.api.{TableConfig, TableException}
     import org.apache.flink.table.plan.nodes.FlinkConventions
    -import org.apache.flink.table.plan.nodes.datastream.DataStreamRowStreamJoin
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin
     import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
     import org.apache.flink.table.plan.schema.RowSchema
    -import org.apache.flink.table.runtime.join.JoinUtil
    +import org.apache.flink.table.runtime.join.WindowJoinUtil
     
    -class DataStreamRowStreamJoinRule
    +class DataStreamWindowJoinRule
       extends ConverterRule(
           classOf[FlinkLogicalJoin],
           FlinkConventions.LOGICAL,
           FlinkConventions.DATASTREAM,
           "DataStreamJoinRule") {
     
    +  /** Time indicator type **/
    +  private var timeType: RelDataType = _
    +
    +  /** left input lower boudary **/
    +  private var leftLowerBoundary: Long = _
    +
    +  /** left input upper boudary **/
    +  private var leftUpperBoundary: Long = _
    +
    +  /** remain join condition exclude equal condition and time condition **/
    +  private var remainCondition: Option[RexNode] = _
    +
       override def matches(call: RelOptRuleCall): Boolean = {
         val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
     
         val joinInfo = join.analyzeCondition
     
    -    JoinUtil.isStreamStreamJoin(
    -      joinInfo.getRemaining(join.getCluster.getRexBuilder),
    -      join.getRowType)
    +    try {
    +      val leftRowSchema = new RowSchema(join.getLeft.getRowType)
    +
    +      val result =
    +        WindowJoinUtil.analyzeTimeBoundary(
    +          joinInfo.getRemaining(join.getCluster.getRexBuilder),
    +          leftRowSchema.logicalType.getFieldCount,
    +          leftRowSchema.physicalType.getFieldCount,
    +          join.getRowType,
    +          join.getCluster.getRexBuilder,
    +          TableConfig.DEFAULT)
    +      timeType = result._1
    +      leftLowerBoundary = result._2
    +      leftUpperBoundary = result._3
    +      remainCondition = result._4
    +      true
    --- End diff --
    
    Should we check the type here?  I think the DataStreamWindowJoin can support inner and outer join, so i check the jointype in translateToPlan for convenient expansion.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r126987096
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamWindowJoinRule.scala ---
    @@ -20,28 +20,60 @@ package org.apache.flink.table.plan.rules.datastream
     
     import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
     import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.`type`.RelDataType
     import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.table.api.{TableConfig, TableException}
     import org.apache.flink.table.plan.nodes.FlinkConventions
    -import org.apache.flink.table.plan.nodes.datastream.DataStreamRowStreamJoin
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin
     import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
     import org.apache.flink.table.plan.schema.RowSchema
    -import org.apache.flink.table.runtime.join.JoinUtil
    +import org.apache.flink.table.runtime.join.WindowJoinUtil
     
    -class DataStreamRowStreamJoinRule
    +class DataStreamWindowJoinRule
       extends ConverterRule(
           classOf[FlinkLogicalJoin],
           FlinkConventions.LOGICAL,
           FlinkConventions.DATASTREAM,
           "DataStreamJoinRule") {
     
    +  /** Time indicator type **/
    +  private var timeType: RelDataType = _
    +
    +  /** left input lower boudary **/
    +  private var leftLowerBoundary: Long = _
    +
    +  /** left input upper boudary **/
    +  private var leftUpperBoundary: Long = _
    +
    +  /** remain join condition exclude equal condition and time condition **/
    +  private var remainCondition: Option[RexNode] = _
    +
       override def matches(call: RelOptRuleCall): Boolean = {
         val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
     
         val joinInfo = join.analyzeCondition
     
    -    JoinUtil.isStreamStreamJoin(
    -      joinInfo.getRemaining(join.getCluster.getRexBuilder),
    -      join.getRowType)
    +    try {
    +      val leftRowSchema = new RowSchema(join.getLeft.getRowType)
    +
    +      val result =
    +        WindowJoinUtil.analyzeTimeBoundary(
    +          joinInfo.getRemaining(join.getCluster.getRexBuilder),
    +          leftRowSchema.logicalType.getFieldCount,
    +          leftRowSchema.physicalType.getFieldCount,
    +          join.getRowType,
    +          join.getCluster.getRexBuilder,
    +          TableConfig.DEFAULT)
    +      timeType = result._1
    +      leftLowerBoundary = result._2
    +      leftUpperBoundary = result._3
    +      remainCondition = result._4
    +      true
    --- End diff --
    
    `DataStreamWindowJoin` should support outer joins, but at the moment it does not. 
    Until it supports outer joins, I would not translate to `DataStreamWindowJoin`.


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/3715#discussion_r127113760
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/rules/datastream/DataStreamWindowJoinRule.scala ---
    @@ -20,28 +20,60 @@ package org.apache.flink.table.plan.rules.datastream
     
     import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
     import org.apache.calcite.rel.RelNode
    +import org.apache.calcite.rel.`type`.RelDataType
     import org.apache.calcite.rel.convert.ConverterRule
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.table.api.{TableConfig, TableException}
     import org.apache.flink.table.plan.nodes.FlinkConventions
    -import org.apache.flink.table.plan.nodes.datastream.DataStreamRowStreamJoin
    +import org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin
     import org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin
     import org.apache.flink.table.plan.schema.RowSchema
    -import org.apache.flink.table.runtime.join.JoinUtil
    +import org.apache.flink.table.runtime.join.WindowJoinUtil
     
    -class DataStreamRowStreamJoinRule
    +class DataStreamWindowJoinRule
       extends ConverterRule(
           classOf[FlinkLogicalJoin],
           FlinkConventions.LOGICAL,
           FlinkConventions.DATASTREAM,
           "DataStreamJoinRule") {
     
    +  /** Time indicator type **/
    +  private var timeType: RelDataType = _
    +
    +  /** left input lower boudary **/
    +  private var leftLowerBoundary: Long = _
    +
    +  /** left input upper boudary **/
    +  private var leftUpperBoundary: Long = _
    +
    +  /** remain join condition exclude equal condition and time condition **/
    +  private var remainCondition: Option[RexNode] = _
    +
       override def matches(call: RelOptRuleCall): Boolean = {
         val join: FlinkLogicalJoin = call.rel(0).asInstanceOf[FlinkLogicalJoin]
     
         val joinInfo = join.analyzeCondition
     
    -    JoinUtil.isStreamStreamJoin(
    -      joinInfo.getRemaining(join.getCluster.getRexBuilder),
    -      join.getRowType)
    +    try {
    +      val leftRowSchema = new RowSchema(join.getLeft.getRowType)
    +
    +      val result =
    +        WindowJoinUtil.analyzeTimeBoundary(
    +          joinInfo.getRemaining(join.getCluster.getRexBuilder),
    +          leftRowSchema.logicalType.getFieldCount,
    +          leftRowSchema.physicalType.getFieldCount,
    +          join.getRowType,
    +          join.getCluster.getRexBuilder,
    +          TableConfig.DEFAULT)
    +      timeType = result._1
    +      leftLowerBoundary = result._2
    +      leftUpperBoundary = result._3
    +      remainCondition = result._4
    +      true
    --- End diff --
    
    Hi @fhueske , I think a good reason to put the check in `translateToPlan` is for throwing a proper exception message, such as `Left join is not supported...`. If we put the check in rule's matches or throw the exception in rule's matches, it will throw a obscure message with the logical plan.
    
    IMO, a meaningful message is important for users. 


Github user fhueske commented on the issue:

    https://github.com/apache/flink/pull/4266
  
    Thanks for the update @hongyuhong!
    I will take this PR from here. The logic looks very good but I would like to refactor some parts (mainly  the `WindowJoinUtil`). 
    
    I will open a new PR with your work and my commit on top, probably later today. 
    It would be great if you could review and check my PR.
    
    @wuchong your review is of course also highly welcome :-)
    
    Thank you, Fabian


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r126680468
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -0,0 +1,349 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConverters._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object WindowJoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition           join condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  inputSchema         join rowtype schema
    +    * @param  rexBuilder          util to build rexNode
    +    * @param  config              table environment config
    +    * @return isRowTime, left lower boundary, right lower boundary, remain condition
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      inputSchema: RowSchema,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (Boolean, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) = cnfCondition match {
    +      case c: RexCall if cnfCondition.getKind == SqlKind.AND =>
    +        c.getOperands.asScala
    +          .map(analyzeCondtionTermType(_, leftLogicalFieldCnt, inputSchema.logicalType))
    +          .reduceLeft((l, r) => {
    +            (l._1 ++ r._1, l._2 ++ r._2)
    +          })
    +      case _ =>
    +        throw new TableException("A time-based stream join requires exactly " +
    +          "two join predicates that bound the time in both directions.")
    +    }
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("A time-based stream join requires exactly " +
    +        "two join predicates that bound the time in both directions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    --- End diff --
    
    minor typo: conditon -> condition


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r126671328
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamWindowJoin.scala ---
    @@ -0,0 +1,187 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.{JoinInfo, JoinRelType}
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamQueryConfig, StreamTableEnvironment, TableException}
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{ProcTimeWindowInnerJoin, WindowJoinUtil}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.table.updateutils.UpdateCheckUtils
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamWindowJoin(
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinType: JoinRelType,
    +    leftSchema: RowSchema,
    +    rightSchema: RowSchema,
    +    schema: RowSchema,
    +    isRowTime: Boolean,
    +    leftLowerBound: Long,
    +    leftUpperBound: Long,
    +    remainCondition: Option[RexNode],
    +    ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +    with CommonJoin
    +    with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamWindowJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinCondition,
    +      joinType,
    +      leftSchema,
    +      rightSchema,
    +      schema,
    +      isRowTime,
    +      leftLowerBound,
    +      leftUpperBound,
    +      remainCondition,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +    joinToString(
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    joinExplainTerms(
    +      super.explainTerms(pw),
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def translateToPlan(
    +      tableEnv: StreamTableEnvironment,
    +      queryConfig: StreamQueryConfig): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    val isLeftAppendOnly = UpdateCheckUtils.isAppendOnly(left)
    +    val isRightAppendOnly = UpdateCheckUtils.isAppendOnly(right)
    +    if (!isLeftAppendOnly || !isRightAppendOnly) {
    +      throw new TableException(
    +        "Windowed stream join does not support updates.")
    +    }
    +
    +    val leftDataStream = left.asInstanceOf[DataStreamRel].translateToPlan(tableEnv, queryConfig)
    +    val rightDataStream = right.asInstanceOf[DataStreamRel].translateToPlan(tableEnv, queryConfig)
    +
    +    // get the equality keys and other condition
    +    val joinInfo = JoinInfo.of(leftNode, rightNode, joinCondition)
    +    val leftKeys = joinInfo.leftKeys.toIntArray
    +    val rightKeys = joinInfo.rightKeys.toIntArray
    +
    +    // generate join function
    +    val joinFunction =
    +    WindowJoinUtil.generateJoinFunction(
    +      config,
    +      joinType,
    +      leftSchema.physicalTypeInfo,
    +      rightSchema.physicalTypeInfo,
    +      schema,
    +      remainCondition,
    +      ruleDescription)
    +
    +    joinType match {
    +      case JoinRelType.INNER =>
    +        isRowTime match {
    --- End diff --
    
    I think using a `if (isRowTime) else ` here is more simple.


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r126671128
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -0,0 +1,349 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    --- End diff --
    
    remove unused import


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r126683783
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/JoinITCase.scala ---
    @@ -0,0 +1,117 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.api.scala.stream.sql
    +
    +import org.apache.flink.api.scala._
    +import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
    +import org.apache.flink.table.api.TableEnvironment
    +import org.apache.flink.table.api.scala._
    +import org.apache.flink.table.api.scala.stream.utils.{StreamITCase, StreamingWithStateTestBase}
    +import org.apache.flink.types.Row
    +import org.junit._
    +
    +import scala.collection.mutable
    +
    +class JoinITCase extends StreamingWithStateTestBase {
    +
    +  val data = List(
    +    (1L, 1, "Hello"),
    +    (2L, 2, "Hello"),
    +    (3L, 3, "Hello"),
    +    (4L, 4, "Hello"),
    +    (5L, 5, "Hello"),
    +    (6L, 6, "Hello"),
    +    (7L, 7, "Hello World"),
    +    (8L, 8, "Hello World"),
    +    (20L, 20, "Hello World"))
    +
    +  /** test process time inner join **/
    +  @Test
    +  def testProcessTimeInnerJoin(): Unit = {
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    val tEnv = TableEnvironment.getTableEnvironment(env)
    +    env.setStateBackend(getStateBackend)
    +    StreamITCase.testResults = mutable.MutableList()
    +    env.setParallelism(1)
    +
    +    val sqlQuery = "SELECT t2.a, t2.c, t1.c from T1 as t1 join T2 as t2 on t1.a = t2.a and " +
    +      "t1.proctime between t2.proctime - interval '5' second and t2.proctime + interval '5' second"
    +
    +    val data1 = new mutable.MutableList[(Int, Long, String)]
    +    data1.+=((1, 1L, "Hi1"))
    +    data1.+=((1, 2L, "Hi2"))
    +    data1.+=((1, 5L, "Hi3"))
    +    data1.+=((2, 7L, "Hi5"))
    +    data1.+=((1, 9L, "Hi6"))
    +    data1.+=((1, 8L, "Hi8"))
    +
    +    val data2 = new mutable.MutableList[(Int, Long, String)]
    +    data2.+=((1, 1L, "HiHi"))
    +    data2.+=((2, 2L, "HeHe"))
    +
    +    val t1 = env.fromCollection(data1).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +    val t2 = env.fromCollection(data2).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime)
    +
    +    tEnv.registerTable("T1", t1)
    +    tEnv.registerTable("T2", t2)
    +
    +    val result = tEnv.sql(sqlQuery).toAppendStream[Row]
    +    result.addSink(new StreamITCase.StringSink[Row])
    +    env.execute()
    +  }
    +
    +  /** test process time inner join with other condition **/
    +  @Test
    +  def testProcessTimeInnerJoinWithOtherCondition(): Unit = {
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    val tEnv = TableEnvironment.getTableEnvironment(env)
    +    env.setStateBackend(getStateBackend)
    +    StreamITCase.testResults = mutable.MutableList()
    --- End diff --
    
    You can simply do `StreamITCase.clear` instead of this.


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r126683700
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/JoinITCase.scala ---
    @@ -0,0 +1,117 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.api.scala.stream.sql
    +
    +import org.apache.flink.api.scala._
    +import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
    +import org.apache.flink.table.api.TableEnvironment
    +import org.apache.flink.table.api.scala._
    +import org.apache.flink.table.api.scala.stream.utils.{StreamITCase, StreamingWithStateTestBase}
    +import org.apache.flink.types.Row
    +import org.junit._
    +
    +import scala.collection.mutable
    +
    +class JoinITCase extends StreamingWithStateTestBase {
    +
    +  val data = List(
    +    (1L, 1, "Hello"),
    +    (2L, 2, "Hello"),
    +    (3L, 3, "Hello"),
    +    (4L, 4, "Hello"),
    +    (5L, 5, "Hello"),
    +    (6L, 6, "Hello"),
    +    (7L, 7, "Hello World"),
    +    (8L, 8, "Hello World"),
    +    (20L, 20, "Hello World"))
    +
    +  /** test process time inner join **/
    +  @Test
    +  def testProcessTimeInnerJoin(): Unit = {
    +    val env = StreamExecutionEnvironment.getExecutionEnvironment
    +    val tEnv = TableEnvironment.getTableEnvironment(env)
    +    env.setStateBackend(getStateBackend)
    +    StreamITCase.testResults = mutable.MutableList()
    --- End diff --
    
    You can simply do `StreamITCase.clear` instead of this.


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r127142150
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -0,0 +1,326 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftLowerBound
    +  *        the left stream lower bound, and -leftLowerBound is the right stream upper bound
    +  * @param leftUpperBound
    +  *        the left stream upper bound, and -leftUpperBound is the right stream lower bound
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeWindowInnerJoin(
    +    private val leftLowerBound: Long,
    +    private val leftUpperBound: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  private val leftStreamWinSize: Long = if (leftLowerBound < 0) -leftLowerBound else 0
    +  private val rightStreamWinSize: Long = if (leftUpperBound > 0) leftUpperBound else 0
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +    cRowWrapper.setChange(true)
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process left stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWinSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      -leftUpperBound,     // right stream lower
    +      -leftLowerBound,     // right stream upper
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process right stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWinSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftLowerBound,    // left stream upper
    +      leftUpperBound,    // left stream upper
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWinSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWinSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoLowerBound: Long,
    +      oppoUpperBound: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoLowerTime = curProcessTime + oppoLowerBound
    +    val oppoUpperTime = curProcessTime + oppoUpperBound
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the other stream elements
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoLowerTime) {
    +        listToRemove.add(eleTime)
    +      } else if (eleTime <= oppoUpperTime) {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        if (isLeft) {
    +          while (i < oppoRowList.size) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +            i += 1
    +          }
    +        } else {
    +          while (i < oppoRowList.size) {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +            i += 1
    +          }
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +  }
    +
    +  /**
    +    * Removes records which are outside the join window from the state.
    +    * Registers a new timer if the state still holds records after the clean-up.
    +    */
    +  private def expireOutTimeRow(
    +      curTime: Long,
    +      winSize: Long,
    +      rowMapState: MapState[Long, JList[Row]],
    +      timerState: ValueState[Long],
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // Search for expired timestamps.
    +    // If we find a non-expired timestamp, remember the timestamp and leave the loop.
    +    // This way we find all expired timestamps if they are sorted without doing a full pass.
    +    while (keyIter.hasNext && nextTimer == 0) {
    +      val recordTime = keyIter.next
    +      if (recordTime < expiredTime) {
    +        listToRemove.add(recordTime)
    +      } else {
    +        nextTimer = recordTime
    +      }
    +    }
    +
    +    // Remove expired records from state
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      rowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +
    +    // If the state has non-expired timestamps, register a new timer.
    +    // Otherwise clean the complete state for this input.
    +    if (nextTimer != 0) {
    +      ctx.timerService.registerProcessingTimeTimer(nextTimer + winSize + 1)
    --- End diff --
    
    The `nextTimer` maybe not the smallest or greatest timestamp among the non-expired timestamps. Is it better to register a `curTime + winSize + 1` timer? 


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r127129047
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -0,0 +1,349 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConverters._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object WindowJoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    +    * and return remain condition.
    +    *
    +    * @param  condition           join condition
    +    * @param  leftLogicalFieldCnt left stream logical field num
    +    * @param  inputSchema         join rowtype schema
    +    * @param  rexBuilder          util to build rexNode
    +    * @param  config              table environment config
    +    * @return isRowTime, left lower boundary, right lower boundary, remain condition
    +    */
    +  private[flink] def analyzeTimeBoundary(
    +      condition: RexNode,
    +      leftLogicalFieldCnt: Int,
    +      inputSchema: RowSchema,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (Boolean, Long, Long, Option[RexNode]) = {
    +
    +    // Converts the condition to conjunctive normal form (CNF)
    +    val cnfCondition = RexUtil.toCnf(rexBuilder, condition)
    +
    +    // split the condition into time indicator condition and other condition
    +    val (timeTerms, remainTerms) = cnfCondition match {
    +      case c: RexCall if cnfCondition.getKind == SqlKind.AND =>
    +        c.getOperands.asScala
    +          .map(analyzeCondtionTermType(_, leftLogicalFieldCnt, inputSchema.logicalType))
    +          .reduceLeft((l, r) => {
    +            (l._1 ++ r._1, l._2 ++ r._2)
    +          })
    +      case _ =>
    +        throw new TableException("A time-based stream join requires exactly " +
    +          "two join predicates that bound the time in both directions.")
    +    }
    +
    +    if (timeTerms.size != 2) {
    +      throw new TableException("A time-based stream join requires exactly " +
    +        "two join predicates that bound the time in both directions.")
    +    }
    +
    +    // extract time offset from the time indicator conditon
    +    val streamTimeOffsets =
    +    timeTerms.map(x => extractTimeOffsetFromCondition(x._3, x._2, rexBuilder, config))
    +
    +    val (leftLowerBound, leftUpperBound) =
    +      streamTimeOffsets match {
    +        case Seq((x, true), (y, false)) => (x, y)
    +        case Seq((x, false), (y, true)) => (y, x)
    +        case _ =>
    +          throw new TableException(
    +            "Time-based join conditions must reference the time attribute of both input tables.")
    +      }
    +
    +    // compose the remain condition list into one condition
    +    val remainCondition =
    +    remainTerms match {
    +      case Seq() => None
    +      case _ =>
    +        // Converts logical field references to physical ones.
    +        Some(remainTerms.map(inputSchema.mapRexNode).reduceLeft((l, r) => {
    +          RelOptUtil.andJoinFilters(rexBuilder, l, r)
    +        }))
    +    }
    +
    +    val isRowTime: Boolean = timeTerms(0)._1 match {
    +      case x if FlinkTypeFactory.isProctimeIndicatorType(x) => false
    +      case _ => true
    +    }
    +    (isRowTime, leftLowerBound, leftUpperBound, remainCondition)
    +  }
    +
    +  /**
    +    * Split the join conditions into time condition and non-time condition
    +    *
    +    * @return (Seq(timeTerms), Seq(remainTerms)),
    +    */
    +  private def analyzeCondtionTermType(
    +      conditionTerm: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): (Seq[(RelDataType, Boolean, RexNode)], Seq[RexNode]) = {
    +
    +    conditionTerm match {
    +      case c: RexCall if Seq(SqlKind.GREATER_THAN, SqlKind.GREATER_THAN_OR_EQUAL,
    +        SqlKind.LESS_THAN, SqlKind.LESS_THAN_OR_EQUAL).contains(c.getKind) =>
    +        val timeIndicators = extractTimeIndicatorAccesses(c, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(c))
    +          case Seq(v1, v2) =>
    +            if (v1._1 != v2._1) {
    +              throw new TableException(
    +                "Both time attributes in a join condition must be of the same type.")
    +            }
    +            if (v1._2 == v2._2) {
    +              throw new TableException("Time-based join conditions " +
    +                "must reference the time attribute of both input tables.")
    +            }
    +            (Seq((v1._1, v1._2, c)), Seq())
    +          case _ =>
    +            throw new TableException(
    +              "Time-based join conditions must reference the time attribute of both input tables.")
    +        }
    +      case other =>
    +        val timeIndicators = extractTimeIndicatorAccesses(other, leftFieldCount, inputType)
    +        timeIndicators match {
    +          case Seq() =>
    +            (Seq(), Seq(other))
    +          case _ =>
    +            throw new TableException("Time indicators can not be used in non time-condition.")
    +        }
    +    }
    +  }
    +
    +  /**
    +    * Extracts all time indicator attributes that are accessed in an expression.
    +    *
    +    * @return seq(timeType, is left input time indicator)
    +    */
    +  def extractTimeIndicatorAccesses(
    +      expression: RexNode,
    +      leftFieldCount: Int,
    +      inputType: RelDataType): Seq[(RelDataType, Boolean)] = {
    +
    +    expression match {
    +      case i: RexInputRef =>
    +        val idx = i.getIndex
    +        inputType.getFieldList.get(idx).getType match {
    +          case t: TimeIndicatorRelDataType if idx < leftFieldCount =>
    +            // left table time indicator
    +            Seq((t, true))
    +          case t: TimeIndicatorRelDataType =>
    +            // right table time indicator
    +            Seq((t, false))
    +          case _ => Seq()
    +        }
    +      case c: RexCall =>
    +        c.operands.asScala
    +          .map(extractTimeIndicatorAccesses(_, leftFieldCount, inputType))
    +          .reduce(_ ++ _)
    +      case _ => Seq()
    +    }
    +  }
    +
    +  /**
    +    * Computes the absolute bound on the left operand of a comparison expression and
    +    * whether the bound is an upper or lower bound.
    +    *
    +    * @return window boundary, is left lower bound
    +    */
    +  def extractTimeOffsetFromCondition(
    +      timeTerm: RexNode,
    +      isLeftExprBelongLeftTable: Boolean,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (Long, Boolean) = {
    +
    +    val timeCall: RexCall = timeTerm.asInstanceOf[RexCall]
    +
    +    val isLeftLowerBound: Boolean =
    +      timeTerm.getKind match {
    +        // e.g a.proctime > b.proctime - 5 sec, then it's the lower bound of a and the value is -5
    +        // e.g b.proctime > a.proctime - 5 sec, then it's not the lower bound of a but upper bound
    +        case kind@(SqlKind.GREATER_THAN | SqlKind.GREATER_THAN_OR_EQUAL) =>
    +          isLeftExprBelongLeftTable
    +        // e.g a.proctime < b.proctime + 5 sec, the the upper bound of a is 5
    +        case kind@(SqlKind.LESS_THAN | SqlKind.LESS_THAN_OR_EQUAL) =>
    +          !isLeftExprBelongLeftTable
    +        case _ =>
    +          throw new TableException("Unsupported time-condition.")
    +      }
    +
    +    val (leftLiteral, rightLiteral) =
    +      reduceTimeExpression(
    +        timeCall.operands.get(0),
    +        timeCall.operands.get(1),
    +        rexBuilder,
    +        config)
    +    val tmpTimeOffset: Long =
    +      if (isLeftExprBelongLeftTable) rightLiteral - leftLiteral else leftLiteral - rightLiteral
    +
    +    val boundary =
    +      tmpTimeOffset.signum * (
    +        if (timeTerm.getKind == SqlKind.LESS_THAN || timeTerm.getKind == SqlKind.GREATER_THAN) {
    +          tmpTimeOffset.abs - 1
    +        } else {
    +          tmpTimeOffset.abs
    +        })
    +
    +    (boundary, isLeftLowerBound)
    +  }
    +
    +  /**
    +    * Calculates the time boundary by replacing the time attribute by a zero literal
    +    * and reducing the expression.
    +    * For example:
    +    * b.proctime - interval '1' second - interval '2' second will be translated to
    +    * 0 - 1000 - 2000
    +    */
    +  private def reduceTimeExpression(
    +      leftRexNode: RexNode,
    +      rightRexNode: RexNode,
    +      rexBuilder: RexBuilder,
    +      config: TableConfig): (Long, Long) = {
    +
    +    /**
    +      * replace the rowtime/proctime with zero literal.
    +      */
    +    def replaceTimeFieldWithLiteral(expr: RexNode): RexNode = {
    +      expr match {
    +        case c: RexCall =>
    +          // replace in call operands
    +          val newOps = c.operands.asScala.map(replaceTimeFieldWithLiteral(_)).asJava
    +          rexBuilder.makeCall(c.getType, c.getOperator, newOps)
    +        case i: RexInputRef if FlinkTypeFactory.isTimeIndicatorType(i.getType) =>
    +          // replace with timestamp
    +          rexBuilder.makeZeroLiteral(expr.getType)
    +        case _: RexInputRef =>
    +          throw new TableException("Time join condition may only reference time indicator fields.")
    +        case _ => expr
    +      }
    +    }
    +
    +    val literalLeftRex = replaceTimeFieldWithLiteral(leftRexNode)
    +    val literalRightRex = replaceTimeFieldWithLiteral(rightRexNode)
    +
    +    val exprReducer = new ExpressionReducer(config)
    +    val originList = new util.ArrayList[RexNode]()
    +    originList.add(literalLeftRex)
    +    originList.add(literalRightRex)
    +    val reduceList = new util.ArrayList[RexNode]()
    +    exprReducer.reduce(rexBuilder, originList, reduceList)
    +
    +    val literals = reduceList.asScala.map(f => f match {
    --- End diff --
    
    Can be simplified to  
    
    ```scala
    val literals = reduceList.asScala.map {
          case literal: RexLiteral =>
            literal.getValue2.asInstanceOf[Long]
          case _ =>
            throw TableException(
              "Time condition may only consist of time attributes, literals, and arithmetic operators.")
        }
    ```


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r127153991
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -0,0 +1,326 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftLowerBound
    +  *        the left stream lower bound, and -leftLowerBound is the right stream upper bound
    +  * @param leftUpperBound
    +  *        the left stream upper bound, and -leftUpperBound is the right stream lower bound
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeWindowInnerJoin(
    +    private val leftLowerBound: Long,
    +    private val leftUpperBound: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  private val leftStreamWinSize: Long = if (leftLowerBound < 0) -leftLowerBound else 0
    +  private val rightStreamWinSize: Long = if (leftUpperBound > 0) leftUpperBound else 0
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +    cRowWrapper.setChange(true)
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process left stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWinSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      -leftUpperBound,     // right stream lower
    +      -leftLowerBound,     // right stream upper
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process right stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWinSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftLowerBound,    // left stream upper
    +      leftUpperBound,    // left stream upper
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWinSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWinSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoLowerBound: Long,
    +      oppoUpperBound: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoLowerTime = curProcessTime + oppoLowerBound
    +    val oppoUpperTime = curProcessTime + oppoUpperBound
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    --- End diff --
    
    I'm not sure about this.  For example, `a.proctime between b.proctime - 5 and b.proctime`. In this case, we will buffer stream `a` for a window size 5, but will not buffer stream `b` because the right window size is 0. 
    
    Suppose the input elements are  [a1, 1], [a2, 2], [b1, 5], [a3, 5]. The first field in the tuple indicates which stream it belongs to. The seconds field in the tuple is the processing timestamp. The expected result should be `a1, b1`, `a2, b1`, `a3, b1`. But the actual result misses `a3, b1`. Because we didn't buffer the elements  from `b` stream. 
    
    So I think, even if the window size is 0, we still need to store the elements. Of course, we will register a `curTime +1` timer to clean the states.


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r126684002
  
    --- Diff: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/scala/stream/sql/JoinITCase.scala ---
    @@ -0,0 +1,117 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.api.scala.stream.sql
    +
    +import org.apache.flink.api.scala._
    +import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
    +import org.apache.flink.table.api.TableEnvironment
    +import org.apache.flink.table.api.scala._
    +import org.apache.flink.table.api.scala.stream.utils.{StreamITCase, StreamingWithStateTestBase}
    +import org.apache.flink.types.Row
    +import org.junit._
    +
    +import scala.collection.mutable
    +
    +class JoinITCase extends StreamingWithStateTestBase {
    +
    +  val data = List(
    --- End diff --
    
    Looks like the `data` is never used, can we remove it?


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r126680044
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/WindowJoinUtil.scala ---
    @@ -0,0 +1,349 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.flink.table.runtime.join
    +
    +import java.math.{BigDecimal => JBigDecimal}
    +import java.util
    +
    +import org.apache.calcite.plan.RelOptUtil
    +import org.apache.calcite.rel.`type`.RelDataType
    +import org.apache.calcite.rel.core.JoinRelType
    +import org.apache.calcite.rex._
    +import org.apache.calcite.sql.SqlKind
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.typeinfo.TypeInformation
    +import org.apache.flink.table.api.{TableConfig, TableException}
    +import org.apache.flink.table.calcite.FlinkTypeFactory
    +import org.apache.flink.table.codegen.{CodeGenerator, ExpressionReducer}
    +import org.apache.flink.table.plan.schema.{RowSchema, TimeIndicatorRelDataType}
    +import org.apache.flink.types.Row
    +
    +import scala.collection.JavaConverters._
    +
    +/**
    +  * An util class to help analyze and build join code .
    +  */
    +object WindowJoinUtil {
    +
    +  /**
    +    * Analyze time-condtion to get time boundary for each stream and get the time type
    --- End diff --
    
    minor typo: condtion -> condition


Github user fhueske commented on the issue:

    https://github.com/apache/flink/pull/4266
  
    Thanks for the review @wuchong.
    I'll address your comments in my upcoming PR.


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r127165003
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamWindowJoin.scala ---
    @@ -0,0 +1,187 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.{JoinInfo, JoinRelType}
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamQueryConfig, StreamTableEnvironment, TableException}
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{ProcTimeWindowInnerJoin, WindowJoinUtil}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.table.updateutils.UpdateCheckUtils
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamWindowJoin(
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinType: JoinRelType,
    +    leftSchema: RowSchema,
    +    rightSchema: RowSchema,
    +    schema: RowSchema,
    +    isRowTime: Boolean,
    +    leftLowerBound: Long,
    +    leftUpperBound: Long,
    +    remainCondition: Option[RexNode],
    +    ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +    with CommonJoin
    +    with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamWindowJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinCondition,
    +      joinType,
    +      leftSchema,
    +      rightSchema,
    +      schema,
    +      isRowTime,
    +      leftLowerBound,
    +      leftUpperBound,
    +      remainCondition,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +    joinToString(
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    joinExplainTerms(
    +      super.explainTerms(pw),
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def translateToPlan(
    +      tableEnv: StreamTableEnvironment,
    +      queryConfig: StreamQueryConfig): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    val isLeftAppendOnly = UpdateCheckUtils.isAppendOnly(left)
    --- End diff --
    
    We should use `DataStreamRetractionRules.isAccRetract(input)` to check whether the input will produces updates.  


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r127166606
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamWindowJoin.scala ---
    @@ -0,0 +1,187 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.{JoinInfo, JoinRelType}
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamQueryConfig, StreamTableEnvironment, TableException}
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{ProcTimeWindowInnerJoin, WindowJoinUtil}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.table.updateutils.UpdateCheckUtils
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamWindowJoin(
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinType: JoinRelType,
    +    leftSchema: RowSchema,
    +    rightSchema: RowSchema,
    +    schema: RowSchema,
    +    isRowTime: Boolean,
    +    leftLowerBound: Long,
    +    leftUpperBound: Long,
    +    remainCondition: Option[RexNode],
    +    ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +    with CommonJoin
    +    with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamWindowJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinCondition,
    +      joinType,
    +      leftSchema,
    +      rightSchema,
    +      schema,
    +      isRowTime,
    +      leftLowerBound,
    +      leftUpperBound,
    +      remainCondition,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +    joinToString(
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    joinExplainTerms(
    +      super.explainTerms(pw),
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def translateToPlan(
    +      tableEnv: StreamTableEnvironment,
    +      queryConfig: StreamQueryConfig): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    val isLeftAppendOnly = UpdateCheckUtils.isAppendOnly(left)
    --- End diff --
    
    `isAccRetract` only checks how updates are encoded but not whether there are updates.
    The current approach is correct, IMO.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r127173843
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -0,0 +1,326 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftLowerBound
    +  *        the left stream lower bound, and -leftLowerBound is the right stream upper bound
    +  * @param leftUpperBound
    +  *        the left stream upper bound, and -leftUpperBound is the right stream lower bound
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeWindowInnerJoin(
    +    private val leftLowerBound: Long,
    +    private val leftUpperBound: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  private val leftStreamWinSize: Long = if (leftLowerBound < 0) -leftLowerBound else 0
    +  private val rightStreamWinSize: Long = if (leftUpperBound > 0) leftUpperBound else 0
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +    cRowWrapper.setChange(true)
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process left stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWinSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      -leftUpperBound,     // right stream lower
    +      -leftLowerBound,     // right stream upper
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process right stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWinSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftLowerBound,    // left stream upper
    +      leftUpperBound,    // left stream upper
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWinSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWinSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoLowerBound: Long,
    +      oppoUpperBound: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoLowerTime = curProcessTime + oppoLowerBound
    +    val oppoUpperTime = curProcessTime + oppoUpperBound
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    --- End diff --
    
    I think you are right @wuchong. I'll remove that condition.
    OTOH, this is a processing time join which cannot guarantee strict results anyway ;-)


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r127173868
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamWindowJoin.scala ---
    @@ -0,0 +1,187 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.{JoinInfo, JoinRelType}
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamQueryConfig, StreamTableEnvironment, TableException}
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{ProcTimeWindowInnerJoin, WindowJoinUtil}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.table.updateutils.UpdateCheckUtils
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamWindowJoin(
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinType: JoinRelType,
    +    leftSchema: RowSchema,
    +    rightSchema: RowSchema,
    +    schema: RowSchema,
    +    isRowTime: Boolean,
    +    leftLowerBound: Long,
    +    leftUpperBound: Long,
    +    remainCondition: Option[RexNode],
    +    ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +    with CommonJoin
    +    with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamWindowJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinCondition,
    +      joinType,
    +      leftSchema,
    +      rightSchema,
    +      schema,
    +      isRowTime,
    +      leftLowerBound,
    +      leftUpperBound,
    +      remainCondition,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +    joinToString(
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    joinExplainTerms(
    +      super.explainTerms(pw),
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def translateToPlan(
    +      tableEnv: StreamTableEnvironment,
    +      queryConfig: StreamQueryConfig): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    val isLeftAppendOnly = UpdateCheckUtils.isAppendOnly(left)
    --- End diff --
    
    The following SQL `select a, sum(b), a+1 from t1 group by a` will optimized into the following nodes:
    
    ```
    DataStreamCalc (AccRetract,  producesUpdates=false)
        DataStreamGroupAggregate (AccRetract, producesUpdates=true)
           DataStreamScan (Acc, producesUpdates=fale)
    ```
    The DataStreamCalc is append only, but is in AccRetract mode which means the output contains retraction. 
    
    I think we want to check whether the input contains retraction, right? 
    
    
        


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r127176219
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/join/ProcTimeWindowInnerJoin.scala ---
    @@ -0,0 +1,326 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.runtime.join
    +
    +import java.util
    +import java.util.{List => JList}
    +
    +import org.apache.flink.api.common.functions.FlatJoinFunction
    +import org.apache.flink.api.common.state._
    +import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
    +import org.apache.flink.api.java.typeutils.ListTypeInfo
    +import org.apache.flink.configuration.Configuration
    +import org.apache.flink.streaming.api.functions.co.CoProcessFunction
    +import org.apache.flink.table.codegen.Compiler
    +import org.apache.flink.table.runtime.CRowWrappingCollector
    +import org.apache.flink.table.runtime.types.CRow
    +import org.apache.flink.types.Row
    +import org.apache.flink.util.Collector
    +import org.slf4j.LoggerFactory
    +
    +/**
    +  * A CoProcessFunction to support stream join stream, currently just support inner-join
    +  *
    +  * @param leftLowerBound
    +  *        the left stream lower bound, and -leftLowerBound is the right stream upper bound
    +  * @param leftUpperBound
    +  *        the left stream upper bound, and -leftUpperBound is the right stream lower bound
    +  * @param element1Type  the input type of left stream
    +  * @param element2Type  the input type of right stream
    +  * @param genJoinFuncName    the function code of other non-equi condition
    +  * @param genJoinFuncCode    the function name of other non-equi condition
    +  *
    +  */
    +class ProcTimeWindowInnerJoin(
    +    private val leftLowerBound: Long,
    +    private val leftUpperBound: Long,
    +    private val element1Type: TypeInformation[Row],
    +    private val element2Type: TypeInformation[Row],
    +    private val genJoinFuncName: String,
    +    private val genJoinFuncCode: String)
    +  extends CoProcessFunction[CRow, CRow, CRow]
    +    with Compiler[FlatJoinFunction[Row, Row, Row]]{
    +
    +  private var cRowWrapper: CRowWrappingCollector = _
    +
    +  /** other condition function **/
    +  private var joinFunction: FlatJoinFunction[Row, Row, Row] = _
    +
    +  /** tmp list to store expired records **/
    +  private var listToRemove: JList[Long] = _
    +
    +  /** state to hold left stream element **/
    +  private var row1MapState: MapState[Long, JList[Row]] = _
    +  /** state to hold right stream element **/
    +  private var row2MapState: MapState[Long, JList[Row]] = _
    +
    +  /** state to record last timer of left stream, 0 means no timer **/
    +  private var timerState1: ValueState[Long] = _
    +  /** state to record last timer of right stream, 0 means no timer **/
    +  private var timerState2: ValueState[Long] = _
    +
    +  private val leftStreamWinSize: Long = if (leftLowerBound < 0) -leftLowerBound else 0
    +  private val rightStreamWinSize: Long = if (leftUpperBound > 0) leftUpperBound else 0
    +
    +  val LOG = LoggerFactory.getLogger(this.getClass)
    +
    +  override def open(config: Configuration) {
    +    LOG.debug(s"Compiling JoinFunction: $genJoinFuncName \n\n " +
    +      s"Code:\n$genJoinFuncCode")
    +    val clazz = compile(
    +      getRuntimeContext.getUserCodeClassLoader,
    +      genJoinFuncName,
    +      genJoinFuncCode)
    +    LOG.debug("Instantiating JoinFunction.")
    +    joinFunction = clazz.newInstance()
    +
    +    listToRemove = new util.ArrayList[Long]()
    +    cRowWrapper = new CRowWrappingCollector()
    +    cRowWrapper.setChange(true)
    +
    +    // initialize row state
    +    val rowListTypeInfo1: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element1Type)
    +    val mapStateDescriptor1: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row1mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo1)
    +    row1MapState = getRuntimeContext.getMapState(mapStateDescriptor1)
    +
    +    val rowListTypeInfo2: TypeInformation[JList[Row]] = new ListTypeInfo[Row](element2Type)
    +    val mapStateDescriptor2: MapStateDescriptor[Long, JList[Row]] =
    +      new MapStateDescriptor[Long, JList[Row]]("row2mapstate",
    +        BasicTypeInfo.LONG_TYPE_INFO.asInstanceOf[TypeInformation[Long]], rowListTypeInfo2)
    +    row2MapState = getRuntimeContext.getMapState(mapStateDescriptor2)
    +
    +    // initialize timer state
    +    val valueStateDescriptor1: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate1", classOf[Long])
    +    timerState1 = getRuntimeContext.getState(valueStateDescriptor1)
    +
    +    val valueStateDescriptor2: ValueStateDescriptor[Long] =
    +      new ValueStateDescriptor[Long]("timervaluestate2", classOf[Long])
    +    timerState2 = getRuntimeContext.getState(valueStateDescriptor2)
    +  }
    +
    +  /**
    +    * Process left stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement1(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      leftStreamWinSize,
    +      timerState1,
    +      row1MapState,
    +      row2MapState,
    +      -leftUpperBound,     // right stream lower
    +      -leftLowerBound,     // right stream upper
    +      true
    +    )
    +  }
    +
    +  /**
    +    * Process right stream records
    +    *
    +    * @param valueC The input value.
    +    * @param ctx   The ctx to register timer or get current time
    +    * @param out   The collector for returning result values.
    +    *
    +    */
    +  override def processElement2(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow]): Unit = {
    +
    +    processElement(
    +      valueC,
    +      ctx,
    +      out,
    +      rightStreamWinSize,
    +      timerState2,
    +      row2MapState,
    +      row1MapState,
    +      leftLowerBound,    // left stream upper
    +      leftUpperBound,    // left stream upper
    +      false
    +    )
    +  }
    +
    +  /**
    +    * Called when a processing timer trigger.
    +    * Expire left/right records which earlier than current time - windowsize.
    +    *
    +    * @param timestamp The timestamp of the firing timer.
    +    * @param ctx       The ctx to register timer or get current time
    +    * @param out       The collector for returning result values.
    +    */
    +  override def onTimer(
    +      timestamp: Long,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext,
    +      out: Collector[CRow]): Unit = {
    +
    +    if (timerState1.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        leftStreamWinSize,
    +        row1MapState,
    +        timerState1,
    +        ctx
    +      )
    +    }
    +
    +    if (timerState2.value == timestamp) {
    +      expireOutTimeRow(
    +        timestamp,
    +        rightStreamWinSize,
    +        row2MapState,
    +        timerState2,
    +        ctx
    +      )
    +    }
    +  }
    +
    +  /**
    +    * Puts an element from the input stream into state and search the other state to
    +    * output records meet the condition, and registers a timer for the current record
    +    * if there is no timer at present.
    +    */
    +  private def processElement(
    +      valueC: CRow,
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#Context,
    +      out: Collector[CRow],
    +      winSize: Long,
    +      timerState: ValueState[Long],
    +      rowMapState: MapState[Long, JList[Row]],
    +      oppoRowMapState: MapState[Long, JList[Row]],
    +      oppoLowerBound: Long,
    +      oppoUpperBound: Long,
    +      isLeft: Boolean): Unit = {
    +
    +    cRowWrapper.out = out
    +
    +    val value = valueC.row
    +
    +    val curProcessTime = ctx.timerService.currentProcessingTime
    +    val oppoLowerTime = curProcessTime + oppoLowerBound
    +    val oppoUpperTime = curProcessTime + oppoUpperBound
    +
    +    // only when windowsize != 0, we need to store the element
    +    if (winSize != 0) {
    +      // register a timer to expire the element
    +      if (timerState.value == 0) {
    +        ctx.timerService.registerProcessingTimeTimer(curProcessTime + winSize + 1)
    +        timerState.update(curProcessTime + winSize + 1)
    +      }
    +
    +      var rowList = rowMapState.get(curProcessTime)
    +      if (rowList == null) {
    +        rowList = new util.ArrayList[Row]()
    +      }
    +      rowList.add(value)
    +      rowMapState.put(curProcessTime, rowList)
    +
    +    }
    +
    +    // loop the other stream elements
    +    val oppositeKeyIter = oppoRowMapState.keys().iterator()
    +    while (oppositeKeyIter.hasNext) {
    +      val eleTime = oppositeKeyIter.next()
    +      if (eleTime < oppoLowerTime) {
    +        listToRemove.add(eleTime)
    +      } else if (eleTime <= oppoUpperTime) {
    +        val oppoRowList = oppoRowMapState.get(eleTime)
    +        var i = 0
    +        if (isLeft) {
    +          while (i < oppoRowList.size) {
    +            joinFunction.join(value, oppoRowList.get(i), cRowWrapper)
    +            i += 1
    +          }
    +        } else {
    +          while (i < oppoRowList.size) {
    +            joinFunction.join(oppoRowList.get(i), value, cRowWrapper)
    +            i += 1
    +          }
    +        }
    +      }
    +    }
    +
    +    // expire records out-of-time
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      oppoRowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +  }
    +
    +  /**
    +    * Removes records which are outside the join window from the state.
    +    * Registers a new timer if the state still holds records after the clean-up.
    +    */
    +  private def expireOutTimeRow(
    +      curTime: Long,
    +      winSize: Long,
    +      rowMapState: MapState[Long, JList[Row]],
    +      timerState: ValueState[Long],
    +      ctx: CoProcessFunction[CRow, CRow, CRow]#OnTimerContext): Unit = {
    +
    +    val expiredTime = curTime - winSize
    +    val keyIter = rowMapState.keys().iterator()
    +    var nextTimer: Long = 0
    +    // Search for expired timestamps.
    +    // If we find a non-expired timestamp, remember the timestamp and leave the loop.
    +    // This way we find all expired timestamps if they are sorted without doing a full pass.
    +    while (keyIter.hasNext && nextTimer == 0) {
    +      val recordTime = keyIter.next
    +      if (recordTime < expiredTime) {
    +        listToRemove.add(recordTime)
    +      } else {
    +        nextTimer = recordTime
    +      }
    +    }
    +
    +    // Remove expired records from state
    +    var i = listToRemove.size - 1
    +    while (i >= 0) {
    +      rowMapState.remove(listToRemove.get(i))
    +      i -= 1
    +    }
    +    listToRemove.clear()
    +
    +    // If the state has non-expired timestamps, register a new timer.
    +    // Otherwise clean the complete state for this input.
    +    if (nextTimer != 0) {
    +      ctx.timerService.registerProcessingTimeTimer(nextTimer + winSize + 1)
    --- End diff --
    
    yes, that makes sense to me.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r127178517
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamWindowJoin.scala ---
    @@ -0,0 +1,187 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.{JoinInfo, JoinRelType}
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamQueryConfig, StreamTableEnvironment, TableException}
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{ProcTimeWindowInnerJoin, WindowJoinUtil}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.table.updateutils.UpdateCheckUtils
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamWindowJoin(
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinType: JoinRelType,
    +    leftSchema: RowSchema,
    +    rightSchema: RowSchema,
    +    schema: RowSchema,
    +    isRowTime: Boolean,
    +    leftLowerBound: Long,
    +    leftUpperBound: Long,
    +    remainCondition: Option[RexNode],
    +    ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +    with CommonJoin
    +    with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamWindowJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinCondition,
    +      joinType,
    +      leftSchema,
    +      rightSchema,
    +      schema,
    +      isRowTime,
    +      leftLowerBound,
    +      leftUpperBound,
    +      remainCondition,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +    joinToString(
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    joinExplainTerms(
    +      super.explainTerms(pw),
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def translateToPlan(
    +      tableEnv: StreamTableEnvironment,
    +      queryConfig: StreamQueryConfig): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    val isLeftAppendOnly = UpdateCheckUtils.isAppendOnly(left)
    --- End diff --
    
    the ` UpdateCheckUtils.isAppendOnly` recursively checks if any downstream operator produces updates. As soon as any downstream operator produces updates, the given operator has to be able to handle them. 
    
    Updates can be encodes as retraction or be implicit per key-wise updates if the update producing and receiving operators use the same keys. Retraction updates are encoded as two messages. Non-retraction updates are encoded as single message and require a key to which they relate (`CRow.change == true` -> insert or update per key, `CRow.change == false`  -> delete on key). Right now, only UpsertTableSinks use non-retraction updates, but other operators such as unbounded joins will use it as well.
    
    So even if `AccRetract` is false, the input might produce updates but those updates are differently encoded, i.e., in a single message. The window stream join is not able to handle updates (it ignores the `CRow.change` flag). Therefore, we must ensure that the inputs do not produce updates.


Github user wuchong commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r127192628
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamWindowJoin.scala ---
    @@ -0,0 +1,187 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.{JoinInfo, JoinRelType}
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamQueryConfig, StreamTableEnvironment, TableException}
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{ProcTimeWindowInnerJoin, WindowJoinUtil}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.table.updateutils.UpdateCheckUtils
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamWindowJoin(
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinType: JoinRelType,
    +    leftSchema: RowSchema,
    +    rightSchema: RowSchema,
    +    schema: RowSchema,
    +    isRowTime: Boolean,
    +    leftLowerBound: Long,
    +    leftUpperBound: Long,
    +    remainCondition: Option[RexNode],
    +    ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +    with CommonJoin
    +    with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamWindowJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinCondition,
    +      joinType,
    +      leftSchema,
    +      rightSchema,
    +      schema,
    +      isRowTime,
    +      leftLowerBound,
    +      leftUpperBound,
    +      remainCondition,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +    joinToString(
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    joinExplainTerms(
    +      super.explainTerms(pw),
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def translateToPlan(
    +      tableEnv: StreamTableEnvironment,
    +      queryConfig: StreamQueryConfig): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    val isLeftAppendOnly = UpdateCheckUtils.isAppendOnly(left)
    --- End diff --
    
    Thank you for the explanation, that makes sense to me.  But I find `DataStreamOverAggregate` and `DataStreamGroupWindowAggregate` use `DataStreamRetractionRules.isAccRetract`, is that a misusage.


Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/flink/pull/4266#discussion_r127193974
  
    --- Diff: flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamWindowJoin.scala ---
    @@ -0,0 +1,187 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.flink.table.plan.nodes.datastream
    +
    +import org.apache.calcite.plan._
    +import org.apache.calcite.rel.core.{JoinInfo, JoinRelType}
    +import org.apache.calcite.rel.{BiRel, RelNode, RelWriter}
    +import org.apache.calcite.rex.RexNode
    +import org.apache.flink.api.java.functions.NullByteKeySelector
    +import org.apache.flink.streaming.api.datastream.DataStream
    +import org.apache.flink.table.api.{StreamQueryConfig, StreamTableEnvironment, TableException}
    +import org.apache.flink.table.plan.nodes.CommonJoin
    +import org.apache.flink.table.plan.schema.RowSchema
    +import org.apache.flink.table.runtime.join.{ProcTimeWindowInnerJoin, WindowJoinUtil}
    +import org.apache.flink.table.runtime.types.{CRow, CRowTypeInfo}
    +import org.apache.flink.table.updateutils.UpdateCheckUtils
    +
    +/**
    +  * Flink RelNode which matches along with JoinOperator and its related operations.
    +  */
    +class DataStreamWindowJoin(
    +    cluster: RelOptCluster,
    +    traitSet: RelTraitSet,
    +    leftNode: RelNode,
    +    rightNode: RelNode,
    +    joinCondition: RexNode,
    +    joinType: JoinRelType,
    +    leftSchema: RowSchema,
    +    rightSchema: RowSchema,
    +    schema: RowSchema,
    +    isRowTime: Boolean,
    +    leftLowerBound: Long,
    +    leftUpperBound: Long,
    +    remainCondition: Option[RexNode],
    +    ruleDescription: String)
    +  extends BiRel(cluster, traitSet, leftNode, rightNode)
    +    with CommonJoin
    +    with DataStreamRel {
    +
    +  override def deriveRowType() = schema.logicalType
    +
    +  override def copy(traitSet: RelTraitSet, inputs: java.util.List[RelNode]): RelNode = {
    +    new DataStreamWindowJoin(
    +      cluster,
    +      traitSet,
    +      inputs.get(0),
    +      inputs.get(1),
    +      joinCondition,
    +      joinType,
    +      leftSchema,
    +      rightSchema,
    +      schema,
    +      isRowTime,
    +      leftLowerBound,
    +      leftUpperBound,
    +      remainCondition,
    +      ruleDescription)
    +  }
    +
    +  override def toString: String = {
    +    joinToString(
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def explainTerms(pw: RelWriter): RelWriter = {
    +    joinExplainTerms(
    +      super.explainTerms(pw),
    +      schema.logicalType,
    +      joinCondition,
    +      joinType,
    +      getExpressionString)
    +  }
    +
    +  override def translateToPlan(
    +      tableEnv: StreamTableEnvironment,
    +      queryConfig: StreamQueryConfig): DataStream[CRow] = {
    +
    +    val config = tableEnv.getConfig
    +
    +    val isLeftAppendOnly = UpdateCheckUtils.isAppendOnly(left)
    --- End diff --
    
    Yes, I think you are right. These checks should also check for updates and not retraction mode. 
    
    Maybe it makes sense to integrate the whole append-only/updates check into the decoration rules. Same for the inference of unique keys (the other method in `UpdateCheckUtils`).


GitHub user fhueske opened a pull request:

    https://github.com/apache/flink/pull/4324

    [FLINK-6232] [table] Add processing time window inner join to SQL.

    This is continuation and extension of PR #3715 and #4266 by @hongyuhong.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fhueske/flink table-join

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/4324.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4324
    
----
commit 10b219678f13c0c21889f97f267dcf4c517045e5
Author: hongyuhong <hongyuhong@huawei.com>
Date:   2017-07-06T03:24:04Z

    [FLINK-6232] [table] Add support for processing time inner windowed stream join.

commit 3d671a2d1867aea2f3d4eee30b2772045917d6d4
Author: Fabian Hueske <fhueske@apache.org>
Date:   2017-07-12T22:49:30Z

    [FLINK-6232] [table] Add SQL documentation for time window join.
    
    - Add support for window join predicates in WHERE clause.
    - Refactoring of WindowJoinUtil.
    - Minor refactorings of join classes.

----


Github user fhueske commented on the issue:

    https://github.com/apache/flink/pull/4266
  
    Hi @hongyuhong and @wuchong, I opened a new PR which extends this PR. 
    Please have a look and give feedback.
    
    @hongyuhong can you close the PRs #3715 and this one? 
    
    Thank you, Fabian


Github user hongyuhong closed the pull request at:

    https://github.com/apache/flink/pull/3715


Github user hongyuhong closed the pull request at:

    https://github.com/apache/flink/pull/4266


Github user wuchong commented on the issue:

    https://github.com/apache/flink/pull/4324
  
    The code looks good to me. Good refactoring!
    
    +1 to merge.
    
    BTW, I have a question. This time-windowed join is different with `DataStream.join(...).window(...)`.  The `DataStream.join.window` is joining two streams on a same window (such as 1 hour tumbling window).  Do we have plan to support it? 


Github user fhueske commented on the issue:

    https://github.com/apache/flink/pull/4324
  
    Thanks for the review @wuchong. 
    
    The `DataStream` join semantics can be implemented with this join by first window grouping both inputs with a UDAGG that collects all records in a list, and then joins the lists on the same time with `left.t BETWEEN right.t AND right.t` (we do not support a single `==` predicate on time so we need `>=` and `<=`, but this could be added, IMO). And then a UDF that crosses the lists. 
    
    I think this is how you would need to do it in SQL (you could use UNNEST to do the Cartesian product). The Table API could have a syntactic shortcut for that but the internal logical representation would again be the same. 
    
    Not sure if it adds much value. I think the windowed join semantics of this PR are nicer because they do not have cut-off points at the window boundaries (two records could be only 3 msecs apart but would not join because they are in different tumbling windows).


Github user fhueske commented on the issue:

    https://github.com/apache/flink/pull/4324
  
    Hi @hongyuhong, it would be great if you could have a look as well since this PR is mostly your work.  
    I'll appreciate any feedback.
    
    Thank you, Fabian


Github user hongyuhong commented on the issue:

    https://github.com/apache/flink/pull/4324
  
    Hi @fhueske, sorry for the late feedback. The refactoring looks pretty great to me, i think it's good to be merged. The only thing i worry about is that there is no enough message for the user to figure out the exact error, do we have plan to improve this?
    
    Thanks very much.
    Yuhong


Github user fhueske commented on the issue:

    https://github.com/apache/flink/pull/4324
  
    Hi @hongyuhong, thanks for your feedback!
    I agree, passing good error messages to users is important. However, this is very difficult in the optimizer because the optimizer is exploring different paths. If we throw an exception in the optimizer, we would kill the exploration and miss a valid plan. That's why I'm very hesitant to throw exceptions in rules. However, if the optimizer does not find a valid execution plan, it quits with an error message that tells the user that no valid plan could be found but does not point to a particular problem (such as an invalid join condition). 
    
    A good solution for this is still needed. Maybe we can collect potential problems during optimization and provide these when the translation fails.
    



Github user fhueske commented on the issue:

    https://github.com/apache/flink/pull/4324
  
    Merging.
    
    I'll open a JIRA to discuss how to pass error messages to users.
    
    Thanks @hongyuhong and @wuchong!


Github user asfgit closed the pull request at:

    https://github.com/apache/flink/pull/4324


Implemented for 1.4.0 with ba6c59e6d744958db7332f88ffdd46effc9ad400 and 471345c0ea9930a582786cc08bd290d374b28c5a

