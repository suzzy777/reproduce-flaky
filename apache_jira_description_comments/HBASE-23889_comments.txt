{quote}I see there is a invalidateConnection method in HBaseTestingUtilities but it needs a very big refactoring to make all the UTs work like this.
{quote}
{quote}So here I suggest we switch back to ZKConnectionRegistry by default, and open a new feature branch to finish the TODOs and the refactoring on UTs.
{quote}
AFAICT, I fixed all the unit tests as a part of HBASE-18095. Do you see some test in the master branch that is failing?

Also, I suggest looking at {{AlwaysStandByHMaster}} for your tests, if you want to simulate a no-active-hmaster situation.. {{StartMiniClusterOption.Builder#numAlwaysStandByMasters()}}

What I can see is that, after mering back the feature branch, the flaky list for master branch became really big.

And to be honestly, I do not think it is the correct way to fix all the UTs by calling an invalidateConnection method, which means I need to get a new Connection everytime after a master restarts.

I stand my point with switching back to ZK based registry for now. We need to fnish this TODO

{code}
TODO: Handle changes to the configuration dynamically without having to restart the client.
{code}

And also, remove the invalidateConnection method in HBTU, just reset the property in Configuration to let the client load the new configuration automatically.

Then I'm OK with setting MasterRegistry as default.

Anyway, I can rewrite the UTs so it can pass for now, and then merge HBASE-22514 back, but I still think we should better polish the MasterRegistry before making it default.

I think there are two separate issues here.

bq. after mering back the feature branch, the flaky list for master branch became really big.

I think the flakiness started after HBASE-23779 and related patches. That increased the fork count and that exposed a lot of flakes. I'm happy to dig into failures that you think are caused by the feature branch.

bq. And also, remove the invalidateConnection method in HBTU, just reset the property in Configuration to let the client load the new configuration automatically.

I do see your point, but I think there are some finer issues here.

1. This pattern of picking random ports after every role restart is very specific to the unit-tests to avoid port conflicts. This is not reflective of a real world usage pattern. Because every restart picks a totally different <port>, the older master:<port> becomes useless. In a normal world where restarts happen (and the master:<port> remains the same), the connections are resilient. 

2. Getting rid of "invalidateConnection()" is an enormous task (like you already noted). Our HBTU usage is pretty erratic and all the tests directly operate on the underlying LocalHBaseCluster. If you see my comment in the code.. This pretty much means that one needs to rewrite most of the tests written over the past 10 (?) years. 

{noformat}
* TODO: There should be a more coherent way of doing this. Unfortunately the way tests are
   *   written, not all start() stop() calls go through this class. Most tests directly operate on
   *   the underlying mini/local hbase cluster. That makes it difficult for this wrapper class to
   *   maintain the connection state automatically. Cleaning this is a much bigger refactor.
{noformat}

3. The whole premise of the feature is that the new bootstrap set of nodes for clients is the HMaster group. Earlier ZK had this role. This means that client expects the masters to be generally available to perform basic operations.  This resulted in outdated test scenarios (like test-connection-when-cluster-is-not-up etc). This is the reason, we have some test utilities like {{AlwaysStandByHMaster}} to help transition the tests into the newer world.

That said, I still agree with you that having dynamic reconfiguration on the client side will nice-to-have feature, but IMHO that alone shouldn't be the reason to switch the default registry back to ZK based. WDYT?

In our deployment, we put zookeeper behind a lvs, so it is free for us to change the machine for zookeeper, and use do not need to change their configurations when zookeeper machines are changed.

But for HMaster, it is another story. It is not easy to put them behind a lvs, as we have builtin active and backup master logic in the code of hbase-client, not all masters are the same. So if we want to change the machine of a master deploy, changing the configuration of all the clients are already a big problem but anyway using a central configuration system can partly solve it. But if this requires all client to restart then this will be a very very big problem.

Anyway, this has to be done before 3.0.0 release. If we can not finish the feature before 3.0.0 release, we have to switch back to zk based registry.

Fair point, let me think of a way to fix this. Out of curiosity (and for some data points), how often do you change / recycle masters?

