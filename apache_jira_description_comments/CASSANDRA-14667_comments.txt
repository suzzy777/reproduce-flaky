I am trying to run a build here https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2380/

[~smiklosovic] Thank you for helping with this, waiting for the build results.

It would be cool if we also tested that this version also runs flawlessly on Java 17.

If I recall correctly, I was told long ago that this will be a breaking change for tools, but I was not provided any other details so you might want to investigate or even ask on the mailing list. 

Sure, this is good notice. I have found that version 3.1.5 is no longer maintained by Dropwizard, so it might cause some problems for us with JDK 17.

The 4.2.17 seems to be OK according to the build matrix, so I decided to raise the PR with an upgrade
https://github.com/dropwizard/metrics/blob/release/4.2.x/.github/workflows/maven.yml#L16



{quote}Sure, this is good notice. I have found that version 3.1.5 is no longer maintained by Dropwizard, so it might cause some problems for us with JDK 17.
{quote}
At least in our CI I do not see any related failures but I agree that it is a good call. Thank you!

Now everyone can already test very easily with JDK 17 in CircleCI after CASSANDRA-18247 got committed.

The CI run from the other day is available here [#2321|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=18247-trunk-v2] and the current failures are described in the CASSANDRA-16895 description with attached respective tickets. Nothing strikes me to be related but this doesn't mean there may be submarines...

 

a lot of failures https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2380/#showFailuresLink

like this 
{code}
java.lang.NoClassDefFoundError: com/codahale/metrics/JmxReporter
	at com.datastax.driver.core.Metrics.<init>(Metrics.java:146)
	at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1667)
	at com.datastax.driver.core.Cluster.init(Cluster.java:214)
	at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:387)
	at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:366)
	at com.datastax.driver.core.Cluster.connect(Cluster.java:311)
	at org.apache.cassandra.metrics.TrieMemtableMetricsTest.setup(TrieMemtableMetricsTest.java:87)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.lang.ClassNotFoundException: com.codahale.metrics.JmxReporter
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
{code}

Something is not right ...

Ahaa I know what is going on ... the Java driver we use for tests itself uses Dropwizard of a different version we just stopped to use and imports are different suddenly.

driver we are using, of version 3.11.0, uses  <metrics.version>3.2.2</metrics.version> in https://central.sonatype.com/artifact/com.datastax.cassandra/cassandra-driver-parent/3.11.0

This parent https://central.sonatype.com/artifact/com.datastax.oss/java-driver-parent/4.15.0 uses metrics <metrics.version>4.1.18</metrics.version>

However, what we could do is to do this:

{code}
        try (com.datastax.driver.core.Cluster c = com.datastax.driver.core.Cluster.builder()
                                                                                  .withoutMetrics()
                                                                                  .addContactPoint("127.0.0.1").build();
{code}

Do you see that ".withoutMetrics()" method? Once there, the test passes (e.g. tests extending AbstractNetstatsStreaming, modified populateData and changeReplicationFactor methods).

So, we either put this "withoutMetrics()" everywhere where we are building Cluster, or we update the project to use Cassandra driver 4 (that is pretty challenging!)

I am not completely sure if we use these "metrics" via driver, I do not think so but should be double checked. I think that for the sake of tests we do not need them at all.

The draft where I tried to fix it is here (1). [~mmuzaf] feel free to take and build on top of this. Build is being run here (2) but today Jenkins is somehow sluggish, lets see what it builds if anything.

I noticed that this would need to be turned off for tooling as well. E.g. FQL replayer is doing some CQL stuff so we would need to turn this off explicitly there as well. 

JavaDriverClient used for Stress tool uses "withoutMetrics()" already. This seems like a known issue we can circumvent like this for tools.

One consequence of putting Cluster.builder().withoutMetrics() everywhere is that people would need to set it like that every single time they want to build Cluster. This is quite error-prone. One solution would be to introduce a checkstyle rule that "Cluster.builder()" can be called only in some helper method, nowhere else (plus some toolings) and that helper method would contain "Cluster.builder().withoutMetrics()" on it. The result is that people would be forced to use this method where "withoutMetrics" is so they could not miss that.

(1) https://github.com/apache/cassandra/pull/2240
(2) https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2383/

https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2389/

[~e.dimitrova] [~mmuzaf] [~brandonwilliams] any feedback on my last comment here? This relates to CASSANDRA-15472. I think that bumping Dropwizard to 4.x for Cassandra 5.0 would be ideal.

Bumping Dropwizard to 4.x is significantly easier than bumping Datastax Java Driver to 4. Bumping Dropwizard to 4 does not prevent us from bumping the driver later on as driver of 4 already includes Dropwizard 4.x.

{quote}I am not completely sure if we use these "metrics" via driver, I do not think so but should be double checked. I think that for the sake of tests we do not need them at all.
{quote}
[https://docs.datastax.com/en/developer/java-driver/3.11/manual/metrics/]

We might not need them in our tests but it appears to me by adding everywhere *withoutMetrics()* we just cover a problem, no?

[~blerer] do I recall correctly you mentioned once you looked into this dropwizard upgrade? Any thoughts?

What is there to cover if we do not use it. If you look closely they are really used almost exclusively in the tests. JavaDriverClient used for Stress tool uses "withoutMetrics()" already btw so this seems like pretty innocent change.

Hello everyone,

I have researched the problem and found a few options that might also work (none of which are good). Firstly, the problem with just bumping up the Dropwizard version is that the {{JmxPerporter}} class has moved from the {{com.codahale.metrics}} package in the 3.x version to the {{com.codahale.metrics.*jmx*}} package in the 4.x version. We can remove the dependency of the Cassandra itself from the {{JmxPerporter}} class (I've done this in the PR), but the cassandra-driver-core 3.1.1 (used in tests now) requires this class in the classpath to run and as the class has changed the package we are seeing the ClassNotFoundException.

So what we can do here:

# Stefan has mentioned adding the _withoutMetrics()_, this will solve the problem, I guess this limits us to using metrics in tests until the driver is updated;
# I've linked the problems to the driver upgrade: CASSANDRA-15750, CASSANDRA-17231. This will allow us to easily update the Dropwizard right after the driver version changes;
# We can use the maven-shade-plugin to relocate classes from the metrics-jmx (the {{JmxPerporter}} has been moved to it) from com.codahale.metrics.*jmx* to the com.codahale.metrics package. This copies the classes bytecode into a new package and create an additional jar. As these classes are fully compatible (only the package has changed), they can be used by the 3.1.1 driver from the classpath as they are now used. Drawbacks: maven is used for the build, I haven't found a corresponding ant for it;
# I've been thinking that as the Dropwizard has the Apache 2.0 licences, we can copy the {{JmxPerporter}} to our tests (and to the classpath accordingly) with the _right_ package, so that the required classes will be in the classpath to run our tests with the old driver until the driver is updated.

I think the first option is our best choice, right?
Thoughts?


 

Option 4) would work too imho.

So this would make metrics functional in tests? If it is just about copying a class and moving it to appropriate package, that seems to me like the least invasive change plus metrics would work too. That is better than metrics not working and putting withoutMetrics everywhere.

What is the cost of that custom class (classes) in the repo from the maintenance perspective? Just copy&forget?


Sent from ProtonMail mobile



\

[~smiklosovic] 

Yes. I haven't tested it for all cases except the distributed/stress tests, but option #4 works. We will need to remove these classes as soon as we update the driver version to 4.x, at no extra cost before then. 
If this is OK, I can submit the patch and check the CI results to check the rest of the cases.

Let's give it a shot! I guess we need to see this first.

The approach mentioned about works perfectly, here are the tests results:
https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2506/
https://butler.cassandra.apache.org/?#/ci/upstream/compare/Cassandra-trunk/cassandra-14667

I have updated the PR with the changes we discussed.

Looks great! [~e.dimitrova]  wdyt?

Hey, thank you both!

I can try to take a look into this next week

[~e.dimitrova]  any progress by any chance?

Just one minute ago, I texted [~mmuzaf] in Slack I am going to look into this today or tomorrow :) 

telepathy!

So I did not get to the code, but I did some digging around. This ticket was not completed initially, as the tools in the ecosystem were not yet upgraded to Dropwizard 4.

This was long ago, so we should go to the @user and @dev ML with a call for feedback on where we are. We can then move on depending on the input gathered. WDYT? [~mmuzaf] and [~smiklosovic] - anyone of you wants to drive this discussion and figure out the current state of the ecosystem? I guess enough time passed, and we can do it with the major release, but it will be nice to make some noise and give an early heads-up to the community if this will break many. 

[~e.dimitrova] Do you have the list of tools to look at before we go to the dev/user lists to get feedback on this change? If some of these tools share a classpath with cassandra's lib directory, it might be a problem and we'll have to upgrade them too, but if not - they already provide the dropwizard metrics library themselves and it doesn't affect this change. The latter is more relevant for us though.

{quote}[~e.dimitrova] Do you have the list of tools to look at before we go to the dev/user lists to get feedback on this change?
{quote}
Unfortunately not. I also asked for exact tools but did not get a particular answer. One more reason why it is good to double-check If someone knows more I guess

The issue update.

I have changed the version from 4.2.17 to *4.2.19* and renamed the variable name to *metrics.version* in accordance with the documentation.
https://metrics.dropwizard.io/4.2.0/getting-started.html



I have scheduled the CI:
https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2530/

[~e.dimitrova] , [~smiklosovic] 



I have analysed the test results and found that copying JmxReporter (and associated classes) as I suggested earlier is not sufficient and only covers the subset of the problems. The following tools use the driver and  in turn, initialise the Cassandra's Dwopwizard library is upgraded to 4.x:
- BulkLoader (NativeSSTableLoaderClient)
- FQLQueryLogTool (QueryReplayer)
- Stress (JavaDriverClient) - already have withoutMetrics() https://github.com/jasonwee/cassandra/commit/0d0acac6c59d3fa703a3d504f9cfd063e4d111b7

So, the complete solution is to use withoutMetrics() for the other tools as well. This should be safe as these tools by themselves are not designed to fetch the metrics. The PR was updated accordingly.


Although we can move forward with the patch, I still have some concerns for users who want to use the 3.x java-driver with Cassandra 5.x and are likely to encounter the same (if the patch is accepted). To remove these concerns I have prepared a small patch to the cassandra-java-driver project that includes the dropwizard metrics as shaded classes into the jar (the approach is similar to the netty). The patch is here and of the 3.x only:
https://github.com/datastax/java-driver/pull/1685

I have tested locally, so it works without the `withoutMetrics()` or copying the JMXReporter classes like a charm. I haven't tested it in the CI. In this case the plan might be:
- release java-driver 3.11.4
- update the cassandra's dependency up to 3.11.4
- upgrade the dropwziard metrics library to 4.2.19 without doing anything else;

The problem here is that the 3.x driver is likely to become obsolete alongside the 5.x release, as the release policy [1] only supports 2 LTS releases. This may not be related to the driver itself, and it may live on for a while and if so it is worth considering to release 3.11.4 as the changes are related to the driver build only.

WDYT?


[1] https://cwiki.apache.org/confluence/display/CASSANDRA/Patching%2C+versioning%2C+and+LTS+releases

[~e.dimitrova], [~smiklosovic] Do we have a chance of getting this into 5.0? wdyt?

The solution for incorporating changes into the java-driver is here:
https://github.com/datastax/java-driver/pull/1685

{quote}To remove these concerns I have prepared a small patch to the cassandra-java-driver project that includes the dropwizard metrics as shaded classes into the jar (the approach is similar to the netty). The patch is here and of the 3.x only:
[https://github.com/datastax/java-driver/pull/1685]
{quote}
 

I am unsure about the drivers, I haven't worked on them, and I am also unsure about releases. Not sure what to tell you, honestly.

[~absurdfarce] , any thoughts here?

Hey [~e.dimitrova] , I just finished up some other work and started looking at [~mmuzaf] 's PR today.  I'll try to get some feedback on there in the next few days or so.

Thanks, [~absurdfarce] !

[~absurdfarce] Thanks, I look forward to your comments :-)

Comments are on the PR itself, but the gist is that we're looking at accepting this shading approach for the short-term and aiming to upgrade dropwizard-metrics inline (and remove the shading) in the longer term.  That question pretty quickly leads to a conversation about Java version compatibility with 3.x, and given that that conversation is far from clear at the moment it seemed easier to go with the shaded approach for now and unblock work on 5.x.

[~absurdfarce] thank you for the summary. So the first step is to wait for 3.11.5 to be released, and then I need to update the Cassandra-related part of the PR right after that, right? Or is some kind of release preparation help needed on my part?

Nope, you have it right [~mmuzaf] .  We need to get 3.11.5 out and then this PR can be updated.  I'll post an update here once I have a better idea of timelines for a 3.11.5 release but we'll try to make it soon-ish.

 

Also, for the record: [JAVA-3114|https://datastax-oss.atlassian.net/browse/JAVA-3114] is tracking the changes necessary on the driver side.  At the moment this is mostly a placeholder for [~mmuzaf] 's PR.

I created a general discussion thread as the community is about to vote for a 5.0 alpha version, and there were different opinions around dependency management in an alpha release. https://lists.apache.org/thread/pdq1y2xks629lh6h7nfc6mt5gp145bs6
Also, how much effort would it be to update the driver? [~mmuzaf], did you try to bump it?

[~e.dimitrova] I checkouted the java-driver repo, prepared a new driver version with the above changes, and tested everything locally. I was not able to run all of the dtest on CI, but the tests that I saw failed due to missing classes in the classpath, they started to pass. 

Changes here (without the driver upgrade) also work, but with shading metrics in the driver itself we don't need to copy the required classes from the metrics library and 3.11.5 will reduce the cassandra-related patch up to ~10-20 lines of code (if we have 3.11.5 we won't need to copy missing classes and won't need to use withoutMetrics()) which seems to be the correct way to go.
https://github.com/apache/cassandra/pull/2238

Hi folks. Just popping in to let you know [3.11.5 with dropwizard shading is now available|https://groups.google.com/a/lists.datastax.com/g/java-driver-user/c/mEA0R3Heauo/m/7wYtdBcLBAAJ], let us know if you have any issues with the new version! Thanks, Henry

great! [~mmuzaf]  I think we are unblocked here, right?

The PR has been updated to reflect the latest changes:
https://github.com/apache/cassandra/pull/2238/files

I'll try to verify the patch on CI.

{quote}The PR has been updated to reflect the latest changes:
[https://github.com/apache/cassandra/pull/2238/files]

I'll try to verify the patch on CI.
{quote}
Thanks! I think we need to run all possible tests here pre-commit, including the costly Python upgrade tests. Let me know if you need help with that

[~e.dimitrova] I was thinking of running test suites one by one on my free CircleCi, but I would really appreciate it if you could run them all at once, so I kindly ask you for help :-)

Unfortunately, even on a one-by-one job basis, you cannot successfully run all Python tests with the free tier.

Full CI started here for the 5.0 branch - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=14667-5.0]

On success I will push also trunk

So, I've checked the results. The good news is that our approach to shade the dropwizard metrics in the java driver itself works. All the dtests that were previously failed now pass and in general everything looks good (the {{test_failing_manual_rebuild_index}}  is here CASSANDRA-18361 ). The bad news is that there were some unit tests that had a hardcoded version of the java driver we were using, so a tiny fix was needed to make them work. I've pushed the fix to the branch 

I was thinking that we could read the driver version from the pom and push the version to junit tests, when realized that it increases the complexity of the version upgrade patch and it is probably better to do this in another issue if we want to.



testForcedNormalRepairWithOneNodeDown - known from CASSANDRA-18440
test_failing_manual_rebuild_index - CASSANDRA-18361
j11_cqlsh_dtests_py311_offheap - something environmental, restarted here - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2500/workflows/2cfb1ffc-ff09-4012-a64c-d4368d769ddf/jobs/40033]
test_bulk_round_trip_with_single_core - This is something we want to open a ticket for. It doesn't seem related, and Butler shows only recent timeouts for it, but this one is assertion failure. It's good to spin it in a loop and see what happens(which branches affect, potentially bisect it, etc).
j17_cqlsh_dtests_py311_vnode - needs restart - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2500/workflows/cc5a39bb-353b-46ba-aae1-ea395c7fcf85]

Thanks for fixing the two unit tests. We don't need to run full CI again because of them. I checked the code and ran them locally. 
I think we need a second committer and trunk CI run. [~adelapena], could you look at this one?

I only have one question - we are pulling logback as an additional dependency. [~mmuzaf] , can you check this, please? Was this expected? Is it already presented and we update it with dropwizard?
{code:java}
[resolver:resolve] Downloading https://repo1.maven.org/maven2/ch/qos/logback/logback-core/1.2.12/logback-core-1.2.12.pom
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/ch/qos/logback/logback-core/1.2.12/logback-core-1.2.12.pom (4 KB at 103.8 KB/sec)
[resolver:resolve] Downloading https://repo1.maven.org/maven2/ch/qos/logback/logback-parent/1.2.12/logback-parent-1.2.12.pom
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/ch/qos/logback/logback-parent/1.2.12/logback-parent-1.2.12.pom (19 KB at 442.7 KB/sec)
[resolver:resolve] Downloading https://repo1.maven.org/maven2/ch/qos/logback/logback-classic/1.2.12/logback-classic-1.2.12.pom
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/ch/qos/logback/logback-classic/1.2.12/logback-classic-1.2.12.pom (10 KB at 235.0 KB/sec)
{code}
Sometimes, we exclude transitive dependencies. Do we need something like that here or not? Please check it
{quote}I was thinking that we could read the driver version from the pom and push the version to junit tests, when realized that it increases the complexity of the version upgrade patch and it is probably better to do this in another issue if we want to.
{quote}
Agreed, let's do this in a follow up ticket.

We are already depending on 1.2.9 in 5.0-alpha1. of logback-core and logback-classic (that is at least what mvn dependency:tree says). We should either bump the version in Cassandra to 1.2.12 and exclude these or we just exclude them and we will go with what we ship - 1.2.9. It is probably safer to update to 1.2.12 so dropwizard gets the dependencies it wants.

Thanks [~smiklosovic] !
{quote}It is probably safer to update to 1.2.12 so dropwizard gets the dependencies it wants.
{quote}
+1 on updating it and excluding for dropwizard

[~e.dimitrova], [~smiklosovic] The PR has been updated.

I think you're both right and we should exclude transitive dependency and update the version. I'm seeing the same behavior for the slf4j-api dependency, I've excluded it as well from dropwizard transitive dependencies, but should we update the version up to the latest? I think it's better to do this in another ticket, but I'd like to hear your thoughts here as well. wdyt?

Preferrably in another ticket for better transparency.

I dont know if we want the latest here. If I was paranoid I would say we dont know if the latest works with Dropwizard.
\

{quote}Preferrably in another ticket for better transparency.

I dont know if we want the latest here. If I was paranoid I would say we dont know if the latest works with Dropwizard.
{quote}
In the past, we handled it in the ticket that triggered the need. In this case, my understanding is that we explicitly get an older version and dropwizard asks for a newer one but probably not the latest one?
{quote}we dont know if the latest works with Dropwizard.
{quote}
Let's test. What if we upgrade in another ticket and then dropwizard doesn't work with the latest version we updated to?

An example of a similar case handled in the past is CASSANDRA-18049. There was also an ML discussion thread.

{quote}In this case, my understanding is that we explicitly get an older version and dropwizard asks for a newer one but probably not the latest one?{quote}

We are using 1.7.25, the dropwizard provides 1.7.36, since this is the API module, it should not affect anything. So it must be safe for us to rely on a different fix version. Here is the full changelog for the slfj library that seems to be doesn't contain any API changes between these versions. Hence I though we could change the version of slfj-api later.
https://www.slf4j.org/news.html


1.7.25 is version of slf4j-api, log4j-over-slf4j and jcl-over-slf4j.

1.2.9 is version of logback-core and logback-classic. Is not that a different thing?

TBH I feel a little bit uneasy about bumping 1.2.9 to 1.4.11 (latest stuff for logback-core afaik). I think it is way more easier to just bump a patch version from 1.2.9 to 1.2.12 rather than "testing if dropwizard works with 1.4.11". What does even "to test" mean in this context?

We can contemplate about bumping it to 1.4.11 some other time, let's just bump it to 1.2.11 so Dropwizard do not pull that and that's it.

And you know what ... yes, lets just bump 1.2.9 -> 1.2.12 directly as part of this ticket. Patch release is really a detail.

Bump to the latest patch version as part of this ticket SGTM. 1.2.9 -> 1.2.12 Still, we need to check the change log. 

Then, exclude from dropwizard and use the explicitly downloaded version now.
{quote}We are using 1.7.25, the dropwizard provides 1.7.36, since this is the API module, it should not affect anything.
{quote}
I am not following. I did not see dropwizard pulling a new version of slf4j? Which pom are you looking at?

 

{quote}I am not following. I did not see dropwizard pulling a new version of slf4j? Which pom are you looking at?{quote}

As I understand it, the dependency only needs to be downloaded once and then it lives in the .m2/repository cache directory, so it doesn't need to be downloaded from scratch every time. So, the most important thing for us to look at is the presence of a library in the build/jar directory. This directory defines the application classpath we use (correct me if I'm wrong). 

If we take a step back and have a closer look at the build/jar directory, there is only one logback-core library of version 1.2.9 and there is no 1.2.12 (which was transitevly inherited from the dropwizard library). I don't know why the original concerns were raised, but as I can see from the block of code you posted it only downloads the pom files required to resolve the dropwizard dependency description itself. There is still no 1.2.12 jar in the resulting classpath, which seems correct to me. 

I'm not arguing against the 1.2.9 -> 1.2.12 upgrade as that seems to be more natural to me as well, but by the same logic we used  applied with adding an exclusion for the logback library to the dropwizard dependency I have also raised some concerns about adding an exclusion (and upgrade as well) of the slf4j-api dependency to the dropwizard. For example, the same exclusion was previously added for the ohc here:
https://github.com/apache/cassandra/blob/trunk/.build/parent-pom-template.xml#L965

No matter how we are currently manage dependencies, there are no duplicates in the resulting classpath. 

{quote} Which pom are you looking at?{quote}

The metrics-core has a dependency on slf4j-api (1.7.36):
https://github.com/dropwizard/metrics/blob/release/4.2.x/metrics-core/pom.xml#L34

The metrics-jvm has a dependency on slf4j-api (1.7.36):
https://github.com/dropwizard/metrics/blob/release/4.2.x/metrics-jvm/pom.xml#L45

In the example in the above message the ohc has a dependency on slf4j-api (1.7.32) that is excluded in cassandra-parent pom:
https://github.com/snazy/ohc/blob/develop/pom.xml#L137
https://github.com/apache/cassandra/blob/trunk/.build/parent-pom-template.xml#L965

But all of them do not exist in the resulting classpath in the build/jar no matter if they are excluded or not, according to my local checks.

You wrote:

I can see from the block of code you posted it only downloads the pom files required to resolve the dropwizard dependency description itself. There is still no 1.2.12 jar in the resulting classpath, which seems correct to me.

This is correct, because of this: (I took that from Maven documentation).

Dependency mediation - this determines what version of an artifact will be chosen when multiple versions are encountered as dependencies. Maven picks the "nearest definition". That is, it uses the version of the closest dependency to your project in the tree of dependencies. You can always guarantee a version by declaring it explicitly in your project's POM.

We explicitly added dependencies into our POM so that means even there is a dependency which depends on a newer version, it will not be used because our dependency is "closer" to the root in the hierarchy.

So we see it is downloaded but it is not used.

However, we see that Dropwizard uses 1.2.12 and we nailed it down to 1.2.9. Why not to make Dropwizard happy?

Sent from ProtonMail mobile



\

For that ohc and exlusion. I think the reason behind that is that if somebody adds cassandra-all to their project as a dependency than without doing anything they would automatically depend on that.

If they added cassandra-all as the first dependency in their pom and they specified a dependency which would depend on that, it would be on the same depth level and in that case Maven would use the dependency version, believe or not, which is declared "first" in our pom ...

"Note that if two dependency versions are at the same depth in the dependency tree, the first declaration wins."


Sent from ProtonMail mobile



\

If all the dependencies are managed in accordance with the way Maven does it everything should be fine. We use ant + maven, and I haven't seen this piece of code before, so I wasn't sure that everything works this way. So, the 1.2.12 pom was downloaded to resolve the library description and the 1.2.9 version is in the resulting classpath.

Anyway, changes that will hopefully make us and the dropwizard happy are in the PR :-)
https://github.com/apache/cassandra/pull/2238/files

I think we do not need to exclude dependencies for metrics-logback. Because we already depend on logback-core and logback-classic in that pom.xml. So exclusion does virtually nothing if we depend on that anyway. We just aligned the versions.

{quote}I think we do not need to exclude dependencies for metrics-logback. Because we already depend on logback-core and logback-classic in that pom.xml. So exclusion does virtually nothing if we depend on that anyway. We just aligned the versions.
{quote}
My understanding was that even if we alingn versions, we would add the exclusions for readability. That makes it more obvious that we rely on specific version that is assigned.

But I might be wrong. [~mck] , [~aratnofsky] - is there a rule for that? Asking as you were reworking the dependency management in 5.0. 

I am not sure I have ever encountered this "code style" elsewhere. Nothing indicates that exclusions are used for readability (1). As I understand that, that is used solely and primarily for excluding unwanted dependencies on purpose. 

As we are effectively not excluding anything as it is on the classpath anyway, maybe just putting a comment that logback-core and logback-classic "shadow" the respective dependecies in metrics-logback would be just enough.

If we bumped the version of metric-logback in the future which would use newer versions of these logback libraries, the older libraries declared directly in the pom would be used because of the rule that "closer dep to the root wins". 

On the other hand, if we bumped the versions of the dependencies in pom.xml directly, based on that rule metrics-logback would use newer ones. As long as we use patch releases it should all just play together.

In neither case there is any reason we should exclude it. A comment is just enough.

Or maybe there is some hidden meaning here which I am not aware of.

https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html

Take care that the ant resolver plugin has some bugs in it and does not resolve transitive dependencies (and their versions) the same way as maven does. ( Yeah :( "closer dep to the root wins" doesn't always work here.)

Trying to follow the conversation above, 
 - we should exclude logging implementation transitive dependencies, and only declare them directly,
 - we should not exclude logging api transitive dependencies, as those direct dependencies that use them would break if the other transitive dependency path is later removed,
 
No dependency should be dictating what logging implementation to use if its using a logging api (abstraction layer). i.e. folk upstream are not always doing their pom correctly.

Thanks, [~mck]!

To be clear on my understanding - we need to do the exclusion and explicit addition because resolver might not resolve things the way we would expect it to (as [~smiklosovic] explained that mvn would do)

 

With the latest changes, tests that previously failed now pass.

https://app.circleci.com/pipelines/github/Mmuzaf/cassandra/355/workflows/f4ff68a6-daf4-43ec-b12f-7b25f6582e5a/jobs/26762
https://app.circleci.com/pipelines/github/Mmuzaf/cassandra/355/workflows/f4ff68a6-daf4-43ec-b12f-7b25f6582e5a/jobs/26758
https://app.circleci.com/pipelines/github/Mmuzaf/cassandra/355/workflows/f4ff68a6-daf4-43ec-b12f-7b25f6582e5a/jobs/26759

I have no technical arguments for choosing one option over the other. For me, it will be better to fix the transitive dependency resolver process when an error occurs than to exclude dependencies in advance. However, since we already have such an example of exclusion in the source code we can include a similar one for these changes. 

[~smiklosovic] are you OK with it taking into account the last comments or do you still think the exclusion is a blocker for the patch itself?

Sure let's exclude that, better be safe than sorry. My comments were written with a proper Maven resolver in mind. I have not expected we do not have the same thing here. It would be still worth to investigate where it diverges concretely from Maven on some examples but that is not the goal of this ticket. It would be great to have a clear understanding how the resolver we use behaves in various scenarios.

j17_utests_system_keyspace_directory has also passed.
https://app.circleci.com/pipelines/github/Mmuzaf/cassandra/355/workflows/f4ff68a6-daf4-43ec-b12f-7b25f6582e5a/jobs/26967

I have checked the fixes between 1.2.9 -> 1.2.12 for the logback and found no suspicious updates for us. 
https://logback.qos.ch/news.html

[~e.dimitrova] would you mind running the full CI to make sure that our exclusion doesn't cause any errors? I think this is the last thing we have to check before going further.

RP trunk:
https://github.com/apache/cassandra/pull/2238

RP 5.0:
https://github.com/apache/cassandra/pull/2705

I have a question: does anyone know why we opted in for JmxReporter at the first place in NodeProbe - I believe the usage of JmxReporter was added in CASSANDRA-14523. Why was it not used CassandraMetricsRegistry as we do [now|https://github.com/apache/cassandra/pull/2238/files#diff-92f8748902f03f37a4f7db56b4dfb7d226adcf3839141e6adb8ebbc575020d57R1835]? It is also done everywhere else in NodeProbe. [~cnlwsu] ?
{quote}Better be safe than sorry.
{quote}
+1
{quote}ant resolver plugin has some bugs in it 
{quote}
[~mck] , do you have links? I agree with [~smiklosovic]  and [~mmuzaf]  that it is good to follow up on that. At least to be sure we do things correctly. But that is, of course, not a blocker for this ticket. Let's collect the info we already have and follow up on this.

I also reviewed the logback release notes (Thank you for the link!!). The only breaking change I saw was regarding DBAPPENDER, which we do not use. (Do we?) I saw RollingFileAppender, ConsoleAppender, AsyncAppender, InstrumentedAppender, FileAppender.

*Question:* Do we need to highlight anything specific in the NEWS.txt regarding the new dropwizard version, or just the CHANGES.txt entry that we bumped the version is enough? 

For completeness, below is the changelog for the drivers:

[https://github.com/datastax/java-driver/tree/3.x/changelog]

I noticed we exclude with the driver - netty-buffer, netty-codec, netty-handler, netty-transport and a few other dependencies. [~smiklosovic], you've been working on some new exclusions for netty after the last upgrade; anything else we need to do here with the driver in that regard? 

I will push the full CI when we confirm what we do with slf4j-api and the drivers' exclusions. I want to run the full test suite with upgrade tests, etc., and I would like to prevent doing it 4 instead of only 2 times (5.0 and trunk). 

bq. ant resolver plugin has some bugs in it 

https://issues.apache.org/jira/browse/CASSANDRA-18049?focusedCommentId=17706782&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17706782

[~e.dimitrova] I didn't see a clear agreement to update the slf4j-api library, so I didn't include it in the patch.

For me, the there are a few reasons for not doing this in this patch: 
- The slf4j-api dependency is important in its own right and deserves its own patch, in case we have to revert the following patch due to some undiscovered problem and/or slf4j still needs to be preserved;
- The slf4j-api module is a module that provides a public API, and according to the slf4j release policy only changes between the major releases, even if you still see the patch versions it is released as a maven module without any changes and nothing more. For example, the latest change related to this module itself is change c6c7a98a7ee1e71382348ae3e41ff7818dfafd65 dated Dec 16, 2019 despite the latest 1.7.36 version is released on Feb 8, 2022.
- We've already removed this library from the transitive dependencies for the dropwizrd library.

In summary, including the slf4j-api version update is not required for this patch. The API that is provided by slf4j and that is used by Cassandra is the same and it only overcomplicates the overall solution by causing unnecessary questions for those developers who will be faced with this patch later. 

So I suggest proceeding with this patch without the slf4j-api alongside the update and if we still want to - create another issue for it, but for the same reasons that were mentioned above there is no need for the 1.7.24 > 1.7.36 upgrade as it gives us nothing.

Actually, there are some commits that I have missed for the slf4j-api
- [SLF4J-469|https://www.mail-archive.com/slf4j-dev@qos.ch/msg02569.html] Potential memory leaks if there is no underlying implementation
- [SLF4j-466|https://www.mail-archive.com/slf4j-dev@qos.ch/msg02499.html] Add test for all happy flow cases
- [SLF4J-460|https://www.mail-archive.com/search?l=slf4j-dev@qos.ch&q=subject:%22%5C%5Bslf4j%5C-dev%5C%5D+%5C%5BJIRA%5C%5D+%5C%28SLF4J%5C-460%5C%29+EventRecodingLogger+debug+logs+as%09TRACE%22&o=newest&f=1)] EventRecodingLogger debug logs as TRACE

These last three issues were fixed in 1.7.30 and no changes have been applied since then. So, I think we can create another issue to do the update 1.7.24 > 1.7.36.

Thank you, [~mmuzaf] 
{quote}So, I think we can create another issue to do the update 1.7.24 > 1.7.36.
{quote}
I agree with you, even low priority one 

 

The slf4j upgdate is here:
https://issues.apache.org/jira/browse/CASSANDRA-18882

18882 is done.

I think I need to answer some questions around java driver dependencies / exclusions. I will return to this shortly, hopefully. 

I do not see anything wrong with java driver deps. We are excluding the ones there which are brought to the class path by netty-all dependency (minus its own exclusions there) which is correct.

[~e.dimitrova]  I think you might proceed with the final builds.

{quote}I do not see anything wrong with java driver deps. We are excluding the ones there which are brought to the class path by netty-all dependency (minus its own exclusions there) which is correct.

[~e.dimitrova]  I think you might proceed with the final builds.
{quote}

I will do the builds shortly, thanks for double-checking the dependencies! Appreciate it! 


Rebased and pushed CI runs, still running:
https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=C14667-5.0 
https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=C14667-trunk

[~mmuzaf], please double-check if I missed anything, but I do not think so. 

5.0: test_decommission - test failure, timeout; I think it was just because we fired all possible tests; even CircleCI cannot catch up at times. The test doesn't fail locally for me. 
trunk: LGTM; one of the containers is marked in red, but:


{code:java}
===End Flaky Test Report===
============ 23 passed, 3 skipped, 4 warnings in 3141.13s (0:52:21) ============

Too long with no output (exceeded 15m0s): context deadline exceeded
{code}

I've seen this before in CircleCI, but it has nothing to do with the tests themselves.

+1



Thanks, 

The same changes have been passed before, so I think the two test timeout failures are some kind of CircleCI related issues. For example, the most valuable dtest for us is here:
[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2500/workflows/2cfb1ffc-ff09-4012-a64c-d4368d769ddf/jobs/39589]

I was searching for the Cassandra Jira issues to resolve the right cause of the timeout but failed to find any.

+1, I think we can ship this!

I would like to say thank you to ALL of you for the collaboration we have had. I am happy finally to see these changes merged into the upcoming release so that users will no longer face compatibility issues. Again, thank you for your time, especially [~smiklosovic], [~e.dimitrova], [~absurdfarce] !

Thank you, [~mmuzaf] and [~absurdfarce] for all the work here to release a new driver version and do all the updates needed! Always great to collaborate with the Cassandra community :-) 

