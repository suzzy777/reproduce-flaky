

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12881269/HIVE-17291.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 11002 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=240)
org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.testCliDriver[insert_overwrite_dynamic_partitions_merge_move] (batchId=243)
org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.testCliDriver[insert_overwrite_dynamic_partitions_merge_only] (batchId=243)
org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.testCliDriver[insert_overwrite_dynamic_partitions_move_only] (batchId=243)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning_mapjoin_only] (batchId=170)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=100)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] (batchId=99)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=235)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=235)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testDummyTxnManagerOnAcidTable (batchId=284)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=180)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6344/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6344/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6344/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12881269 - PreCommit-HIVE-Build

Failures are not related.

[~lirui], [~xuefuz], could you please review?

Thanks,
Peter

Thanks for working on this, [~pvary]. The patch looks good. However, I was a little confused. The description suggests that we are fixing the case when dynamic allocation is not enabled. However, the code seemingly will get executed in either case. I'm not sure if it's proper to use {{spark.executor.instances}} when dynamic allocation is enabled. Any thoughts?

Hi [~xuefuz],

I might not see the whole picture here, but my intention was to modify only the case when the dynamic allocation is not enabled.

The patch modifies only the {{SparkSessionImpl.getMemoryAndCores()}} method which is only used by {{SetSparkReducerParallelism.getSparkMemoryAndCores()}} method which looks like this:
{code:title=SetSparkReducerParallelism}
  private void getSparkMemoryAndCores(OptimizeSparkProcContext context) throws SemanticException {
    if (sparkMemoryAndCores != null) {
      return;
    }
    if (context.getConf().getBoolean(SPARK_DYNAMIC_ALLOCATION_ENABLED, false)) {
      // If dynamic allocation is enabled, numbers for memory and cores are meaningless. So, we don't
      // try to get it.
      sparkMemoryAndCores = null;
      return;
    }

[..]
    try {
[..]
      sparkMemoryAndCores = sparkSession.getMemoryAndCores();
    } catch (HiveException e) {
[..]
    }
  }
{code}

If the above statements are true, then in case of dynamic allocation we do not use this data, and the number of reducers based only on the size of the data:
{code:title=SetSparkReducerParallelism}
  @Override
  public Object process(Node nd, Stack<Node> stack,
      NodeProcessorCtx procContext, Object... nodeOutputs)
      throws SemanticException {
[..]
          // Divide it by 2 so that we can have more reducers
          long bytesPerReducer = context.getConf().getLongVar(HiveConf.ConfVars.BYTESPERREDUCER) / 2;
          int numReducers = Utilities.estimateReducers(numberOfBytes, bytesPerReducer,
              maxReducers, false);

          getSparkMemoryAndCores(context);     <-- In case of dynamic allocation this sets sparkMemoryAndCores to null
          if (sparkMemoryAndCores != null &&
              sparkMemoryAndCores.getFirst() > 0 && sparkMemoryAndCores.getSecond() > 0) {
            // warn the user if bytes per reducer is much larger than memory per task
            if ((double) sparkMemoryAndCores.getFirst() / bytesPerReducer < 0.5) {
              LOG.warn("Average load of a reducer is much larger than its available memory. " +
                  "Consider decreasing hive.exec.reducers.bytes.per.reducer");
            }

            // If there are more cores, use the number of cores
            numReducers = Math.max(numReducers, sparkMemoryAndCores.getSecond());
          }
          numReducers = Math.min(numReducers, maxReducers);
          LOG.info("Set parallelism for reduce sink " + sink + " to: " + numReducers +
              " (calculated)");
          desc.setNumReducers(numReducers);
[..]
  }
{code}

Might missed something, since I am quite newby in this part of the code.

Thanks for taking the time and looking at this!
Peter

You're right. Thanks for the explanation.
+1 to the patch.


Is this just to avoid unstable test output? If so, it seems we're doing something similar in {{QTestUtil}}: https://github.com/apache/hive/blob/master/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java#L1188

[~lirui]: You might be using black magic, or similar :)
In the middle of the night I woke up, that something is not good with the patch - some of the query output should be changed as a result of the change, so I decided that check out some stuff, and saw your comment which immediately answered my question. What I am not sure of is wether you were using black magic to wake me up, because my patch was not good, or you were using white magic to answer my unasked question :) :) :)

I found this problem with the help of the qtest outputs, but I think this signals a more important usage problem. Maybe you have more experience how users use HoS, and how they tune their queries, but when my wife optimizes an Oracle query she often iterates through explain / hint change / explain loops. I imagine with hive it looks like this: explain / change config / explain. When we change the spark configuration the RpcServer is killed, and a new one is started (I might be wrong here, tell me if it is not like this). The result of this is that the first explain is different than the next one this defeats the whole purpose of the optimization.

What happens when a query uses a wrong number of reducers? The query will run, but will result in slower execution? Out of memory?

Also the test results of HIVE-17292 showed me, that with multiple reducers configured the output of the {{hiveSparkClient.getExecutorCount()}} is even less reliable. The possible outcomes are:
- *0* - if only the Application Master is started
- *1* - if the AM, and 1 executor is started
- *2* - if the AM, and both executors are started

So we should not base any decision on it neither in QTestUtil, nor in {{sparkSession.getMemoryAndCores()}}, only if we made sure that all of the executors are stared which will be running at the time of the query. I starting to understand the wisdom of [~xuefuz] statement:

??I also had some doubts on the original idea which was to automatically and dynamically deciding the parallelism based on available memory/cores. Maybe we should back to the basis, where the number of reducers is solely determined (statically) by the total shuffled data size (stats) divided by the configuration "bytes per reducer". I'm open to all proposals, including doing this for dynamic allocation and using spark.executor.instances for static allocation.??

In the light of these findings I think we should use the proposed algorithm to calculate the parallelism (as proposed by [~xuefuz]):
- for dynamic allocation - number of reducers is solely determined (statically) by the total shuffled data size (stats) divided by the configuration "bytes per reducer"
- for static allocation - using spark.executor.instances

What do you think?

Sorry for dragging you through the whole thinking process :(

Thanks,
Peter

[~pvary], thanks so much for working in the middle of the night - I'll make sure to take time difference into consideration next time I use any magic :) :)
bq. When we change the spark configuration the RpcServer is killed, and a new one is started
More precisely, it's the spark session gets killed and started again, not the RpcServer. Rpc configs are made immutable by HIVE-16876.
bq. What happens when a query uses a wrong number of reducers? The query will run, but will result in slower execution? Out of memory?
Yes. But I don't think using mem/core will lead to "wrong number of reducers". We always firstly compute numReducers based on data size. Then mem/core is used as an attempt to have more reducers, not fewer:
{code}
            // If there are more cores, use the number of cores
            numReducers = Math.max(numReducers, sparkMemoryAndCores.getSecond());
{code}
 The reason is spark tasks are cheap but tend to require more memory. So it's safer to have more of them when we can. On the other hand, we still don't want too much than really needed, therefore it's better to use the available cores than the configured number - it ensures when taking effect, all the reducers finish in one round (assuming static allocation). So I think this is a useful, at least harmless, optimization when automatically deciding numReducers.
For stable tests, I'd prefer to stick to what we do in QTestUtil. Let me know your opinions. Thanks.

Thanks [~lirui], helpful as always!

What I do not get, why we do not see flakiness on TestMiniSparkOnYarnCliDriver tests, when the we change a spark configuration value, and the spark session is invalidated. Until now I thought, that in this case new containers are created by yarn, and we will have a race condition when checking {{hiveSparkClient.getExecutorCount()}} again. But obviously this is not the case, since I do not see any flakiness in the test results in HIVE-17292.

Thanks,
Peter

For test result stability, I'm fine with whatever solution. It would be interesting to find out why this is an issue even with Rui's magic in QTestUtil.

As to production, most likely dynamic allocation is enabled for sharing resources among users. The problem I see with dynamically determining parallelism with available executors is the dynamic nature of the executors. Typically, each user or user group has a queue, which is specified when a query is submitted. The capacity of the queue is decided in YARN. Further, the availability of the yarn containers is not guaranteed, and this is apparent when the cluster is busy. In this mode, using available executors can underestimate the number of reducers needed. This also happens when Spark client is initially launched. In addition, with dynamic allocation, minExecutors (usually 0) and initialExecutors (usually a small number) are not guaranteed either, and maxExecutors (usually a large number) only puts an upper limit. In a production env, users are less likely to overwrite what admin sets as default. Given such an uncertainly, I don't think we should determine parallelism based on available executors. I'd propose that we use "size per reducer" to decide the number of reducers, which might be further constrained under what maxExecutors allows.

Static allocation is mostly useful for benchmarking, but less likely used in a multi-tenant env. Even under this mode, {{spark.executor.instances}} are not guaranteed either. However, once the client gets an executor, it never gives it up. Thus, it's useful to determine the number of reducers using available executors. This comes with catch, which is about the first query run when the executors are starting. For this case, I think it should be okay to use {{spark.executor.instances}}. In short, with static allocation, we can use available executors to determine reducer parallelism and use {{spark.executor.instances}} when info about available executor is not available.

Any thoughts?

To clarify: I see the flakiness appear only when running the HoS tests against *real clusters*. I am still not sure why I do not see it with [~lirui]'s magic. When running the tests agains an appropriately configured HoS cluster I am using the TestBeeLineDriver to run the query tests, and I see the flakiness there. The BeeLineDriver could not use the QTestUtil magic, since it runs the query against a real HS2 instance with the out of the box Sessions. So the current state is not that bad!

Quick question: Do we have a way to determine when info about the available executors is reliable? The current implementation assumes that if we have a value which is bigger than 0, then it is reliable. But it still might be invalid if we requested multiple executors, but not yet received all.

Thanks,
Peter

Getting a value greater than 0 doesn't necessarily mean that all requested executors are up unless only one executor is requested. For testing, you might alter Rui's logic such that it waits until all expected cores are returned.

For testing this is exactly what I did in HIVE-17292.3.patch. I am not sure why we do not have the flakiness in this case when the configuration is changed, and the spark session is killed and restarted. Logic suggests, that in this case we will request new executors based on the new settings, and we are in the middle of the query test file, so [~lirui]'s magic does not applies here.
For example:
{code:title=spark_dynamic_partition_pruning_2.q}
EXPLAIN SELECT d1.label, count(*), sum(agg.amount) 
[..]

set hive.spark.dynamic.partition.pruning.max.data.size=1;   <-- I think new session is started here

EXPLAIN SELECT d1.label, count(*), sum(agg.amount)
[..]
{code}

Did you confirm that the configuration is updated and new session is launched? It might be possible that the configuration is updated via other code path or the configuration update is ignored by the test. Every qtest files was executed in a single session in the past and later this was changed to sharing a session for multiple qtest files. I'm not sure if there is also some magic.

[~pvary],
bq. Logic suggests, that in this case we will request new executors based on the new settings, and we are in the middle of the query test file, so Rui Li's magic does not applies here
I think it's because when we get a spark session, we'll set it to the SessionState:
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java#L127
In QTestUtil, we override the {{setSparkSession}} method, so each time we create a new spark session, we'll wait for it to init. Currently, we have only one executor, and we wait until numCores > 1. So we don't have any flakiness. With your change in HIVE-17292, we'll need to wait until we have 4 cores.

[~xuefuz], your concerns about dynamic allocation are valid. I think current implementation only uses "size per reducer" when dynamic allocation is enabled. For static allocation, as I mentioned, the available executor/core can only give us more reducers than needed (decided by "size per reducer"). As long as we have a reasonable {{hive.exec.reducers.bytes.per.reducer}}, we won't underestimate the num of reducers. So I think we're good with the current logic. Does that make sense?

[~lirui], that makes sense.

Ok. So based on this discussion we should leave the logic which generates the number of reducers without any change, since this results in the most useful output for the user.

So when I test against a real cluster, I have to mask the reducer numbers before comparing the output files.

Closing this jira as a won't fix, if you do not mind.

Thanks,
Peter

