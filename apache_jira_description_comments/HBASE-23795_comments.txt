Step one is to get to know the tests very well. This is normally a tall order for a mature distributed systems application and the scale of the HBase tests is beyond what I have run into as normal (>1 hour for all tests in a dev run). Because of this, I filed a few issues above and then kind of put on blinders for a bit. Unfortunetly, it doesn't help matters that the tests hate my current dns and vpn and osx environments out of the box. Anyway, getting to know the cast of characters. I'm almost done and as soon as I am, I will wrap up my PR's for at least two of those issues. 127.0.0.1 is likely a bit longer of a journey to fully complete.

I'll have many more issues to file for step 2.

I'll start with small and medium tests in HBASE-23849 Harden small and medium tests for lots of parallel runs with re-used jvms.

Good way to get some solid, easy progress before the mountain that is large tests.

In the mean time, I've been working in parallel on things related to HBASE-23806 Provide a much faster and efficient alternate option to maven and surefire for running tests. Not likely to be shared any time soon, but providing it's own benefit to this related issue.

I've started making some progress here.

With some tuning, these large tests are not so large after all.

I have to do some work to make sure they clean up after themselves so that JVMs dont get dirtier and dirtier over time, but all of these tests can run in parallel and much faster than they are now.

Now that I have spent enough time to understand what is possible here, to kind of zoom out, here is what would need to happen (this can take some time, but is easily done in parts):
 * Tune HBase and hdfs settings to make sense for the small world we are creating in tests. If you wash a test out with threads, it's really not very realistic at all. This means pinning settings for the JVM so that the right number of threads is created, settings for HBase and hdfs that make sense - handler counts and pool sizes netty 'stuff' - and scaling down thousands of threads to 200-300 or something. Not many of these threads are running at the same time - the rest is just flooding and eating resources.
 * Make things close and shutdown and enforce this. Lots and lots of various leaks currently. Once mostly removed, you can start reusing JVM's, but I think there are other benefits to actually closing and stopping your resources explicitly. Not closing or shutting down some things explicit can have lingering OS ramifications even when creating new JVM's.
 * Have tests clean up their expensive or config statics and reset sys props, with enforcement. Needed for JVM reuse.
 * Clean up any heavy resource usage you can't currently control or doesn't seem to make a lot of sense (I think an ipc thread pool is set to core and max size 200?) 

That's kind of the core of it, there is a lot of related useful stuff to do I think.

A lot of these tests are reasonably fast now. They could be even faster with a little work, but even without that, they are not that slow. Loading up a JVM, loading up 10-20k classes, warming up JIT, blah blah, that is super costly. Just getting to rerunning tests in the same JVM will be super helpful. There is a lot that can be done after that as well, but such a large win - good enough goal for now. Most of these tests don't even need that much RAM. They are just not running with sensible resources.

So, after getting to know the tests more individually, they did not match up with experience of trying to run the test suite. So I jumped ahead a bit.

I reduced resources and improvement. But the test suite still did not make sense.

I started shutting down resources and resolving deadlock or deadlock like situations on shutdown.

I removed System.exit type calls that not appropriate for Junit tests and kill JVM’s on us.

Again, I saw improvements but the test runs still didn’t make sense.

So I started hacking and providing higher limits to find out why nothing made sense.

In the end, lots of connections are fired up, few are closed, unless you run the tests fairly slowly, you will bed DOS attacked by them.

The worst of the leaks appears to be within the rpc client.

These tests are much faster, the rest of the flakies become conquerable, running tests with more parallel JVMs and even in the same JVM is now not that difficult, though some of the code does make it a little exercise. Good news all around.


I think the fail safe finalizer asking everyone to be polite citizens and close connections may be both failing in it's politeness and making a lot of the test situation even worse at best. You have all these extra costs and bad affects on GC with a finalizer, and the amount of connections these tests roll through and do not close is very high. I think it's quite easily overwhelmed and I think it contributes various  bad things that probably varies by the garbage collector in place.

Timely, proper connection closes and verification of reuse is a very fast gc agnostic recipe.  Those good citizens could use an assist, so I filed HBASE-23918 Track sensitive resources to ensure they are closed and assist devs in finding leaks.

So I've gone through this pretty thoroughly, though the tests failed on me a lot more than they are currently at the time.

If you use a statics checker and clear large statics and you reinit the right test and runtime statics, and shutdown additional threads and resources that can remain outstanding, these tests  can run in the same JVM about as well as Lucene and Solr tests do, taking advantage of class caching and hotspot, etc.

I've seen all the tests run on my 16-core machine in about 30-40 minutes with a new JVM for every test and parallelism lifted to max load, so perhaps 20 minutes would be in sight with reuse.

