Here is the analysis around what's happening among 3 masters as one of them is being paused and CreateTable request and OpenTable request in next iteration.
 [https://github.com/apache/kudu/blob/master/src/kudu/master/dynamic_multi_master-test.cc#L598-L610]
{code:java}
      LOG(INFO) << "Pausing and resuming individual masters";
      string table_name = kTableName;
      for (int i = 0; i < expected_num_masters; i++) {
        ASSERT_OK(migrated_cluster.master(i)->Pause());
        cluster::ScopedResumeExternalDaemon resume_daemon(migrated_cluster.master(i));
        NO_FATALS(cv.CheckRowCount(table_name, ClusterVerifier::EXACTLY, 0));

        // See MasterFailoverTest.TestCreateTableSync to understand why we must
        // check for IsAlreadyPresent as well.
        table_name = Substitute("table-$0", i);
        Status s = CreateTable(&migrated_cluster, table_name);
        ASSERT_TRUE(s.ok() || s.IsAlreadyPresent());
      }
{code}
Consider 3 masters A, B, C.
 - A is the leader and B & C are followers
 - A gets paused
 - B becomes the leader
 - Create table request which gets propagated to B and C forming a quorum.
 - Now A is resumed
 - While A is coming back up, B is paused.
 - C becomes candidate and tries to become leader asking for vote from A. But A itself was the leader before it was paused and for some reason doesn't vote.
 - Open table request now goes to table A (the leader) and gets table not found error because A didn't receive the create table request when it was down.
 - Moments later B resumes (which was leader before it was paused) and wins the election and A steps down. But by this time the test has failed.

A=eb86b1c4913647fc927d576f744c3d27
B=3978845170a24b80a8903036a6e97382
C=00e14f42918b444bb2a05e9b4f2ac855

Log snippets:

{noformat}
I0317 17:04:14.694448 17546 raft_consensus.cc:479] T 00000000000000000000000000000000 P 00e14f42918b444bb2a05e9b4f2ac855 [term 4 FOLLOWER]: Starting leader election (detected failure of leader eb86b1c4913647fc927d576f744c3d27
I0317 17:04:18.775841 17549 sys_catalog.cc:437] T 00000000000000000000000000000000 P 3978845170a24b80a8903036a6e97382 [sys.catalog]: This master's current role is: LEADER
I0317 17:04:18.776212 17553 sys_catalog.cc:437] T 00000000000000000000000000000000 P 00e14f42918b444bb2a05e9b4f2ac855 [sys.catalog]: This master's current role is: FOLLOWER
{noformat}

eb86b1c4913647fc927d576f744c3d27 coming back from pause thinks it's the leader.
While 00e14f42918b444bb2a05e9b4f2ac855 is trying to become leader.

{noformat}
I0317 17:04:19.643288 14988 tablet_service.cc:1729] Received RequestConsensusVote() RPC: tablet_id: "00000000000000000000000000000000" candidate_uuid: "00e14f42918b444bb2a05e9b4f2ac855" candidate_term: 5 candidate_status { last_received { term: 4 index: 3513 } } ignore_live_leader: false dest_uuid: "eb86b1c4913647fc927d576f744c3d27" is_pre_election: true
I0317 17:04:19.644940 17503 consensus_queue.cc:571] T 00000000000000000000000000000000 P eb86b1c4913647fc927d576f744c3d27 [LEADER]: Leader has been unable to successfully communicate with peer 3978845170a24b80a8903036a6e97382 for more than 4 seconds (6.459s)
I0317 17:04:19.645022 14987 tablet_service.cc:1729] Received RequestConsensusVote() RPC: tablet_id: "00000000000000000000000000000000" candidate_uuid: "00e14f42918b444bb2a05e9b4f2ac855" candidate_term: 5 candidate_status { last_received { term: 4 index: 3513 } } ignore_live_leader: false dest_uuid: "eb86b1c4913647fc927d576f744c3d27"
I0317 17:04:19.645114 17503 sys_catalog.cc:434] T 00000000000000000000000000000000 P eb86b1c4913647fc927d576f744c3d27 [sys.catalog]: SysCatalogTable state changed. Reason: Peer health change. Latest consensus state: current_term: 4 leader_uuid: "eb86b1c4913647fc927d576f744c3d27" committed_config { opid_index: 2852 OBSOLETE_local: false peers { permanent_uuid: "eb86b1c4913647fc927d576f744c3d27" member_type: VOTER last_known_addr { host: "127.0.92.253" port: 37459 } } peers { permanent_uuid: "3978845170a24b80a8903036a6e97382" member_type: VOTER last_known_addr { host: "127.0.92.252" port: 45331 } } peers { permanent_uuid: "00e14f42918b444bb2a05e9b4f2ac855" member_type: VOTER last_known_addr { host: "127.0.92.254" port: 43853 } attrs { promote: false } } }
I0317 17:04:19.645200 17503 sys_catalog.cc:437] T 00000000000000000000000000000000 P eb86b1c4913647fc927d576f744c3d27 [sys.catalog]: This master's current role is: LEADER


I0317 17:04:19.645022 14987 tablet_service.cc:1729] Received RequestConsensusVote() RPC: tablet_id: "00000000000000000000000000000000" candidate_uuid: "00e14f42918b444bb2a05e9b4f2ac855" candidate_term: 5 candidate_status { last_received { term: 4 index: 3513 } } ignore_live_leader: false dest_uuid: "eb86b1c4913647fc927d576f744c3d27"

I0317 17:04:19.645114 17503 sys_catalog.cc:434] T 00000000000000000000000000000000 P eb86b1c4913647fc927d576f744c3d27 [sys.catalog]: SysCatalogTable state changed. Reason: Peer health change. Latest consensus state: current_term: 4 leader_uuid: "eb86b1c4913647fc927d576f744c3d27" committed_config { opid_index: 2852 OBSOLETE_local: false peers { permanent_uuid: "eb86b1c4913647fc927d576f744c3d27" member_type: VOTER last_known_addr { host: "127.0.92.253" port: 37459 } } peers { permanent_uuid: "3978845170a24b80a8903036a6e97382" member_type: VOTER last_known_addr { host: "127.0.92.252" port: 45331 } } peers { permanent_uuid: "00e14f42918b444bb2a05e9b4f2ac855" member_type: VOTER last_known_addr { host: "127.0.92.254" port: 43853 } attrs { promote: false } } }

/data0/somelongdirectorytoavoidrpathissues/src/kudu/src/kudu/integration-tests/cluster_verifier.cc:119: Failure
Failed                                                                                              
Bad status: Not found: Unable to open table: the table does not exist: table_name: "table-1"        
/data0/somelongdirectorytoavoidrpathissues/src/kudu/src/kudu/master/dynamic_multi_master-test.cc:603: Failure
Expected: cv.CheckRowCount(table_name, ClusterVerifier::EXACTLY, 0) doesn't generate new fatal failures in the current thread.
  Actual: it does.                                                                                  
I0317 17:04:19.667603   371 external_mini_cluster.cc:1294] Killing /tmp/dist-test-task6JYMlq/build/debug/bin/kudu with pid 15089
I0317 17:04:19.673735 15061 raft_consensus.cc:1223] T 00000000000000000000000000000000 P 3978845170a24b80a8903036a6e97382 [term 6 LEADER]: Rejecting Update request from peer eb86b1c4913647fc927d576f744c3d27 for earlier term 4. Current term is 6. Ops: []
I0317 17:04:19.676132 14988 tablet_service.cc:1729] Received RequestConsensusVote() RPC: tablet_id: "00000000000000000000000000000000" candidate_uuid: "3978845170a24b80a8903036a6e97382" candidate_term: 6 candidate_status { last_received { term: 4 index: 3513 } } ignore_live_leader: false dest_uuid: "eb86b1c4913647fc927d576f744c3d27"
I0317 17:04:19.676213 14987 tablet_service.cc:1729] Received RequestConsensusVote() RPC: tablet_id: "00000000000000000000000000000000" candidate_uuid: "3978845170a24b80a8903036a6e97382" candidate_term: 6 candidate_status { last_received { term: 4 index: 3513 } } ignore_live_leader: false dest_uuid: "eb86b1c4913647fc927d576f744c3d27" is_pre_election: true
I0317 17:04:19.676512 14986 raft_consensus.cc:3027] T 00000000000000000000000000000000 P eb86b1c4913647fc927d576f744c3d27 [term 4 LEADER]: Stepping down as leader of term 4
I0317 17:04:19.676553 14986 raft_consensus.cc:726] T 00000000000000000000000000000000 P eb86b1c4913647fc927d576f744c3d27 [term 4 LEADER]: Becoming Follower/Learner. State: Replica: eb86b1c4913647fc927d576f744c3d27, State: Running, Role: LEADER
I0317 17:04:19.676688 14986 consensus_queue.cc:257] T 00000000000000000000000000000000 P eb86b1c4913647fc927d576f744c3d27 [NON_LEADER]: Queue going to NON_LEADER mode. State: All replicated index: 0, Majority replicated index: 3513, Committed index: 3513, Last appended: 4.3513, Last appended by leader: 3513, Current term: 4, Majority size: -1, State: 0, Mode: NON_LEADER, active raft config: opid_index: 2852 OBSOLETE_local: false peers { permanent_uuid: "eb86b1c4913647fc927d576f744c3d27" member_type: VOTER last_known_addr { host: "127.0.92.253" port: 37459 } } peers { permanent_uuid: "3978845170a24b80a8903036a6e97382" member_type: VOTER last_known_addr { host: "127.0.92.252" port: 45331 } } peers { permanent_uuid: "00e14f42918b444bb2a05e9b4f2ac855" member_type: VOTER last_known_addr { host: "127.0.92.254" port: 43853 } attrs { promote: false } }

{noformat}

Examined another failure in ParameterizedRecoverMasterTest.TestRecoverDeadMasterSysCatalogCopy/1, where GetParam() = 3

6fb is the leader that's paused and cba becomes the leader.
cd7 is the follower.

{noformat}
I0317 09:18:20.449790 21057 raft_consensus.cc:479] T 00000000000000000000000000000000 P cba6c57c99f44acfabfb650e6cb94d06 [term 2 FOLLOWER]: Starting leader election (detected failure of leader 6fb6e93836bb45ae882edd7e0d26c852)
I0317 09:18:20.449846 21057 raft_consensus.cc:3032] T 00000000000000000000000000000000 P cba6c57c99f44acfabfb650e6cb94d06 [term 2 FOLLOWER]: Advancing to term 3
I0317 09:18:20.459255 21057 raft_consensus.cc:683] T 00000000000000000000000000000000 P cba6c57c99f44acfabfb650e6cb94d06 [term 3 LEADER]: Becoming Leader. State: Replica: cba6c57c99f44acfabfb650e6cb94d06, State: Running, Role: LEADER


I0317 09:18:20.841820 21058 sys_catalog.cc:434] T 00000000000000000000000000000000 P cd7cbe8654e7426ca818c1c667cef824 [sys.catalog]: SysCatalogTable state changed. Reason: New leader cba6c57c99f44acfabfb650e6cb94d06. Latest consensus state: current_term: 3 leader_uuid: "cba6c57c99f44acfabfb650e6cb94d06" committed_config { opid_index: 2860 OBSOLETE_local: false peers { permanent_uuid: "cba6c57c99f44acfabfb650e6cb94d06" member_type: VOTER last_known_addr { host: "127.0.92.125" port: 42749 } } peers { permanent_uuid: "cd7cbe8654e7426ca818c1c667cef824" member_type: VOTER last_known_addr { host: "127.0.92.124" port: 35709 } } peers { permanent_uuid: "6fb6e93836bb45ae882edd7e0d26c852" member_type: VOTER last_known_addr { host: "127.0.92.126" port: 41791 } attrs { promote: false } } }

I0317 09:18:20.842010 21058 sys_catalog.cc:437] T 00000000000000000000000000000000 P cd7cbe8654e7426ca818c1c667cef824 [sys.catalog]: This master's current role is: FOLLOWER
{noformat}


Table creation request which could have been replicated to leader cba and follower cd7 but not 6fb.
{noformat}
I0317 09:18:22.128876 17932 catalog_manager.cc:1617] Servicing CreateTable request from {username='slave'} at 127.0.0.1:47764:
name: "table-0"
{noformat}

Looks like previous leader 6fb is up and leader cba is paused
{noformat}
I0317 09:18:22.342823 18013 raft_consensus.cc:1223] T 00000000000000000000000000000000 P cd7cbe8654e7426ca818c1c667cef824 [term 3 FOLLOWER]: Rejecting Update request from peer 6fb6e93836bb45ae882edd7e0d26c852 for earlier term 2. Current term is 3. Ops: []
{noformat}

Open table request fails since it likely went to 6fb which is leader from previous term among itself and follower cd7.
{noformat}
Bad status: Not found: Unable to open table: the table does not exist: table_name: "table-0"        
/data0/somelongdirectorytoavoidrpathissues/src/kudu/src/kudu/master/dynamic_multi_master-test.cc:603: Failure
Expected: cv.CheckRowCount(table_name, ClusterVerifier::EXACTLY, 0) doesn't generate new fatal failures in the current thread.
  Actual: it does
{noformat}

Moments later 6fb steps down as cd7 is paused and cba (term 3) becomes the leader.
{noformat}
W0317 09:18:22.404738 17916 leader_election.cc:334] T 00000000000000000000000000000000 P cba6c57c99f44acfabfb650e6cb94d06 [CANDIDATE]: Term 3 pre-election: RPC error from VoteRequest() call to peer 6fb6e93836bb45ae882edd7e0d26c852 (127.0.92.126:41791): Timed out: connection negotiation to 127.0.92.126:41791 for RPC RequestConsensusVote timed out after 1.923s (ON_OUTBOUND_QUEUE)
I0317 09:18:22.407025 17942 raft_consensus.cc:1223] T 00000000000000000000000000000000 P cba6c57c99f44acfabfb650e6cb94d06 [term 3 LEADER]: Rejecting Update request from peer 6fb6e93836bb45ae882edd7e0d26c852 for earlier term 2. Current term is 3. Ops: []
I0317 09:18:22.411103 20992 consensus_queue.cc:1038] T 00000000000000000000000000000000 P 6fb6e93836bb45ae882edd7e0d26c852 [LEADER]: Peer responded invalid term: Peer: permanent_uuid: "cd7cbe8654e7426ca818c1c667cef824" member_type: VOTER last_known_addr { host: "127.0.92.124" port: 35709 }, Status: INVALID_TERM, Last received: 2.3568, Next index: 3569, Last known committed idx: 3572, Time since last communication: 0.000s
I0317 09:18:22.411545 21016 raft_consensus.cc:3027] T 00000000000000000000000000000000 P 6fb6e93836bb45ae882edd7e0d26c852 [term 2 LEADER]: Stepping down as leader of term 2
I0317 09:18:22.411592 21016 raft_consensus.cc:726] T 00000000000000000000000000000000 P 6fb6e93836bb45ae882edd7e0d26c852 [term 2 LEADER]: Becoming Follower/Learner. State: Replica: 6fb6e93836bb45ae882edd7e0d26c852, State: Running, Role: LEADER
{noformat}

Uploaded log files here: [https://drive.google.com/drive/folders/1u3WRQOa4A7YuA0TjGp3895QEj5sVVK_z?usp=sharing]

Failures have been observed in mostly ASAN builds and in DEBUG builds as well.

Fix: https://gerrit.cloudera.org/c/17211/

Commit ac09d6205486908df05af4d64dc7618aae7c37bf in kudu's branch refs/heads/master from Bankim Bhavsar
[ https://gitbox.apache.org/repos/asf?p=kudu.git;h=ac09d62 ]

[test] KUDU-3266 Fix flakiness in dynamic_multi_master test

Flakiness was reported in dynamic_multi_master test after
the introduction of test for recovering dead master,
commit 4b4a8c0f2f.

See KUDU-3266 for the analysis.

This change wraps the check for row count under ASSERT_EVENTUALLY
to ensure the resumed master and the remaining master are given
a chance to communicate Raft messages and become up to date.

Tests:
- Reproduced the issue with ASAN build with dist-test.
- Verified no failures over 100 iterations with the fix
on ASAN build.

Change-Id: Ifac1d95707064b6ac2624d3f52336d6c39afd3c8
Reviewed-on: http://gerrit.cloudera.org:8080/17211
Tested-by: Bankim Bhavsar <bankim@cloudera.com>
Reviewed-by: Andrew Wong <awong@cloudera.com>
Reviewed-by: Mahesh Reddy <mreddy@cloudera.com>
Reviewed-by: Alexey Serbin <aserbin@cloudera.com>


Commit ac09d6205486908df05af4d64dc7618aae7c37bf in kudu's branch refs/heads/master from Bankim Bhavsar
[ https://gitbox.apache.org/repos/asf?p=kudu.git;h=ac09d62 ]

[test] KUDU-3266 Fix flakiness in dynamic_multi_master test

Flakiness was reported in dynamic_multi_master test after
the introduction of test for recovering dead master,
commit 4b4a8c0f2f.

See KUDU-3266 for the analysis.

This change wraps the check for row count under ASSERT_EVENTUALLY
to ensure the resumed master and the remaining master are given
a chance to communicate Raft messages and become up to date.

Tests:
- Reproduced the issue with ASAN build with dist-test.
- Verified no failures over 100 iterations with the fix
on ASAN build.

Change-Id: Ifac1d95707064b6ac2624d3f52336d6c39afd3c8
Reviewed-on: http://gerrit.cloudera.org:8080/17211
Tested-by: Bankim Bhavsar <bankim@cloudera.com>
Reviewed-by: Andrew Wong <awong@cloudera.com>
Reviewed-by: Mahesh Reddy <mreddy@cloudera.com>
Reviewed-by: Alexey Serbin <aserbin@cloudera.com>


