The test performed was 11% read of slices on a clustering key, so 89% write; workload from prod.

What I saw in CPU profiles was that the chunk cache was a decent part of compaction and the read path, but not in a positive way.  With compaction also populating the cache, it caused a constant churn from the cache, but all reads had to pay the cost for it.

Rerunning the test with chunk cache enabled, seeing the following for hits/misses

{code}
OneMinuteRate

Hits
392.07075331154306
Misses
10004.73634589861
{code}

So a 3.8% cache hit rate, this makes sense to me since compaction keeps populating the cache with files that are about to be deleted

CI is Yellow; known failing tests only.

[~jasonstack] was hoping I could get your feedback on this patch?  From my testing, if there are writes going on the cluster the cache hit ratio tanks and chunk cache gets more costly than having it disabled; most workloads I see are write heavy, hence the default I went with.

I took a first pass through.  I definitely think we should be able to disable the {{ChunkCache}} without impacting the \{{BufferPool}} and seem like they should be two separate configuration entries to me.

The code changes look good too. 

The only thing that needs more discussion is the change to make the ChunkCache be disabled by default. A single benchmark isn't enough evidence to convince me. 

We could either leave the default as enabled (in which case I think it's already good to go once CI is happy), or if we are able to collect a few more benchmarks (read-heavy, write-heavy, compaction-heavy, hot-key) to justify disabling then we should do so.

Fair point.  I am spinning up two clusters, with and without CC, and can run a few benchmarks Monday.

The current test used was a write heavy clustering key test with slices on clustering, but mostly looked at the selects for this patch.  Given the logic I see the next test I will run is a 100% read test with pre-populated data.

Attached test which fetches rows using IN clause, this test is 11% reads as well.  For the graph oss-pushbutton-62-selects is a cluster without this patch, and oss-pushbutton-63-selects is a cluster with this patch and the default of off.

What I see is that up to 99% they are roughly equal, and that over 99% disabling chunk cache is more stable.

Added a read only workload; select = with chunk cache, select-2 is without chunk cache.  What I see is up to 99% they are equal, after with chunk cache has an advantage, then at 99.99 without chunk cache has the advantage.

{code}
selects.hdr
#[Mean    =     49.88099, StdDeviation   =     37.74398]
#[Max     =   1826.77504, Total count    =      2880010]
#[Buckets =           25, SubBuckets     =       262144]
selects-2.hdr
#[Mean    =     51.66994, StdDeviation   =     44.94572]
#[Max     =   1810.85798, Total count    =      2880036]
#[Buckets =           25, SubBuckets     =       262144]
{code}

The mean has a diff of about 1ms.

In summary of what I see

1) if cluster has no writes and only does reads, there is a small advantage 
2) if the cluster is doing compactions, there are no advantages

Will redo the tests since just saw that the runners were in a different DC than the cluster.

Here are the results rerunning again (didn't populate data before test, relied on the same clusters)

cluster 62 = with chunk cache
cluster 63 = without chunk cache

clustering slice:
read rate: 11%
!https://issues.apache.org/jira/secure/attachment/13009751/clustering-slice_latency_selects_baseline.png!
!https://issues.apache.org/jira/secure/attachment/13009750/clustering-slice_latency_under90_selects_baseline.png!

clustering in-clause:
read rate: 11%
!https://issues.apache.org/jira/secure/attachment/13009749/clustering-in-clause_latency_selects_baseline.png!
!https://issues.apache.org/jira/secure/attachment/13009748/clustering-in-clause_latency_under90_selects_baseline.png!

Attempt 3
!https://issues.apache.org/jira/secure/attachment/13009803/clustering-in-clause_latency_selects_baseline_attempt3.png!
!https://issues.apache.org/jira/secure/attachment/13009802/clustering-in-clause_latency_under90_selects_baseline_attempt3.png!

medium blob:
Read rate: 100%
!https://issues.apache.org/jira/secure/attachment/13009747/medium-blobs_latency_selects_baseline.png!
!https://issues.apache.org/jira/secure/attachment/13009746/medium-blobs_latency_under90_selects_baseline.png!


Partition single row read
Read rate: 100%
!https://issues.apache.org/jira/secure/attachment/13009745/partition-single-row-read_latency_selects_baseline.png!
!https://issues.apache.org/jira/secure/attachment/13009744/partition-single-row-read_latency_under90_selects_baseline.png!

Here are the results I am seeing (baseline is 3.0); all read only tests are expected to have zero compaction going on, the cluster only ran the specific workload at the time, multi workload traffic has not happened.

* Point reads show roughly equal performance
* Scans around 99.5% we see cache doing better but under that non cache is better
* In clause shows improvement with cache

From past testing, anything greater than 99.99% is too noisy to really look at, so ignoring that when making my statements.  Given this the only test out of the 4 that had any benefit was the in-clause test (will run a few more times to see how stable), and the scan tests were generally regressed.  The tests which were designed to highlight this feature (medium blob, and Partition single row read) showed no difference with or without the cache.

In my experience, in clause is rather rare but scans are much more common (partition read, read range by clustering, read latest N rows, etc.).  If compaction is constantly running (unlikely in these tests, much more likely in production) then the cache is invalidated very frequently which should yield worse results than what these tests show (homogeneous query pattern vs heterogeneous query pattern); given this I am still in favor of the default of false.

[~jmeredithco] can you review again?  [~jasonstack] can I get your feedback as well?

[~dcapwell] sorry, won't be able to look deeper into chunk cache this week. But based on the comparison between 3.0 baseline, 4.0 chunk cache, and 4.0 no chunk cache, disabling chunk-cache didn't bridge the gap between 3.0 and 4.0. I wonder if something else is affecting the perf instead of chunk cache. do you have JFR?

bq. I wonder if something else is affecting the perf instead of chunk cache.

Yeah, it looks like more is going on, so have to look closer.  I was curious about networking since I knew two different JIRA were going on with regard to networking perf, since the two tests which regressed returns more data, thought it could be some how related.

bq. do you have JFR?

JFR no, but I did have a flame graph from the first attempt

!https://issues.apache.org/jira/secure/attachment/13009804/async-profile.collapsed.svg!



Thanks for doing the extra benchmarking.  Results seem to be fairly close, and to get the best out of things users would need to optimize and benchmark anyway to work out how to use their memory budget.

+1 from me, with a minor nit that it would be good to update {{conf/cassandra.yaml}} with a commented out entry and some description of what the setting controls.

Thanks jon, will update conf/cassandra.yaml.

bq. work out how to use their memory budget.

Yeah, this is why I still lean to this default.  I have not found a use case which benefits from chunk cache and find some use cases which are roughly equal with and without, so favor disabling in these cases to save memory (a much limited resource in production).

[~jmeredithco] pushed changes to the yaml to document this flag.

[~jasonstack] would be great to get your feedback.  This patch does not fix the perf regression, so hoping we can defer that to CASSANDRA-16078.  This mostly just disables as it doesn't appear I can find a use case which benefits from the feature, so feel that we should be explicit about having the feature on rather than default to it.

I wonder if the chunk-cache regression is related to CASSANDRA-15229, let me run some tests from CASSANDRA-15229.

 

Also, I am not a committer, you may need to find one more...

!16036_128mb.png!

 

Above is write perf in mixed-read-write test using 128mb cache between 16036-disable-chunk-cache and its base line. Disabling chunk cache significantly improves latency.  (read perf is similar to write perf)

 

 

!15229_128mb.png!

Above is write perf in mixed-read-write test using 128mb cache between 15229-disable-chunk-cache and 15229-improved-buffer-pool. Disabling chunk cache show some improvement on latency.  (read perf is similar to write perf)

+1 to disable chunk cache until we get CASSANDRA-15229 and other improvements (eg. fixed buffer size) into chunk cache.

Awesome, thanks for the feedback.

Now that [~jasonstack] is a committer (congrats!) I will move forward to merge this later today (after rerunning CI on it).

CI results (pending):

Circle: https://app.circleci.com/pipelines/github/dcapwell/cassandra?branch=commit_remote_branch%2FCASSANDRA-16036-trunk-B4951B6C-9967-4B3D-A93A-5C5539DDE804
Jenkins: https://ci-cassandra.apache.org/job/Cassandra-devbranch/51/

rebase and broke JMX (since it isn't enabled) so enabled check cache in testing and tests are passing.  I am running CI against trunk as the failing tests are consistent and failing for other branches, so isolating the changes to make sure those tests are not broken here.

ok so looks like the read_repair tests and the gossiper test was broken by https://issues.apache.org/jira/browse/CASSANDRA-15833, so can ignore in this results.  Will rerun the tests with the commit to enable the cache in tests.

Updated CI results

Circle: https://app.circleci.com/pipelines/github/dcapwell/cassandra?branch=commit_remote_branch%2FCASSANDRA-16036-trunk-2EBAD3E9-4394-4D42-9213-69A6590F37E2 (expected test failures caused by other JIRA, and 1 flaky test in no-vnode case but not in vnode case)
Jenkins: https://ci-cassandra.apache.org/job/Cassandra-devbranch/52/

trunk baseline: https://app.circleci.com/pipelines/github/dcapwell/cassandra/574/workflows/19f38f3c-9da3-42d5-ba5f-269f0285b791

