I checked the differences in the logs and in the code.

The problem occures when:
- a node is restarted 
- Peer Sync failed (no "/get" handler for instance, should it become mandatory ?)
- the node is already synced (nothing to replicate)

or :

- a node is restarted and this is the leader (I do not know if it only appends with a lonely leader...)
- the node is already synced (nothing to replicate)

For the first case,

I think this is a side effect of the modification in SOLR-4965. 

If Peer Sync is succesfull, in the code an explicit commit is called. And there's a comment which says:

{code:title=RecoveryStrategy.java|borderStyle=solid}
            // force open a new searcher
            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));
{code}

This is not the case if Peer Sync failed.
Just adding this line is enough to correct this issue.

Here is a patch with a test which reproduces the problem and the correction (to be applied to the branch 4x).

I am working on the second case.

Second patch which also solves the second case.

A second test added too.

These two tests are quite slow: more than 2 minutes on my machine, is it an issue ?




Is this really the case? Why are nodes that are not yet able to serve queries consideres being active?

Here's a patch I cooked up for this issue (against 6.3.0) ... it's not ready to commit and needs a test, but does resolve my issue.

Here is patch with the fix and tests.

All in all, the following are the ways which cause this issue:
# Replica is the only one in the shard -- hence on startup becomes leader, skips recovery, becomes active without waiting for warming to complete
# Replica goes into recovery, discovers that it is the leader, becomes active without waiting for warming to complete
# When peersync fails but replication reports that there is nothing to replicate then replica becomes active without waiting for warming to complete. This can happen in the following ways:
## peersync could be skipped if firstTime=false which can happen if recovering on startup we discover that last operation in ulog had flag gap set
## peersync could be skipped if firstTime=false because the previous recovery attempt failed and we're retrying recovery
## peersync could fail if the peersync request failed due to exceptions

The fix in this patch (based on Tim's patch) ensure that there is a registered searcher before we publish any replica as active. Doing it inside RecoveryStrategy was not sufficient because that does not cover scenario 1. The tests in this patch inject failure into peer sync and simulate wrong index fingerprint computation to reproduce the problem. The scenarios that I could not reliably simulate were 2 and 3.1 but the fix should cover both of them. Tim's patch had a bug where openSearcher was called without returnSearcher=true. This can return a null future sometimes if there is an on deck searcher already and no new registration is needed. The correct fix is to send returnSearcher=true as well as waitFuture and check for both.

The test in the earlier patch was flaky. This patch fixes those test problems. I beasted this test 200 times to be sure. This is ready.

Commit 90da5ce81cea82424dad6ba9ab1bf12d34d196e2 in lucene-solr's branch refs/heads/master from [~shalin]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=90da5ce ]

SOLR-6086: Replica is active during autowarming resulting in queries being sent to a replica that may not have a registered searcher. This causes spikes in response times when adding a replica in busy clusters


Commit b1a65c8f5572004cabc2b8d5548bf07f22fd2b3e in lucene-solr's branch refs/heads/master from [~shalin]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b1a65c8 ]

SOLR-6086: Remove unused import


Thanks Ludovic and Tim!

Thanks [~shalin].

Commit 7aeef02bdf515b33a93e0e3a460ac461b8f70165 in lucene-solr's branch refs/heads/branch_7x from [~shalin]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7aeef02 ]

SOLR-6086: Replica is active during autowarming resulting in queries being sent to a replica that may not have a registered searcher. This causes spikes in response times when adding a replica in busy clusters

(cherry picked from commit 90da5ce)


Commit 98f90094305628198866a52d2bd9289002cbbe9c in lucene-solr's branch refs/heads/branch_7x from [~shalin]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=98f9009 ]

SOLR-6086: Remove unused import

(cherry picked from commit b1a65c8)


I had forgotten to cherry pick the changes on branch_7x. I just did that now.

Bulk close after 7.1.0 release

