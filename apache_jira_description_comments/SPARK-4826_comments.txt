Actually, it turns out that this has happened twice.  Here's a link to another occurrence of the same failure pattern: https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/1147/

I committed a documentation typo fix to {{master}} and {{branch-1.2}} at the same time, which caused a huge number of Maven builds to kick off simultaneously in Jenkins (since it was otherwise idle), and alll of these builds failed due to tests in WriteAheadLogBackedBlockRDDSuite; ; it also broke the master SBT build.

I wonder if there's some kind of sharing / contention where multiple copies of the test are attempting to write to the same directory.

[~hshreedharan], it would be great to get your help with this to see if you can spot any potential problems in that test suite.

It looks like there is some issue with the directories/files existing (though we use random names for files/dirs). I will see try to get something ready later today

This raises an interesting test infrastructure question: Do we have a way of invoking multiple copies of the same test (on the same or across multiple JVMs) to check a test's level of isolation? If not, that might be a good thing to look into.

I just cooked up a quick way of invoking this test multiple times in parallel using [GNU parallel|http://www.gnu.org/software/parallel/]:

{code}
parallel 'sbt/sbt -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -Pkinesis-asl -Phive -Phive-thriftserver "testOnly org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite"' ::: '' '' '' ''
{code}

This will fire up 4 copies of that one test in parallel. I ran it a couple of times on my laptop without issue, but appears to be due to some sbt locking that prevents the tests from actually running in parallel.

I've [posted a question on Stack Overflow|http://stackoverflow.com/questions/27474000/how-can-i-run-multiple-copies-of-the-same-test-in-parallel] about this.

I suspect that the nextString is conflicting and producing strings that are likely conflicting (since createTempDir is atomic). Using monotonically increasing names for the file counter will likely fix the issue.

User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/3695

I pushed a hotfix disabling these tests, but let's re-enable them once things are working.

User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3704

Issue resolved by pull request 3704
[https://github.com/apache/spark/pull/3704]

