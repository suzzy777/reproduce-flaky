We need to support multiple start/stop of sentry service. Therefore, start sentry service should be moved to what start() will execute. The reason e2e test fails is because it tries to start/stop sentry service multiple times. This change should fix the e2e test failure.

code review link https://reviews.apache.org/r/58221/


The review link isn't accessible:

{code}
You don't have access to this review request.

This review request is private. You must be a requested reviewer, either directly or on a requested group, and have permission to access the repository in order to view this review request.
{code}

Sorry, I forgot to publish it. Can you check if it is accessible now?

Thanks, this is accessible now.

move the scheduling of HMS follower to runServer(). 

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12862160/SENTRY-1649.001-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2485/console

This message is automatically generated.

update based on code review. change code style and output exception

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12862351/SENTRY-1649.002-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to an error

{color:red}ERROR:{color} failed to apply patch (exit code 1):
The patch does not appear to apply with p0, p1, or p2



Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2486/console

This message is automatically generated.

Create HMSFollower instance in SentryService constructor. start HMSFollower in SentryService.runServer and stop HMSFollower in SentryService.stop

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12862503/SENTRY-1649.003-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to an error

{color:red}ERROR:{color} failed to apply patch (exit code 1):
The patch does not appear to apply with p0, p1, or p2



Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2490/console

This message is automatically generated.

Looking at the way HMSFollower is implemented I don't see why do you need to actually destroy one instance and create a new one when you want to start/stop it. The service is managed using the executor so the only thing you need to do is to stop or start *executing* the service.

My latest update does not destroy one instance. You can take a look at "https://reviews.apache.org/r/58221/diff/3#1" sentry/service/thrift/SentryService.java. It is created in SentryService constructor.
  

fix the build error by including all changes in a single patch file. 

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12862879/SENTRY-1649.004-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2496/console

This message is automatically generated.

1. Implement HMSFollower.close(). Remove its start() and stop().
2. Add SentryService.close(), executor service shutdown are moved there. in stop(), cancel the future of the tasks instead of shutting down executor services. 
3. In SentryService$CommandImpl.run(), call SentryService.stop() and close() instead of just call serviceExecutor.shutdown().

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12862969/SENTRY-1649.005-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2499/console

This message is automatically generated.

version 006 is patch after rebase that includes all latest commites from remote sentry-ha-redesign

Why do you want to use future for de-scheduling? Are you sure that it does the right thing? What's wrong with the simple shutdown()?

Looking more at this it looks like it is possible to cancel execution using future's cancel method, but all this seems to be rather complicated - in this case shutting down the executor seems simpler. And you do need to shut it down on exit anyway.

update for suggestions in code review by Sasha.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12862970/SENTRY-1649.006-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.dbprovider.TestDbSentryOnFailureHookLoading

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2500/console

This message is automatically generated.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12862973/SENTRY-1649.007-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.dbprovider.TestDbSentryOnFailureHookLoading

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2501/console

This message is automatically generated.

change the order of scheduling SentryStoreCleaner, trying to fix build error below:
----------------
org.apache.sentry.tests.e2e.dbprovider.TestDbSentryOnFailureHookLoading.org.apache.sentry.tests.e2e.dbprovider.TestDbSentryOnFailureHookLoading

Failing for the past 1 build (Since Failed#2500 )
Took 4.3 sec.
Error Message

got: <[null]>, expected: each not null
Stacktrace

org.junit.internal.AssumptionViolatedException: got: <[null]>, expected: each not null
	at org.apache.sentry.tests.e2e.dbprovider.TestDbSentryOnFailureHookLoading.setup(TestDbSentryOnFailureHookLoading.java:48)

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863198/SENTRY-1649.008-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2502/console

This message is automatically generated.

shutdown service executor at SentryService.stop()

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863285/SENTRY-1649.009-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.generic.service.persistent.TestPrivilegeOperatePersistence

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2503/console

This message is automatically generated.

with minor change to get build machine why has "[DEBUG - org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:321)] Failed to detect a valid hadoop home directory
java.io.IOException: HADOOP_HOME or hadoop.home.dir are not set." in org.apache.sentry.provider.db.generic.service.persistent.TestPrivilegeOperatePersistence.testGrantPrivilegeExternalComponentInvalidConf

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863285/SENTRY-1649.009-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2504/console

This message is automatically generated.

the patch file name is SENTRY-1649.010-sentry-ha-redesign.patch

with minor change to get build machine why has "[DEBUG - org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:321)] Failed to detect a valid hadoop home directory
java.io.IOException: HADOOP_HOME or hadoop.home.dir are not set." in org.apache.sentry.provider.db.generic.service.persistent.TestPrivilegeOperatePersistence.testGrantPrivilegeExternalComponentInvalidConf

SENTRY-1649.011-sentry-ha-redesign.patct
create sentryStoreCleanService at very beginning to avoid timing issue. That may be the cause of the test failure.


Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863308/SENTRY-1649.011-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2506/console

This message is automatically generated.

test build SENTRY-1649.012-sentry-ha-redesign.patch

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863305/SENTRY-1649.010-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2505/console

This message is automatically generated.

SENTRY-1649.013-sentry-ha-redesign.patch
only call startSentryStoreCleaner() at constructor, and see if it fixes the test failure at building machine.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863331/SENTRY-1649.012-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 4 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.thrift.TestSentryWebServerWithSSL
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2508/console

This message is automatically generated.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863335/SENTRY-1649.013-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2509/console

This message is automatically generated.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863335/SENTRY-1649.013-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2510/console

This message is automatically generated.

SENTRY-1649.014-sentry-ha-redesign.patch


Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863351/SENTRY-1649.014-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2512/console

This message is automatically generated.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863356/SENTRY-1649.015-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2513/console

This message is automatically generated.

add patch SENTRY-1649.016-sentry-ha-redesign.patch. Remove formatting changes

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863476/SENTRY-1649.016-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to an error

{color:red}ERROR:{color} failed to apply patch (exit code 1):
The patch does not appear to apply with p0, p1, or p2



Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2521/console

This message is automatically generated.

full change patch SENTRY-1649.017-sentry-ha-redesign.patch

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863477/SENTRY-1649.017-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2522/console

This message is automatically generated.

some format change SENTRY-1649.018-sentry-ha-redesign.patch

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863499/SENTRY-1649.018-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2524/console

This message is automatically generated.

only start cleaner at runServer in patch version 19

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863680/SENTRY-1649.019-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2527/console

This message is automatically generated.

remove extra code with patch v20

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863731/SENTRY-1649.020-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2529/console

This message is automatically generated.

move starting HMSfollower to end of runServer(). V21

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12863938/SENTRY-1649.021-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.metastore.TestDbNotificationListenerSentryDeserializer

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2535/console

This message is automatically generated.

After moving starting HMSfollower to end of runServer(). the following test fail

Regression

org.apache.sentry.tests.e2e.metastore.TestDbNotificationListenerSentryDeserializer.testAlterPartition

Failing for the past 1 build (Since Failed#2535 )
Took 0.61 sec.
Error Message

Database N_db17 already exists
Stacktrace

org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Database N_db17 already exists
Standard Output

2017-04-19 04:54:17,617 (Thread-0) [INFO - org.apache.sentry.tests.e2e.hive.AbstractTestWithStaticConfiguration.setupTestStaticConfiguration(AbstractTestWithStaticConfiguration.java:284)] AbstractTestWithStaticConfiguration setupTestStaticConfiguration
2017-04-19 04:54:17,634 (Thread-0) [INFO - org.apache.sentry.tests.e2e.hive.AbstractTestWithStaticConfiguration.setupTestStaticConfiguration(AbstractTestWithStaticConfiguration.java:293)] BaseDir = /tmp/1492577657634-0
2017-04-19 04:54:18,329 (Thread-0) [INFO - org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:446)] starting cluster: numNameNodes=1, numDataNodes=2
Formatting using clusterid: testClusterID
2017-04-19 04:54:18,948 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:716)] No KeyProvider found.
2017-04-19 04:54:18,948 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:726)] fsLock is fair:true
2017-04-19 04:54:18,999 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:239)] dfs.block.invalidate.limit=1000
2017-04-19 04:54:19,000 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:245)] dfs.namenode.datanode.registration.ip-hostname-check=true
2017-04-19 04:54:19,001 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:71)] dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-04-19 04:54:19,003 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:76)] The block deletion will start around 2017 Apr 19 04:54:19
2017-04-19 04:54:19,006 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:354)] Computing capacity for map BlocksMap
2017-04-19 04:54:19,006 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:355)] VM type       = 64-bit
2017-04-19 04:54:19,008 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:356)] 2.0% max memory 1.8 GB = 36.4 MB
2017-04-19 04:54:19,009 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:361)] capacity      = 2^22 = 4194304 entries
2017-04-19 04:54:19,045 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createBlockTokenSecretManager(BlockManager.java:358)] dfs.block.access.token.enable=false
2017-04-19 04:54:19,046 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:344)] defaultReplication         = 2
2017-04-19 04:54:19,046 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:345)] maxReplication             = 512
2017-04-19 04:54:19,048 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:346)] minReplication             = 1
2017-04-19 04:54:19,049 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:347)] maxReplicationStreams      = 2
2017-04-19 04:54:19,049 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:348)] replicationRecheckInterval = 3000
2017-04-19 04:54:19,049 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:349)] encryptDataTransfer        = false
2017-04-19 04:54:19,049 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:350)] maxNumBlocksToLog          = 1000
2017-04-19 04:54:19,056 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:746)] fsOwner             = jenkins (auth:SIMPLE)
2017-04-19 04:54:19,057 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:747)] supergroup          = supergroup
2017-04-19 04:54:19,057 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:748)] isPermissionEnabled = true
2017-04-19 04:54:19,057 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:759)] HA Enabled: false
2017-04-19 04:54:19,060 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796)] Append Enabled: true
2017-04-19 04:54:19,309 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:354)] Computing capacity for map INodeMap
2017-04-19 04:54:19,310 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:355)] VM type       = 64-bit
2017-04-19 04:54:19,310 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:356)] 1.0% max memory 1.8 GB = 18.2 MB
2017-04-19 04:54:19,310 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:361)] capacity      = 2^21 = 2097152 entries
2017-04-19 04:54:19,312 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:235)] ACLs enabled? false
2017-04-19 04:54:19,312 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:239)] XAttrs enabled? true
2017-04-19 04:54:19,312 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:247)] Maximum size of an xattr: 16384
2017-04-19 04:54:19,313 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:298)] Caching file names occuring more than 10 times
2017-04-19 04:54:19,322 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:354)] Computing capacity for map cachedBlocks
2017-04-19 04:54:19,322 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:355)] VM type       = 64-bit
2017-04-19 04:54:19,323 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:356)] 0.25% max memory 1.8 GB = 4.6 MB
2017-04-19 04:54:19,323 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:361)] capacity      = 2^19 = 524288 entries
2017-04-19 04:54:19,325 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo.<init>(FSNamesystem.java:5166)] dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-04-19 04:54:19,325 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo.<init>(FSNamesystem.java:5167)] dfs.namenode.safemode.min.datanodes = 0
2017-04-19 04:54:19,325 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo.<init>(FSNamesystem.java:5168)] dfs.namenode.safemode.extension     = 0
2017-04-19 04:54:19,329 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:65)] NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-04-19 04:54:19,329 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:67)] NNTop conf: dfs.namenode.top.num.users = 10
2017-04-19 04:54:19,330 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:69)] NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-04-19 04:54:19,331 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:905)] Retry cache on namenode is enabled
2017-04-19 04:54:19,332 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:913)] Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-04-19 04:54:19,335 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:354)] Computing capacity for map NameNodeRetryCache
2017-04-19 04:54:19,335 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:355)] VM type       = 64-bit
2017-04-19 04:54:19,335 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:356)] 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2017-04-19 04:54:19,335 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:361)] capacity      = 2^16 = 65536 entries
2017-04-19 04:54:19,419 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:158)] Allocated new BlockPoolId: BP-1490004680-67.195.81.141-1492577659354
2017-04-19 04:54:19,443 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.NNStorage.format(NNStorage.java:552)] Storage directory /tmp/1492577657634-0/dfs/name1 has been successfully formatted.
2017-04-19 04:54:19,445 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.NNStorage.format(NNStorage.java:552)] Storage directory /tmp/1492577657634-0/dfs/name2 has been successfully formatted.
2017-04-19 04:54:19,590 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.getImageTxIdToRetain(NNStorageRetentionManager.java:203)] Going to retain 1 images with txid >= 0
2017-04-19 04:54:19,592 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1417)] createNameNode []
2017-04-19 04:54:19,636 (Thread-0) [WARN - org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:125)] Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-04-19 04:54:19,720 (Thread-0) [INFO - org.apache.hadoop.metrics2.impl.MetricsSystemImpl.startTimer(MetricsSystemImpl.java:377)] Scheduled snapshot period at 10 second(s).
2017-04-19 04:54:19,721 (Thread-0) [INFO - org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:192)] NameNode metrics system started
2017-04-19 04:54:19,725 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.NameNode.setClientNamenodeAddress(NameNode.java:397)] fs.defaultFS is hdfs://127.0.0.1:0
2017-04-19 04:54:19,768 (Thread-0) [INFO - org.apache.hadoop.hdfs.DFSUtil.httpServerTemplateForNNAndJN(DFSUtil.java:1703)] Starting Web-server for hdfs at: http://localhost:0
2017-04-19 04:54:19,845 (Thread-0) [INFO - org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)] Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-04-19 04:54:19,854 (Thread-0) [INFO - org.apache.hadoop.security.authentication.server.AuthenticationFilter.constructSecretProvider(AuthenticationFilter.java:284)] Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-04-19 04:54:19,859 (Thread-0) [INFO - org.apache.hadoop.http.HttpRequestLog.getRequestLog(HttpRequestLog.java:80)] Http request log for http.requests.namenode is not defined
2017-04-19 04:54:19,865 (Thread-0) [INFO - org.apache.hadoop.http.HttpServer2.addGlobalFilter(HttpServer2.java:710)] Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-04-19 04:54:19,869 (Thread-0) [INFO - org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:685)] Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-04-19 04:54:19,869 (Thread-0) [INFO - org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:693)] Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-04-19 04:54:19,888 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.initWebHdfs(NameNodeHttpServer.java:86)] Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-04-19 04:54:19,890 (Thread-0) [INFO - org.apache.hadoop.http.HttpServer2.addJerseyResourcePackage(HttpServer2.java:609)] addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-04-19 04:54:19,904 (Thread-0) [INFO - org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:915)] Jetty bound to port 40824
2017-04-19 04:54:19,904 (Thread-0) [INFO - org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)] jetty-6.1.26
2017-04-19 04:54:19,933 (Thread-0) [INFO - org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)] Extract jar:file:/home/jenkins/jenkins-slave/workspace/PreCommit-SENTRY-Build/maven-repo/org/apache/hadoop/hadoop-hdfs/2.7.2/hadoop-hdfs-2.7.2-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_40824_hdfs____.z4wlaw/webapp
2017-04-19 04:54:20,110 (Thread-0) [INFO - org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)] Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40824
2017-04-19 04:54:20,119 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:716)] No KeyProvider found.
2017-04-19 04:54:20,119 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:726)] fsLock is fair:true
2017-04-19 04:54:20,121 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:239)] dfs.block.invalidate.limit=1000
2017-04-19 04:54:20,121 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:245)] dfs.namenode.datanode.registration.ip-hostname-check=true
2017-04-19 04:54:20,122 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:71)] dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-04-19 04:54:20,122 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:76)] The block deletion will start around 2017 Apr 19 04:54:20
2017-04-19 04:54:20,122 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:354)] Computing capacity for map BlocksMap
2017-04-19 04:54:20,122 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:355)] VM type       = 64-bit
2017-04-19 04:54:20,123 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:356)] 2.0% max memory 1.8 GB = 36.4 MB
2017-04-19 04:54:20,123 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:361)] capacity      = 2^22 = 4194304 entries
2017-04-19 04:54:20,126 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createBlockTokenSecretManager(BlockManager.java:358)] dfs.block.access.token.enable=false
2017-04-19 04:54:20,127 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:344)] defaultReplication         = 2
2017-04-19 04:54:20,127 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:345)] maxReplication             = 512
2017-04-19 04:54:20,127 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:346)] minReplication             = 1
2017-04-19 04:54:20,127 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:347)] maxReplicationStreams      = 2
2017-04-19 04:54:20,127 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:348)] replicationRecheckInterval = 3000
2017-04-19 04:54:20,127 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:349)] encryptDataTransfer        = false
2017-04-19 04:54:20,127 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:350)] maxNumBlocksToLog          = 1000
2017-04-19 04:54:20,128 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:746)] fsOwner             = jenkins (auth:SIMPLE)
2017-04-19 04:54:20,128 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:747)] supergroup          = supergroup
2017-04-19 04:54:20,128 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:748)] isPermissionEnabled = true
2017-04-19 04:54:20,129 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:759)] HA Enabled: false
2017-04-19 04:54:20,129 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796)] Append Enabled: true
2017-04-19 04:54:20,130 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:354)] Computing capacity for map INodeMap
2017-04-19 04:54:20,130 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:355)] VM type       = 64-bit
2017-04-19 04:54:20,130 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:356)] 1.0% max memory 1.8 GB = 18.2 MB
2017-04-19 04:54:20,130 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:361)] capacity      = 2^21 = 2097152 entries
2017-04-19 04:54:20,132 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:235)] ACLs enabled? false
2017-04-19 04:54:20,132 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:239)] XAttrs enabled? true
2017-04-19 04:54:20,133 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:247)] Maximum size of an xattr: 16384
2017-04-19 04:54:20,133 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:298)] Caching file names occuring more than 10 times
2017-04-19 04:54:20,134 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:354)] Computing capacity for map cachedBlocks
2017-04-19 04:54:20,134 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:355)] VM type       = 64-bit
2017-04-19 04:54:20,135 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:356)] 0.25% max memory 1.8 GB = 4.6 MB
2017-04-19 04:54:20,135 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:361)] capacity      = 2^19 = 524288 entries
2017-04-19 04:54:20,136 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo.<init>(FSNamesystem.java:5166)] dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-04-19 04:54:20,136 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo.<init>(FSNamesystem.java:5167)] dfs.namenode.safemode.min.datanodes = 0
2017-04-19 04:54:20,137 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo.<init>(FSNamesystem.java:5168)] dfs.namenode.safemode.extension     = 0
2017-04-19 04:54:20,137 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:65)] NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-04-19 04:54:20,138 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:67)] NNTop conf: dfs.namenode.top.num.users = 10
2017-04-19 04:54:20,138 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:69)] NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-04-19 04:54:20,138 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:905)] Retry cache on namenode is enabled
2017-04-19 04:54:20,139 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:913)] Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-04-19 04:54:20,139 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:354)] Computing capacity for map NameNodeRetryCache
2017-04-19 04:54:20,139 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:355)] VM type       = 64-bit
2017-04-19 04:54:20,140 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:356)] 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2017-04-19 04:54:20,140 (Thread-0) [INFO - org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:361)] capacity      = 2^16 = 65536 entries
2017-04-19 04:54:20,148 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:715)] Lock on /tmp/1492577657634-0/dfs/name1/in_use.lock acquired by nodename 20965@asf921.gq1.ygridcore.net
2017-04-19 04:54:20,151 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:715)] Lock on /tmp/1492577657634-0/dfs/name2/in_use.lock acquired by nodename 20965@asf921.gq1.ygridcore.net
2017-04-19 04:54:20,157 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FileJournalManager.recoverUnfinalizedSegments(FileJournalManager.java:362)] Recovering unfinalized segments in /tmp/1492577657634-0/dfs/name1/current
2017-04-19 04:54:20,158 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FileJournalManager.recoverUnfinalizedSegments(FileJournalManager.java:362)] Recovering unfinalized segments in /tmp/1492577657634-0/dfs/name2/current
2017-04-19 04:54:20,159 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:669)] No edit log streams selected.
2017-04-19 04:54:20,185 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeSection(FSImageFormatPBINode.java:255)] Loading 1 INodes.
2017-04-19 04:54:20,197 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:181)] Loaded FSImage in 0 seconds.
2017-04-19 04:54:20,198 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:970)] Loaded image for txid 0 from /tmp/1492577657634-0/dfs/name1/current/fsimage_0000000000000000000
2017-04-19 04:54:20,206 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:982)] Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-04-19 04:54:20,207 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegment(FSEditLog.java:1218)] Starting log segment at 1
2017-04-19 04:54:20,302 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.NameCache.initialized(NameCache.java:143)] initialized with 0 entries 0 lookups
2017-04-19 04:54:20,303 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:688)] Finished loading FSImage in 159 msecs
2017-04-19 04:54:20,487 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:342)] RPC server is binding to localhost:0
2017-04-19 04:54:20,495 (Thread-0) [INFO - org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:53)] Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-04-19 04:54:20,509 (Socket Reader #1 for port 33849) [INFO - org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:606)] Starting Socket Reader #1 for port 33849
2017-04-19 04:54:20,541 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:652)] Clients are to use localhost:33849 to access this namenode/service.
2017-04-19 04:54:20,544 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerMBean(FSNamesystem.java:6031)] Registered FSNamesystemState MBean
2017-04-19 04:54:20,579 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.LeaseManager.getNumUnderConstructionBlocks(LeaseManager.java:136)] Number of blocks under construction: 0
2017-04-19 04:54:20,579 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.LeaseManager.getNumUnderConstructionBlocks(LeaseManager.java:136)] Number of blocks under construction: 0
2017-04-19 04:54:20,579 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initializeReplQueues(FSNamesystem.java:1182)] initializing replication queues
2017-04-19 04:54:20,580 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo.leave(FSNamesystem.java:5241)] STATE* Leaving safe mode after 0 secs
2017-04-19 04:54:20,580 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo.leave(FSNamesystem.java:5253)] STATE* Network topology has 0 racks and 0 datanodes
2017-04-19 04:54:20,581 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo.leave(FSNamesystem.java:5256)] STATE* UnderReplicatedBlocks has 0 blocks
2017-04-19 04:54:20,592 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateHeartbeatState(DatanodeDescriptor.java:451)] Number of failed storage changes from 0 to 0
2017-04-19 04:54:20,593 (Replication Queue Initializer) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:2763)] Total number of blocks            = 0
2017-04-19 04:54:20,594 (Replication Queue Initializer) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:2764)] Number of invalid blocks          = 0
2017-04-19 04:54:20,594 (Replication Queue Initializer) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:2765)] Number of under-replicated blocks = 0
2017-04-19 04:54:20,594 (Replication Queue Initializer) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:2766)] Number of  over-replicated blocks = 0
2017-04-19 04:54:20,594 (Replication Queue Initializer) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:2768)] Number of blocks being written    = 0
2017-04-19 04:54:20,594 (Replication Queue Initializer) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:2770)] STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 15 msec
2017-04-19 04:54:20,641 (IPC Server Responder) [INFO - org.apache.hadoop.ipc.Server$Responder.run(Server.java:836)] IPC Server Responder: starting
2017-04-19 04:54:20,641 (IPC Server listener on 33849) [INFO - org.apache.hadoop.ipc.Server$Listener.run(Server.java:676)] IPC Server listener on 33849: starting
2017-04-19 04:54:20,646 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices(NameNode.java:695)] NameNode RPC up at: localhost/127.0.0.1:33849
2017-04-19 04:54:20,646 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1103)] Starting services required for active state
2017-04-19 04:54:20,654 (CacheReplicationMonitor(827881346)) [INFO - org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:160)] Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-04-19 04:54:20,672 (Thread-0) [INFO - org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1413)] Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/tmp/1492577657634-0/dfs/data/data1,[DISK]file:/tmp/1492577657634-0/dfs/data/data2
2017-04-19 04:54:20,759 (Thread-0) [WARN - org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-04-19 04:54:20,768 (Thread-0) [INFO - org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:159)] DataNode metrics system started (again)
2017-04-19 04:54:20,776 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.datanode.BlockScanner.<init>(BlockScanner.java:172)] Initialized block scanner with targetBytesPerSec 1048576
2017-04-19 04:54:20,777 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:427)] Configured hostname is 127.0.0.1
2017-04-19 04:54:20,785 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1100)] Starting DataNode with maxLockedMemory = 0
2017-04-19 04:54:20,798 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.datanode.DataNode.initDataXceiver(DataNode.java:898)] Opened streaming server at /127.0.0.1:35465
2017-04-19 04:54:20,802 (Thread-0) [INFO - org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler.<init>

move start HMSFollower at the start of runServer(). v22

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12864020/SENTRY-1649.022-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2536/console

This message is automatically generated.

create HMSFollower in startHMSFollower and await termination after HMSFollower executor shutdownNow(). V23

attach v23 again to trigger build

same as v23, but change version to v24 to trigger build

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12864313/SENTRY-1649.024-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.thrift.TestSentryWebServerWithSSL

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2543/console

This message is automatically generated.

re-attach v24 to have another build

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12864322/SENTRY-1649.024-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2544/console

This message is automatically generated.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12864343/SENTRY-1649.024-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2546/console

This message is automatically generated.

create HMSFollower in constructor and startHMSFollower. See if that fixes the unit test failure. 

unit test failure at TestSentryWebServerWithSSL with java.io.IOException: HADOOP_HOME or hadoop.home.dir are not set
v25

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12864347/SENTRY-1649.025-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.thrift.TestSentryWebServerWithSSL

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2547/console

This message is automatically generated.

v24 again

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12864376/SENTRY-1649.024-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2549/console

This message is automatically generated.

The same as v24, but rebased on the latest changes in sentry-ha-redesign branch. V26

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12864430/SENTRY-1649.026-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.generic.service.persistent.TestPrivilegeOperatePersistence

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2554/console

This message is automatically generated.

v26 again

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12864486/SENTRY-1649.026-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2555/console

This message is automatically generated.

v26 the third time

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12864523/SENTRY-1649.026-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 4 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.generic.service.persistent.TestPrivilegeOperatePersistence

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2556/console

This message is automatically generated.

v26 4th times

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12864707/SENTRY-1649.026-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to an error

{color:red}ERROR:{color} failed to apply patch (exit code 1):
The patch does not appear to apply with p0, p1, or p2



Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2558/console

This message is automatically generated.

rebase on SENTRY-1587. Same as v24 and v26

v27 again. not build result from previous attachment

rebase for SENTRY-1691. v28

v28 again to trigger build

v29. implement shutdownAndAwaitTermination in SentryServiceUtil, and use it in SentryService

v30. create HMSFollower in constructor to avoid unit test failures

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12865005/SENTRY-1649.030-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2565/console

This message is automatically generated.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12865005/SENTRY-1649.030-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.thrift.TestSentryWebServerWithSSL

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2569/console

This message is automatically generated.

both sentry-1685 build 2567 and sentry-1649 build 2569 have the same failed test TestSentryWebServerWithSSL.setup, and both are caused by java.io.FileNotFoundException:

v30 again. to trigger build

build 2567, 2569 and 2571 all fail at TestSentryWebServerWithSSL.setup(), and the exception is "Caused by: java.io.FileNotFoundException: /home/jenkins/jenkins-slave/workspace/PreCommit-SENTRY-Build%402/sentry-provider/sentry-provider-db/target/test-classes/keystore.jks (No such file or directory)" It happened for 3 different sentry issues with different patches. This issue was reported in SENTRY-1400 on 12/Jul/16 15:24

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12865055/SENTRY-1649.030-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationEnd2End

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2573/console

This message is automatically generated.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12865055/SENTRY-1649.030-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationEnd2End

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2576/console

This message is automatically generated.

build 2573 and 2576 failed at "TestHDFSIntegrationEnd2End.testColumnPrivileges". It is not a stable test. And is reported in  (SENTRY-1723) Flaky test TestHDFSIntegrationEnd2End.

"Error Message

Error at verifying Path action : /user/hive/warehouse/db2.db/p1 ; expected:<READ_EXECUTE> but was:<null>
Stacktrace
java.lang.AssertionError: Error at verifying Path action : /user/hive/warehouse/db2.db/p1 ; expected:<READ_EXECUTE> but was:<null>
	at org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationEnd2End.testColumnPrivileges(TestHDFSIntegrationEnd2End.java:505)"

v31. rebased on latest commits. identical to v30

v31 again to trigger build

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12865261/SENTRY-1649.031-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 3 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.service.persistent.TestSentryStore

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2580/console

This message is automatically generated.

v31. to trigger build.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12865619/SENTRY-1649.031-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationEnd2End

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2601/console

This message is automatically generated.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12865619/SENTRY-1649.031-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationEnd2End

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2605/console

This message is automatically generated.

v32. merge with latest updates

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12865776/SENTRY-1649.032-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.provider.db.generic.service.persistent.TestPrivilegeOperatePersistence

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2613/console

This message is automatically generated.

It looks like your changes exposed some deeper problem with the tests.

Avoid creating local metastore from HMSFollower

Trying with disabled local metastore from HMSFollower

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12865897/SENTRY-1649.033-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationEnd2End

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2623/console

This message is automatically generated.

I think SENTRY-1747 should address the test failures. My previous attempt here had a bug, need to retry with the right fix.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12865997/SENTRY-1649.033-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationEnd2End

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2626/console

This message is automatically generated.

I look at the log message for succeeded test and failed test. The reason of the test failure is
a) TestHDFSIntegrationBase.hiveServer2 is static (protected static InternalHiveServer hiveServer2;). 
b) When the test_1 is cleaning up and removing roles, test_2 has started and created new hiveServer2, and the roles created are removed by test_1. So test_2 fails. 

Log messages:

1. For the succeeded test, the clean up messages, such as "Starting command: drop role col_role", from TestHDFSIntegrationBase.cleanAfterTest() only appear at the end of the log when the test ends.
2017-05-02 10:37:37,309 (HiveServer2-Handler-Pool: Thread-256) [INFO - org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:185)] Parsing command: drop role col_role
2017-05-02 10:37:37,349 (HiveServer2-Background-Pool: Thread-273) [INFO - org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1317)] Starting command: drop role col_role
2017-05-02 10:37:37,363 (pool-6-thread-3) [INFO - org.apache.sentry.provider.db.service.thrift.SentryPolicyStoreProcessor.drop_sentry_role(SentryPolicyStoreProcessor.java:441)] {"serviceName":"Sentry-Service","userName":"hive","impersonator":"","ipAddress":"/127.0.0.1","operation":"DROP_ROLE","eventTime":"1493739457363","operationText":"DROP ROLE col_role","allowed":"true","databaseName":null,"tableName":null,"column":null,"resourcePath":null,"objectType":"ROLE"}


2. For failed test, the clean up messages, such as "Starting command: drop role col_role", appear at the beginning of the log and the end of log. Inidcating cleanAfterTest() was called at the beginning of the test and the end of the test. This removes the configuration setup for the test, and causes the test to fail.
2017-04-26 10:07:22,986 (HiveServer2-Background-Pool: Thread-1365) [INFO - org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1317)] Starting command: drop role col_role
2017-04-26 10:07:23,007 (pool-86-thread-3) [INFO - org.apache.sentry.provider.db.service.thrift.SentryPolicyStoreProcessor.drop_sentry_role(SentryPolicyStoreProcessor.java:441)] {"serviceName":"Sentry-Service","userName":"hive","impersonator":"","ipAddress":"/127.0.0.1","operation":"DROP_ROLE","eventTime":"1493201243007","operationText":"DROP ROLE col_role","allowed":"true","databaseName":null,"tableName":null,"column":null,"resourcePath":null,"objectType":"ROLE"}

2017-04-26 10:06:00,674 (HiveServer2-Handler-Pool: Thread-255) [INFO - org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:185)] Parsing command: drop role 
col_role
2017-04-26 10:06:00,738 (HiveServer2-Background-Pool: Thread-446) [INFO - org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1317)] Starting command: drop role col_role
2017-04-26 10:06:00,762 (pool-6-thread-3) [INFO - org.apache.sentry.provider.db.service.thrift.SentryPolicyStoreProcessor.drop_sentry_role(SentryPolicyStoreProcessor.java:441)] {"serviceName":"Sentry-Service","userName":"hive","impersonator":"","ipAddress":"/127.0.0.1","operation":"DROP_ROLE","eventTime":"1493201160762","operationText":"DROP ROLE col_role","allowed":"true","databaseName":null,"tableName":null,"column":null,"resourcePath":null,"objectType":"ROLE"}


Can you clarify why would one test start before another test completes? This shouldn't happen.



v34. use fix from 1747 and add more logging message in failed test

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12866115/SENTRY-1649.034-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationEnd2End

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2627/console

This message is automatically generated.

v35. start HMSFollower sooner and check needHiveSnapshot at constructor

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12866206/SENTRY-1649.035-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2629/console

This message is automatically generated.

[~lina.li] There are a lot of updates in this JIRA, can you summarize the result - what you are changing and what were the issues with tests that you had to get around or address.

Patch v.35 with code review comments fixed.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12866318/SENTRY-1649.036-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 4 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationAdvanced
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.dbprovider.TestDbSentryOnFailureHookLoading
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationEnd2End

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2634/console

This message is automatically generated.

v37. create HMSFollower in startHMSFollower. Update based on comments

The changes are
1. move starting HMSFollower from SentryService constructor to runServer() when starting SentryService. This allows getting notification after start()/stop() SentryService
2. create HMSFollower in runServer(). This allows HMSFollower to pick up different hive configuration after stop()/start() of SentryService.
3. HMSFollower.run() checks if the meta store URL is set in configuration. If not, return. This avoid creating local meta store.
4. Change HMSFollower initial start delay to 0 ms. Before, it was 60K ms (1 minutes). This zero delay allows test verification happens after HMSFollower gets permission from hive. 
5. Close connections when HMSFollower.close() is called, which is called in SentryService.Stop().
6. Keep setting if we should get full snapshot in HMSFollower constructor. Otherwise, when the first full snapshot has nothing, a default AuthzObj is created in sentrystore. Since the latest notification ID is still 0, next run tries to get full snapshot again. When the second full snapshot has nothing, we have "SentryAlreadyExistsException: AuthzObj: default already exists". So I move the code for setting needHiveSnapshot back to constructor. "SENTRY-1751 HMSFollower should not persist empty full snapshot" is created for this issue.
7. disable the unit test due to "SENTRY-1750 HMSFollower does not handle view update correctly". When TestHDFSIntegrationEnd2End.testViews() runs before TestHDFSIntegrationEnd2End.testColumnPrivileges(), the create view notification event causes exception in processing, and HMSFollower does not process any following event. That causes testColumnPrivileges() to fail. 
8. Increase the time to wait from 100 ms to to twice of the HMSFollower task interval in testColumnPrivileges and related tests. In this way, HMSFollower should get the updates before verification. So the test is reliable.
9. fixed the failed test TestDbSentryOnFailureHookLoading.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12866333/SENTRY-1649.037-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 4 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationAdvanced
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.dbprovider.TestDbSentryOnFailureHookLoading
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationEnd2End

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2635/console

This message is automatically generated.

v35. To verify it works

v37. local test succeeds. check if build works

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12866427/SENTRY-1649.037-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.dbprovider.TestDbSentryOnFailureHookLoading

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2636/console

This message is automatically generated.

v39. fix unit test TestDbSentryOnFailureHookLoading. Increase wait time before verify in tests

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12866545/SENTRY-1649.039-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} mvn test exited 1
{color:red}ERROR:{color} Failed: org.apache.sentry.tests.e2e.hdfs.TestHDFSIntegrationEnd2End

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2641/console

This message is automatically generated.

v40. disable testViews due to sentry-1750

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12866635/SENTRY-1649.040-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2645/console

This message is automatically generated.

Here are the results of testing the latest attachment
https://issues.apache.org/jira/secure/attachment/12866635/SENTRY-1649.040-sentry-ha-redesign.patch against sentry-ha-redesign.

{color:green}Overall:{color} +1 all checks pass

{color:green}SUCCESS:{color} all tests passed

Console output: https://builds.apache.org/job/PreCommit-SENTRY-Build/2650/console

This message is automatically generated.

Transferring +1 from reviewboard.

[~lina.li] Thank you for your contribution!

