I tries it under 1.6.1, but I did not see anything that fixes that in 2.0.

anyone?

I don't think partitioning is supposed to maintain ordering.

Of course it does, every technology that supports partitioning supports ordering in the files themselves....
Otherwise you just don't provide a good solution for queries.

The fix is pretty small, I can work on it myself - how can I do that?

Yes, but a partitioning operation does not necessarily preserve ordering. Of course you can order it after that . Can you provide a pull request to show what you mean?

Usually you partition the data, and then you order it - this way you preserve ordering.
The problem here occurs in the writer itself, the DataFrame itself is partitioned and ordered correctly.

I would have some time to work on it next week or something like that, can I just do a pull request and put it here?


Hi,
I only got a chance to work on it now.
I saw that the whole class tree got changed - I changed the code in org.apache.spark.sql.execution.datasources.FileFormatWriter.
The problem is I cannot seem to run a mvn clean install...A lot of tests fail (not relevant to my change, and happen without it) - And I do want to make sure there are relevant tests (though I did not find any).

Any Ideas?

Also I cannot create a pull request, I get 403.

Ran,

but the whole thing is that the partitioning here actually happens a minute before writing it to the file itself - so no you can reorder it after that - it is already in the file.

Look, I think this is a serious bug  - it makes parquet files and orc files bigger than they have to be and really not efficient for reading.

I don't have any problems building or making PRs now, and Jenkins is OK. I'm not sure what error your'e seeing.

Hey, I'll do a pull and try again....
How to I gain access to create a pull request?

So...
Can you help?
Or at least point me to someone who can?

BTW On spark-core I get test errors like these:
org.apache.spark.JavaAPISuite.map(org.apache.spark.JavaAPISuite)
  Run 1: JavaAPISuite.setUp:85 Â» NoClassDefFound Could not initialize class org.apache....
  Run 2: JavaAPISuite.tearDown:92 NullPointer

I think it has something to do with this error that I also see:
Error while locating file spark-version-info.properties

See recent JIRAs about JavaAPISuite on Windows. I assume you might be on Windows? develop on Linux or OS X if you can, since I don't think Windows entirely works for development of Spark. I am not sure what other problems you're having because you didn't say, but, again I can build and pass all tests right now and so can Jenkins (again, modulo a non-trivial amount of test flakiness) https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/

I am running thee build on linux, so this is not it.
It is something to do with spark-version-info.properties missing.

I have basiaclly cloned the repository from https://github.com/apache/spark and ran "build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 clean install"

This always fails for me....can you point me to someone who can help me?

Not sure, I have no failures on OS X, on Ubuntu, and all of the Jenkins tests pass. I don't see reports of problems building.

[~ran.haim@optimalplus.com] I'm not sure how to proceed on this. Above you say that sort-then-partition doesn't preserve the sort, which is correct, it does not. For example, if I sort people by name, then partition by age, there's no way in general that they can stay sorted by name. Younger people may have names alphabetically after older people. Here you seem to be talking about partitioning then sorting. That leaves it sorted, though may change the partitioning. Do you mean partitioning by the same value you sort on? or partitioning one way and sorting within partitions another way?

If you're not able to open a PR, can you give a short example illustrating the issue?

Hi,
When you want to write your data to orc files or perquet files,
even if the dataframe is partitioned correctly, you have to tell the writer how to partition the data.
This means that when you want to write your data in a partitioned folder you lose sorting, and this is unacceptable when thinking on read performance and data on disk size.

I already changed the code locally, and it works as excpeted - but I have no permissions to create a PR, and I do not know how to get it.

Opening a pull request is available to everyone. Maybe you are trying to push a branch to apache/spark, which you can't do. You push branches to your own fork. In any event, can you say anything at all about the proposed change here?

Sure , I propose to stop using UnsafeKVExternalSorter, and just use a HashMap[String, ArrayBuffer[UnsafeRow]] - that is it basically.

It seems that in spark 2.1 code, the sorting issue is resolved.
The sorter does consider inner sorting in the sorting key - but I think it will be faster to just insert the rows to a list in a hash map.
In any case I suggest to change this issue to minor.


OK, if this seems to you resolved as of 2.1 and you aren't able to proceed with suggesting changes anyway, I'm going to resolve this.

Hi [~ran.haim@optimalplus.com], [~srowen],

How sure are we that this is not a problem anymore?  I've been testing some of my application code that relies on preserving ordering (for performance) on 2.1 (build from head of branch-2.1) and it still seems that after sorting within partitions, saving to parquet, then re-reading from parquet, that the rows are not correctly ordered.

{code}
val random = new scala.util.Random
val raw = sc.parallelize((1 to 1000000).map { _ => random.nextInt }).toDF("A").repartition(100)
raw.sortWithinPartitions("A").write.mode("overwrite").parquet("maybeSorted.parquet")
val maybeSorted = spark.read.parquet("maybeSorted.parquet")
val isSorted = maybeSorted.mapPartitions { rows =>
  val isSorted = rows.map(_.getInt(0)).sliding(2).forall { case List(x, y) => x <= y }
  Iterator(isSorted)
}.reduce(_ && _)
{code}

Hi,
I did not actually use 2.1 yet - so I cannot be 100% sure.
I can see that on org.apache.spark.sql.execution.datasources.FileFormatWriter it is creating a sorter using the inner sorting used:
val sortingExpressions: Seq[Expression] =
        description.partitionColumns ++ bucketIdExpression ++ sortColumns

The best way to test it is by using orcdump/parquet-tools by querying the individual files and making sure they are sorted.

Ahh, I think you are correct.  The issue on the write seems to be resolved as of 2.1+ (checked using parquet-tools).

On a read, some optimizations were made in https://github.com/apache/spark/pull/12095 that would mean that some files may get merged into a single partition (which will almost certainly be not sorted).  I mentioned the issue mailing list, but had missed the (useful) responses: http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-within-partitions-is-not-maintained-in-parquet-td18618.html

If I set spark.sql.files.openCostInBytes very high, the re-read test now seems fine.

Thanks!

Hi,
I think we need to reopen this.
It seems that on org.apache.spark.sql.execution.datasources.FileFormatWriter the sortColumns are the columns used by the buckting.
There is no way of telling the writer how to sort the data, when not using buckting.

I suggest to separate the sortby and the buckting data in the dataframewriter, and allow to use sortby even when not using buckting.

