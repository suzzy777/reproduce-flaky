+1. Thanks for filing this issue.

Another interesting question is whether we can easily make these suites run in parallel so that we don't slow down precommits too much.


Do you (or anyone) know how to set up Python version on Jenkins ?

Hey [~alanmyrvold], [~markflyhigh], [~yifanzou] 

Do you know  how do we configure the version of Python interpreter on Jenkins?

We eventually would like to have multiple versions of the interpreter, so I wonder what the best practices are to achieve that.

One option may be to simply install multiple versions of interpreter on Jenkins machines and rely on tox to discern, another - try to run test suites in a docker container with all necessary dependencies as desired. 

Do you have any advice here or perhaps you're working on something similar already?  

AFAIK, the python is installed with ubuntu-1604 in Jenkins slaves that configured by Apache Infra. [https://github.com/apache/infrastructure-puppet/blob/deployment/data/ubuntu/ubuntu_1604.yaml.]

We do have plans to try running tests in a docker container. https://docs.google.com/document/d/1y0YuQj_oZXC0uM5-gniG7r9-5gv2uiDhzbtgYYJW48c/edit

Thanks [~yifanzou]. I think we should find a way to parallelize the test execution, before adding more test suites, since slow precommits will impact developer productivity.

I heard from [~udim] that Bazel has better isolation of test environments and natively supports parallelizing test execution, on test suite and test level. 

My investigation from April 2018 is here: https://github.com/udim/beam/commit/3c08b21a9950e39bf71b150a087ae7bf7350f7c3

I haven't test with cython yet, but I just checked and Bazel has experimental support for pip: https://github.com/bazelbuild/rules_python/tree/master/examples/version


Lowered the priority to 'Minor' for now - I suggest reserving 'Major' priorities for issues that we must finish for Beam to be viable on Python 3.   

As an experiment, I [set up Python 3 environments on travis|https://github.com/manuzhang/beam/blob/travis/.travis.yml] based on recent master and [only Python 3.5 passes|https://travis-ci.org/manuzhang/beam] among 3.4, 3.5, 3.6 and 3.7. 

Thanks, [~mauzhang]. Unfortunately I can't see linked test results but if 3.5 is the only suite that passes, it's definitely a concern and a sign that we may want to add testing in other environments sooner than later. To avoid a significant Presubmit slow down, we can start with adding an on-demand Jenkins suite that will run tests in all environments. It would be also great to reach a common understanding what an efficiently parallelizable  Beam Python test suite will look like, cc: [~yifanzou] [~markflyhigh].  

I've summarized my thoughts on running tests using Bazel and Docker here:
https://lists.apache.org/thread.html/4be0f687135b7c6778224dd76389a39f9ebf78a3cf9c4cb4e76ebb73@%3Cdev.beam.apache.org%3E

[~tvalentyn], I fixed the travis link and you should be able to see now.

Thanks, [~mauzhang]. I looked at the logs, and also verifed myself that some tests that pass on Python 3.5 on Jenkins, fail in other versions of the interpreter. FYI [~matthiasml6] [~RobbeSneyders] [~splovyt] [~Juta].

 For example:

python ./setup.py test -s apache_beam.typehints.typed_pipeline_test.SideInputTest.test_basic_side_input_hint  fails on Python 3.4 with:

======================================================================
ERROR: test_basic_side_input_hint (apache_beam.typehints.typed_pipeline_test.SideInputTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/beam/sdks/python/apache_beam/typehints/typed_pipeline_test.py", line 173, in test_basic_side_input_hint
    self._run_repeat_test(repeat)
  File "/beam/sdks/python/apache_beam/typehints/typed_pipeline_test.py", line 144, in _run_repeat_test
    self._run_repeat_test_good(repeat)
  File "/beam/sdks/python/apache_beam/options/pipeline_options.py", line 803, in wrapper
    f(*args, **kwargs)
  File "/beam/sdks/python/apache_beam/typehints/typed_pipeline_test.py", line 150, in _run_repeat_test_good
    result = ['a', 'bb', 'c'] | beam.Map(repeat, 3)
  File "/beam/sdks/python/apache_beam/transforms/ptransform.py", line 496, in __ror__
    p.run().wait_until_finish()
  File "/beam/sdks/python/apache_beam/pipeline.py", line 403, in run
    self.to_runner_api(), self.runner, self._options).run(False)
  File "/beam/sdks/python/apache_beam/pipeline.py", line 416, in run
    return self.runner.run_pipeline(self)
  File "/beam/sdks/python/apache_beam/runners/direct/direct_runner.py", line 139, in run_pipeline
    return runner.run_pipeline(pipeline)
  File "/beam/sdks/python/apache_beam/runners/portability/fn_api_runner.py", line 231, in run_pipeline
    return self.run_via_runner_api(pipeline.to_runner_api())
  File "/beam/sdks/python/apache_beam/runners/portability/fn_api_runner.py", line 234, in run_via_runner_api
    return self.run_stages(*self.create_stages(pipeline_proto))
  File "/beam/sdks/python/apache_beam/runners/portability/fn_api_runner.py", line 967, in create_stages
    pcoll.coder_id = coders.get_id(coder)
  File "/beam/sdks/python/apache_beam/runners/pipeline_context.py", line 79, in get_id
    self._id_to_proto[id] = obj.to_runner_api(self._pipeline_context)
  File "/beam/sdks/python/apache_beam/coders/coders.py", line 259, in to_runner_api
    component_coder_ids=[context.coders.get_id(c) for c in components])
  File "/beam/sdks/python/apache_beam/coders/coders.py", line 259, in <listcomp>
    component_coder_ids=[context.coders.get_id(c) for c in components])
  File "/beam/sdks/python/apache_beam/runners/pipeline_context.py", line 79, in get_id
    self._id_to_proto[id] = obj.to_runner_api(self._pipeline_context)
  File "/beam/sdks/python/apache_beam/coders/coders.py", line 250, in to_runner_api
    urn, typed_param, components = self.to_runner_api_parameter(context)
  File "/beam/sdks/python/apache_beam/coders/coders.py", line 276, in to_runner_api_parameter
    google.protobuf.wrappers_pb2.BytesValue(value=serialize_coder(self)),
  File "/beam/sdks/python/apache_beam/coders/coders.py", line 67, in serialize_coder
    pickler.dumps(coder))
TypeError: unsupported operand type(s) for %: 'bytes' and 'tuple'


[~mauzhang] It also seems that your runs also included some tests that we currently skip in Python 3, for example I think 3.4 logs include a skipped test apache_beam.runners.portability.fn_api_runner_test.FnApiRunnerTestWithGrpc.test_pardo_metrics
 

[~tvalentyn], I simply run "./gradlew testPython3" for each environment. The Python 3.4 test is much much longer. Is there a flag that doesn't work in 3.4 ?

I may be wrong but I suspect for some reason `@unittest.skipif` annotation did not get trigger in your test suite, and then the suite ran into  BEAM-5623 which take long to finish. The Travis logs are truncated so I could see if we ran those tests or not.

I tried to run py3 tox test suite from python:3.4-strech conatiner (see: https://s.apache.org/beam-py3-conversion-quick-start) and 117 tests didn't pass. The test suite finished within few minutes. 

I don't think 3.4 is important to target (it's already in security-bugfix-only mode). 3.6 and 3.7, yes. 

https://issues.apache.org/jira/browse/INFRA-17335 is created on Infra team to install other versions of python 3 on Jenkins.

Per [https://devguide.python.org/,] EOL for Python 3.4 is 2019-03-16, so I agree with [~robertwb] that it's no longer important to target.

cc: [~RobbeSneyders].

agree. I'll update the INFRA ticket.

[~markflyhigh], [~tvalentyn] and [~robertwb], it seems there's been no response from the infra for a while. How can we push this forward ?

I have reached out to #asfinfra via [http://infra.chat|http://infra.chat/], and [~gmcdonald] graciously offered help. Thanks, [~gmcdonald].

