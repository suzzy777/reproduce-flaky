First of all, this is really great. Thanks for creating the issue.

I have looked into little bit of Mahout's Spark Shell, how it works.
In my understanding, Mahout's Spark Shell is creating REPL from IMain with sdc injected. (Mahout distributed context). 
If it is, how about load Mahout as a library in existing spark interpreter.

I tried quick test and screenshot attached (mahout-existing-spark-test.png). 
Which is simply loading "org.apache.mahout:mahout-spark_2.10:0.10.1" and importing necessary packages and create org.apache.mahout.sparkbindings.SparkDistributedContext.
I'd like to hear about opinion about this approach.

Thank you! That's exactly the guidance i was looking for. It would not seem to be much point in doing mahout as standalone interpreter -- it is just bunch of imports and libraries. 

it does require setting spark context in a specific way though; e.g. it supports kryo serialization for its matrices and vectors only. so context needs to be factored in a certain way (there's a mahout wrapper around context in each case. mahout package functions require this context to be an implicit val (just like in a spark shell) as well. 

Hi, I'm having trouble getting this to work. I understand that the sparkcontext needs to be configured to use the kryo serialization, but I can't find where the conf for embedded spark instance is. Can someone clarify the posted example with the serialization setting?

EDIT: It looks like there's no way to set it currently without modifying the source. I added the line conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer"); into the code that generates the SparkContext in the SparkInterpreter and Mahout started to work.


Could you try 'Interpreter' menu?
It allows you modify configuration of spark context.
A screenshot attached.

Perfect, that works! No source code modification is needed. Thank you Moon Soo!

is it possible to enable standard zeppelin plotting? 
Also, is it possible to exchange plotting data between different interpreters that potentially can plot better? i.e. get data from the mahout+spark and pass it over for plotting to python?

Sorry if i am asking questions too simple. i am not quite familiar with Zeppelin architecture and premises. 

What i am getting at, would there be a scenario to use e.g. matplotlib to plot say contour plots on data summaries obtained from mahout scripting?

also i suppose we still need an additional code to extract data for plotting. 

If you mean built in charts, please see "Display as Table, Chart" section on http://zeppelin.incubator.apache.org/docs/display.html. Your Interpreter/Library just need to print data in this format to leverage built in charts.

Exchanging data between different interpreters are possible, if interpreters are in the same group.
For example, SparkInterpreter, SparkSqlInterpreter, PysparkInterpreter are all in the same group "spark', and they share single SparkContext instance.

Let me know if it helps.

Thank you. 

what is definition of group?

For example, SparkInterpreter, https://github.com/apache/incubator-zeppelin/blob/master/spark/src/main/java/org/apache/zeppelin/spark/SparkInterpreter.java#L91 is defining group.

Basically Zeppelin trying to provide sandbox environment for each interpreter instance to prevent conflict their dependency library and standard io.
However, if the interpreter's are in the same group, they run inside of the same JVM process and they can exchange some information.


I see. thank you very much.

I'm having two issues with the procedure tested in the screenshot

issue 1 : The first one is concerning the dependency part which is asking me the following : 
          
          "Must be used before SparkInterpreter (%spark) initialized"

solution: I'm not sure if that is supposed to happen but my solution for that was creating a new interpreter and loading the dependency in it
          
          %spark
          z.load("org.apache.mahout:mahout-spark_2.10:0.10.1")

issue 2 : The second issue was with the mahout libraries import

          import org.apache.mahout.math._
          import scalabindings._
          import RLikeOps._
          import drm._
          import RLikeDrmOps._

          import org.apache.mahout.sparkbindings._
          import collection.JavaConversions._

          @transient implicit val sdc : org.apache.mahout.math.drm.DistributedContext = new org.apache.mahout.sparkbindings.SparkDistributedContext(sc)

output : 

import org.apache.mahout.math._
import scalabindings._
import RLikeOps._
import drm._
import RLikeDrmOps._
import org.apache.mahout.sparkbindings._
import collection.JavaConversions._
<console>:96: error: type mismatch;
 found   : org.apache.spark.org.apache.spark.org.apache.spark.org.apache.spark.org.apache.spark.SparkContext
 required: org.apache.spark.org.apache.spark.org.apache.spark.org.apache.spark.org.apache.spark.SparkContext
       @transient implicit val sdc : org.apache.mahout.math.drm.DistributedContext = new org.apache.mahout.sparkbindings.SparkDistributedContext(sc)

no solution for now for the second issue. So I was wondering also if that is supposed to happen?

Issue 1 is expected. The reason is dependency library need to loaded with scala compiler creation. You can either restart interpreter or restart Zeppelin, when you see the message.

issue 2 : i've just tested again with both local mode, standalone mode of spark cluster. Both works without error. Could you share how did you install/configure Zeppelin?


I haven't done much edits : 

$> sudo mvn clean install -Pspark-1.4 -DskipTests -Drat.skip=true # I needed the last option to workaround the RatCheckException error

I have set the server.port to 8989. It's running locally.

I have exported the MASTER to match my spark master address and 
ZEPPELIN_JAVA_OPTS=export ZEPPELIN_JAVA_OPTS="-Dspark.executor.memory=8g -Dspark.cores.max=4".

That's it!



Sorry to necromance here, but has there been any movement on this recently?

I was getting ready to take a crack at it, but I don't want to duplicate efforts. 

Also, I know there has been quite a bit of refactoring since last July- anyone want to chime in on gotchyas that I should be on the lookout for?

GitHub user rawkintrevo opened a pull request:

    https://github.com/apache/incubator-zeppelin/pull/928

    [ZEPPELIN-116] Add Mahout Support for Spark Interpreter

    ### What is this PR for?
    This PR adds Mahout functionality for the Spark Interpreter.
    
    
    ### What type of PR is it?
    Improvement 
    
    ### Todos
    [ ] - Implement Mahout Interpreter in Spark
    [ ] - Add Unit Tests
    [ ] - Add Documentation
    [ ] - Add Example Notebook
    
    ### What is the Jira issue?
    https://issues.apache.org/jira/browse/ZEPPELIN-116
    
    ### How should this be tested?
    Open a Spark Notebook with Mahout enabled and run a few simple commands using the R-Like DSL and Spark Distributed Context (Mahout Specific)
    
    ### Screenshots (if appropriate)
    
    ### Questions:
    * Does the licenses files need update?
    No
    * Is there breaking changes for older versions?
    No
    * Does this needs documentation?
    Yes
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rawkintrevo/incubator-zeppelin mahout-terp

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-zeppelin/pull/928.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #928
    
----
commit f7a70f200c31816696cb5bfed151e347dc1a728c
Author: Trevor Grant <trevor.d.grant@gmail.com>
Date:   2016-05-23T22:29:25Z

    [ZEPPELIN-116] Add Mahout Support for Spark Interpreter

----


Github user bzz commented on the pull request:

    https://github.com/apache/incubator-zeppelin/pull/928#issuecomment-222579453
  
    @rawkintrevo thank you for contribution!
    
    In my experience, if CI is failing and no reason is investigated\posted on PR by the author - it takes much longer to get the PR reviewed by committers, as passing CI is a precondition for getting things merged.


Github user Leemoonsoo commented on the pull request:

    https://github.com/apache/incubator-zeppelin/pull/928#issuecomment-222592450
  
    Thanks @rawkintrevo for the contribution.
    
    I think mahout support can be 
    
    a) integrated into SparkInterpreter (like this patch) 
    b) provide documentation to user to add mahout dependency, import package and create `SparkDistributedContext` (without code integration)
    
    I think both have pros and cons. If you elaborate pros and cons of both approaches, that would be really helpful.


Github user rawkintrevo commented on the pull request:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    @bzz Thanks for the comments, It builds fine on my machine, I wanted to see how it did against CI.  Been out of town for a wedding over the holiday weekend (in US Monday was a holiday). I didn't think anyone would look at it before today, and apologies for not outlining things more clearly.  Looking at the fail logs, it seems like flakey fails, but I'll investigate further this evening.  
    
    @Leemoonsoo also thanks for the comments. 
    I do have some documentation on this as well at:
    https://trevorgrant.org/2016/05/19/visualizing-apache-mahout-in-r-via-apache-zeppelin-incubating/
    
    I agree there are those two approaches, and will present a 3rd where 'Mahout' lives as it's own interpreter.
    
    so approach (1) a patch, approach (2) documentation only, approach (3) a stand alone interpreter.
    
    In general, options 1 and 3 make integrating Apache Mahout into notebooks more 'turn-key' and since one of the primary use cases of Apache Zeppelin is data discovery analytics, this seems like a natural fit that would benefit both projects AND end users.  Cons of 1 and 3 are that they add code that must be maintained; as far as invasiveness of patch and code which must be maintained, 1 is vastly superior to 3 (for 3 we would be basically copy and pasting the spark interpretter over with this patch applied). I do agree with Moon's comments above for paramter setting based on 'spark.mahout=true/false" this is to bzz's point WIP.
    
    So pros/cons of approach 2 is in essence the inverse of 1 and 3. The pro: no additional code to maintain (though a blog post which must be maintained as I've learned the hard way re my post on Installing Zeppelin with Spark and Flink), the cons of which: while Zeppelin does make all of the steps required fairly easy, the steps required to add Mahout support would be at least fairly considered an "intermediate" Zeppelin skill level.  This is a barrier to entry to several Apache Mahout users who are excited to see this integration as a means for visualization in Mahout.
    
    My .02. 
    



Github user Leemoonsoo commented on the pull request:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    @rawkintrevo Thanks for the explanation. Approach 1 to lower the barrier and keep maintenance minimum make sense.
    
    **License**
    Following new dependencies are introduced. They need to be addressed in https://github.com/apache/incubator-zeppelin/tree/master/zeppelin-distribution/src/bin_license
    ```
    [INFO] |  +- it.unimi.dsi:fastutil:jar:7.0.11:compile
    [INFO] |  \- com.tdunning:t-digest:jar:3.1:compile
    [INFO] |  +- com.github.scopt:scopt_2.10:jar:3.3.0:compile
    [INFO] |  \- com.github.haifengl:smile-plot:jar:1.1.0:compile
    [INFO] |     +- com.github.haifengl:smile-core:jar:1.1.0:compile
    [INFO] |     |  +- com.github.haifengl:smile-data:jar:1.1.0:compile
    [INFO] |     |  +- com.github.haifengl:smile-math:jar:1.1.0:compile
    [INFO] |     |  \- com.github.haifengl:smile-graph:jar:1.1.0:compile
    [INFO] |     \- org.swinglabs:swingx:jar:1.6.1:compile
    [INFO] |        +- com.jhlabs:filters:jar:2.0.235:compile
    [INFO] |        \- org.swinglabs:swing-worker:jar:1.1:compile
    [INFO] |     +- com.thoughtworks.xstream:xstream:jar:1.4.4:compile
    [INFO] |     |  +- xmlpull:xmlpull:jar:1.1.3.1:compile
    [INFO] |     |  \- xpp3:xpp3_min:jar:1.1.4c:compile
    ```
    Check dependency tree with `mvn package dependency:tree` command
    
    **Dependency confliction**
    Following dependency versions are different from dependency brought by Spark.
    
    Mahout
    ```
    com.esotericsoftware.kryo:kryo:jar:2.24:compile
    com.fasterxml.jackson.core:jackson-core:jar:2.7.2:compile
    ```
    
    Spark (1.6)
    ```
    com.esotericsoftware.kryo:kryo:jar:2.21:compile
    com.fasterxml.jackson.core:jackson-core:jar:2.4.4:compile
    ```
    Can all conflicted dependencies be excluded ?
    
    
    **Packaging**
    If mahout requires to be loaded in spark executor's classpath, then adding mahout dependency in pom.xml will not be enough to work with Spark cluster. Could you clarify if mahout need to be loaded in spark executor?



Github user rawkintrevo commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    Refactored the dependencies out of pom.xml
    
    Shouldn't bee any conflicts now.
    
    It is on the use to specify the directory which contain the nessecary Mahout jars.  Talking to the Mahout community to make these exist in a consistent place in src/binaries of Mahout


Github user rawkintrevo commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    And intellij took a lot of liberties refactoring my imports... going to change those back and re push


Github user Leemoonsoo commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    And i think it's good idea to add new configuration to https://github.com/apache/incubator-zeppelin/blob/master/docs/interpreter/spark.md#configuration


Github user rawkintrevo commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    Good call out @Leemoonsoo I have plans to put together a nice documentation page, once we settle on the approach/API/settings (which I believe we now have done? feedback welcome).  I would not have remembered to add to the spark-config page.
    
    I'm going to go ahead and begin writing docs.  
    
    Thoughts on unit tests? I don't think the build env will magically have the required Mahout jars laying around (new dependency light version expects the users to copy 3-4 jars to a user defined directory).
    
    How was the unit testing handled with R?



Github user rawkintrevo commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    Thoughts on why this isn't building?


Github user Leemoonsoo commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    You'll need resolve conflicts (rebase or merge master) to trigger CI.
    
    One approach possible is, i think if Mahout jars are available in maven repository, unittest can use [DependencyResolver](https://github.com/apache/incubator-zeppelin/blob/master/zeppelin-interpreter/src/main/java/org/apache/zeppelin/dep/DependencyResolver.java#L89) to download them.


Github user dlyubimov commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    is it possible to have a dialect, meaning requiring something like 
    
        %spark.mahout
        ... 
    
    while sharing configuration of most of the settings with the %spark?


Github user felixcheung commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    IMHO extending the scala interpreter is preferred instead of creating another interpreter (%spark.mahout suggestion) since it is very similar..


Github user Leemoonsoo commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    I think whether creating mahout interpreter or not will be depends on expectation that how user uses mahout.
    
    If usecase is let spark user uses mahout during their work along with all the other libraries, it make sense to load mahout as a library.
    
    But if usecase is more like mahout user leverages spark, flink, etc as a execution engine to run mahout algorithms and more tight integration between Zeppelin and Mahout required in the future, then creating an interpreter more make sense. 
    
    @rawkintrevo What do you think?


Github user rawkintrevo commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    @Leemoonsoo definitely the latter... *sigh*.  Beginning work on a refactor (actually already begun bc I had reached the same conclusion). 



Github user felixcheung commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    Then perhaps it should be a separate one (eg. %mahout)


Github user rawkintrevo commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    yea, thats what I'm thinking, then we'll have mahout.spark , mahout.flink, mahout.h20
    



Github user felixcheung commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    how would we support different languages in this case? for instance, Apache Spark can be in Java/Scala/Python/R, Apache Flink can be in Java/Scala/Python, H2O can be in Java/Python/R


Github user rawkintrevo commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    my thought is only support spark.  If you want to use pyspark/spark/ blah - use the spark interpreter. 
    We'll only support additional langs as Mahout introduces interpreters for them. eg. right now only Spark, coming soon, flink-spark, I'm not sure the time tables on H20-Mahout interpreter. 
    
    Then we're going to use the Resource pool to move tables from Mahout to R/Python for plotting. you can stringify a tsv put it in the resourcepool, fish it out in python or R then make a dataframe out of it and plot it. Since its only for plotting, there is a natural limit on how big it should be in the first place. 


Github user rawkintrevo commented on the issue:

    https://github.com/apache/incubator-zeppelin/pull/928
  
    Total refactor- extends spark interpreter, (not done before as far as I know, but make more sense imho).  Keeps settings local to mahout, not interfering with main spark terp. 
    
    Best of both worlds- seperates out into %mahout interpreter, keeps settings seperate from main spark interpreter, but still has very low overhead in terms of new code (easily maintainable). 
    
    Please lmk your thoughts. 
    
    Still needs docs/tests. 


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    This says no conflicts but didn't try to rebuild...? I merged with master hoping to retrigger a build. Not sure what I did wrong here but obviously something...


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    If someone could help me out I'd appreciate it...
    
    First of all, this works fine as expected in the notebooks (either way). 
    
    In MahoutSparkInterpreter.java line 89, there is a jar I can load. 
    
    If I don't load it, on testing only, I get the following error:
    ```
    No Import : org.apache.mahout:mahout-spark-shell_2.10:0.12.1
    <console>:35: error: type mismatch;
     found   : org.apache.spark.SparkContext
     required: org.apache.mahout.sparkbindings.SparkDistributedContext
           implicit val sdc: SparkDistributedContext = sc
                                                       ^
    ```
    
    If I do load it, again in testing only, I get the following errors (truncated):
    ```
    With org.apache.mahout:mahout-spark-shell_2.10:0.12.1:
    
    == Enclosing template or block ==
    
    Import( // val <import>: ImportType(org.apache.mahout.sparkbindings)
      "org"."apache"."mahout"."sparkbindings" // final package sparkbindings in package mahout, tree.tpe=org.apache.mahout.sparkbindings.type
      List(
        ImportSelector(_,1227,null,-1)
      )
    )
    
    uncaught exception during compilation: scala.reflect.internal.FatalError
    <console>:4: error: illegal inheritance;
     self-type line2125238280$23.$read does not conform to scala.Serializable's selftype Serializable
    class $read extends Serializable {
                        ^
    <console>:4: error: Serializable does not have a constructor
    class $read extends Serializable {
                ^
    <console>:6: error: illegal inheritance;
     self-type $read.this.$iwC does not conform to scala.Serializable's selftype Serializable
      class $iwC extends Serializable {
                         ^
    <console>:6: error: Serializable does not have a constructor
      class $iwC extends Serializable {
                 ^
    <console>:11: error: illegal inheritance;
     self-type $iwC does not conform to scala.Serializable's selftype Serializable
    class $iwC extends Serializable {
                       ^
    ...
    ```
    
    Since this works in the actual notebooks, I'm not sure what I'm doing wrong in setting up the testing env.  Any advice would be appreciated.
    
    Thanks,
    
    tg



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    This appears to be working, but there is a bug when doing OLS regarding the thift server. It is the same error message one normally gets when trying to use incompatible versoin of spark. Is triggered by ".collect" method on drm.  But works fine for other operations.  Very confusing.



Github user dlyubimov commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    what's the message? 
    `DRMLike.collect()`, eventually,  is a translation to `RDD.collect()`


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Consider the code
    ```scala
    %mahout
    
    val mxRnd = Matrices.symmetricUniformView(5000, 2, 1234)
    val drmRand = drmParallelize(mxRnd)
    
    
    val drmSin = drmRand.mapBlock() {case (keys, block) =>  
      val blockB = block.like()
      for (i <- 0 until block.nrow) {
        blockB(i, 0) = block(i, 0) 
        blockB(i, 1) = Math.sin((block(i, 0) * 8))
      }
      keys -> blockB
    }
    
    val drmRand = drmParallelize(mxRnd)
    drmSampleToTSV(drmRand, 0.5)
    ```
    Works fine, I can take the resulting string and do things with it.
    
    However when I run
    ```scala
    drmRand.collect(::, 1)
    ```
    
    The error depends if I am using the Zeppelin Dependency Resolver (e.g. downloading jars from Maven) or pointing to `MAHOUT_HOME`.  
    
    If using the dependency resolver, the error is:
    org.apache.thrift.transport.TTransportException
        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
        at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
    ...
    
    ^^ This is a standard error message when Zepplin-Spark and the Spark on the Cluster are version mismatched, however this was curious as I was running Spark in local mode. 
    I added <scope>provided</scope> to the zeppelin-spark dependency in the mahout pom.xml,  now both local and MAHOUT_HOME give the same error message which is as follows.  This lead me to believe (and I still do) the problem lies in version mismatching?:
    
    org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 192.168.86.152): java.lang.IllegalStateException: unread block data
    	at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2431)
    	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1383)
    	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
    	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)
    	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:207)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    	at java.lang.Thread.run(Thread.java:745)
    Driver stacktrace:
    	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
    	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
    	at scala.Option.foreach(Option.scala:236)
    	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
    	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
    	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)
    	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
    	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
    	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
    	at org.apache.spark.rdd.RDD.collect(RDD.scala:926)
    	at org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark.collect(CheckpointedDrmSpark.scala:128)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:54)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:59)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:61)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:63)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:65)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:67)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:69)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:71)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:73)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:75)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:77)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:79)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:81)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:83)
    	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:85)
    	at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:87)
    	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:89)
    	at $iwC$$iwC$$iwC.<init>(<console>:91)
    	at $iwC$$iwC.<init>(<console>:93)
    	at $iwC.<init>(<console>:95)
    	at <init>(<console>:97)
    	at .<init>(<console>:101)
    	at .<clinit>(<console>)
    	at .<init>(<console>:7)
    	at .<clinit>(<console>)
    	at $print(<console>)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:606)
    	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
    	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
    	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
    	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
    	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
    	at org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:793)
    	at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:736)
    	at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:729)
    	at org.apache.zeppelin.mahout.MahoutSparkInterpreter.interpret(MahoutSparkInterpreter.java:141)
    	at org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)
    	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)
    	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:345)
    	at org.apache.zeppelin.scheduler.Job.run(Job.java:176)
    	at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
    	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    	at java.lang.Thread.run(Thread.java:745)
    Caused by: java.lang.IllegalStateException: unread block data
    	at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2431)
    	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1383)
    	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
    	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)
    	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:207)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    	... 1 more



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    UPDATE: 
    
    Sorry for the quick one-two punch. But the above error only occurs in Spark cluster mode, not in Spark local mode. Leading me to believe jars aren't getting loaded up. 


Github user dlyubimov commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    the only i thing i can think of is if somehow to different versions of
    spark or any of its dependencies were loaded into two different
    classloaders. I think spark uses a dedicated classloader even for tasks.
    but i definitely saw it before and although i do not recollect the context
    of what was wrong, eventually i think it was down to that, several versions
    of the same class were present, which were transitive dependencies of
    something.
    
    This is a very nasty situation to trace, i can remember that too.
    
    On Thu, Jun 16, 2016 at 4:36 PM, Trevor Grant <notifications@github.com>
    wrote:
    
    > UPDATE:
    >
    > Sorry for the quick one-two punch. But the above error only occurs in
    > Spark cluster mode, not in Spark local mode. Leading me to believe jars
    > aren't getting loaded up.
    >
    > —
    > You are receiving this because you commented.
    > Reply to this email directly, view it on GitHub
    > <https://github.com/apache/zeppelin/pull/928#issuecomment-226643983>, or mute
    > the thread
    > <https://github.com/notifications/unsubscribe/AAf7_xQ4PcXMJMdHuM1ssSa9q-_EKTc3ks5qMd4XgaJpZM4IpU0W>
    > .
    >



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    As I am playing with this, things seem to stop/start working at random... The Thrift Server error in the Zepplin context, with Java Heap Space errors related to the kryo serializer in the logs.  Some sort of mis-configuration errors (?) 



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Per this:
    http://stackoverflow.com/questions/32498891/spark-read-and-write-to-parquet-leads-to-outofmemoryerror-java-heap-space
    
    and this:
    http://spark.apache.org/docs/latest/configuration.html#compression-and-serialization
    
    I set the buffer to 64k and the buffer.max to 1g, and that seems to have solved it.


Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Great work, @rawkintrevo ! It looks like a rebase on the latest master is needed now


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @bzz not quite done.  A little more testing and I realized, jars aren't being properly loaded when Spark is in cluster mode.  Think you could take a peek and try to give me a hint why that might be?
    
    Either way, I hope to to have it all worked out by EOD tomorrow.  Will rebase when I get cluster working then should be gtg!


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @bzz and @Leemoonsoo 
    
    A big part of the refactor was introducing no new dependencies- instead loading from maven or MAHOUT_HOME at interpretter start up via dependency resolver.
    
    This was done to get around version mismatches.
    
    Here is output from `mvn dependency:tree`  I'm not super familiar, so I'm not 100% sure there are nothing new, but I am like 97% sure. 
    ```
    [INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ zeppelin-mahout ---
    [INFO] org.apache.zeppelin:zeppelin-mahout:jar:0.6.0-SNAPSHOT
    [INFO] +- org.slf4j:slf4j-api:jar:1.7.10:compile
    [INFO] +- org.slf4j:slf4j-log4j12:jar:1.7.10:compile
    [INFO] |  \- log4j:log4j:jar:1.2.17:compile
    [INFO] +- org.apache.zeppelin:zeppelin-interpreter:jar:0.6.0-SNAPSHOT:provided
    [INFO] +- org.apache.zeppelin:zeppelin-spark:jar:0.6.0-SNAPSHOT:compile
    [INFO] +- org.apache.zeppelin:zeppelin-spark-dependencies:jar:0.6.0-SNAPSHOT:compile
    [INFO] +- com.google.code.gson:gson:jar:2.2:compile
    [INFO] +- org.datanucleus:datanucleus-core:jar:3.2.10:compile
    [INFO] +- org.datanucleus:datanucleus-api-jdo:jar:3.2.6:compile
    [INFO] +- org.datanucleus:datanucleus-rdbms:jar:3.2.9:compile
    [INFO] +- org.scala-lang:scala-library:jar:2.10.4:compile
    [INFO] +- org.scala-lang:scala-compiler:jar:2.10.4:compile
    [INFO] +- org.scala-lang:scala-reflect:jar:2.10.4:compile
    [INFO] \- junit:junit:jar:4.11:test
    [INFO]    \- org.hamcrest:hamcrest-core:jar:1.3:test
    ```


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Lot's of connection test failures when building Zeppelin-Server?


Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    That sounds great @rawkintrevo on licenses !
    Sorry, in preparation to 0.6 release people get quite busy.. 


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @bzz I can imagine. Builds are still failling at random,and I'm not sure what to make of it.  mvn clean verify works well on my system.  Let me know if there is anymore I can do to test/work on this for you.
    
    tg



Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Thank you for your patience!
    Could you rebase your work on latest master please and I'll be happy to look into the CI build failures again?


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Merged Master- still failing on Zeppelin Server at 
    ```
    15:27:30,479 ERROR org.apache.zeppelin.rest.AbstractTestRestApi:251 - Exception in AbstractTestRestApi while checkIfServerIsRunning 
    java.net.ConnectException: Connection refused
    ```


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Also OOM errors in the actual Mahout interpretter...


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Also OOM errors in the actual Mahout interpretter...


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @bzz I've rebased to master twice now.  The failures are on:
    - Zeppelin Server: Connection Refused
    - Mahout : Out of Memory
    
    `mvn clean verify` works for me locally under a variety of circumstances.  I don't know much about trouble shooting travis builds.  I know you have 0.6 rolling out and are busy, but please take a quick look and / or point me to resources/give me some hints for solving these issues. 
    
    Thanks!
    
    tg


Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    ### CI failure debug approach
    First thing to do in order to debug such issues: 
     - link all raw build logs
     - extact and systematize failure resaons from logs (usulaly it's last 'exited with 1' message + first exception)
     - cross-check them with list of [flaky-tests in JIRA](https://issues.apache.org/jira/browse/ZEPPELIN-862?jql=project%20%3D%20ZEPPELIN%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20labels%20%3D%20flaky-test)
    
    Usually, after all this is done and posted - it is much easier for other people to jump in and help in the drive-by reviews.
    
    ### CI failures in this PR
    
     1. `ZeppelinSparkClusterTest` in Zeppelin Server  failing
    
      Profiles # [1](https://api.travis-ci.org/jobs/141222530/log.txt?deansi=true), [2](https://api.travis-ci.org/jobs/141222531/log.txt?deansi=true), [3](https://api.travis-ci.org/jobs/141222532/log.txt?deansi=true),
    
      ```
    Failed tests: 
      ZeppelinSparkClusterTest.pySparkDepLoaderTest:231 expected:<FINISHED> but was:<ERROR>
      ZeppelinSparkClusterTest.pySparkAutoConvertOptionTest:152 expected:<FINISHED> but was:<ERROR>
      ZeppelinSparkClusterTest.pySparkTest:127 expected:<FINISHED> but was:<ERROR>
    
    Tests run: 63, Failures: 3, Errors: 0, Skipped: 0
      ```
    
      Profile [4](https://api.travis-ci.org/jobs/141222533/log.txt?deansi=true) has only single test failure
      ```
    Results :
    
    Failed tests: 
      ZeppelinSparkClusterTest.pySparkTest:127 expected:<FINISHED> but was:<ERROR>
    
    Tests run: 40, Failures: 1, Errors: 0, Skipped: 0
      ```
    
      Profile [6](https://api.travis-ci.org/jobs/141222535/log.txt?deansi=true) have even more tests
    
      ```
    Running org.apache.zeppelin.rest.ZeppelinSparkClusterTest
    ....
    5:06:37,836  INFO org.apache.zeppelin.notebook.Paragraph:252 - run paragraph 20160630-050637_1289828172 using spark org.apache.zeppelin.interpreter.LazyOpenInterpreter@62b97ff1
    Exception in thread "pool-1-thread-3" java.lang.IllegalAccessError: tried to access method com.google.common.collect.MapMaker.softValues()Lcom/google/common/collect/MapMaker; from class org.apache.spark.SparkEnv
    	at org.apache.spark.SparkEnv.<init>(SparkEnv.scala:75)
    	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:272)
    	at org.apache.spark.SparkContext.<init>(SparkContext.scala:204)
    	at org.apache.zeppelin.spark.SparkInterpreter.createSparkContext(SparkInterpreter.java:338)
    	at org.apache.zeppelin.spark.SparkInterpreter.getSparkContext(SparkInterpreter.java:122)
    	at org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:513)
    	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)
    	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.getProgress(LazyOpenInterpreter.java:110)
    	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer.getProgress(RemoteInterpreterServer.java:404)
    	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Processor$getProgress.getResult(RemoteInterpreterService.java:1509)
    	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Processor$getProgress.getResult(RemoteInterpreterService.java:1494)
    	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
    	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
    	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    	at java.lang.Thread.run(Thread.java:745)
    .....
    Failed tests: 
      ZeppelinSparkClusterTest.pySparkDepLoaderTest:195->getSparkVersionNumber:250 expected:<FINISHED> but was:<ERROR>
      ZeppelinSparkClusterTest.pySparkAutoConvertOptionTest:139->getSparkVersionNumber:250 expected:<FINISHED> but was:<ERROR>
      ZeppelinSparkClusterTest.basicRDDTransformationAndActionTest:81 expected:<FINISHED> but was:<ERROR>
      ZeppelinSparkClusterTest.pySparkTest:115->getSparkVersionNumber:250 expected:<FINISHED> but was:<ERROR>
      ZeppelinSparkClusterTest.zRunTest:180 expected:<FINISHED> but was:<ERROR>
      ZeppelinSparkClusterTest.sparkRTest:90->getSparkVersionNumber:250 expected:<FINISHED> but was:<ERROR>
      ```  
    
     1. `ZeppelinSparkClusterTest` hangs AKA [ZEPPELIN-862](https://issues.apache.org/jira/browse/ZEPPELIN-862)
      Profile [5](https://api.travis-ci.org/jobs/141222534/log.txt?deansi=true)
      ```
    Running org.apache.zeppelin.rest.ZeppelinSparkClusterTest
    ...
    
    No output has been received in the last 10 minutes, this potentially indicates a stalled build or something wrong with the build itself.
    
    The build has been terminated
    ```
      This guys is known and flaky, so it's better to focus on the rest issues for now.
    
     1.  `SparkParagraphIT.testPySpark` failure - never seen before, might deserve a separate JIRA issue with `flaky-test` label.
    
      Profile [7](https://api.travis-ci.org/jobs/141222536/log.txt?deansi=true) fails in somehow a new way
    
      ```
    
    Results :
    
    Failed tests: 
      SparkParagraphIT.testPySpark:132 Paragraph from SparkParagraphIT of testPySpark status: 
    Expected: "FINISHED"
         but: was "ERROR"
      SparkParagraphIT.testPySpark:139 Paragraph from SparkParagraphIT of testPySpark result: 
    Expected: "test loop 0\ntest loop 1\ntest loop 2"
         but: was "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8754372370659284789.py\", line 20, in <module>\n    from py4j.java_gateway import java_import, JavaGateway, GatewayClient\nImportError: No module named py4j.java_gateway\npyspark is not responding Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8754372370659284789.py\", line 20, in <module>\n    from py4j.java_gateway import java_import, JavaGateway, GatewayClient\nImportError: No module named py4j.java_gateway"
    
    Tests run: 16, Failures: 2, Errors: 0, Skipped: 0
      ```
    
    Please feel free to add, in case I'm missing something here.
    
    ---------------------------
    
    From the first glance `java.lang.IllegalAccessError: tried to access method` - means that there is different version of some class in run-time, from the one you expect\have in compile time. 
    This happens a lot when transitive dependencies bring new versions of the library you already have.
    
    I would start looking at Guava dependency version that you have in `mvn dependency:tree` to determine the "offender".
    
    Hope this helps @rawkintrevo !


Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Let's focus on fixing CI profiles one by one.
    
    [1st profile](https://s3.amazonaws.com/archive.travis-ci.org/jobs/141500057/log.txt)  failure now is consistent with before (whish is a good sign - it's reproducable!) and is due to failing tests in `ZeppelinSparkClusterTest` :
    
    ```
    01:34:08,427  INFO org.apache.zeppelin.server.ZeppelinServer:133 - Bye
    
    Results :
    
    Failed tests: 
      ZeppelinSparkClusterTest.pySparkDepLoaderTest:231 expected:<FINISHED> but was:<ERROR>
      ZeppelinSparkClusterTest.pySparkAutoConvertOptionTest:152 expected:<FINISHED> but was:<ERROR>
      ZeppelinSparkClusterTest.pySparkTest:127 expected:<FINISHED> but was:<ERROR>
    
    Tests run: 64, Failures: 3, Errors: 0, Skipped: 0
    ```
    
    If we go up the logs to the origin, we will see mahout mentioned as well, which is a sign that this failure is related to the PR. 
    And then the exception happens:
    
    ```
    Running org.apache.zeppelin.rest.ZeppelinSparkClusterTest
    01:32:21,442  INFO org.apache.zeppelin.rest.AbstractTestRestApi:116 - Test Zeppelin stared.
    SPARK HOME detected /home/travis/build/apache/zeppelin/spark-1.6.1-bin-hadoop2.3
    
    .....
    
    01:32:50,555  INFO org.apache.zeppelin.notebook.Paragraph:252 - run paragraph 20160701-013250_988735914 using dep org.apache.zeppelin.interpreter.LazyOpenInterpreter@1c84bbdb
    01:32:50,556  INFO org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess:143 - Run interpreter process [..//bin/interpreter.sh, -d, ../interpreter/mahout, -p, 35986, -l, ..//local-repo/2BRE4D28P]
    ..//bin/interpreter.sh: line 159: /home/travis/build/apache/zeppelin/run/zeppelin-interpreter-mahout-travis-testing-worker-linux-docker-726dc48a-3376-linux-16.pid: No such file or directory
    SLF4J: Class path contains multiple SLF4J bindings.
    SLF4J: Found binding in [jar:file:/home/travis/build/apache/zeppelin/interpreter/mahout/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: Found binding in [jar:file:/home/travis/build/apache/zeppelin/interpreter/mahout/zeppelin-spark-dependencies-0.6.0-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: Found binding in [jar:file:/home/travis/build/apache/zeppelin/interpreter/mahout/zeppelin-spark-0.6.0-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: Found binding in [jar:file:/home/travis/build/apache/zeppelin/zeppelin-interpreter/target/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
    01:32:51,068  INFO org.apache.zeppelin.interpreter.remote.RemoteInterpreter:170 - Create remote interpreter org.apache.zeppelin.spark.DepInterpreter
    01:32:51,110 ERROR org.apache.zeppelin.scheduler.RemoteScheduler:276 - Can't get status information
    org.apache.thrift.transport.TTransportException
    	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
    	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
    	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
    	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
    	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
    	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
    	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.recv_getStatus(RemoteInterpreterService.java:389)
    	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.getStatus(RemoteInterpreterService.java:375)
    	at org.apache.zeppelin.scheduler.RemoteScheduler$JobStatusPoller.getStatus(RemoteScheduler.java:262)
    	at org.apache.zeppelin.scheduler.RemoteScheduler$JobStatusPoller.run(RemoteScheduler.java:211)
    ```
    
    The most suspicious part here is
    ```
    1:32:50,554  INFO org.apache.zeppelin.interpreter.InterpreterFactory:608 - Interpreter org.apache.zeppelin.spark.DepInterpreter 478460891 created
    01:32:50,555  INFO org.apache.zeppelin.notebook.Paragraph:252 - run paragraph 20160701-01:32:50,556  INFO org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess:143 - Run interpreter process [..//bin/interpreter.sh, -d, ../interpreter/mahout, -p, 35986, -l, ..//local-repo/2BRE4D28P]
    ..//bin/interpreter.sh: line 159: /home/travis/build/apache/zeppelin/run/zeppelin-interpreter-mahout-travis-testing-worker-linux-docker-726dc48a-3376-linux-16.pid: No such file or directory
    ...
    01:32:51,068  INFO org.apache.zeppelin.interpreter.remote.RemoteInterpreter:170 - Create remote interpreter org.apache.zeppelin.spark.DepInterpreter
    ```
    
    Could you confirm that scenarios, similar to one in [ZeppelinSparkClusterTest.pySparkDepLoaderTest](https://github.com/apache/zeppelin/blob/master/zeppelin-server/src/test/java/org/apache/zeppelin/rest/ZeppelinSparkClusterTest.java#L218) work, if you run those manually on local machine a new notebook?


Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Looks at latest CI results, I do not think that comment above is relevant any more, esp for the first and most general CI profile.
    
    Great progress @rawkintrevo !


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @Bzz,  Re: the three Pyspark related tests in Zeppelin Spark Cluster- Similar scenarios would always work on my machine, and I am unable to reproduce the test failure locally.  (Have been the whole time).  With the exception of bumping to 0.7.0, I successfully run `mvn clean verify` locally on each run of code.  
    
    Since updating to `0.7.0`  running `mvn clean package -Ppyspark` I get the following errors:
    
    ```
    Tests in error: 
      HeliumApplicationFactoryTest.testUnloadOnInterpreterUnbind:227 » ClassCast jav...
      HeliumApplicationFactoryTest.testLoadRunUnloadApplication:145 » ClassCast java...
      HeliumApplicationFactoryTest.testUnloadOnInterpreterRestart:272 » ClassCast ja...
      HeliumApplicationFactoryTest.testUnloadOnParagraphRemove:189 » ClassCast java....
    ```
    
    **tog** was having similiar issues, (he attached his error log in the original email) [thread](https://lists.apache.org/thread.html/e71088130d5e71058890f09fa91df5c1c5111ecb401c63f27a9762a7@%3Cdev.zeppelin.apache.org%3E).  I tried building with 
    - `mvn clean package`
    - `mvn clean install`
    - `mvn clean package -Pbuild-dist`
    
    All will yield said error in testing.  Though, I can successfully build and check the cases where the pyspark tests noted above are failing.  (However, even before I was unable to recreate said tests locally).  
    
    So the several pushes I made yesterday were related to the fact that I am basically flying *quasi* blind locally. ( I can still `mvn clean package` in the `zeppelin/mahout` directory )
    
    For brevity I will not re copy and paste, but here is my thought:
    - I never messed with anything in pyspark, or the Zeppelin Server.  
    - Functionality still works full when actually running Zeppelin
    - Thus this is an issue of test configuration.
    
    Upon reviewing the failed tests I noticed, the note_id they are using (which was originally designated for Spark Tests) is the sameone being used by the Mahout tests, even though they are in different interpreter groups. 
    
    So I was attempting to change the note IDs, which is when I got the `null pointer` errors of the last test.  
    
    That issue I was able to recreate locally, it had to do with not setting `Resource Pool` when creating the context in the MahoutTest. I've fixed that locally and have just pushed.  The fact that this seemingly small oversight was calling failures leads me to believe that the *note* which Spark Test and Mahout Test are using, are also somehow being used by Zeppelin server, and this mash up being the root of the problem.  
    
    



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    last commit was to get around that web application fail. 
    
    Added my own logging to shed some light on the situation:
    
    `21:59:21,200  INFO org.apache.zeppelin.rest.AbstractTestRestApi:134 - pySparkTest: %text pyspark is not responding`


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/418
  
    @bzz so much has changed since I wrote this I want to go through it once and make sure everything is still accurate.  I'm f/t trying to get [ZEPPELIN-116](https://github.com/apache/zeppelin/pull/928) done. Will do this as soon as that is complete.


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @bzz, I can't recreate the build failure.
    
    I can say
    - Spark, pySpark, and Mahout notebooks and paragraphs run as expected.
    - Spark and pySpark tests pass. Also, integration tests pass in `zeppelin-server`.  The only thing that fails is the Spark Cluster test. 
    - The part of the Spark Cluster Test that fails is python not being found when testing via the REST API
    - I can also confirm that all of the failing tests ALSO work as expected against a built Zeppein (see following python script to recreate tests)
    
    
    ``` python
    
    # build zeppelin like this:
    #
    # mvn clean package -DskipTests -Psparkr -Ppyspark -Pspark-1.6
    
    from requests import post, get, delete
    from json import dumps
    
    ZEPPELIN_SERVER = "localhost"
    ZEPPELIN_PORT = 8080
    base_url = "http://%s:%i" % (ZEPPELIN_SERVER, ZEPPELIN_PORT)
    
    
    
    def create_notebook(name_of_new_notebook):
        payload = {"name": name_of_new_notebook}
        notebook_url = base_url + "/api/notebook"
        r = post(notebook_url, dumps(payload))
        return r.json()
    
    def delete_notebook(notebook_id):
        target_url = base_url + "/api/notebook/%s" % notebook_id
        r = delete(target_url)
        return r
    
    
    def create_paragraph(code, notebook_id, title=""):
        target_url = base_url + "/api/notebook/%s/paragraph" % notebook_id
        payload = { "title": title, "text": code }
        r = post(target_url, dumps(payload))
        return r.json()["body"]
    
    
    notebook_id = create_notebook("test1")["body"]
    
    test_codes = [
    "%spark print(sc.parallelize(1 to 10).reduce(_ + _))",
        "%r localDF <- data.frame(name=c(\"a\", \"b\", \"c\"), age=c(19, 23, 18))\n" +
        "df <- createDataFrame(sqlContext, localDF)\n" +
        "count(df)",
        "%pyspark print(sc.parallelize(range(1, 11)).reduce(lambda a, b: a + b))",
        "%pyspark print(sc.parallelize(range(1, 11)).reduce(lambda a, b: a + b))",
        "%pyspark\nfrom pyspark.sql.functions import *\n"
        + "print(sqlContext.range(0, 10).withColumn('uniform', rand(seed=10) * 3.14).count())",
        "%spark z.run(1)"
    ]
    
    para_ids = [create_paragraph(c, notebook_id) for c in test_codes]
    
    # run all paragraphs:
    post(base_url + "/api/notebook/job/%s" % notebook_id)
    
    #delete_notebook(notebook_id)
    ```
    
    After two weeks of chasing dead ends and my tail, I call this is an issue with the testing env, not the mahout interpreter.


Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Got it, thank you so much for digging into it!
    
    Let me try to look into it more this week


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    All failures due to Mahout doesn't support scala 2.11 yet.  Adding logic to detect, similar to detecting spark v < 1.5, and adding to testing suite. 
    
    @bzz, pyspark issues seemed to have resolved themselves. 


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Currently failing to download deps.
    
    ```
    [ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.4:process (default) on project zeppelin: Error downloading resources archive. Could not transfer artifact org.apache.apache.resources:apache-jar-resource-bundle:jar:1.5-SNAPSHOT from/to codehaus-snapshots (https://nexus.codehaus.org/snapshots/): nexus.codehaus.org: Name or service not known
    ```
    
    Will try again in a little while. 
    



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    I resubmitted this a few times, there is something going awry with finding `org.apache.apache.resources:apache-jar-resource-bundle:jar:1.5-SNAPSHOT`
    
    
    The build fails on random profiles, all with the error:
    ```
    [ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.4:process (default) on project zeppelin: Error downloading resources archive. Could not transfer artifact org.apache.apache.resources:apache-jar-resource-bundle:jar:1.5-SNAPSHOT from/to apache-snapshots (https://repository.apache.org/snapshots/): Connect to repository.apache.org:443 [repository.apache.org/207.244.88.143] failed: Connection timed out
    [ERROR] org.apache.apache.resources:apache-jar-resource-bundle:jar:1.5-SNAPSHOT
    ```
    Everything else builds nicely.



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @bzz and @jongyoul 
    OK, so two things I think have happened.  (In case some thing like this happens again). 
    
    When I rebased, some version of ZeppelinClusterTest.java that had the [PySpark tests commented out](https://github.com/apache/zeppelin/pull/928/commits/d5963a9844ea9765ff2a34c5bcaf19f05a42d837#diff-64a9440e811c5fba6ac1b61157fa6912L121) got worked in, so of course it passed, the failing tests were skipped. 
    
    However, I also happened to notice [here](https://github.com/apache/zeppelin/pull/928/commits/d5963a9844ea9765ff2a34c5bcaf19f05a42d837#diff-69b17249ac9d265091d730130d973f4aR493) that `org.apache.zeppelin.mahout.MahoutSparkInterpreter` was listed before `org.apache.zeppelin.spark.PySparkInterpreter` so I move the Mahout one down, and the tests seem to be passing now.  
    
    This led me to believe, that is a very fragile test that should be looked into, but then I also realized I forgot to put a comma after the MahoutSparkInterpreter, so I'm going to do that now, and I'll bet what was happening, when it made the cluster: it didn't know how to make the pySpark or any of the other interpreters because of the missing comma, so mystery solved. 
    
    Thanks for all of your help/sorry for wasting your time... I owe you both beers next time we're in the same spot. 
    



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @bzz  only difference between this and last was squash of commit. all fails are related to downloading dependencies still. 


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    ## CI Failures in This PR
    
    #### Profile [2](https://s3.amazonaws.com/archive.travis-ci.org/jobs/148308638/log.txt) Hangs a la [ZEPPELIN-862](https://issues.apache.org/jira/browse/ZEPPELIN-862?jql=project%20%3D%20ZEPPELIN%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20labels%20%3D%20flaky-test)
    
    `No output has been received in the last 10 minutes, this potentially indicates a stalled build or something wrong with the build itself.`
    
    #### Profile [6](https://s3.amazonaws.com/archive.travis-ci.org/jobs/148308642/log.txt), [8](https://s3.amazonaws.com/archive.travis-ci.org/jobs/148308647/log.txt) Network issues, can't download dependencies
    `[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.4:process (default) on project zeppelin: Error downloading resources archive. Could not transfer artifact org.apache.apache.resources:apache-jar-resource-bundle:jar:1.5-SNAPSHOT from/to codehaus-snapshots (https://nexus.codehaus.org/snapshots/): nexus.codehaus.org: Name or service not known`
    
    `ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.4:process (default) on project zeppelin: Error downloading resources archive. Could not transfer artifact org.apache.apache.resources:apache-jar-resource-bundle:jar:1.5-SNAPSHOT from/to apache-snapshots (https://repository.apache.org/snapshots/): Connect to repository.apache.org:443 [repository.apache.org/207.244.88.143] failed: Connection timed out`
    
    #### Profile [7 ](https://s3.amazonaws.com/archive.travis-ci.org/jobs/148308645/log.txt) This is a [known flaky test with no coresponding JIRA](https://github.com/apache/zeppelin/pull/1201)
    `31m- should provide onclick method *** FAILED ***[0m
    [31m  The code passed to eventually never returned normally. Attempted 1 times over 206.957211 milliseconds. Last failure message: 0 was not equal to 1. (AbstractAngularElemTest.scala:72)[0m`


Github user rawkintrevo closed the pull request at:

    https://github.com/apache/zeppelin/pull/928


GitHub user rawkintrevo reopened a pull request:

    https://github.com/apache/zeppelin/pull/928

    [ZEPPELIN-116] Add Apache Mahout Interpreter

    ### What is this PR for?
    This PR adds Mahout functionality for the Spark Interpreter.
    
    
    ### What type of PR is it?
    Improvement 
    
    ### Todos
    - [x] Implement Mahout Interpreter in Spark
    - [x] Add Unit Tests
    - [x] Add Documentation
    - [x] Add Example Notebook
    
    ### What is the Jira issue?
    https://issues.apache.org/jira/browse/ZEPPELIN-116
    
    ### How should this be tested?
    Open a Spark Notebook with Mahout enabled and run a few simple commands using the R-Like DSL and Spark Distributed Context (Mahout Specific)
    
    ### Screenshots (if appropriate)
    
    ### Questions:
    * Does the licenses files need update?
    No
    * Is there breaking changes for older versions?
    No
    * Does this needs documentation?
    Yes
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rawkintrevo/incubator-zeppelin mahout-terp

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/zeppelin/pull/928.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #928
    
----
commit af935152ce0b70578317ad0f629fd7cf7043f9e0
Author: Trevor Grant <trevor.d.grant@gmail.com>
Date:   2016-06-08T17:05:25Z

    [ZEPPELIN-116] Add Mahout Interpreter

----


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    [Build 6281](https://travis-ci.org/apache/zeppelin/builds/149199571)
    
    Profiles [1](https://s3.amazonaws.com/archive.travis-ci.org/jobs/149199572/log.txt), [2](https://s3.amazonaws.com/archive.travis-ci.org/jobs/149199574/log.txt), and [3](https://travis-ci.org/apache/zeppelin/jobs/149199575):
    
    All stalled a la [ZEPPELIN-862](https://issues.apache.org/jira/browse/ZEPPELIN-862?jql=project%20%3D%20ZEPPELIN%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20labels%20%3D%20flaky-test)



Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @rawkintrevo you did titanic job, struggling with CI here.. but now, after Spark 2.0 and Scala 2.11 support got merged CI fails even on *master*
    
    Would it make sense to take a break until 0.6.1 gets released and master get's sustainably green?
    
    I'm sure we are going finish this one very fast as soon as it's done.


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @bzz I was really hoping to get it in by 0.6.1 but it's whatever. 
    
    [Profile 1](https://s3.amazonaws.com/archive.travis-ci.org/jobs/150945703/log.txt) is the only failure now- 
    `ParallelSchedulerTest.testRun:55 expected:<RUNNING> but was:<PENDING>`
    
    I think it's unrelated- other people were having the issue yesterday too...
    [PR-1290](https://github.com/apache/zeppelin/pull/1290)



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    for what it's worth (probably not much) this passed travis CI on my branch- 
    [https://travis-ci.org/rawkintrevo/incubator-zeppelin/builds/154183905](https://travis-ci.org/rawkintrevo/incubator-zeppelin/builds/154183905)
    
    Failing on profile 1, onClick in webserver hits time out (unrelated).
    [https://travis-ci.org/apache/zeppelin/jobs/154156408#L16359](https://travis-ci.org/apache/zeppelin/jobs/154156408#L16359)



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    *bump* @bzz should we continue work on this or pursue another route? 


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @bzz and @Leemoonsoo -
    
    Was talking to some other about this- I think we should take a new tack on this as Mahout currently only supports Spark v1.5 officially, but also 1.6 in practice.  Given Zeppelin ships with Spark 2.0 as default, and you can't have multiple SparkContexts (e.g. a Spark 1.6 and a Spark 2.0) it's likely this 'terp would be dead on arrival for many users.  
    
    New thought, based on a lot of work I've been doing recently- create a python script that in essence updates `conf/interpreter.json` with a new interpreter based on the current Spark interpreter, and adds the required libraries and configurations (such as the Kryo serializers), resulting in something called `%sparkMahout`
    
    A much lighter weight version of what you can see here in principal:
    [create a new interpreter](https://github.com/rawkintrevo/bluemix-extra-services/blob/master/data/services/zeppelin.py#L166)
     [update a spark interpreter with Mahout configs](https://github.com/rawkintrevo/bluemix-extra-services/blob/master/data/services/zeppelin.py#L68)
    
    In addition, a page or two of documentation in Zeppelin saying, "If you want to use Mahout with Apache Spark, you have to have Spark 1.5 or 1.6 (until Mahout is Spark2 compliant), go here run this python script, it will be created.  Here is some basic useage on Mahout and links to some other good getting started material"
    
    So in short we would scrap all of this code, and replace it with some docs and a python script that would create a more resilient (based on my experience) interpreter, that will be far less likely to give users a negative experience with Zeppelin or Mahout.
    
    Thoughts? 


Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @rawkintrevo thank you for sharing the ideas and keeping it up! 
    This sounds like very reasonable approach to me, at least until mahout is Spark2 compliant.
    
    So the idea is to provide an automation inside Zeppelin that would add a pre-configure interpreter, for Spark 1.5-1.6 with Mahout dependencies, update the docs on how to use it and provide an example notebook?


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @bzz, thanks! That is exactly the idea- and I can do the same thing for Flink.
    
    The cool thing there is, the exact same code* will run on %mahoutSpark and %mahoutFlink, so that will bring the 'mahout interpreter' to users even if they are running Spark 2.0, before Mahout gets bumped up to run on Spark 2.
    
    



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Refactored as previously discussed.  Added script that sets up two new terps, one for mahout on flink, one for mahout on spark.  Added docs and a robust tutorial that shows useage.


Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Sounds awesome, @rawkintrevo, thank you for keeping up a good work!
    
    Let me try it this week and get back to you.


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Hey @bzz, did you get a chance to check this out? Anything I can do to help?


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    bump- status or thoughts on this?


Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Sorry for delay, @rawkintrevo ! 
    
    Thank you for kind reminder on this great stuff. I want to mention this work on ApacheCon, so hope we can merge this asap :)
    
    I tied it and got `IOError: [Errno 28] No space left on device` while downloading Mahout. While I'm cleaning the space, here are few things that I noticed:
      - script does not "fail fast", meaning that if downloading of Mahout failed due to disk space, it went on trying to create dirs, update .json, etc. How do you think, is it worth adding it?
    
      - output uses the term `terp`
    
      ```
    created new terp 'sparkMahout' from terp 'spark
    created new terp 'flinkMahout' from terp 'flink
      ```
      I assume it's used instead of longer `interpreter`. But other users may get confused at this point, as documentation does not mention such term. Do you think we use `interpreter` consistently here as well?
    
    - is `python scripts/mahout/add_mahout.py` idempotent? 
    
      Meaning, if it failed once for some reason, is it safe to re-run again with same args and not get 2 more interpreters created?
    
    I will clean up some space and try it again and post back.


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    I can address the 3 bullet points in the next day or two.
    
    Re; the disk space- yea Mahout adds another 300MB or so. I recall seeing some chat on the mailing list that Zeppelin was already getting 'too fat', the original Mahout interpreter was adding that 300MB as well.  That was another motivation for going the 'setup script' route. 
    
    Will update, rebase, and push soon.  Thanks for working on it and I am also super motivated to get merged pre-ApacheCon!


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    - [x] Fail fasts: changed `call` to [`check_call`](https://docs.python.org/2/library/subprocess.html#check_call) where it is appropriate to fail and exit. 
    - [x] Slang in standard out: Updated strings to use 'interpreter' instead of 'terp'.  'terp' is still short hand in the code; imho anyone who starts monkeying around in there will understand it quickly as the shortening is done very consistenly
    - [x] Idempotent: `createTerp` function checks if an interpreter with the new name exists, and if it does deletes it. 


Github user felixcheung commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Interesting approach!



Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @rawkintrevo thank you for addressing the feedback promptly.
    
    I tried examples and everything worked very well!
    
    One thing though - on the second run of `add_mahout.py` I got
    
    ```
    $python scripts/mahout/add_mahout.py
    --zeppelin_home not specified, using .../zeppelin
    ZEPPELIN_HOME validated
    apache-mahout-distribution-0.12.2.tar.gz found, skipping download
    created new interpreter 'sparkMahout' from interpreter 'spark
    created new interpreter 'flinkMahout' from interpreter 'flink
    updating 'sparkMahout' with Apache Mahout dependencies and settings
    updating 'flinkMahout' with Apache Mahout dependencies and settings
    'export MAHOUT_HOME=...' already exists in zeppelin_env.sh, not appending
    restarting Apache Zeppelin to load new interpreters...
    Zeppelin is not running
    Zeppelin start                                             [  OK  ]
    ---------------------------------------------------------------------------------------------------------------
    all done! Thanks for using Apache Mahout
    bye
    ```
    
    It didn't add more interpreters indeed but, do you think in such case, from user experience perspective it could be better just to stop the execution and notify user in the CLI that he already has it configured, nothing was touched and skip starting Zeppelin, etc? 
    
    One last thing, shall we may be rename script to `add_mahout_interpreter.py` to make its purpos more obvious?
    
    One possible further improvement (in the further PRs) could be - auto-import things from `org.apache.mahout` and define `SparkDistributedContext` so user do not have to do that every time. But right now Zeppelin lacks the means to do so, without writing an interpreter impl.
    
    Looks awesome to me, let's merge after final updates, if there is no further discussion!


Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @bzz, 
    - I could see usecases either way.  So i've added the `--overwrite_existing` flag.  If that flag is called it will delete current `%sparkMahout` and `%flinkMahout` interpreters. 
    - renamed to add_mahout_intpreters.py (currently spark, flink, possibly beam in future, etc)
    - +1 on an `init` section of interpreter.json. I can think of usecases for that beyond this interpreter.  Specifically, I might have an interpreter called "sparkImageProcessing" which has all of my image processing jars in the class path. So it makes sense when ever I am using that interpreter I'm going to import the usual suspects, and that always ends up being an "init" paragraph at the top of my notebook.  For another day I suppose. 



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    @felixcheung thanks.  Apache Mahout is a kind of special case, in that really we're a set of libraries that can run on currently Spark, Flink, H20 runners, and possible additionals ones (such as beam in the future).  We are however not super aggressive when it comes to supporting spark version changes [MAHOUT-1889](https://issues.apache.org/jira/browse/MAHOUT-1889).  But just because a user chooses to use Spark 2.0, doesn't mean they can't play with flink based Mahout. 
    
    Finally, when trying to write a 'traditional interpreter' the additional dependencies were increasing the size of Zeppelin binary by about 200 MB. The Python script is outside the box, but an elegeent solution to a unique problem imho, and one that it may make sense for others to follow. 
    
    tg



Github user rawkintrevo commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Profile 2 failed. 
    
    ```
    16/11/06 14:18:51 INFO PySparkInterpreter: File /tmp/zeppelin_pyspark-1526538107058607857.py created
    Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 31.378 sec <<< FAILURE! - in org.apache.zeppelin.spark.PySparkInterpreterTest
    testBasicIntp(org.apache.zeppelin.spark.PySparkInterpreterTest)  Time elapsed: 31.137 sec  <<< FAILURE!
    java.lang.AssertionError: expected:<SUCCESS> but was:<ERROR>
    	at org.junit.Assert.fail(Assert.java:88)
    	at org.junit.Assert.failNotEquals(Assert.java:743)
    	at org.junit.Assert.assertEquals(Assert.java:118)
    	at org.junit.Assert.assertEquals(Assert.java:144)
    	at org.apache.zeppelin.spark.PySparkInterpreterTest.testBasicIntp(PySparkInterpreterTest.java:142)
    ```
    
    Considering this PR only adds docs and an intert python script (which isn't run during testing), I must conclude this is is an unrelated failure. 


Github user bzz commented on the issue:

    https://github.com/apache/zeppelin/pull/928
  
    Looks great to me, thank you @rawkintrevo 
    
    CI failure looks not relevant to the changes.
    
    Merging to master if there is no further discussion.


Github user asfgit closed the pull request at:

    https://github.com/apache/zeppelin/pull/928


