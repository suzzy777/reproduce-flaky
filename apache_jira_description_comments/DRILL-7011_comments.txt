arina-ielchiieva commented on pull request #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#discussion_r268603780
 
 

 ##########
 File path: exec/java-exec/src/test/java/org/apache/drill/exec/store/easy/text/compliant/TestCsvWithSchema.java
 ##########
 @@ -82,6 +167,468 @@ public void testSchema() throws Exception {
           .addRow(10, new LocalDate(2019, 3, 20), "it works!", 1234.5D, 20L, "")
           .build();
       RowSetUtilities.verify(expected, actual);
+    } finally {
+      resetV3();
+      resetSchema();
+    }
+  }
+
+
+  /**
+   * Use a schema with explicit projection to get a consistent view
+   * of the table schema, even if columns are missing, rows are ragged,
+   * and column order changes.
+   * <p>
+   * Force the scans to occur in distinct fragments so the order of the
+   * file batches is random.
+   */
+  @Test
+  public void testMultiFileSchema() throws Exception {
+    RowSet expected1 = null;
+    RowSet expected2 = null;
+    try {
+      enableV3(true);
+      enableSchema(true);
+      enableMultiScan();
+      String tablePath = buildTwoFileTable("multiFileSchema", raggedMulti1Contents, reordered2Contents);
+      run(SCHEMA_SQL, tablePath);
+
+      // Wildcard expands to union of schema + table. In this case
+      // all table columns appear in the schema (though not all schema
+      // columns appear in the table.)
+
+      String sql = "SELECT id, `name`, `date`, gender, comment FROM " + tablePath;
+      TupleMetadata expectedSchema = new SchemaBuilder()
+          .add("id", MinorType.INT)
+          .add("name", MinorType.VARCHAR)
+          .addNullable("date", MinorType.DATE)
+          .add("gender", MinorType.VARCHAR)
+          .add("comment", MinorType.VARCHAR)
+          .buildSchema();
+      expected1 = new RowSetBuilder(client.allocator(), expectedSchema)
+          .addRow(1, "arina", new LocalDate(2019, 1, 18), "female", "ABC")
+          .addRow(2, "javan", new LocalDate(2019, 1, 19), "male", "ABC")
+          .addRow(4, "albert", new LocalDate(2019, 5, 4), "", "ABC")
+          .build();
+      expected2 = new RowSetBuilder(client.allocator(), expectedSchema)
+          .addRow(3, "bob", new LocalDate(2001, 1, 16), "NA", "ABC")
+          .build();
+
+      // Loop 10 times so that, as the two reader fragments read the two
+      // files, we end up with (acceptable) races that read the files in
+      // random order.
+
+      for (int i = 0; i < 10; i++) {
+        boolean sawSchema = false;
+        boolean sawFile1 = false;
+        boolean sawFile2 = false;
+        Iterator<DirectRowSet> iter = client.queryBuilder().sql(sql).rowSetIterator();
+        while (iter.hasNext()) {
+          RowSet result = iter.next();
+          if (result.rowCount() == 3) {
+            sawFile1 = true;
+            new RowSetComparison(expected1).verifyAndClear(result);
+          } else if (result.rowCount() == 1) {
+            sawFile2 = true;
+            new RowSetComparison(expected2).verifyAndClear(result);
+          } else {
+            assertEquals(0, result.rowCount());
+            sawSchema = true;
+          }
+        }
+        assertTrue(sawSchema);
+        assertTrue(sawFile1);
+        assertTrue(sawFile2);
+      }
+    } finally {
+      expected1.clear();
+      expected2.clear();
+      client.resetSession(ExecConstants.ENABLE_V3_TEXT_READER_KEY);
+      client.resetSession(ExecConstants.STORE_TABLE_USE_SCHEMA_FILE);
+      client.resetSession(ExecConstants.MIN_READER_WIDTH_KEY);
+    }
+  }
+
+  /**
+   * Test the schema we get in V2 when the table read order is random.
+   * Worst-case: the two files have different column counts and
+   * column orders.
+   * <p>
+   * Though the results are random, we iterate 10 times which, in most runs,
+   * shows the random variation in schemas:
+   * <ul>
+   * <li>Sometimes the first batch has three columns, sometimes four.</li>
+   * <li>Sometimes the column `id` is in position 0, sometimes in position 1
+   * (correlated with the above).</li>
+   * <li>Due to the fact that sometimes the first file (with four columns)
+   * is returned first, sometimes the second file (with three columns) is
+   * returned first.</li>
+   * </ul>
+   */
+  @Test
+  public void testSchemaRaceV2() throws Exception {
+    try {
+      enableV3(false);
+      enableSchema(false);
+      enableMultiScan();
+      String tablePath = buildTwoFileTable("schemaRaceV2", multi1Contents, reordered2Contents);
+      boolean sawFile1First = false;
+      boolean sawFile2First = false;
+      boolean sawFullSchema = false;
+      boolean sawPartialSchema = false;
+      boolean sawIdAsCol0 = false;
+      boolean sawIdAsCol1 = false;
+      String sql = "SELECT * FROM " + tablePath;
+      for (int i = 0; i < 10; i++) {
+        Iterator<DirectRowSet> iter = client.queryBuilder().sql(sql).rowSetIterator();
+        int batchCount = 0;
+        while(iter.hasNext()) {
+          batchCount++;
+          RowSet result = iter.next();
+          TupleMetadata resultSchema = result.schema();
+          if (resultSchema.size() == 4) {
+            sawFullSchema = true;
+          } else {
+            assertEquals(3, resultSchema.size());
+            sawPartialSchema = true;
+          }
+          if (resultSchema.index("id") == 0) {
+            sawIdAsCol0 = true;
+          } else {
+            assertEquals(1, resultSchema.index("id"));
+            sawIdAsCol1 = true;
+          }
+          if (batchCount == 1) {
+            RowSetReader reader = result.reader();
+            assertTrue(reader.next());
+            String id = reader.scalar("id").getString();
+            if (id.equals("1")) {
+              sawFile1First = true;
+            } else {
+              assertEquals("3", id);
+              sawFile2First = true;
+            }
+          }
+          result.clear();
+        }
+      }
+
+      // Outcome is random (which is the key problem). Don't assert on these
+      // because doing so can lead to a flakey test.
+
+      if (!sawFile1First || ! sawFile2First || !sawFullSchema || !sawPartialSchema || !sawIdAsCol0 || !sawIdAsCol1) {
+        System.out.println("Some variations did not occur");
 
 Review comment:
   I know PR is not complete yet, just a note to remove `System.out.println` during final clean up.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on issue #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#issuecomment-476166889
 
 
   @paul-rogers 
   Actually when I was presenting the schema provisioning design, there were a proposal to add schema property `drill.is_full_schema`. By default it’s `false`, thus we assume that schema is partial.
   If user wants to indicate that schema is strict and to ignore all columns except of those indicated in schema, he needs to create schema the following way:
   	`create schema (col int) properties ('drill.is_full_schema' = 'true')`
   
   Since most of the `default` property problems are related to star queries, we can state the following:
   1. For queries with defined list of columns (aka projection queries: `select id, name from t`), we apply schema consistently.
   2. For star queries and when schema property `drill.is_full_schema` is set to `false`, we might get inconsistent results with default values but it's ok since we discover schema on the read.
   3. For star queries and when schema property `drill.is_full_schema` is set to `true`, we project only those columns indicated in schema.
   What do you think?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on issue #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#issuecomment-476166889
 
 
   @paul-rogers 
   Actually when I was presenting the schema provisioning design, there were a proposal to add schema property `drill.is_full_schema`. By default it’s `false`, thus we assume that schema is partial.
   If user wants to indicate that schema is strict and to ignore all columns except of those indicated in schema, he needs to create schema the following way:
   	`create schema (col int) for table dfs.tmp.t. properties ('drill.is_full_schema' = 'true')`
   
   Since most of the `default` property problems are related to star queries, we can state the following:
   1. For queries with defined list of columns (aka projection queries: `select id, name from t`), we apply schema consistently.
   2. For star queries and when schema property `drill.is_full_schema` is set to `false`, we might get inconsistent results with default values but it's ok since we discover schema on the read.
   3. For star queries and when schema property `drill.is_full_schema` is set to `true`, we project only those columns indicated in schema.
   What do you think?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on pull request #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#discussion_r268603780
 
 

 ##########
 File path: exec/java-exec/src/test/java/org/apache/drill/exec/store/easy/text/compliant/TestCsvWithSchema.java
 ##########
 @@ -82,6 +167,468 @@ public void testSchema() throws Exception {
           .addRow(10, new LocalDate(2019, 3, 20), "it works!", 1234.5D, 20L, "")
           .build();
       RowSetUtilities.verify(expected, actual);
+    } finally {
+      resetV3();
+      resetSchema();
+    }
+  }
+
+
+  /**
+   * Use a schema with explicit projection to get a consistent view
+   * of the table schema, even if columns are missing, rows are ragged,
+   * and column order changes.
+   * <p>
+   * Force the scans to occur in distinct fragments so the order of the
+   * file batches is random.
+   */
+  @Test
+  public void testMultiFileSchema() throws Exception {
+    RowSet expected1 = null;
+    RowSet expected2 = null;
+    try {
+      enableV3(true);
+      enableSchema(true);
+      enableMultiScan();
+      String tablePath = buildTwoFileTable("multiFileSchema", raggedMulti1Contents, reordered2Contents);
+      run(SCHEMA_SQL, tablePath);
+
+      // Wildcard expands to union of schema + table. In this case
+      // all table columns appear in the schema (though not all schema
+      // columns appear in the table.)
+
+      String sql = "SELECT id, `name`, `date`, gender, comment FROM " + tablePath;
+      TupleMetadata expectedSchema = new SchemaBuilder()
+          .add("id", MinorType.INT)
+          .add("name", MinorType.VARCHAR)
+          .addNullable("date", MinorType.DATE)
+          .add("gender", MinorType.VARCHAR)
+          .add("comment", MinorType.VARCHAR)
+          .buildSchema();
+      expected1 = new RowSetBuilder(client.allocator(), expectedSchema)
+          .addRow(1, "arina", new LocalDate(2019, 1, 18), "female", "ABC")
+          .addRow(2, "javan", new LocalDate(2019, 1, 19), "male", "ABC")
+          .addRow(4, "albert", new LocalDate(2019, 5, 4), "", "ABC")
+          .build();
+      expected2 = new RowSetBuilder(client.allocator(), expectedSchema)
+          .addRow(3, "bob", new LocalDate(2001, 1, 16), "NA", "ABC")
+          .build();
+
+      // Loop 10 times so that, as the two reader fragments read the two
+      // files, we end up with (acceptable) races that read the files in
+      // random order.
+
+      for (int i = 0; i < 10; i++) {
+        boolean sawSchema = false;
+        boolean sawFile1 = false;
+        boolean sawFile2 = false;
+        Iterator<DirectRowSet> iter = client.queryBuilder().sql(sql).rowSetIterator();
+        while (iter.hasNext()) {
+          RowSet result = iter.next();
+          if (result.rowCount() == 3) {
+            sawFile1 = true;
+            new RowSetComparison(expected1).verifyAndClear(result);
+          } else if (result.rowCount() == 1) {
+            sawFile2 = true;
+            new RowSetComparison(expected2).verifyAndClear(result);
+          } else {
+            assertEquals(0, result.rowCount());
+            sawSchema = true;
+          }
+        }
+        assertTrue(sawSchema);
+        assertTrue(sawFile1);
+        assertTrue(sawFile2);
+      }
+    } finally {
+      expected1.clear();
+      expected2.clear();
+      client.resetSession(ExecConstants.ENABLE_V3_TEXT_READER_KEY);
+      client.resetSession(ExecConstants.STORE_TABLE_USE_SCHEMA_FILE);
+      client.resetSession(ExecConstants.MIN_READER_WIDTH_KEY);
+    }
+  }
+
+  /**
+   * Test the schema we get in V2 when the table read order is random.
+   * Worst-case: the two files have different column counts and
+   * column orders.
+   * <p>
+   * Though the results are random, we iterate 10 times which, in most runs,
+   * shows the random variation in schemas:
+   * <ul>
+   * <li>Sometimes the first batch has three columns, sometimes four.</li>
+   * <li>Sometimes the column `id` is in position 0, sometimes in position 1
+   * (correlated with the above).</li>
+   * <li>Due to the fact that sometimes the first file (with four columns)
+   * is returned first, sometimes the second file (with three columns) is
+   * returned first.</li>
+   * </ul>
+   */
+  @Test
+  public void testSchemaRaceV2() throws Exception {
+    try {
+      enableV3(false);
+      enableSchema(false);
+      enableMultiScan();
+      String tablePath = buildTwoFileTable("schemaRaceV2", multi1Contents, reordered2Contents);
+      boolean sawFile1First = false;
+      boolean sawFile2First = false;
+      boolean sawFullSchema = false;
+      boolean sawPartialSchema = false;
+      boolean sawIdAsCol0 = false;
+      boolean sawIdAsCol1 = false;
+      String sql = "SELECT * FROM " + tablePath;
+      for (int i = 0; i < 10; i++) {
+        Iterator<DirectRowSet> iter = client.queryBuilder().sql(sql).rowSetIterator();
+        int batchCount = 0;
+        while(iter.hasNext()) {
+          batchCount++;
+          RowSet result = iter.next();
+          TupleMetadata resultSchema = result.schema();
+          if (resultSchema.size() == 4) {
+            sawFullSchema = true;
+          } else {
+            assertEquals(3, resultSchema.size());
+            sawPartialSchema = true;
+          }
+          if (resultSchema.index("id") == 0) {
+            sawIdAsCol0 = true;
+          } else {
+            assertEquals(1, resultSchema.index("id"));
+            sawIdAsCol1 = true;
+          }
+          if (batchCount == 1) {
+            RowSetReader reader = result.reader();
+            assertTrue(reader.next());
+            String id = reader.scalar("id").getString();
+            if (id.equals("1")) {
+              sawFile1First = true;
+            } else {
+              assertEquals("3", id);
+              sawFile2First = true;
+            }
+          }
+          result.clear();
+        }
+      }
+
+      // Outcome is random (which is the key problem). Don't assert on these
+      // because doing so can lead to a flakey test.
+
+      if (!sawFile1First || ! sawFile2First || !sawFullSchema || !sawPartialSchema || !sawIdAsCol0 || !sawIdAsCol1) {
+        System.out.println("Some variations did not occur");
 
 Review comment:
   I know PR is not completed yet, just a note to remove `System.out.println` during final clean up.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


paul-rogers commented on pull request #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#discussion_r269437772
 
 

 ##########
 File path: exec/java-exec/src/test/java/org/apache/drill/exec/store/easy/text/compliant/TestCsvWithSchema.java
 ##########
 @@ -82,6 +167,468 @@ public void testSchema() throws Exception {
           .addRow(10, new LocalDate(2019, 3, 20), "it works!", 1234.5D, 20L, "")
           .build();
       RowSetUtilities.verify(expected, actual);
+    } finally {
+      resetV3();
+      resetSchema();
+    }
+  }
+
+
+  /**
+   * Use a schema with explicit projection to get a consistent view
+   * of the table schema, even if columns are missing, rows are ragged,
+   * and column order changes.
+   * <p>
+   * Force the scans to occur in distinct fragments so the order of the
+   * file batches is random.
+   */
+  @Test
+  public void testMultiFileSchema() throws Exception {
+    RowSet expected1 = null;
+    RowSet expected2 = null;
+    try {
+      enableV3(true);
+      enableSchema(true);
+      enableMultiScan();
+      String tablePath = buildTwoFileTable("multiFileSchema", raggedMulti1Contents, reordered2Contents);
+      run(SCHEMA_SQL, tablePath);
+
+      // Wildcard expands to union of schema + table. In this case
+      // all table columns appear in the schema (though not all schema
+      // columns appear in the table.)
+
+      String sql = "SELECT id, `name`, `date`, gender, comment FROM " + tablePath;
+      TupleMetadata expectedSchema = new SchemaBuilder()
+          .add("id", MinorType.INT)
+          .add("name", MinorType.VARCHAR)
+          .addNullable("date", MinorType.DATE)
+          .add("gender", MinorType.VARCHAR)
+          .add("comment", MinorType.VARCHAR)
+          .buildSchema();
+      expected1 = new RowSetBuilder(client.allocator(), expectedSchema)
+          .addRow(1, "arina", new LocalDate(2019, 1, 18), "female", "ABC")
+          .addRow(2, "javan", new LocalDate(2019, 1, 19), "male", "ABC")
+          .addRow(4, "albert", new LocalDate(2019, 5, 4), "", "ABC")
+          .build();
+      expected2 = new RowSetBuilder(client.allocator(), expectedSchema)
+          .addRow(3, "bob", new LocalDate(2001, 1, 16), "NA", "ABC")
+          .build();
+
+      // Loop 10 times so that, as the two reader fragments read the two
+      // files, we end up with (acceptable) races that read the files in
+      // random order.
+
+      for (int i = 0; i < 10; i++) {
+        boolean sawSchema = false;
+        boolean sawFile1 = false;
+        boolean sawFile2 = false;
+        Iterator<DirectRowSet> iter = client.queryBuilder().sql(sql).rowSetIterator();
+        while (iter.hasNext()) {
+          RowSet result = iter.next();
+          if (result.rowCount() == 3) {
+            sawFile1 = true;
+            new RowSetComparison(expected1).verifyAndClear(result);
+          } else if (result.rowCount() == 1) {
+            sawFile2 = true;
+            new RowSetComparison(expected2).verifyAndClear(result);
+          } else {
+            assertEquals(0, result.rowCount());
+            sawSchema = true;
+          }
+        }
+        assertTrue(sawSchema);
+        assertTrue(sawFile1);
+        assertTrue(sawFile2);
+      }
+    } finally {
+      expected1.clear();
+      expected2.clear();
+      client.resetSession(ExecConstants.ENABLE_V3_TEXT_READER_KEY);
+      client.resetSession(ExecConstants.STORE_TABLE_USE_SCHEMA_FILE);
+      client.resetSession(ExecConstants.MIN_READER_WIDTH_KEY);
+    }
+  }
+
+  /**
+   * Test the schema we get in V2 when the table read order is random.
+   * Worst-case: the two files have different column counts and
+   * column orders.
+   * <p>
+   * Though the results are random, we iterate 10 times which, in most runs,
+   * shows the random variation in schemas:
+   * <ul>
+   * <li>Sometimes the first batch has three columns, sometimes four.</li>
+   * <li>Sometimes the column `id` is in position 0, sometimes in position 1
+   * (correlated with the above).</li>
+   * <li>Due to the fact that sometimes the first file (with four columns)
+   * is returned first, sometimes the second file (with three columns) is
+   * returned first.</li>
+   * </ul>
+   */
+  @Test
+  public void testSchemaRaceV2() throws Exception {
+    try {
+      enableV3(false);
+      enableSchema(false);
+      enableMultiScan();
+      String tablePath = buildTwoFileTable("schemaRaceV2", multi1Contents, reordered2Contents);
+      boolean sawFile1First = false;
+      boolean sawFile2First = false;
+      boolean sawFullSchema = false;
+      boolean sawPartialSchema = false;
+      boolean sawIdAsCol0 = false;
+      boolean sawIdAsCol1 = false;
+      String sql = "SELECT * FROM " + tablePath;
+      for (int i = 0; i < 10; i++) {
+        Iterator<DirectRowSet> iter = client.queryBuilder().sql(sql).rowSetIterator();
+        int batchCount = 0;
+        while(iter.hasNext()) {
+          batchCount++;
+          RowSet result = iter.next();
+          TupleMetadata resultSchema = result.schema();
+          if (resultSchema.size() == 4) {
+            sawFullSchema = true;
+          } else {
+            assertEquals(3, resultSchema.size());
+            sawPartialSchema = true;
+          }
+          if (resultSchema.index("id") == 0) {
+            sawIdAsCol0 = true;
+          } else {
+            assertEquals(1, resultSchema.index("id"));
+            sawIdAsCol1 = true;
+          }
+          if (batchCount == 1) {
+            RowSetReader reader = result.reader();
+            assertTrue(reader.next());
+            String id = reader.scalar("id").getString();
+            if (id.equals("1")) {
+              sawFile1First = true;
+            } else {
+              assertEquals("3", id);
+              sawFile2First = true;
+            }
+          }
+          result.clear();
+        }
+      }
+
+      // Outcome is random (which is the key problem). Don't assert on these
+      // because doing so can lead to a flakey test.
+
+      if (!sawFile1First || ! sawFile2First || !sawFullSchema || !sawPartialSchema || !sawIdAsCol0 || !sawIdAsCol1) {
+        System.out.println("Some variations did not occur");
 
 Review comment:
   Actually, this is a poor person's "test". Can't actually fail when some variations don't occur as test will be flaky. Since this is more of a demo of existing problems, it is not really a test. We'll remove this code when we switch to V3 as the default.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


paul-rogers commented on issue #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#issuecomment-477031299
 
 
   Added wildcard projection handling for both "lenient" and "strict" schemas. Added tests for the scan framework. Added a basic tests for "lenient" wildcard projection with CSV files. Have not yet added CSV tests for strict schema because I need to know how to pass a table-level property with the new CREATE SCHEMA command.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on pull request #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#discussion_r269512903
 
 

 ##########
 File path: exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/scan/project/WildcardProjection.java
 ##########
 @@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.impl.scan.project;
+
+import java.util.List;
+
+import org.apache.drill.exec.physical.impl.scan.project.AbstractUnresolvedColumn.UnresolvedWildcardColumn;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+
+/**
+ * Perform a wildcard projection. In this case, the query wants all
+ * columns in the source table, so the table drives the final projection.
+ * Since we include only those columns in the table, there is no need
+ * to create null columns. Example: SELECT *
+ */
+
+public class WildcardProjection extends ReaderLevelProjection {
+
+  public WildcardProjection(ScanLevelProjection scanProj,
+      TupleMetadata tableSchema,
+      ResolvedTuple rootTuple,
+      List<ReaderProjectionResolver> resolvers) {
+    super(resolvers);
+    for (ColumnProjection col : scanProj.columns()) {
+      if (col instanceof UnresolvedWildcardColumn) {
+        projectAllColumns(rootTuple, tableSchema);
+      } else {
+        resolveSpecial(rootTuple, col, tableSchema);
+      }
+    }
+  }
+
+  /**
+   * Project all columns from table schema to the output, in table
+   * schema order. Since we accept any map columns as-is, no need
+   * to do recursive projection.
+   *
+   * @param tableSchema
 
 Review comment:
   Please add column description to avoid warning in the IDE.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on pull request #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#discussion_r269513816
 
 

 ##########
 File path: exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/scan/BaseScanOperatorExecTest.java
 ##########
 @@ -0,0 +1,180 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.impl.scan;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedScanFramework.ScanFrameworkBuilder;
+import org.apache.drill.exec.physical.impl.scan.ScanTestUtils.ScanFixture;
+import org.apache.drill.exec.physical.impl.scan.ScanTestUtils.ScanFixtureBuilder;
+import org.apache.drill.exec.physical.impl.scan.framework.BasicScanFactory;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.impl.scan.framework.SchemaNegotiator;
+import org.apache.drill.exec.physical.rowSet.ResultSetLoader;
+import org.apache.drill.exec.physical.rowSet.RowSetLoader;
+import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.VectorContainer;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.test.SubOperatorTest;
+import org.apache.drill.test.rowSet.RowSet.SingleRowSet;
+import org.apache.drill.test.rowSet.RowSetUtilities;
+
+/**
+ * Test of the scan operator framework. Here the focus is on the
+ * implementation of the scan operator itself. This operator is
+ * based on a number of lower-level abstractions, each of which has
+ * its own unit tests. To make this more concrete: review the scan
+ * operator code paths. Each path should be exercised by one or more
+ * of the tests here. If, however, the code path depends on the
+ * details of another, supporting class, then tests for that class
+ * appear elsewhere.
+ */
+
+public class BaseScanOperatorExecTest extends SubOperatorTest {
+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BaseScanOperatorExecTest.class);
+
+  /**
+   * Base class for the "mock" readers used in this test. The mock readers
+   * follow the normal (enhanced) reader API, but instead of actually reading
+   * from a data source, they just generate data with a known schema.
+   * They also expose internal state such as identifying which methods
+   * were actually called.
+   */
+
+  protected static abstract class BaseMockBatchReader implements ManagedReader<SchemaNegotiator> {
+    public boolean openCalled;
 
 Review comment:
   Can fields be private / protected?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on pull request #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#discussion_r269512455
 
 

 ##########
 File path: exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/scan/project/ScanLevelProjection.java
 ##########
 @@ -128,6 +129,25 @@
 
 public class ScanLevelProjection {
 
+  public enum ScanProjectionType {
 
 Review comment:
   Please add java doc for each option if possible.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on pull request #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#discussion_r269542539
 
 

 ##########
 File path: exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/scan/file/MetadataColumn.java
 ##########
 @@ -25,6 +25,8 @@
 
 public abstract class MetadataColumn extends ResolvedColumn implements ConstantColumnSpec {
 
+
+  public static final int ID = 15;
 
 Review comment:
   Do we still need ID?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on issue #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#issuecomment-477160154
 
 
   @paul-rogers command syntax is the following:
   ```
   CREATE [OR REPLACE] SCHEMA
   [LOAD 'file:///path/to/file']
   [(column_name data_type nullability,...)]
   [FOR TABLE `table_name`]
   [PATH 'file:///schema_file_path/schema_file_name'] 
   [PROPERTIES ('key1'='value1', 'key2'='value2', ...)]
   ```
   `PROPERTIES` should be provided in parenthesis, in a form of key / value pairs where value follows after the key and equal sign, each enclosed the single quotes. Key / value groups should be separated by commas.
   ```
   create schema
   (col1 int, col2 int)
   for table t
   properties (
   'drill.strict' = 'true',
   'some_other_prop' = 'val')
   ```
   In `.drill.schema` JSON file this would look the following way:
   ```
   {
     "table" : "dfs.tmp.`t`",
     "schema" : {
       "columns" : [
         {
           "name" : "col1",
           "type" : "INT",
           "mode" : "OPTIONAL"
         },
         {
           "name" : "col2",
           "type" : "INT",
           "mode" : "OPTIONAL"
         }
       ],
       "properties" : {
         "drill.strict" : "true",
         "some_other_prop" : "val"
       }
     },
     "version" : 1
   }
   ```
   During deserialization they will be stored in `TupleMetadata` class (use `property(key)`, `properties` methods to extract them).
   
   If you want to add column properties, similar syntax will be used, except instead of parenthesis you need to use curly braces:
   ```
   create schema
   (col1 int, col2 int properties {'drill.strict' = 'true'})
   for table t
   properties (
   'drill.strict' = 'true',
   'some_other_prop' = 'val')
   ```
   JSON output:
   ```
   {
     "table" : "dfs.tmp.`t`",
     "schema" : {
       "columns" : [
         {
           "name" : "col1",
           "type" : "INT",
           "mode" : "OPTIONAL",
           "properties" : {
             "drill.strict" : "true"
           }
         },
         {
           "name" : "col2",
           "type" : "INT",
           "mode" : "OPTIONAL"
         }
       ],
       "properties" : {
         "drill.is_strict_schema" : "true",
         "some_other_prop" : "val"
       }
     },
     "version" : 1
   }
   ```
   Please let me know if there are any other syntax related questions.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on issue #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#issuecomment-477160154
 
 
   @paul-rogers command syntax is the following:
   ```
   CREATE [OR REPLACE] SCHEMA
   [LOAD 'file:///path/to/file']
   [(column_name data_type nullability,...)]
   [FOR TABLE `table_name`]
   [PATH 'file:///schema_file_path/schema_file_name'] 
   [PROPERTIES ('key1'='value1', 'key2'='value2', ...)]
   ```
   `PROPERTIES` should be provided in parenthesis, in a form of key / value pairs where value follows after the key and equal sign, each enclosed the single quotes. Key / value groups should be separated by commas.
   ```
   create schema
   (col1 int, col2 int)
   for table t
   properties (
   'drill.strict' = 'true',
   'some_other_prop' = 'val')
   ```
   In `.drill.schema` JSON file this would look the following way:
   ```
   {
     "table" : "dfs.tmp.`t`",
     "schema" : {
       "columns" : [
         {
           "name" : "col1",
           "type" : "INT",
           "mode" : "OPTIONAL"
         },
         {
           "name" : "col2",
           "type" : "INT",
           "mode" : "OPTIONAL"
         }
       ],
       "properties" : {
         "drill.strict" : "true",
         "some_other_prop" : "val"
       }
     },
     "version" : 1
   }
   ```
   During deserialization they will be stored in `TupleMetadata` class (use `property(key)`, `properties` methods to extract them).
   
   If you want to add column properties, similar syntax will be used, except instead of parenthesis you need to use curly braces:
   ```
   create schema
   (col1 int, col2 int properties {'drill.strict' = 'true'})
   for table t
   properties (
   'drill.strict' = 'true',
   'some_other_prop' = 'val')
   ```
   JSON output:
   ```
   {
     "table" : "dfs.tmp.`t`",
     "schema" : {
       "columns" : [
         {
           "name" : "col1",
           "type" : "INT",
           "mode" : "OPTIONAL",
           "properties" : {
             "drill.strict" : "true"
           }
         },
         {
           "name" : "col2",
           "type" : "INT",
           "mode" : "OPTIONAL"
         }
       ],
       "properties" : {
         "drill.strict" : "true",
         "some_other_prop" : "val"
       }
     },
     "version" : 1
   }
   ```
   Please let me know if there are any other syntax related questions.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on issue #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#issuecomment-477160154
 
 
   @paul-rogers command syntax is the following:
   ```
   CREATE [OR REPLACE] SCHEMA
   [LOAD 'file:///path/to/file']
   [(column_name data_type nullability,...)]
   [FOR TABLE `table_name`]
   [PATH 'file:///schema_file_path/schema_file_name'] 
   [PROPERTIES ('key1'='value1', 'key2'='value2', ...)]
   ```
   `PROPERTIES` should be provided in parenthesis, in a form of key / value pairs where value follows after the key and equal sign, each enclosed the single quotes. Key / value groups should be separated by commas.
   ```
   create schema
   (col1 int, col2 int)
   for table t
   properties (
   'drill.strict' = 'true',
   'some_other_prop' = 'val')
   ```
   In `.drill.schema` JSON file this would look the following way:
   ```
   {
     "table" : "dfs.tmp.`t`",
     "schema" : {
       "columns" : [
         {
           "name" : "col1",
           "type" : "INT",
           "mode" : "OPTIONAL"
         },
         {
           "name" : "col2",
           "type" : "INT",
           "mode" : "OPTIONAL"
         }
       ],
       "properties" : {
         "drill.strict" : "true",
         "some_other_prop" : "val"
       }
     },
     "version" : 1
   }
   ```
   During deserialization they will be stored in `TupleMetadata` class (use `property(key)`, `properties` methods to extract them).
   
   If you want to add column properties, similar syntax will be used, except instead of parenthesis you need to use curly braces:
   ```
   create schema
   (col1 int properties {'drill.strict' = 'true'}, col2 int)
   for table t
   properties (
   'drill.strict' = 'true',
   'some_other_prop' = 'val')
   ```
   JSON output:
   ```
   {
     "table" : "dfs.tmp.`t`",
     "schema" : {
       "columns" : [
         {
           "name" : "col1",
           "type" : "INT",
           "mode" : "OPTIONAL",
           "properties" : {
             "drill.strict" : "true"
           }
         },
         {
           "name" : "col2",
           "type" : "INT",
           "mode" : "OPTIONAL"
         }
       ],
       "properties" : {
         "drill.strict" : "true",
         "some_other_prop" : "val"
       }
     },
     "version" : 1
   }
   ```
   Please let me know if there are any other syntax related questions.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


paul-rogers commented on issue #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#issuecomment-477490790
 
 
   Addressed review comments. Added more implicit type conversions.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on issue #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#issuecomment-477592578
 
 
   It's an impressive PR and it's good that we did code review interactively. Overall, looks good. Please squash the commits and it will be good to go.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


paul-rogers commented on issue #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#issuecomment-477853689
 
 
   Final cleanup to ensure all check style warnings are addressed and unit tests pass.
   As part of this, removed a bunch of unused "suppress warnings" annotations that are no longer needed in Java 8.
   
   This PR should be ready to go. Since this has gotten rather large (sorry!) we'll tackle enforcing "strict" column schema in the next one.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


arina-ielchiieva commented on issue #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711#issuecomment-478020820
 
 
   +1, let's finish pending changes in different PR.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


asfgit commented on pull request #1711: DRILL-7011: Support schema in scan framework
URL: https://github.com/apache/drill/pull/1711
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


Merged into master with commit id d89d5fc769557aef4c74c0b33b9c8f854035da93.

