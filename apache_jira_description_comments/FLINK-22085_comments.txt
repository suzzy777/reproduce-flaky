Also a failure: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15944&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=9071

[~jqin] Could you take a look?

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15996&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=10356

A failure because of creating topic timeout.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16005&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7928

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16232&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7410

[~lindong] Do you mind taking a look?

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16254&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6919

{code}
[ERROR] testAutoOffsetRetrievalAndCommitToKafka(org.apache.flink.connector.kafka.source.KafkaSourceLegacyITCase)  Time elapsed: 30.446 s  <<< FAILURE!
java.lang.AssertionError: expected:<50> but was:<null>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runAutoOffsetRetrievalAndCommitToKafka(KafkaConsumerTestBase.java:386)
	at org.apache.flink.connector.kafka.source.KafkaSourceLegacyITCase.testAutoOffsetRetrievalAndCommitToKafka(KafkaSourceLegacyITCase.java:155)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16281&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6539

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16311&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6862

[~dwysakowicz] Sure, I will take a look.

Thx [~lindong]  I will assign this to you.

Any progress [~lindong] ?

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16470&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

[~dwysakowicz] Sorry, I was busy with other work and didn't realize this is a blocker for 1.13 release until today.

This looks like a flaky test which didn't have any useful log information in the Azure pipeline log. I was not able to reproduce this on my Macbook yet.

Since this is a blocker for 1.13 release and we want to address this asap, I opened [https://github.com/apache/flink/pull/15617] to change the test log level to INFO. Could you help approve this PR?

In the mean time, I will take a deeper look in the test source code and run more tests locally to hopefully reproduce it with INFO level logging.

I am afraid we should not increase the log level. Could you verify that in your personal azure account? https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository

I marked it as a blocker because I've seen quite regularly. My intention is to mainly verify if the problem is in the production code or is a flakiness of the test.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16524&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6862

Sounds good. I will use my personal azure account.

I am able to reproduce it on my Macbook with INFO level logging. The following stacktrace is repeated printed in the log. I am looking into this.

{code:java}
348201 [Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Failed to trigger checkpoint for job a9516740f56272d7a3730023fec0bead.)
org.apache.flink.runtime.checkpoint.CheckpointException: some tasks of job a9516740f56272d7a3730023fec0bead has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
        at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:101) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) ~[?:1.8.0_151]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.11.12.jar:?]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[scala-library-2.11.12.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?]
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
{code}

 
 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16544&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6539

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16574&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6862

[~dwysakowicz] I have updated this JIRA with the root cause explanation. Could you help review https://github.com/apache/flink/pull/15633?

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16590&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6539

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16590&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6539

Fixed in:
* master
** 98424e6383bcce107844cbeecc2e9df4ffb4272a
* 1.13
** 766f7175d62f5ef572027331e3fceb31b1b4cc4b

[~lindong] {{KafkaSourceLegacyITCase}} timed out in [that build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16789&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6593]. The build itself included the fix [98424e6|https://github.com/apache/flink/commit/98424e6383bcce107844cbeecc2e9df4ffb4272a] provided by this issue. May you have another look at it to double-check whether it's related or a completely different issue/

Thanks for the information [~mapohl]. Previously I have verified that I can successfully run KafkaSourceLegacyITCase::testBrokerFailure 100 times without issue.

Let me try to reproduce the issue by running KafkaSourceLegacyITCase.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16858&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16849&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6916

Any luck reproducing the issue [~lindong]?

[~dwysakowicz] Yeah I am able to reproduce the test hang once. I found the reason why the test hangs (after a test failure) and uploaded [https://github.com/apache/flink/pull/15713] to fix this this.

I am still trying to debug the test failure.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16996&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6883

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16996&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=41635

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16996&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17001&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=42003

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17000&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=41341

Merge another fix in:
* 1.14 (master)
** 58a7c80fa35424608ad44d1d6691d1407be0092a
* 1.13
** 28eea07cea69f1d7bf0d2062e65e08f7671b8ebc

[~dwysakowicz] FYI, after applying https://github.com/apache/flink/pull/15713, I am able to run KafkaSourceLegacyITCase 100 times without any failure. 

The bug fixed by https://github.com/apache/flink/pull/15713 would cause test hang only if there is a Flink job that keeps running. After reading through related code, I still could not find a full explanation of why there is such a Flink job in the first place.

I will stop investigating this bug for now. Let's see if there is still test failure and if we can find more useful information from the Azure pipeline log.




Thanks a lot [~lindong]. Really good job on stabilising the test! Your plan sounds good to me to wait and see if it still occurs.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17206&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6612

I will remove timeouts on master to possibly get thread dumps.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17212&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7062

Thank you [~dwysakowicz] for the information.

For the first test failure [1], it could be because the Azure pipeline is very slow and the it takes more than 60 seconds (due to long GC) to complete that test. Maybe we can see if increasing the timeout to 120 seconds could reduce the failure rate of this test.

For the second test failure [2], it appears that the test failed due to "OperatorEvent from an OperatorCoordinator to a task was lost". This is relate to https://github.com/apache/flink/pull/15605 which was committed recently. 

Given that KafkaSourceLegacyITCase no longer hangs and the comment history in this JIRA is already very long, I opened https://issues.apache.org/jira/browse/FLINK-22488 to track the issue of "OperatorEvent from an OperatorCoordinator to a task was lost". Maybe we can close this JIRA and continue the discussion in FLINK-22488.

[1] https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17206&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6612
[2] https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17212&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=7062

One observation:
* some of the sub-tasks finish right away after starting, which makes it impossible to take any checkpoints/savepoints
{code}
[Source: KafkaSource -> Map -> Map (3/8)#1] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
22:02:57,135 [Source: KafkaSource -> Map -> Map (5/8)#1] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Reader received NoMoreSplits event.
22:02:57,136 [Source: KafkaSource -> Map -> Map (4/8)#1] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource -> Map -> Map (4/8)#1 (334496a6fd2c371dd842e7ecb61811d9) switched from RUNNING to FINISHED.


22:05:27,161 [FailingIdentityMapper Status Printer] INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper [] - ============================> Failing mapper  6: count=1000, totalCount=1000
22:05:27,162 [FailingIdentityMapper Status Printer] INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper [] - ============================> Failing mapper  1: count=1000, totalCount=1000
22:05:27,163 [FailingIdentityMapper Status Printer] INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper [] - ============================> Failing mapper  7: count=1000, totalCount=1000
22:05:27,163 [FailingIdentityMapper Status Printer] INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper [] - ============================> Failing mapper  0: count=1000, totalCount=1000
22:05:27,163 [FailingIdentityMapper Status Printer] INFO  org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper [] - ============================> Failing mapper  5: count=1000, totalCount=1000
22:05:27,628 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:28,128 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:28,628 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:29,128 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:29,627 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:30,128 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:30,628 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:31,128 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:31,628 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
22:05:32,128 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job 461c989450b54b7847421b914ddb4f00 since some tasks of job 461c989450b54b7847421b914ddb4f00 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17243&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=41700

[~dwysakowicz] I see. Previously the test would be fail with org.junit.runners.model.TestTimedOutException if it does not end within 60 seconds. I suppose the test is hanging now because we have removed timeouts.

Now that we have verifies that the test would indeed hang if we do not timeout tests, I think we can add back the timeout now to avoid test hang.

Regarding why that test could not end by itself: I have seen and investigated this before. I didn't find the root cause as explained in the earlier comment [1]. Take testMultipleSourcesOnePartition as example, this test might not end by itself if the ValidatingExactlyOnceSink in the Pipeline could not receive the expected number of messages, which means that some message is lost in the pipeline.

Previously I have checked the FlinkKafkaConsumer code but could not find any hypothesis of why message could be lost without triggering an exception in this class. But FLINK-21996 gives me new hint as it suggests that messages could be lost in the pipeline when any AddSplitEvent is lost. I currently don't have enough knowledge to prove or disprove this hypothesis. Will need more time to read the code. 

It will be great if other Flink developers could help provide more ideas or debug how a message could be lost in a Flink job.

[1] https://issues.apache.org/jira/browse/FLINK-22085?focusedCommentId=17330815&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17330815






{quote}
Now that we have verifies that the test would indeed hang if we do not timeout tests, I think we can add back the timeout now to avoid test hang.
{quote}

It's better not to add the timeouts because now we have thread dumps and we know the test actually hangs busy looping. With a timeout it could've hanged in a random spot without an informative traces. Therefore we should not add timeouts.

[~dwysakowicz] Hmm... why would the test hang in a random spot if we add timeouts? Regarding those bugs which we are currently investigating, if we add back the timeout, then the test will just fail fast with TestTimedOutException. Not sure if I missed something here.

Note that if we don't add back the timeout, then those bugs will waste Azure pipeline resource. And it also makes developer wait longer for the test result. I personally believe new thread dumps will give us the same information as the thread dump we already get. Just my 2 cents.

If you have concerns regarding timeouts, please join the discussion at: https://lists.apache.org/thread.html/r0710f90f30e230d3f11b459a6edc5afd5b2e990ab9533b1442adfaf5%40%3Cdev.flink.apache.org%3E

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17993&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6634

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18055&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18052&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18651&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6378

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18851&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6366

[~lindong], any updates on this one?

[~xintongsong] No... after spending 2+ hours over the weekend to look into this, I don't think I will be able to find the answer to this bug any time soon.

Previously I spent considerable (from 4/11 to 4/26) to investigate this bug and found 2+ issues. I couldn't not find any more Kafka-related bugs by reading the code and the thread dump. It is not clear from the thread dump where the deadlock comes from.  In order for me to further figure this out, I would need to have deeper understanding of Flink runtime works by reading the source Flink code, which I have not had time to do since I joined the current team.

If anyone could help read the threadump from the log and give some idea regarding why the test deadlocks, I would be happy to continue the investigation.



https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18957&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6632

[~lindong], thanks for the updates. I'll try to find another pair of eyes to take a look.

Do you mind if I unassign you for the moment? That helps remind us on the kanban board that this ticket is not being taken care of.

[~xintongsong] Sure, please feel free to un-assign me. Thanks!

[~gaoyunhaii], could you help take a look at the stacks from the runtime angles?

[~xintongsong] OK, I'll have a look~

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19003&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6601

Thanks a log, [~gaoyunhaii]. I've assigned you to the ticket.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19048&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19069&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6632

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19124&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6632

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19152&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=6601

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19175&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6687

I checked the last several cases, it seems they are all hang at testMultipleSourcesOnePartition(org.apache.flink.connector.kafka.source.KafkaSourceLegacyITCase) with the last log shows
{code:java}
01:34:47,793 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint for job d9229593ea3e8c25751e0b146ba62ee1 since some tasks of job d9229593ea3e8c25751e0b146ba62ee1 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
{code}
I'll first focus on this issue.

[~gaoyunhaii] Thanks for the investigation. Previously I have looked into this particular INFO message. From what I know, it appears to be expected by this test.

What happens is that some subtask (e.g. 7 out of 8 subtasks) have finished correctly when the checkpoint is triggered. And Flink would not take the checkpoint if any subtask has finished.

The issue here is that a subtask never finishes. According to the logic of this particular test, this could be because the sink of this subtask never received the expected number of messages, which I could not find any explanation so far.


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19202&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

Hi [~lindong] thanks for the explanation and it indeed helps a lot. 

First sorry that I still have not found the root cause, I further checked the case, and the rough process is
  
1. A generate job starts up and generates 5 partitions, each with 1000 records.
2. A consumer job starts up, it has two tasks, the first is kafka source -> failing identity map -> validate map with parallelism 8, and the second  task is the validating exactly-once sink, with parallelism 1. 
3. Since there are only 5 partitions, 3 of the source tasks would finished immediately after startup.
4. One of the failing identity map would trigger failover in the middle. 
5. After restarts, the job would re-run. Note that no checkpoint taken in the first run due to finished tasks, the job would run from scratch after failover. And similar to the first run 3 of the 8 source tasks would finish immediately.
6. The original design is that the sink task would check if 5000 records are received, if so, it would throw a SuccessException() to terminate the job.

The main problem here is that the debug thread prints that each of the 5 alive failing identity map have processed 1000 records, but the sink still do not throw the exception. I could not see other possible execution path for now. To make it clear how many records the sink has received, I first open [a PR to also add debug thread to print the sink status | https://github.com/apache/flink/pull/16233]. Could some one have a look at this PR first? Very thanks~

Logs added in db73b33538ab8b2d9edc91e6fe9795b48e3a2e4d

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19643&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

[~gaoyunhaii], 
New instance found. Please take another look.

Thanks [~xintongsong] for the review and monitoring the tests~ Afterward I tried to repeat this test on Azure: [https://github.com/apache/flink/pull/16317], and it seems that the issue could reproduce in Azure for some probability. With all these instances I found that:
 # The failing identity map indeed process required (namely 1000) records after failover.
 # The sink might lost all the records from some map tasks, but the number of maps get lost is random. And I have not met with the case that a part of records from a single task get lost.

As a whole, I think it seems the case is not related to Kafka, but might have some relationship with local network stack. One possible related point is that the test has set buffer timeout to 0. I'll try to further analyze the causes. 

Hi all, I should have found the reason: for flush timeout = 0, the flush happens only after the records emitted and there is no standalone flush thread, then after failover:
 # The (source -> map -> map) task (parallel = 8) started, it first try to restore the partition state, after this it broadcast EndOfChannelStateEvent, this would block the channel. 
 # Then for the following records emitted by the (source -> map -> map) tasks, it would not notifyDataAvailable since isBlocked = true.
 # After the (sink) task (parallel = 1) received all the EndOfChannelStateEvent, it would resume all the subpartitions. After this it would check if the subpartition is available, if so, it would queue the corresponding local input channel
 # However, if before 3, the (source -> map -> map) task has emitted all the 1000 records, then these record would not be notified during resuming since the subpartition has isBlock = false, but when it check the availability of the subpartition, it would return isAvailable() = false since flush requested = false. Then the data won't be notified in the future

The bug could be reproduced locally by add sleep in the UpstreamRecoveryTrackImpl#handleEndOfRecovery to delay the step 3. 

Thanks you [~gaoyunhaii] for the explanation!

Do you plan to submit a PR to fix this? I will help take a deeper look into this explanation and the PR.

No problem, I'll create a new issue and link it to this issue. I'll also think a bit on the possible fix options~

I think having `notifyDataAvailable` at the end of recovery certainly makes sense. Extra `notifyDataAvailable` are rather cheap when not on the hot-path.

Very thanks [~arvid], I also agree with we could have one more notifyDataAvailable at the end of recovery and other resume cases. I'll have a look at the details~

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19865&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19884&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=41430

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20195&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7&l=6529

I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled "stale-assigned".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.


Since we have fix this case via https://issues.apache.org/jira/browse/FLINK-23223 and it has not pop up for a long time, I'll close the issue for now.

