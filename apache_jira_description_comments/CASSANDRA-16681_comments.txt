I think this is a test issue. I think we have a race [here|https://github.com/apache/cassandra/blob/9a432418f2277c40a1fe4b64049688d6354ecdca/test/burn/org/apache/cassandra/utils/memory/LongBufferPoolTest.java#L276-L284] where, if threads exit after {{doneThreads}} is sampled, the assumptions mentioned in the comment are violated.

I haven't assigned or posted a change because I'm still looking at some other weirdness around this test and trying to understand how it was intended to work.

If anyone else wants to take a look and corroborate or just take the ticket, it's fine by me.

Although I still think there is a race as described, more troubleshooting has indicated that it is not the cause for this assertion error most times. It's happening earlier on in the test when the threads would not be exiting. When it does happen, there is always only a single tracked chunk that has not been recycled a given time interval. It appears that the chunk is not being acquired or cycled in any way after entering this state. Still digging on it, but if anyone with more knowledge in the area wants to take a look, they are welcome.

I think I've found a race. I know [~gianluca] also said he's working on a patch, so I'm just going to post my findings here so we can compare notes.

What I think is happening: 

LocalPool.addChunk [evicts|https://lists.apache.org/thread.html/r75b09da8df530aa382605887a63dfa57b9c8d647b10f9064dd2b027a%40%3Cdev.cassandra.apache.org%3E] a non-empty chunk in thread A. {{evict.release()}} finds a "not free" chunk and does not recycle. Meanwhile, thread B does {{LocalPooll.put}}, freeing the last buffer from the chunk. {{free}} returns -1, but the [status CaS|https://github.com/apache/cassandra/blob/4acfd3bdf1acdb6b28059a49dd39823d7ea0689d/src/java/org/apache/cassandra/utils/memory/BufferPool.java#L808] fails because thread A has not yet {{setEvicted}}. We therefore leave the function with the chunk totally freed, but not recycled.

I have a patch with some synchronization around the release+status update that removes this flakiness. Currently wondering how big an issue it actually is, and if we care. Also pondering if this should be moved out of 4.0 for a number of reasons:

1.) 4.0 is close and I don't have much appetite for touching such integral code
2.) I think this issue is low-impact since the chunk would just be GC'd instead of being recycled
3.) The test is fairly pathological and this is perhaps even less likely to happen in the running server (pure speculation)

Curious to get input on this and hear what Gianluca has found.

bq. I have a patch with some synchronization around the release+status update that removes this flakiness.

Getting back to this after some time, I'm less confident in the patch. I ported the fix without all my instrumentation and I'm back seeing flakiness. I will be looking into it more, but meanwhile I'm open to input if there are other leads.

How did you repro this one [~aholmber]? I have given it a go with no luck so far.

I have never reproduced locally. I could get it occasionally in the test multiplexer in CircleCI, but I have not developed any good leads. I have not spent a lot of time on it more recently. I was thinking of un-assigning and asking whether we should punt on this, unless someone has energy to devote to it. Thoughts?

I haven't bottomed out on this, and I haven't produced a complete solution thus far. I'm going to continue looking at it intermittently as I'm able, but meanwhile I'll un-assign to make sure I'm not dissuading anyone else from taking it up.

{noformat}
[junit-timeout] Testcase: testPoolAllocateWithRecyclePartially(org.apache.cassandra.utils.memory.LongBufferPoolTest):   FAILED
[junit-timeout] null
[junit-timeout] junit.framework.AssertionFailedError
[junit-timeout]         at org.apache.cassandra.utils.memory.LongBufferPoolTest$Debug.check(LongBufferPoolTest.java:106)
[junit-timeout]         at org.apache.cassandra.utils.memory.LongBufferPoolTest.testAllocate(LongBufferPoolTest.java:288)
[junit-timeout]         at org.apache.cassandra.utils.memory.LongBufferPoolTest.testPoolAllocate(LongBufferPoolTest.java:139)
[junit-timeout]         at org.apache.cassandra.utils.memory.LongBufferPoolTest.testPoolAllocateWithRecyclePartially(LongBufferPoolTest.java:127)
[junit-timeout]
[junit-timeout]
[junit-timeout] Testcase: testPoolAllocateWithoutRecyclePartially(org.apache.cassandra.utils.memory.LongBufferPoolTest):        FAILED
[junit-timeout] null
[junit-timeout] junit.framework.AssertionFailedError
[junit-timeout]         at org.apache.cassandra.utils.memory.LongBufferPoolTest$Debug.check(LongBufferPoolTest.java:106)
[junit-timeout]         at org.apache.cassandra.utils.memory.LongBufferPoolTest.testAllocate(LongBufferPoolTest.java:288)
[junit-timeout]         at org.apache.cassandra.utils.memory.LongBufferPoolTest.testPoolAllocate(LongBufferPoolTest.java:139)
[junit-timeout]         at org.apache.cassandra.utils.memory.LongBufferPoolTest.testPoolAllocateWithoutRecyclePartially(LongBufferPoolTest.java:133)
{noformat}

Just adding these since I had not seen them before.

Looking at this more today. I'm wondering if we should review and maybe move forward a patch for the race I described above. As mentioned, I could still get failures following that, but what I realized today:
 - Failures go from ~dozens per hundred runs to single digit
 - Even though one mode of flakiness fails the same assertion, I now believe it may be arriving there by a different mechanism

Is it worth taking a look at those changes in the interest of getting more stability, and possibly spinning out investigation looking for more issues?

That sounds good to me unless [~brandon.williams] beats us to it. If we do that though I would put here a detailed description of the findings and thread interactions because in a few months time, if we revisit this, we don't want to ramp up again on such things.

I've been on and off looking into this ticket too.

What I noticed is that sometimes a worker thread is simply lagging behind and the corresponding chunk doesn't get recycled in the arbitrary window of 10 seconds. But if we were to wait a couple more seconds (instead of failing with the assertion) we'd see it would eventually catch up.
 The problem is that the {{sharedRecycle}} queues fill up more quickly than the other threads can process them.

In order to reproduce the failure locally, I simply increase the number of concurrent worker threads from a factor of 2 to, say, 10 here [https://github.com/apache/cassandra/blob/699a1f74fcc1da1952da6b2b0309c9e2474c67f4/test/burn/org/apache/cassandra/utils/memory/LongBufferPoolTest.java#L139].

I believe some sort of CPU contention is also happening in CircleCI, given the test can't determine the right number of processors available to the Docker container.
 The xlarge instance has only 8 vCPUs, but from the logs we can see the test identifies 36 cores and so it starts 72 threads:
{code:java}
[junit-timeout] INFO  [main] 2021-05-19 16:18:52,488 LongBufferPoolTest.java:264 - 2021/05/19 16:18:52 - testing 72 threads for 2m
{code}
I tested with different instance sizes (small, medium, xlarge) and they all report 36 cores.
 This seems to be a problem with CircleCI in general:

[https://circleci.com/docs/2.0/configuration-reference/#resourceclass]
{quote}Note: Java, Erlang and any other languages that introspect the /proc directory for information about CPU count may require additional configuration to prevent them from slowing down when using the CircleCI 2.0 resource class feature. Programs with this issue may request 32 CPU cores and run slower than they would when requesting one core. Users of languages with this issue should pin their CPU count to their guaranteed CPU resources.
{quote}
I have a patch to set a fixed number of workers threads (16) for this test that should help with this issue: [https://github.com/grighetto/cassandra/pull/8]

All other assertions in the test that deal with integrity/correctness always passed, which also indicates this is really just a timing issue.

FWIW, I can reproduce this on a plain linux machine running java 8 without any modification.

Without the modification, I've seen it cut pretty close too (meaning the last chunk got recycled just the before the latch timeout), but increasing the threads favors it.
But anything that causes a delay has the potential to fail that assertion because it's time based. There are many random choices going on in the test, if so it happens that many workers are queuing their buffers for their neighbours to release them, but their neighbours don't get scheduled to clear the queues, it has the potential to fail too.

My initial proposal consisted in reducing the ratio of queuing/direct release: https://github.com/grighetto/cassandra/pull/7/commits/aa3ba87f137f0ccba94957a1f100de3953b172bf
That also works for me, even with the increased number of threads.

Anyway, there might be other issues too, but I'm pretty sure CPU contention is playing a part on CircleCI because {{Runtime.getRuntime().availableProcessors()}} doesn't work properly there.

bq. All other assertions in the test that deal with integrity/correctness always passed, which also indicates this is really just a timing issue.

I agree with your analysis.  Even doubling your reduction ratio to 1 out of 20 and reducing the threads to 2, this test still fails for me, not that I'm surprised.

I believe like many other tests that rely on timing, we're never going to be able to fully prevent this from being flaky in the real world where things can be slower than theory.

For 4.0.0 I suggest we disable this test and examine rewriting it later.

Saw this here: https://ci-cassandra.apache.org/job/Cassandra-devbranch/897/

I just found this in 3.11 - https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1119/testReport/junit/org.apache.cassandra.utils.memory/LongBufferPoolTest/testAllocate/

New error on trunk: [https://ci-cassandra.apache.org/job/Cassandra-trunk/1032/testReport/org.apache.cassandra.utils.memory/LongBufferPoolTest/testPoolAllocateWithRecyclePartially_2/]

It can be reproduced [with CircleCI|https://app.circleci.com/pipelines/github/adelapena/cassandra/1406/workflows/ef197b74-0805-404f-bdcd-851cdd75a846].

See the other (duplicate) ticket. There are more modes of failure than mentioned here, and this is likely not (only) a test issue. 
The code under test is not properly synchronized in at least one place (MicroQueueOfChunks is one obvious place).

I submitted a patch for 4.0+ version.
Now looking into 3.11 branch, but it turns out it is different and it actually doesn't have some of the issues I fixed. 
For instance, 3.11 doesn't seem to have the biggest thread-safety issue in the MicroQueueOfChunks caused by Chunk#recycler  reference, because it doesn't have the recycler field at all (and no tinyPool either). 
It also doesn't seem to have the memory in use bug addressed by my first patch. 

However, it does seem to rely on the same assumptions in the LongBufferPoolTest, so the test is probably flaky there as well. 
Do you think it is worth backporting the relevant fixes to 3.11? 


CC [~stefania_alborghetti] [~jasonstack]

3.11 is a supported version, so unless sbdy corrects me I am afraid we have to.

Ok, I created the following PRs:

casssandra-3.11: https://github.com/apache/cassandra/pull/1617
cassandra-4.0: https://github.com/apache/cassandra/pull/1614
cassandra-4.1: https://github.com/apache/cassandra/pull/1618
trunk: https://github.com/apache/cassandra/pull/1619

Basically all patches go to 4.0 and higher.
For 3.11, I backported only the LongBufferPoolTest patch, because the other issues in the BufferPool are not present there (also 3.11 version of BufferPool is a lot simpler than 4.0).



[~pkolaczk] could you provide more clear justifications for your perceived bugs, and the resultant refactors, to guide review?

Which other threads do you think are accessing {{LocalPool.release()}} via what access path? The bugs you perceive seem to hinge on this eventuality, however I can see only two locations invoking {{LocalPool.release}}: {{LocalBufferPoolAllocator.release()}} and {{localPool.onRemoval}}. The former invokes on its own private pool, and has only single ownership semantics, and so should be fine. The latter should only be invoked when the thread is terminating? It could in principle be invoked at another time, but I do not think it is.

Chunks are released only via this path, or when there are no references to the chunk from application threads. So, if the above is fine this is likely also fine?



[~benedict] it is all described in the commit message. It answers exactly your questions.

Quoting the relevant part of the commit message for your convenience:

1. LocalPool is meant to be managed by a single thread.
Unfortunately there was a code path that allowed
a thread enter the context of the LocalPool owned by another
thread by attempting to recycle a chunk using its
recycler reference. This led to a situation where unsynchronized
data structures were modified from multiple threads.
The patch ensures LocalPool state is modified by the owning thread
only.

The following sequence triggered the bug:
a) The app asks the BufferPool for a tiny chunk on Thread A.
We get the chunk from the thread local LocalPool A, we set
its recycler to the parent LocalPool and owner to its tinyPool.
b) Eventually the tiny chunk gets evicted. Now its owner is set
to null, but the chunk is not free yet, so nothing more happens.
c) In the meantime buffer(s) of the tiny chunk get(s)
transferred to Thread B on the client side.
d) Eventually the client releases the last buffer of the tiny chunk
by calling BufferPool#put, but it does that on Thread B.
e) BufferPool grabs a reference to the thread local LocalPool B and
calls put on it. So far all correct. We are returning the buffer
to the LocalPool B, owned by Thread B.
f) First we call free to mark appropriate freeSlots of the chunk.
It is the last buffer, so all bits turn to ones (freeSlots == -1).
The chunk has been evicted, so its owner == null.
We're not the owner, so we don't need to remove the chunk from
the micro queue (no need because it is not there) and we can
give it back to the pool we got the chunk from.
So we call chunk.tryRecycle.
g) chunk.tryRecycle realizes freeSlots == -1, CAS succeeds,
now freeSlots == 0 and the chunk needs to be recycled.
h) chunk.recycle gets called and calls chunk.recycler.recycle()
underneath. But the chunk was allocated by thread A,
so chunk.recycler points to LocalPool A owned by Thread A.
Here we enter the context of the wrong thread!
i) chunk.recycler.recycle() gets the parent chunk and its slab
and calls put again. Notice we're calling put the second time now.
We're calling it on a LocalPool A, but we are still on Thread B.
Danger!
j) LocalPool A is the owner of chunk's slab we got the buffer
from in step b). So owner == this.
k) Now, we're additionally unlucky, and it turns out this was
the last buffer of the normal chunk as well.
So we have free = 0 and owner == this, the first condition after
free is true, so we remove the chunk from the micro queue.
The micro queue is managed by Thread A, and we're
doing this on Thread B. Here we crash.

 

BTW: You will not see this path in cassandra-3.x branches, because the `recycler` thing and tinypool was added on cassandra-4.0. So the bug in the BufferPool applies to 4.0 only.

 

> Chunks are released only via this path, or when there are no references to the chunk from application threads

 

That holds only on 3.x.

In 4.x chunks may be released following the `recycler` reference, which doesn't guarantee pointing to the LocalPool of the current thread.

Thank you, that explains the bug and I agree it is one. 

However, I believe there is a much simpler solution to this problem, which is to introduce an additional parameter to `put` that indicates if it has been invoked by the owning pool directly, and indicate this is not the case when invoked by the `Recycler` implementation in `LocalPool`. This can actually replace the `owner == this` test, and we can introduce a method overload that computes `owner == this` to pass to an `isOwner` parameter. This has the added benefit of not breaking use cases where a `BufferPool` may be (safely) shared between threads - not currently a problem, I don't think, but better to avoid surprises in future.

# How is that simpler than an internal check for currentThread? It adds a parameter to the public API of LocalPool and moves the burden of correctness on the caller.
 # It would make the code analysis harder - it is not immediately obvious the correct thread would touch the private non-synchronized stuff. In my patch it is obvious.
 # I expect the majority of buffers are returned by the same thread which allocated it. So if you disable recycling of buffers invoked from tinypool in all cases of indirect returns, it would be overly conservative.

BTW point 3 leads me to thinking - do we even really need it that much to recycle at all when we notice an owned buffer is completely free? Why not keep it for future allocations from that thread? I can imagine eager recycling could get us in some pathological edge-case performance problem if the caller requests just one buffer, returns it, requests another, returns etc. In this case such buffer would constantly travel between the GlobalPool and LocalPool which kinda defeats the purpose of local pools.

 

bq. How is that simpler than an internal check for currentThread? It adds a parameter to the public API of LocalPool and moves the burden of correctness on the caller.

Because we do not modify the logic in any other way, we just pass `isOwner`. The public API is not modified, it is entirely internal to `BufferPool`.

bq. It would make the code analysis harder - it is not immediately obvious the correct thread would touch the private non-synchronized stuff. In my patch it is obvious.

See above. Code analysis is simpler, as less is changed. I also disagree this makes it significantly more complex even from first principles, particularly if we suitably name the methods to make clear the semantics.

bq. I expect the majority of buffers are returned by the same thread which allocated it. So if you disable recycling of buffers invoked from tinypool in all cases of indirect returns, it looks overly conservative.

No, this would behave correctly as we would be invoking it on the tinyPool that we own, and so we would pass `isOwner = true`

bq. do we even really need it that much to recycle at all when we notice an owned buffer is completely free

Lazy recycling can introduce more pathological behaviour, particularly for `TinyPool` as we may have large allocations that have all been freed, and a single tiny slither in a number of tiny pools that prevent them from being released. If these are themselves part of mostly free _chunks_ we could end up exhausting our global pool entirely without any of it being in use.

Hmm, we have another option of course which is to make `MicroQueueOfChunks` thread safe for this use case. If we remove `count` and simply read all three registers, so that we insert into the first null entry, this should work fine, as Thread B can only null these entries, which is entirely safe for Thread A (it will either see it, or not; the attempt to allocate will be correctly linearised and it would just remove it again, if still present). 

This potentially scopes the changes even more narrowly, logically at least.

That is to say, we permit only exclusive access for `add` but permit `removeIf` and `forEach` to be invoked by any thread safely.

My goal here is to avoid significant changes to the logic of `BufferPool`, both to reduce the risk of the changes and also because I have been asked to review it, and I do not have the time.

> Code analysis is simpler, as less is changed. 

How adding a new parameter + modifying all callers is "less change" than a single `Thread.currentThread() == owningThread` check that is explicit and guaranteed to do the right thing?

The thread check was the only thing needed to correct for that particular problem described in point 1. 

Anyway, I'm fine as long as we still keep an `assert Thread.currentThread() == owningThread` in that place, to make it obvious and fail fast in case someone gets the boolean flag wrong.

The fact it was broken and multiple people didn't see it for many months already proves the property of "only one thread can enter here" is totally not obvious.

And now it would be even harder when we have one more parameter, because now the reader of the code would have to analyze all the callers who call put if they set this flag properly.

 

> The public API is not modified, it is entirely internal to `BufferPool`.

Technically you're right, but this is one of the main APIs of a major internal component, which in itself is quite complex and has many callers.

LocalPool's put is called from the main API of the BufferPool as well  as indirectly from chunks when they get recycled, including recursive call from other put (you can have put twice in a call trace). 

 

> No, this would behave correctly as we would be invoking it on the tinyPool that we own, and so we would pass `isOwner = true`

But what about the already evicted tiny buffers? Owner == null for those.

 

> Lazy recycling can introduce more pathological behaviour, particularly for `TinyPool` as we may have large allocations that have all been freed, and a single tiny slither in a number of tiny pools that prevent them from being released. If these are themselves part of mostly free _chunks_ we could end up exhausting our global pool entirely without any of it being in use.

Indeed, good point.

> Hmm, we have another option of course which is to make `MicroQueueOfChunks` thread safe for this use case.

 

I considered that, but then this gets us into a pandora box of Java memory model peculiarities. IMHO risky approach, and I actually like the "only one thread owns this" approach much better.

bq. The thread check was the only thing needed to correct for that particular problem described in point 1.

If you want to submit a patch that modifies only this, I can review it simply. It retains the problem I mentioned before, of preventing an allocator being safely shared between threads, and it does so unnecessarily. But I'm fine with it.

bq. The fact it was broken and multiple people didn't see it for many months already proves the property of "only one thread can enter here" is totally not obvious.

The amount of time is sort of meaningless here, since nobody is looking at it. The problem here was that ownership was being calculated by the method itself; by calculating it in the caller we must consider at each call-site if this can be known, and if not propagate the parameter to the caller (until we do know). So, I think this adequately mitigates the risk.

bq. LocalPool's put is called from the main API of the BufferPool

I suggested overloading this method, so the overload with the additional method can be made private to indicate it is not a part of the "main API" (although this is all internal to the class, so this is only communicative to the reader)

bq. But what about the already evicted tiny buffers? Owner == null for those.

Good point. Perhaps my proposal of making `MicroQueueOfChunks` thread-safe is superior, then, so that we may in fact recycle these in all cases and avoid more pathological situations if tiny buffers are shared between threads?



> My goal here is to avoid significant changes to the logic of `BufferPool`, both to reduce the risk of the changes and also because I have been asked to review it, and I do not have the time.

 

Fair enough, but the problem 1. was not the *only* problem. Actually after fixing it, some other things popped up. For example there was another race condition related to marking chunks evicted. Which led to a slightly bigger refactor how partial recycling is handled; because partial recycling relies on proper chunk statuses.

bq. I considered that, but then this gets us into a pandora box of Java memory model peculiarities.

What JMM peculiarities are you worried about? We would just need to ensure that every method invoke by `removeIf` was itself thread-safe, which they are but we might want to annotate them and comment that they must be idempotent. We would also need to be certain the `Chunk` are themselves safe-published, but since all their member-fields are final or volatile this is already true as well.

That said, if most of the refactors you introduced were not necessary to fix these concerns I would be happy with simply reducing the scope of changes to those strictly necessary to fix the issue.

bq. Actually after fixing it, some other things popped up. For example there was another race condition related to marking chunks evicted. 

Hmm, ok. Well that scuppers that plan, then.

So, the remaining bugs are unfamiliar to me as I wasn't involved in introducing the {{status}} logic. [~aleksey] perhaps you might be able to take a look at this, since you reviewed the patch that introduced it?

I will take a look anyway, since this code is fairly important, but I don't know I will have time to familiarise myself with it all adequately for a sufficiently confident review.

Also, [~pkolaczk], apologies - I see your earlier commit message is very thorough, explaining these details. I only saw the later commit message in GitHub, which was itself very detailed but did not outline all of the bugs or the precipitating scenario to the above bug.

> The amount of time is sort of meaningless here, since nobody is looking at it. 

I was referring to a few people to whom I showed the bug initially by saying "here, I have the evidence the MicroQueueOfChunks is being accessed from multiple threads" and the response was:

"how so? I looked at the code and it looks correct".

For example here:

>  I can see only two locations invoking {{{}LocalPool.release{}}}: {{LocalBufferPoolAllocator.release()}} and {{{}localPool.onRemoval{}}}. The former invokes on its own private pool, and has only single ownership semantics, and so should be fine. The latter should only be invoked when the thread is terminating? It could in principle be invoked at another time, but I do not think it is. Chunks are released only via this path, or when there are no references to the chunk from application threads. So, if the above is fine this is likely also fine?

;)

And BTW, that was also *my* initial reaction when I saw the exceptions for the first time and required many days until we figured this out. Hence I conclude, it was far from obvious and I wanted to make the code a bit more explicit so the next time noone has to figure this out again.

Anyway, let me get some time to split this patch into just the essential part and the nice-to-have cosmetic / simplification parts, because indeed maybe some stuff could be left out.

> What JMM peculiarities are you worried about?

The fact this is a non-atomic data structure that supports both writes and reads already puts it in territory of "lockless concurrent data structures", which is by definition hard and risky.

E.g. this structure has a counter of non-null fields. When either adding or removing it updates the fields and updates the counter. How do you ensure the other threads see a consistent state? Not sure, but I'm afraid at least some CAS would be needed somewhere. 

I'm not saying it cannot be done, but IMHO keeping LocalPool really thread local avoids a lot of complexity. 

 

bq. E.g. this structure has a counter of non-null fields. When either adding or removing it updates the fields and updates the counter. How do you ensure the other threads see a consistent state? Not sure, but I'm afraid at least some CAS would be needed somewhere.

I was proposing removing the count entirely, as it is a minor optimisation. Simply use the three fields and their null/non-null status. The owning thread is only permitted to set to non-null (and assures safe publication), and the other threads may only set to null, and may either only do so if they have taken ownership of the act, by first transitioning -1 -> 0, or perhaps would do so using CAS (whereas the owning thread could perhaps do so with e.g. {{putOrdered}} )

This at least is a very narrow piece of code to analyse, which is probably safer than modifying the `BufferPool` which appears to have grown too complex since it was first introduced. Some time in future, we should consider refactoring more fully, to perhaps distinguish the `LocalPool` and `TinyPool` concepts more fully so that we do not make this kind of mistake, as the behaviour is subtly different between the two, and it is easy to forget this and fail to account for the special-casing.

Still, you have indicated that this wouldn't materially simplify the patch (indeed, it sounds like it would make it more complicated), so I agree that's a waste of time.

> I was proposing removing the count entirely, as it is a minor optimisation. Simply use the three fields and their null/non-null status.

I agree, this makes sense as well. So that's actually not that complex as I initially thought. The fact we have a fixed number of items (3) and if we remove the counter, we could allow "add from one thread, remove from any" and probably even volatile would be enough.

Well, now I actually like this idea, and you seem to have (almost) convinced me. :)

There is also some nice symmetry with chunks, which can also only be allocated from by a single thread, yet any thread can return buffers to them.

One minor concern is though, that what about eviction racing with recycle. I mean, the owner evicts a chunk and at the same time the application returns the last buffer into that chunk and triggers recycling (on another thread). I guess one must be careful with first nulling the owner and then removing from the queue to prevent this....

Anyway, that's slightly more complex than the isOwner param / thread check, so let's leave it for the future.

 

>  Some time in future, we should consider refactoring more fully, to perhaps distinguish the `LocalPool` and `TinyPool` concepts more fully so that we do not make this kind of mistake, as the behaviour is subtly different between the two, and it is easy to forget this and fail to account for the special-casing.

 

+1

 

Well, {{volatile}} by itself is probably insufficient, as we can have a race condition where all three chunks have been freed by another thread but not yet cleared from the queue, and the owning thread needs to clear an entry and insert a new one. So we either have to have a yield spin (in which case we don't even need {{volatile}}, we can use plain memory accesses), or we need to use CAS for clearing (which I marginally prefer, as I dislike yield spins, even if it comes at a cost of increased volatile usage)



[~benedict] I have split the patch into smaller ones. Should be easier to review now. The biggest one are the changes related to chunk status and partial recycling, but the test doesn't pass without it, so they are needed.

Thanks [~pkolaczk]. [~aleksey] has agreed to review that part, as he reviewed the initial patch the introduced it. Tagging him here so he can take a look now it's ready.

Cool, tagged self as a reviewer.

Have just noticed the Patch Available transition - will take a look soon.

First 6 out of 7 commits look good to me. Some nice catches there.

Almost done convincing myself that the changes in the very last commit - "Fix and simplify chunk status management" - is safe. Won't be long now, and my apologies for the delay again.

May I suggest that we only commit the first 6 out of 7 changes, without the simplification patch?

It’s not incorrect, it does address the (non-critical) issue of returning fully freed chunks to the partially recycled queue, but in my opinion it’s not definitively significantly simpler enough to justify.

It shifts some of the existing complexity to {{Chunk}} but the coupling with LP recycling logic remains effectively the same, and I’m not the biggest fan of {{recycleFully()}} essentially having to be aware of the call stack to decide which CAS to do.

Do you mind delaying further simplification until a further refactor, in a different ticket, that would also include a {{LocalPool}} refactor wrt handling of the tiny pool?

If you don’t mind, then I’m +1 to commit the initial 6 patches and consider the ticket resolved. Cheers.

We have seen some memory leaks in production that could be related to threading issue that the fifth commit address. So I would love to see the 6 first commits being committed. They look good to me. I do not have a strong opinion for the last one but it might be because I ended up more familiar with the previous version of the code. In any case thanks a lot for your investigation [~pkolaczk]. It is some really nice work.

[~aleksey] I'm not sure if the the last patch didn't fix something actually. Let me try to rerun the test with only 6 changes and see if it works and I get back to you. There was simply many back-and-forth already on this and I don't remember exactly at the moment, but the fact that the original control flow didn't go through the same CAS could be a correctness issue, not just code style issue.

// edit:
Looking at the description, there was a race fixed there, not just code style:
{quote}
   2. When a chunk was evicted, it was first released and later marked as
       EVICTED. The actual order of operations with partial recycling
       enabled was as follows:
       a) release: clear owner -> tryRecycle -> try change status
          EVICTED to IN_USE (fail)
       b) change status to EVICTED
    
       This not only didn't really recycle the chunk, but could also race
       with an other thread trying to recycle the very same chunk and
       accidentally mark an already recycled chunk as EVICTED.
       The order has been fixed and now chunk is marked as evicted before
       attempting to recycle it.

{quote}

The `testPoolAllocateWithRecyclePartially` fails quite reliably  if I exclude the last patch:

{noformat}
INFO  [main] 2022-10-07 09:31:39,811 LongBufferPoolTest.java:329 - 2022/10/07 09:31:39 - testing 16 threads for 2m
INFO  [main] 2022-10-07 09:31:39,811 LongBufferPoolTest.java:330 - Testing BufferPool with memoryUsageThreshold=16777216 and enabling BufferPool.DEBUG
INFO  [test:5] 2022-10-07 09:31:39,829 NoSpamLogger.java:92 - Maximum memory usage reached (16,000MiB), cannot allocate chunk of 8,000MiB

java.lang.AssertionError: Last recycled 5 < last acquired 6
	at org.apache.cassandra.utils.memory.LongBufferPoolTest$Debug.check(LongBufferPoolTest.java:122)
	at org.apache.cassandra.utils.memory.LongBufferPoolTest.testAllocate(LongBufferPoolTest.java:353)
	at org.apache.cassandra.utils.memory.LongBufferPoolTest.testPoolAllocate(LongBufferPoolTest.java:159)
	at org.apache.cassandra.utils.memory.LongBufferPoolTest.testPoolAllocateWithRecyclePartially(LongBufferPoolTest.java:147)
{noformat}
 

The other test `testPoolAllocateWithoutRecyclePartially` works fine with only 6 patches.

So the 7-th patch fixes partial recycling. Not sure how serious this is - looks like some chunks would simply be not recycled, so that could cause a memory leak worst case? 




Yep. That's why I've actually delayed that a bit and only committed the test fix so far. Not placing the recycled chunk in the optimal queue is a minor performance issue, but failing to recycle altogether is more serious than that.

Committed the test fix as dc2acba043c6978b32a9556e0d610251d5535ce6 to 3.11 and merged up (to 4.0, 4.1, and trunk (4.2)).

Committed the {{BufferPool}} fixes and improvements -verbatim - as e13356d75d2d3c200f1636337cf15329bd1b829b to 4.0 and merged up (to 4.1 and trunk (4.2)).

Thank you [~pkolaczk] for the great work and the patience.

