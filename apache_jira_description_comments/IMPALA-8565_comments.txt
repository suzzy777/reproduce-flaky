I'm going to:
# try to repro locally on a release build
# look at adding logging to understand the state of the cluster better (what queries were present on the query page; values of metrics).

It looks like 15 queries were rejected (based on metrics and grepping the output).
{noformat}
MainThread: DeltaSum=30 Deltas={'dequeued': 0, 'admitted': 5, 'released': 0, 'rejected': 15, 'queued': 10, 'timed-out': 0} (Expected=30 for metrics=['admitted', 'queued', 'rejected'])
{noformat}

It looks like at least one query got admitted after the metric snapshot was taken. So I think probably what happened was that some queries got admitted. I see several queries getting admitted around the time that the assert failed, so the metrics were probably inconsistent with what we saw on the queries page.

{noformat}
impalad.impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com.jenkins.log.INFO.20190518-000920.27863:I0518 00:09:32.765684 29056 admission-controller.cc:730] 30443e5c9ae63dc9:239c966a00000000] Admitted queued query id=30443e5c9ae63dc9:239c966a00000000
impalad.impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com.jenkins.log.INFO.20190518-000920.27863:I0518 00:09:32.765748 29086 admission-controller.cc:730] e54417fb2515f75d:93a5ab5e00000000] Admitted queued query id=e54417fb2515f75d:93a5ab5e00000000
impalad.impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com.jenkins.log.INFO.20190518-000920.27863:I0518 00:09:32.765785 29106 admission-controller.cc:730] fa47c93b46542443:e9b997ef00000000] Admitted queued query id=fa47c93b46542443:e9b997ef00000000
impalad.impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com.jenkins.log.INFO.20190518-000920.27863.1:I0518 00:09:32.765684 29056 admission-controller.cc:730] 30443e5c9ae63dc9:239c966a00000000] Admitted queued query id=30443e5c9ae63dc9:239c966a00000000
impalad.impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com.jenkins.log.INFO.20190518-000920.27863.1:I0518 00:09:32.765748 29086 admission-controller.cc:730] e54417fb2515f75d:93a5ab5e00000000] Admitted queued query id=e54417fb2515f75d:93a5ab5e00000000
impalad.impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com.jenkins.log.INFO.20190518-000920.27863.1:I0518 00:09:32.765785 29106 admission-controller.cc:730] fa47c93b46542443:e9b997ef00000000] Admitted queued query id=fa47c93b46542443:e9b997ef00000000
{noformat}

Next step would be to understand why the queries were dequeued after being queued (the test is meant to be designed so that queries don't finish before the test expects them to).


It looks like an instance completing was what caused the dequeuing.
{noformat}
I0518 00:09:32.675745 28992 query-state.cc:649] 6941d938fd6f0baa:68de368c00000003] Instance completed. instance_id=6941d938fd6f0baa:68de368c00000003 #in-flight=2 status=OK
...
I0518 00:09:32.765430 28868 admission-controller.cc:895] Updated mem reserved for hosts:
impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22002: 2.40 GB -> 1.60 GB
I0518 00:09:32.765445 28795 admission-controller.cc:999] Dequeue thread will try to admit 3 requests, pool=default-pool, num_queued=3
I0518 00:09:32.765470 28795 admission-controller.cc:402] Checking agg mem in pool=default-pool : agg_num_running=5, agg_num_queued=10, agg_mem_reserved=9.60 GB,  local_host(local_mem_admitted=4.80 GB, num_admitted_running=2, num_queued=3, backend_mem_reserved=1.60 GB) cluster_mem_needed=2.40 GB pool_max_mem=12.00 GB
I0518 00:09:32.765480 28795 admission-controller.cc:420] Checking memory on host=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22000 mem_reserved=4.00 GB mem_admitted=1.60 GB needs=819.20 MB admit_mem_limit=12.00 GB
I0518 00:09:32.765488 28795 admission-controller.cc:420] Checking memory on host=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22001 mem_reserved=4.00 GB mem_admitted=1.60 GB needs=819.20 MB admit_mem_limit=12.00 GB
I0518 00:09:32.765496 28795 admission-controller.cc:420] Checking memory on host=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22002 mem_reserved=1.60 GB mem_admitted=1.60 GB needs=819.20 MB admit_mem_limit=12.00 GB
I0518 00:09:32.765499 28795 admission-controller.cc:1018] Dequeuing query=30443e5c9ae63dc9:239c966a00000000
{noformat}

The query is:
{noformat}
I0518 00:09:28.261564 28962 Frontend.java:1197] 6941d938fd6f0baa:68de368c00000000] Analyzing query:
select * from alltypesagg where id != 4
union all
select * from alltypesagg where id != 4
union all
select * from alltypesagg where id != 4
union all
select * from alltypesagg where id != 4
union all
select * from alltypesagg where id != 4
union all
select * from alltypesagg where id != 4
union all
select * from alltypesagg where id != 4
{noformat}

The query as a whole didn't take too long to run.
{noformat}
I0518 00:09:28.261394 28962 impala-server.cc:1107] 6941d938fd6f0baa:68de368c00000000] Registered query query_id=6941d938fd6f0baa:68de368c00000000 session_id=20458b387ab25aaa:ff723d2588b53ca9
I0518 00:09:28.261564 28962 Frontend.java:1197] 6941d938fd6f0baa:68de368c00000000] Analyzing query: 
I0518 00:09:28.278061 28962 Frontend.java:1238] 6941d938fd6f0baa:68de368c00000000] Analysis finished.
I0518 00:09:28.628314 28974 admission-controller.cc:617] 6941d938fd6f0baa:68de368c00000000] Schedule for id=6941d938fd6f0baa:68de368c00000000 in pool_name=default-pool per_host_mem_estimate=80.92 MB PoolConfig: max_requests=150 max_queued=10 max_mem=12.00 GB
I0518 00:09:28.628368 28974 admission-controller.cc:622] 6941d938fd6f0baa:68de368c00000000] Stats: agg_num_running=0, agg_num_queued=0, agg_mem_reserved=0,  local_host(local_mem_admitted=0, num_admitted_running=0, num_queued=0, backend_mem_reserved=0)
I0518 00:09:28.628387 28974 admission-controller.cc:402] 6941d938fd6f0baa:68de368c00000000] Checking agg mem in pool=default-pool : agg_num_running=0, agg_num_queued=0, agg_mem_reserved=0,  local_host(local_mem_admitted=0, num_admitted_running=0, num_queued=0, backend_mem_reserved=0) cluster_mem_needed=2.40 GB pool_max_mem=12.00 GB
I0518 00:09:28.628401 28974 admission-controller.cc:420] 6941d938fd6f0baa:68de368c00000000] Checking memory on host=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22000 mem_reserved=0 mem_admitted=0 needs=819.20 MB admit_mem_limit=12.00 GB
I0518 00:09:28.628410 28974 admission-controller.cc:420] 6941d938fd6f0baa:68de368c00000000] Checking memory on host=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22001 mem_reserved=0 mem_admitted=0 needs=819.20 MB admit_mem_limit=12.00 GB
I0518 00:09:28.628420 28974 admission-controller.cc:420] 6941d938fd6f0baa:68de368c00000000] Checking memory on host=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22002 mem_reserved=0 mem_admitted=0 needs=819.20 MB admit_mem_limit=12.00 GB
I0518 00:09:28.628425 28974 admission-controller.cc:654] 6941d938fd6f0baa:68de368c00000000] Admitted query id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:28.628432 28974 admission-controller.cc:1077] 6941d938fd6f0baa:68de368c00000000] For Query TUniqueId(hi=7584582086261935018, lo=7556537199697592320) per_backend_mem_limit set to: 819.20 MB per_backend_mem_to_admit set to: 819.20 MB
I0518 00:09:28.628443 28974 admission-controller.cc:357] 6941d938fd6f0baa:68de368c00000000] Update admitted mem reserved for host=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22000 prev=0 new=819.20 MB
I0518 00:09:28.628451 28974 admission-controller.cc:357] 6941d938fd6f0baa:68de368c00000000] Update admitted mem reserved for host=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22001 prev=0 new=819.20 MB
I0518 00:09:28.628458 28974 admission-controller.cc:357] 6941d938fd6f0baa:68de368c00000000] Update admitted mem reserved for host=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22002 prev=0 new=819.20 MB
I0518 00:09:28.628474 28974 admission-controller.cc:657] 6941d938fd6f0baa:68de368c00000000] Final: agg_num_running=1, agg_num_queued=0, agg_mem_reserved=0,  local_host(local_mem_admitted=2.40 GB, num_admitted_running=1, num_queued=0, backend_mem_reserved=0)
I0518 00:09:28.628486 28974 coordinator.cc:95] 6941d938fd6f0baa:68de368c00000000] Exec() query_id=6941d938fd6f0baa:68de368c00000000 stmt=
I0518 00:09:28.628788 28974 coordinator.cc:355] 6941d938fd6f0baa:68de368c00000000] starting execution on 3 backends for query_id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:28.632356 28978 impala-internal-service.cc:52] 6941d938fd6f0baa:68de368c00000000] ExecQueryFInstances(): query_id=6941d938fd6f0baa:68de368c00000000 coord=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22001 #instances=2
I0518 00:09:28.633471 28985 query-state.cc:641] 6941d938fd6f0baa:68de368c00000000] Executing instance. instance_id=6941d938fd6f0baa:68de368c00000000 fragment_idx=0 per_fragment_instance_idx=0 coord_state_idx=1 #in-flight=1
I0518 00:09:28.635102 28986 query-state.cc:641] 6941d938fd6f0baa:68de368c00000001] Executing instance. instance_id=6941d938fd6f0baa:68de368c00000001 fragment_idx=1 per_fragment_instance_idx=0 coord_state_idx=1 #in-flight=2
I0518 00:09:28.642663 28974 coordinator.cc:369] 6941d938fd6f0baa:68de368c00000000] started execution on 3 backends for query_id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:32.679335 28168 coordinator.cc:712] Backend completed: host=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22002 remaining=3 query_id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:32.679373 28168 coordinator-backend-state.cc:285] query_id=6941d938fd6f0baa:68de368c00000000: first in-progress backend: impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22000
I0518 00:09:34.951083 30101 impala-server.cc:1293] Cancel(): query_id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:34.951108 30101 coordinator.cc:482] ExecState: query id=6941d938fd6f0baa:68de368c00000000 execution cancelled
I0518 00:09:34.951118 30101 coordinator-backend-state.cc:449] Sending CancelQueryFInstances rpc for query_id=6941d938fd6f0baa:68de368c00000000 backend=127.0.0.1:27000
I0518 00:09:34.951344 30101 coordinator-backend-state.cc:449] Sending CancelQueryFInstances rpc for query_id=6941d938fd6f0baa:68de368c00000000 backend=127.0.0.1:27001
I0518 00:09:34.957010 28166 control-service.cc:170] CancelQueryFInstances(): query_id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:34.957022 28166 query-exec-mgr.cc:97] QueryState: query_id=6941d938fd6f0baa:68de368c00000000 refcnt=5
I0518 00:09:34.957027 28166 query-state.cc:666] Cancel: query_id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:34.957032 28166 krpc-data-stream-mgr.cc:326] cancelling all streams for fragment_instance_id=6941d938fd6f0baa:68de368c00000001
I0518 00:09:34.957041 28166 krpc-data-stream-mgr.cc:326] cancelling all streams for fragment_instance_id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:34.957617 30101 coordinator.cc:683] CancelBackends() query_id=6941d938fd6f0baa:68de368c00000000, tried to cancel 2 backends
I0518 00:09:34.957626 30101 coordinator.cc:879] Release admission control resources for query_id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:34.957695 30101 admission-controller.cc:745] Released query id=6941d938fd6f0baa:68de368c00000000 agg_num_running=4, agg_num_queued=7, agg_mem_reserved=12.80 GB,  local_host(local_mem_admitted=0, num_admitted_running=0, num_queued=3, backend_mem_reserved=7.20 GB)
I0518 00:09:34.963088 28985 krpc-data-stream-mgr.cc:295] 6941d938fd6f0baa:68de368c00000000] DeregisterRecvr(): fragment_instance_id=6941d938fd6f0baa:68de368c00000000, node=20
I0518 00:09:34.973379 28986 query-state.cc:649] 6941d938fd6f0baa:68de368c00000001] Instance completed. instance_id=6941d938fd6f0baa:68de368c00000001 #in-flight=9 status=CANCELLED: Cancelled
I0518 00:09:34.973953 28983 query-state.cc:285] 6941d938fd6f0baa:68de368c00000000] UpdateBackendExecState(): last report for 6941d938fd6f0baa:68de368c00000000
I0518 00:09:34.978821 28983 query-state.cc:446] 6941d938fd6f0baa:68de368c00000000] Cancelling fragment instances as directed by the coordinator. Returned status: Cancelled
I0518 00:09:34.978843 28983 query-state.cc:666] 6941d938fd6f0baa:68de368c00000000] Cancel: query_id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:35.013245 28985 query-state.cc:649] 6941d938fd6f0baa:68de368c00000000] Instance completed. instance_id=6941d938fd6f0baa:68de368c00000000 #in-flight=8 status=CANCELLED: Cancelled
I0518 00:09:35.136042 28962 impala-server.cc:1186] UnregisterQuery(): query_id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:35.136047 28962 impala-server.cc:1293] Cancel(): query_id=6941d938fd6f0baa:68de368c00000000
I0518 00:09:35.183235 28962 query-exec-mgr.cc:184] ReleaseQueryState(): deleted query_id=6941d938fd6f0baa:68de368c00000000
{noformat}

But one that one backend it finishes really quickly:
{noformat}
I0518 00:09:28.634873 28987 impala-internal-service.cc:52] 6941d938fd6f0baa:68de368c00000000] ExecQueryFInstances(): query_id=6941d938fd6f0baa:68de368c00000000 coord=impala-ec2-centos74-m5-4xlarge-ondemand-04c0.vpc.cloudera.com:22001 #instances=1
I0518 00:09:28.642482 28992 query-state.cc:641] 6941d938fd6f0baa:68de368c00000003] Executing instance. instance_id=6941d938fd6f0baa:68de368c00000003 fragment_idx=1 per_fragment_instance_idx=2 coord_state_idx=2 #in-flight=1
I0518 00:09:32.675745 28992 query-state.cc:649] 6941d938fd6f0baa:68de368c00000003] Instance completed. instance_id=6941d938fd6f0baa:68de368c00000003 #in-flight=2 status=OK
I0518 00:09:32.675755 28990 query-state.cc:285] 6941d938fd6f0baa:68de368c00000000] UpdateBackendExecState(): last report for 6941d938fd6f0baa:68de368c00000000
I0518 00:09:32.688680 28990 query-exec-mgr.cc:184] 6941d938fd6f0baa:68de368c00000000] ReleaseQueryState(): deleted query_id=6941d938fd6f0baa:68de368c00000000
{noformat}

It looks like the query is pretty badly designed to reliably run slowly. Query execution is super-fast, I think we're relying on the client fetching slowing down the whole thing and applying backpressure. Maybe the one fragment finishes fast because it races the other fragments and fills up the exchange buffer on the coordinator. Or something. Anyway we should redesign the query
{noformat}
+---------------------+--------+----------+----------+---------+------------+----------+---------------+------------------------+
| Operator            | #Hosts | Avg Time | Max Time | #Rows   | Est. #Rows | Peak Mem | Est. Peak Mem | Detail                 |
+---------------------+--------+----------+----------+---------+------------+----------+---------------+------------------------+
| F20:ROOT            | 1      | 952.30ms | 952.30ms |         |            | 0 B      | 0 B           |                        |
| 20:EXCHANGE         | 1      | 4.05ms   | 4.05ms   | 208.98K | 20.90K     | 13.69 MB | 942.25 KB     | UNPARTITIONED          |
| F19:EXCHANGE SENDER | 3      | 19.74ms  | 21.93ms  |         |            | 1.48 KB  | 0 B           |                        |
| 00:UNION            | 3      | 264.38us | 536.74us | 208.98K | 20.90K     | 0 B      | 0 B           |                        |
| |--02:SCAN HDFS     | 3      | 2.61ms   | 3.93ms   | 11.00K  | 1.10K      | 1.42 MB  | 80.00 MB      | functional.alltypesagg |
| |--03:SCAN HDFS     | 3      | 2.69ms   | 3.67ms   | 11.00K  | 1.10K      | 1.25 MB  | 80.00 MB      | functional.alltypesagg |
| |--04:SCAN HDFS     | 3      | 2.24ms   | 3.49ms   | 11.00K  | 1.10K      | 1.25 MB  | 80.00 MB      | functional.alltypesagg |
| |--05:SCAN HDFS     | 3      | 2.83ms   | 3.45ms   | 11.00K  | 1.10K      | 1.15 MB  | 80.00 MB      | functional.alltypesagg |
| |--06:SCAN HDFS     | 3      | 3.19ms   | 5.03ms   | 11.00K  | 1.10K      | 1.25 MB  | 80.00 MB      | functional.alltypesagg |
| |--07:SCAN HDFS     | 3      | 2.39ms   | 2.69ms   | 11.00K  | 1.10K      | 1.59 MB  | 80.00 MB      | functional.alltypesagg |
| |--08:SCAN HDFS     | 3      | 2.29ms   | 3.17ms   | 11.00K  | 1.10K      | 1.42 MB  | 80.00 MB      | functional.alltypesagg |
| |--09:SCAN HDFS     | 3      | 3.26ms   | 6.18ms   | 11.00K  | 1.10K      | 1.30 MB  | 80.00 MB      | functional.alltypesagg |
| |--10:SCAN HDFS     | 3      | 1.96ms   | 2.47ms   | 11.00K  | 1.10K      | 1.25 MB  | 80.00 MB      | functional.alltypesagg |
| |--11:SCAN HDFS     | 3      | 2.59ms   | 4.31ms   | 11.00K  | 1.10K      | 1.59 MB  | 80.00 MB      | functional.alltypesagg |
| |--12:SCAN HDFS     | 3      | 1.84ms   | 2.01ms   | 11.00K  | 1.10K      | 1.59 MB  | 80.00 MB      | functional.alltypesagg |
| |--13:SCAN HDFS     | 3      | 1.73ms   | 2.12ms   | 11.00K  | 1.10K      | 1.61 MB  | 80.00 MB      | functional.alltypesagg |
| |--14:SCAN HDFS     | 3      | 1.75ms   | 1.86ms   | 11.00K  | 1.10K      | 1.42 MB  | 80.00 MB      | functional.alltypesagg |
| |--15:SCAN HDFS     | 3      | 1.78ms   | 1.94ms   | 11.00K  | 1.10K      | 1.42 MB  | 80.00 MB      | functional.alltypesagg |
| |--16:SCAN HDFS     | 3      | 1.64ms   | 1.75ms   | 11.00K  | 1.10K      | 1.59 MB  | 80.00 MB      | functional.alltypesagg |
| |--17:SCAN HDFS     | 3      | 1.44ms   | 1.50ms   | 11.00K  | 1.10K      | 1.59 MB  | 80.00 MB      | functional.alltypesagg |
| |--18:SCAN HDFS     | 3      | 1.63ms   | 1.77ms   | 11.00K  | 1.10K      | 1.59 MB  | 80.00 MB      | functional.alltypesagg |
| |--19:SCAN HDFS     | 3      | 1.53ms   | 1.62ms   | 11.00K  | 1.10K      | 1.59 MB  | 80.00 MB      | functional.alltypesagg |
| 01:SCAN HDFS        | 3      | 2.98ms   | 3.19ms   | 11.00K  | 1.10K      | 1.25 MB  | 80.00 MB      | functional.alltypesagg |
+---------------------+--------+----------+----------+---------+------------+----------+---------------+------------------------+

{noformat}

Commit 84dfbb488089db8c5cb1b169c0c0aa6758b9656a in impala's branch refs/heads/master from Tim Armstrong
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=84dfbb4 ]

IMPALA-8565: larger query in test_admission_controller

It appears that in some rare cases, fragments from the
query used can finish too early, before the client has
started fetching to hit EOS.

Reduce the chances of this happening by increasing
the query size and reducing the fetch frequency.

Testing:
I could not reproduce this rare flake locally, probably
because my hardware is different from the hardware that
the failures occurred on.

I looped the failing test for a while locally to try and
ensure that the flakiness did not at least get worse.

Change-Id: Ifd7716d5d5ccc4b2f79b3acf2fbc880716b4c0fe
Reviewed-on: http://gerrit.cloudera.org:8080/13718
Reviewed-by: Bikramjeet Vig <bikramjeet.vig@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Hopefully the above change is enough to deflake


