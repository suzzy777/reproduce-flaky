For info, this is the reason for the failure of at least some upgrade dtests (typically [this|http://cassci.datastax.com/job/cassandra-2.2_dtest/lastCompletedBuild/testReport/upgrade_tests.paging_test/TestPagingData/test_paging_a_single_wide_row/]).  Basically, the test are issuing a truncate as their first order of business, and the 2.1 closes the connection to the other node due to this, some of the truncation acknowledgment get losts because it's in the queue of that connection, hence ending up in a truncate timeout.

And of course there is the fact that due to this the 2.1 node logs a warning with a stack trace, which might worry operators a bit even though nothing is wrong.


Pushed a branch for this [here|https://github.com/pcmanus/cassandra/commits/9761]. I've resurrected (quite blindly) the "areAllNodesOn21" test we have in 2.1 (but it now tests for 2.2) and the role manager uses that to decide if it should set itself up or not. If not, it re-schedule a try a bit later (it reuses {{cassandra.superuser_setup_delay_ms}} for that which imo is fine. The default is 10s but the {{areAllNodesOn22}} check is pretty cheap. Open to alternatives though if someone feels it's not adequate). This seems to work fine but I'm not all that familiar with the role manager so hopefully I haven't missed anything.

The results for the dtests are [here|http://cassci.datastax.com/job/pcmanus-9761-dtest/1/]. Got 2 weird and apparently unrelated failures on the unit tests however so rebased the branch to the last changes and re-running those, but the patch is ready for review otherwise.


I kind of like the idea of rejecting any role operations until setup is completed, as opposed to just advising against it as we did previously. Unfortunately, this breaks the migration path as the conversion from users to roles uses the public {{createRole}} method and so also the new {{process}}. Perhaps we could just change the semantic slightly from {{isSetup}} to {{isClusterReady}} and set that to true as soon as the {{areAllNodesAtLeast22}} check returns true, before the setup task is executed.

This isn't important, just a potential improvement, but seeing as we're now rescheduling the setup task when the cluster is not ready, perhaps we could do the same on other failuire scenarios. e.g. if {{convertLegacyData}} or {{setupDefaultRole}} runs but fails, only a restart will trigger a re-attempt. Of course, the primary reason for either of those to fail previously was being run on a mixed cluster, so it's far less likely to happen. Still, it seems like some low-hanging fruit.

I've pushed a couple of commits for the above [here|https://github.com/beobal/cassandra/tree/9761].

Sorry for missing the {{createRole}} call. Both additional commits lgtm. Once CI looks good on your branch (do you have links to that?) and if you have no further comments, I'll commit that.

Great, thanks. 
Regarding CI, I only pushed a 2.2 version but I'm pretty sure separate 3.0 & trunk branches aren't really necessary for this.
* [testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-testall/]
* [dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-dtest/]

After my conscience (aka [~thobbs]) got the better of me, I've pushed 3.0 and trunk versions for CI. 

Results will be at:
* [3.0 testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-3.0-testall/]
* [3.0 dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-3.0-dtest/]
* [trunk testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-trunk-testall/]
* [trunk dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-trunk-dtest/]

[~slebresne], I'm not sure how I didn't notice in the first place, but I realised that your original branch was based on 3.0, not 2.2. Was it intentional to skip 2.2 with this fix, or just an oversight?
Seeing as dtests on 3.0/trunk are holding this up anyway,  I've backported to 2.2 and updated my branches for CI.

Patches:
* [2.2 branch|https://github.com/beobal/cassandra/tree/9761-2.2]
* [3.0 branch|https://github.com/beobal/cassandra/tree/9761-3.0]
* [trunk branch|https://github.com/beobal/cassandra/tree/9761-trunk]

CI Tests:
* [2.2 testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-2.2-testall/]
* [2.2 dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-2.2-dtest/]
* [3.0 testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-3.0-testall/]
* [3.0 dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-3.0-dtest/]
* [trunk testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-trunk-testall/]
* [trunk dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9761-trunk-dtest/]


bq.  Was it intentional to skip 2.2 with this fix, or just an oversight?

It's totally an oversight, I myself though I had done it on 2.2.

Seems the CI tests are still ongoing, but once it's done and if there is new failures, +1.

There are some *differences* in the CI tests on these branches vs 3.0/trunk, particularly dtests. I don't see anything which indicates anything other than flaky tests though, there are several fixtures with multiple failures, just different subsets in each branch. So I've committed to 2.2 in {{b22ad4210cbf9c0c0bd6e595265b162f391da3a1}} and merged to 3.0 & trunk.


