Tim, I know you've seen this since you already have a patch :)

See:
http://sandbox.jenkins.cloudera.com/view/Impala/view/Nightly-Builds/job/impala-master-cdh5-exhaustive/434/

{code}
11:40:58 custom_cluster/test_admission_controller.py ....
11:44:21 custom_cluster/test_alloc_fail.py ..
11:45:40 custom_cluster/test_delegation.py ...
11:46:20 custom_cluster/test_hdfs_fd_caching.py .
11:46:49 custom_cluster/test_hive_parquet_timestamp_conversion.py ..
11:47:33 custom_cluster/test_insert_behaviour.py ..
11:48:29 custom_cluster/test_legacy_joins_aggs.py .
11:48:45 custom_cluster/test_parquet_max_page_header.py .
11:49:35 custom_cluster/test_permanent_udfs.py .
11:50:55 custom_cluster/test_query_expiration.py ...
11:51:49 custom_cluster/test_redaction.py ....
11:53:53 custom_cluster/test_scratch_disk.py ....
11:55:56 custom_cluster/test_session_expiration.py .
11:56:17 custom_cluster/test_spilling.py ...F
11:57:29 authorization/test_authorization.py ..
11:58:08 authorization/test_grant_revoke.py .
11:59:37 ../../Impala-auxiliary-tests/tests/aux_custom_cluster_tests/test_ldap.py xxxxx
11:59:38 
11:59:38 =================================== FAILURES ===================================
11:59:38  TestSpilling.test_spilling[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: parquet/none] 
11:59:38 
11:59:38 self = <test_spilling.TestSpilling object at 0x3c00350>
11:59:38 vector = <tests.common.test_vector.TestVector object at 0x291fe10>
11:59:38 
11:59:38     @pytest.mark.execute_serially
11:59:38     @CustomClusterTestSuite.with_args(
11:59:38         impalad_args="--read_size=200000",
11:59:38         catalogd_args="--load_catalog_in_background=false")
11:59:38     def test_spilling(self, vector):
11:59:38       new_vector = deepcopy(vector)
11:59:38       # remove this. the test cases set this explicitly.
11:59:38       del new_vector.get_value('exec_option')['num_nodes']
11:59:38 >     self.run_test_case('QueryTest/spilling', new_vector)
11:59:38 
11:59:38 custom_cluster/test_spilling.py:92: 
11:59:38 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
11:59:38 common/impala_test_suite.py:256: in run_test_case
11:59:38     result = self.__execute_query(target_impalad_client, query, user=user)
11:59:38 common/impala_test_suite.py:451: in __execute_query
11:59:38     return impalad_client.execute(query, user=user)
11:59:38 common/impala_connection.py:161: in execute
11:59:38     return self.__beeswax_client.execute(sql_stmt, user=user)
11:59:38 beeswax/impala_beeswax.py:163: in execute
11:59:38     handle = self.__execute_query(query_string.strip(), user=user)
11:59:38 beeswax/impala_beeswax.py:329: in __execute_query
11:59:38     self.wait_for_completion(handle)
11:59:38 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
11:59:38 
11:59:38 self = <tests.beeswax.impala_beeswax.ImpalaBeeswaxClient object at 0x3c003d0>
11:59:38 query_handle = QueryHandle(log_context='6a438a5486d15ebf:bf36040116f68b95', id='6a438a5486d15ebf:bf36040116f68b95')
11:59:38 
11:59:38     def wait_for_completion(self, query_handle):
11:59:38       """Given a query handle, polls the coordinator waiting for the query to complete"""
11:59:38       while True:
11:59:38         query_state = self.get_state(query_handle)
11:59:38         # if the rpc succeeded, the output is the query state
11:59:38         if query_state == self.query_states["FINISHED"]:
11:59:38           break
11:59:38         elif query_state == self.query_states["EXCEPTION"]:
11:59:38           try:
11:59:38             error_log = self.__do_rpc(
11:59:38               lambda: self.imp_service.get_log(query_handle.log_context))
11:59:38 >           raise ImpalaBeeswaxException("Query aborted:" + error_log, None)
11:59:38 E           ImpalaBeeswaxException: ImpalaBeeswaxException:
11:59:38 E            Query aborted:
11:59:38 E           Memory limit exceeded
11:59:38 E           The memory limit is set too low to initialize spilling operator (id=4). The minimum required memory to spill this operator is 6.48 MB.
11:59:38 E           
11:59:38 E           
11:59:38 E           
11:59:38 E           Memory Limit Exceeded
11:59:38 E           Query(6a438a5486d15ebf:bf36040116f68b95) Limit: Consumption=132.76 MB
11:59:38 E             Fragment 6a438a5486d15ebf:bf36040116f68b9a: Consumption=104.42 MB
11:59:38 E               AGGREGATION_NODE (id=1): Consumption=104.32 MB
11:59:38 E                 Exprs: Consumption=56.07 MB
11:59:38 E               HDFS_SCAN_NODE (id=0): Consumption=0
11:59:38 E               DataStreamSender: Consumption=90.00 KB
11:59:38 E             Block Manager: Limit=60.00 MB Consumption=57.87 MB
11:59:38 E             Fragment 6a438a5486d15ebf:bf36040116f68b97: Consumption=23.79 MB
11:59:38 E               SORT_NODE (id=2): Consumption=4.00 KB
11:59:38 E               AGGREGATION_NODE (id=4): Consumption=7.41 MB
11:59:38 E                 Exprs: Consumption=508.00 KB
11:59:38 E               EXCHANGE_NODE (id=3): Consumption=0
11:59:38 E               DataStreamRecvr: Consumption=16.36 MB
11:59:38 E               DataStreamSender: Consumption=2.28 KB
11:59:38 
11:59:38 beeswax/impala_beeswax.py:349: ImpalaBeeswaxException
11:59:38 ---------------------------- Captured stdout setup -----------------------------
11:59:38 Starting State Store logging to /data/jenkins/workspace/impala-master-cdh5-exhaustive/repos/Impala/cluster_logs/custom_cluster//statestored.INFO
11:59:38 Starting Catalog Service logging to /data/jenkins/workspace/impala-master-cdh5-exhaustive/repos/Impala/cluster_logs/custom_cluster//catalogd.INFO
11:59:38 Starting Impala Daemon logging to /data/jenkins/workspace/impala-master-cdh5-exhaustive/repos/Impala/cluster_logs/custom_cluster//impalad.INFO
11:59:38 Starting Impala Daemon logging to /data/jenkins/workspace/impala-master-cdh5-exhaustive/repos/Impala/cluster_logs/custom_cluster//impalad_node1.INFO
11:59:38 Starting Impala Daemon logging to /data/jenkins/workspace/impala-master-cdh5-exhaustive/repos/Impala/cluster_logs/custom_cluster//impalad_node2.INFO
11:59:38 Waiting for Catalog... Status: 50 DBs / 1067 tables (ready=True)
11:59:38 Waiting for Catalog... Status: 50 DBs / 1067 tables (ready=True)
11:59:38 Waiting for Catalog... Status: 50 DBs / 1067 tables (ready=True)
11:59:38 Impala Cluster Running with 3 nodes.
11:59:38 ---------------------------- Captured stderr setup -----------------------------
11:59:38 MainThread: Found 3 impalad/1 statestored/1 catalogd process(es)
11:59:38 MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-06ae.vpc.cloudera.com:25000
11:59:38 MainThread: Waiting for num_known_live_backends=3. Current value: 0
11:59:38 MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-06ae.vpc.cloudera.com:25000
11:59:38 MainThread: num_known_live_backends has reached value: 3
11:59:38 MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-06ae.vpc.cloudera.com:25001
11:59:38 MainThread: Waiting for num_known_live_backends=3. Current value: 1
11:59:38 MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-06ae.vpc.cloudera.com:25001
11:59:38 MainThread: num_known_live_backends has reached value: 3
11:59:38 MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-06ae.vpc.cloudera.com:25002
11:59:38 MainThread: num_known_live_backends has reached value: 3
11:59:38 MainThread: Found 3 impalad/1 statestored/1 catalogd process(es)
11:59:38 MainThread: Getting metric: statestore.live-backends from impala-boost-static-burst-slave-06ae.vpc.cloudera.com:25010
11:59:38 MainThread: Metric 'statestore.live-backends' has reach desired value: 4
11:59:38 MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-06ae.vpc.cloudera.com:25000
11:59:38 MainThread: num_known_live_backends has reached value: 3
11:59:38 MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-06ae.vpc.cloudera.com:25001
11:59:38 MainThread: num_known_live_backends has reached value: 3
11:59:38 MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-06ae.vpc.cloudera.com:25002
11:59:38 MainThread: num_known_live_backends has reached value: 3
11:59:38 -- connecting to: localhost:21000
11:59:38 ----------------------------- Captured stderr call -----------------------------
11:59:38 -- executing against localhost:21000
11:59:38 use tpch_parquet;
11:59:38 
11:59:38 SET disable_codegen=False;
11:59:38 SET abort_on_error=1;
11:59:38 SET exec_single_node_rows_threshold=0;
11:59:38 SET batch_size=0;
11:59:38 -- executing against localhost:21000
11:59:38 set num_nodes=1;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 
11:59:38 set max_block_mgr_memory=265m;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 
11:59:38 select l_orderkey, count(*)
11:59:38 from lineitem
11:59:38 group by 1
11:59:38 order by 1 limit 10;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 SET NUM_NODES=0;;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 set num_nodes=1;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 
11:59:38 set max_block_mgr_memory=275m;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 
11:59:38 select l_returnflag, l_orderkey, avg(l_tax), min(l_shipmode)
11:59:38 from lineitem
11:59:38 group by 1,2
11:59:38 order by 1,2 limit 3;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 SET NUM_NODES=0;;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 set max_block_mgr_memory=275m;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 
11:59:38 select l_orderkey, count(*)
11:59:38 from lineitem
11:59:38 group by 1
11:59:38 order by 1 limit 10;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 SET MAX_BLOCK_MGR_MEMORY=0;;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 set num_nodes=0;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 
11:59:38 set max_block_mgr_memory=275m;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 
11:59:38 select l_comment, count(*)
11:59:38 from lineitem
11:59:38 group by 1
11:59:38 order by count(*) desc limit 5;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 SET NUM_NODES=0;;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 set num_nodes=0;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 
11:59:38 set max_block_mgr_memory=60m;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 
11:59:38 select l_returnflag, l_orderkey, round(avg(l_tax),2), min(l_shipmode)
11:59:38 from lineitem
11:59:38 group by 1,2
11:59:38 order by 1,2 limit 3;
11:59:38 
11:59:38 -- executing against localhost:21000
11:59:38 SET NUM_NODES=0;;
11:59:38 
{code}