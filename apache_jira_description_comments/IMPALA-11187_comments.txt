I have reviewed this flaky:
{code:python}
"""Other Kudu tests are run with a scan level of READ_AT_SNAPSHOT to have predicable scan results. This test verifies that scans work as expected at the scan level of READ_LATEST by retrying the scan if the results are incorrect."""
{code}
I have tried to reproduce the issue locally with larger inserts, but the tests succeeded. To understand why didn't the result matched the expected values even after 10s we should review the Kudu logs. In case the issue would reoccur, please extract and attach the Kudu logs for analysis.

Commit 1358700740dbeff799f6a6a95f95b1d9fe7281d2 in impala's branch refs/heads/master from Tamas Mate
[ https://gitbox.apache.org/repos/asf?p=impala.git;h=135870074 ]

IMPALA-10733: Fix flaky TestKuduOperations.test_replica_selection

This issue is related to IMPALA-11187, 'test_read_mode' can have
unpredicted results by its nature, it sets the 'kudu_read_mode' to
'READ_LATEST'. The 'test_replica_selection' runs just after
'test_read_mode' and the Kudu tests use the same Impala client, thus
'kudu_read_mode=READ_LATEST' persists.

This commit set 'kudu_read_mode=READ_AT_SNAPSHOT' for
'test_replica_selection'.

Testing:
 - Ran 'test_replica_selection'

Change-Id: Id98edf6af8ac67bab65d1c09d3e3b6f9343da854
Reviewed-on: http://gerrit.cloudera.org:8080/18418
Reviewed-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>
Tested-by: Impala Public Jenkins <impala-public-jenkins@cloudera.com>


Saw this in a build on branch-4.1.1: [https://jenkins.impala.io/job/ubuntu-16.04-dockerised-tests/6196]
{code:java}
query_test/test_kudu.py:551: in test_read_modes
    self._retry_query(cursor, "select count(*) from %s" % table_name, [(103,)])
query_test/test_kudu.py:535: in _retry_query
    assert retries < 3, \
E   AssertionError: Did not get a correct result for select count(*) from test_read_modes_53f93f33.test_read_latest after 3 retries: [(93,)]
E   assert 3 < 3{code}

Just ran into this in https://jenkins.impala.io/job/ubuntu-16.04-from-scratch/18289/.

I looked through Kudu and Impala logs. I don't see anything obvious, although in the middle of this query Kudu tablet servers are doing
{code}
I1207 02:40:47.913925 17518 tablet_service.cc:1512] Processing DeleteTablet for tablet a010773aad414657975ec8fcd20b84fc with delete_type TABLET_DATA_DELETED (Table deleted at 2022-12-07 02:40:47 UTC) from {username='ubuntu'} at 127.0.0.1:48856
I1207 02:40:47.913942 17502 tablet_service.cc:1512] Processing DeleteTablet for tablet 63d011a0c5a340f2909efd885c06b0ed with delete_type TABLET_DATA_DELETED (Table deleted at 2022-12-07 02:40:47 UTC) from {username='ubuntu'} at 127.0.0.1:48856
I1207 02:40:47.913933 17515 tablet_service.cc:1512] Processing DeleteTablet for tablet 387108e0cf094873b8d61319178fcfe1 with delete_type TABLET_DATA_DELETED (Table deleted at 2022-12-07 02:40:47 UTC) from {username='ubuntu'} at 127.0.0.1:48856
I1207 02:40:47.914009 17502 tablet_service.cc:1512] Processing DeleteTablet for tablet 377b32f8e9c542338249be052aae9d20 with delete_type TABLET_DATA_DELETED (Table deleted at 2022-12-07 02:40:47 UTC) from {username='ubuntu'} at 127.0.0.1:48856
I1207 02:40:47.914029 17515 tablet_service.cc:1512] Processing DeleteTablet for tablet 3dac2fd02f6045b3aa01d06e00d7fdf2 with delete_type TABLET_DATA_DELETED (Table deleted at 2022-12-07 02:40:47 UTC) from {username='ubuntu'} at 127.0.0.1:48856
I1207 02:40:47.914047 17502 tablet_service.cc:1512] Processing DeleteTablet for tablet fa987644f7554383a1a9b6984e54cfa2 with delete_type TABLET_DATA_DELETED (Table deleted at 2022-12-07 02:40:47 UTC) from {username='ubuntu'} at 127.0.0.1:48856
I1207 02:40:47.914064 17518 tablet_service.cc:1512] Processing DeleteTablet for tablet b4318238b0e94b718881e5c6d0afd0e5 with delete_type TABLET_DATA_DELETED (Table deleted at 2022-12-07 02:40:47 UTC) from {username='ubuntu'} at 127.0.0.1:48856
I1207 02:40:47.914077 17515 tablet_service.cc:1512] Processing DeleteTablet for tablet daf8246510f14af1bab937551059d615 with delete_type TABLET_DATA_DELETED (Table deleted at 2022-12-07 02:40:47 UTC) from {username='ubuntu'} at 127.0.0.1:48856
I1207 02:40:47.915189  8717 tablet_replica.cc:330] T 63d011a0c5a340f2909efd885c06b0ed P 8292ec3a6791482b847b5728af5c3012: stopping tablet replica
I1207 02:40:47.915246  8717 raft_consensus.cc:2229] T 63d011a0c5a340f2909efd885c06b0ed P 8292ec3a6791482b847b5728af5c3012 [term 1 LEADER]: Raft consensus shutting down.
I1207 02:40:47.915292  8717 raft_consensus.cc:2258] T 63d011a0c5a340f2909efd885c06b0ed P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus is shut down!
I1207 02:40:47.915372  8717 ts_tablet_manager.cc:1824] T 63d011a0c5a340f2909efd885c06b0ed P 8292ec3a6791482b847b5728af5c3012: Deleting tablet data with delete state TABLET_DATA_DELETED
I1207 02:40:47.916625  8717 ts_tablet_manager.cc:1837] T 63d011a0c5a340f2909efd885c06b0ed P 8292ec3a6791482b847b5728af5c3012: tablet deleted with delete type TABLET_DATA_DELETED: last-logged OpId 1.2
I1207 02:40:47.916648  8717 log.cc:1192] T 63d011a0c5a340f2909efd885c06b0ed P 8292ec3a6791482b847b5728af5c3012: Deleting WAL directory at /home/ubuntu/Impala/testdata/cluster/cdh7/node-3/var/lib/kudu/ts/wal/wals/63d011a0c5a340f2909efd885c06b0ed
I1207 02:40:47.916810  8717 ts_tablet_manager.cc:1858] T 63d011a0c5a340f2909efd885c06b0ed P 8292ec3a6791482b847b5728af5c3012: Deleting consensus metadata
I1207 02:40:47.916927  8717 tablet_replica.cc:330] T a010773aad414657975ec8fcd20b84fc P 8292ec3a6791482b847b5728af5c3012: stopping tablet replica
I1207 02:40:47.916961  8717 raft_consensus.cc:2229] T a010773aad414657975ec8fcd20b84fc P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus shutting down.
I1207 02:40:47.916975  8717 raft_consensus.cc:2258] T a010773aad414657975ec8fcd20b84fc P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus is shut down!
I1207 02:40:47.917044  8717 ts_tablet_manager.cc:1824] T a010773aad414657975ec8fcd20b84fc P 8292ec3a6791482b847b5728af5c3012: Deleting tablet data with delete state TABLET_DATA_DELETED
I1207 02:40:47.917963  8717 ts_tablet_manager.cc:1837] T a010773aad414657975ec8fcd20b84fc P 8292ec3a6791482b847b5728af5c3012: tablet deleted with delete type TABLET_DATA_DELETED: last-logged OpId 1.3
I1207 02:40:47.917980  8717 log.cc:1192] T a010773aad414657975ec8fcd20b84fc P 8292ec3a6791482b847b5728af5c3012: Deleting WAL directory at /home/ubuntu/Impala/testdata/cluster/cdh7/node-3/var/lib/kudu/ts/wal/wals/a010773aad414657975ec8fcd20b84fc
I1207 02:40:47.918067  8717 ts_tablet_manager.cc:1858] T a010773aad414657975ec8fcd20b84fc P 8292ec3a6791482b847b5728af5c3012: Deleting consensus metadata
I1207 02:40:47.918195  8717 tablet_replica.cc:330] T 387108e0cf094873b8d61319178fcfe1 P 8292ec3a6791482b847b5728af5c3012: stopping tablet replica
I1207 02:40:47.918227  8717 raft_consensus.cc:2229] T 387108e0cf094873b8d61319178fcfe1 P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus shutting down.
I1207 02:40:47.918239  8717 raft_consensus.cc:2258] T 387108e0cf094873b8d61319178fcfe1 P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus is shut down!
I1207 02:40:47.918305  8717 ts_tablet_manager.cc:1824] T 387108e0cf094873b8d61319178fcfe1 P 8292ec3a6791482b847b5728af5c3012: Deleting tablet data with delete state TABLET_DATA_DELETED
I1207 02:40:47.919057  8717 ts_tablet_manager.cc:1837] T 387108e0cf094873b8d61319178fcfe1 P 8292ec3a6791482b847b5728af5c3012: tablet deleted with delete type TABLET_DATA_DELETED: last-logged OpId 1.2
I1207 02:40:47.919075  8717 log.cc:1192] T 387108e0cf094873b8d61319178fcfe1 P 8292ec3a6791482b847b5728af5c3012: Deleting WAL directory at /home/ubuntu/Impala/testdata/cluster/cdh7/node-3/var/lib/kudu/ts/wal/wals/387108e0cf094873b8d61319178fcfe1
I1207 02:40:47.919144  8717 ts_tablet_manager.cc:1858] T 387108e0cf094873b8d61319178fcfe1 P 8292ec3a6791482b847b5728af5c3012: Deleting consensus metadata
I1207 02:40:47.919350  8717 tablet_replica.cc:330] T 377b32f8e9c542338249be052aae9d20 P 8292ec3a6791482b847b5728af5c3012: stopping tablet replica
I1207 02:40:47.919378  8717 raft_consensus.cc:2229] T 377b32f8e9c542338249be052aae9d20 P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus shutting down.
I1207 02:40:47.919389  8717 raft_consensus.cc:2258] T 377b32f8e9c542338249be052aae9d20 P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus is shut down!
I1207 02:40:47.919442  8717 ts_tablet_manager.cc:1824] T 377b32f8e9c542338249be052aae9d20 P 8292ec3a6791482b847b5728af5c3012: Deleting tablet data with delete state TABLET_DATA_DELETED
I1207 02:40:47.920148  8717 ts_tablet_manager.cc:1837] T 377b32f8e9c542338249be052aae9d20 P 8292ec3a6791482b847b5728af5c3012: tablet deleted with delete type TABLET_DATA_DELETED: last-logged OpId 1.2
I1207 02:40:47.920166  8717 log.cc:1192] T 377b32f8e9c542338249be052aae9d20 P 8292ec3a6791482b847b5728af5c3012: Deleting WAL directory at /home/ubuntu/Impala/testdata/cluster/cdh7/node-3/var/lib/kudu/ts/wal/wals/377b32f8e9c542338249be052aae9d20
I1207 02:40:47.920233  8717 ts_tablet_manager.cc:1858] T 377b32f8e9c542338249be052aae9d20 P 8292ec3a6791482b847b5728af5c3012: Deleting consensus metadata
I1207 02:40:47.920531  8717 tablet_replica.cc:330] T 3dac2fd02f6045b3aa01d06e00d7fdf2 P 8292ec3a6791482b847b5728af5c3012: stopping tablet replica
I1207 02:40:47.920554  8717 raft_consensus.cc:2229] T 3dac2fd02f6045b3aa01d06e00d7fdf2 P 8292ec3a6791482b847b5728af5c3012 [term 1 LEADER]: Raft consensus shutting down.
I1207 02:40:47.920579  8717 raft_consensus.cc:2258] T 3dac2fd02f6045b3aa01d06e00d7fdf2 P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus is shut down!
I1207 02:40:47.920653  8717 ts_tablet_manager.cc:1824] T 3dac2fd02f6045b3aa01d06e00d7fdf2 P 8292ec3a6791482b847b5728af5c3012: Deleting tablet data with delete state TABLET_DATA_DELETED
I1207 02:40:47.921308  8717 ts_tablet_manager.cc:1837] T 3dac2fd02f6045b3aa01d06e00d7fdf2 P 8292ec3a6791482b847b5728af5c3012: tablet deleted with delete type TABLET_DATA_DELETED: last-logged OpId 1.2
I1207 02:40:47.921324  8717 log.cc:1192] T 3dac2fd02f6045b3aa01d06e00d7fdf2 P 8292ec3a6791482b847b5728af5c3012: Deleting WAL directory at /home/ubuntu/Impala/testdata/cluster/cdh7/node-3/var/lib/kudu/ts/wal/wals/3dac2fd02f6045b3aa01d06e00d7fdf2
I1207 02:40:47.921389  8717 ts_tablet_manager.cc:1858] T 3dac2fd02f6045b3aa01d06e00d7fdf2 P 8292ec3a6791482b847b5728af5c3012: Deleting consensus metadata
I1207 02:40:47.921643  8717 tablet_replica.cc:330] T fa987644f7554383a1a9b6984e54cfa2 P 8292ec3a6791482b847b5728af5c3012: stopping tablet replica
I1207 02:40:47.921677  8717 raft_consensus.cc:2229] T fa987644f7554383a1a9b6984e54cfa2 P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus shutting down.
I1207 02:40:47.921687  8717 raft_consensus.cc:2258] T fa987644f7554383a1a9b6984e54cfa2 P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus is shut down!
I1207 02:40:47.921737  8717 ts_tablet_manager.cc:1824] T fa987644f7554383a1a9b6984e54cfa2 P 8292ec3a6791482b847b5728af5c3012: Deleting tablet data with delete state TABLET_DATA_DELETED
I1207 02:40:47.922452  8717 ts_tablet_manager.cc:1837] T fa987644f7554383a1a9b6984e54cfa2 P 8292ec3a6791482b847b5728af5c3012: tablet deleted with delete type TABLET_DATA_DELETED: last-logged OpId 1.3
I1207 02:40:47.922468  8717 log.cc:1192] T fa987644f7554383a1a9b6984e54cfa2 P 8292ec3a6791482b847b5728af5c3012: Deleting WAL directory at /home/ubuntu/Impala/testdata/cluster/cdh7/node-3/var/lib/kudu/ts/wal/wals/fa987644f7554383a1a9b6984e54cfa2
I1207 02:40:47.922526  8717 ts_tablet_manager.cc:1858] T fa987644f7554383a1a9b6984e54cfa2 P 8292ec3a6791482b847b5728af5c3012: Deleting consensus metadata
I1207 02:40:47.922871  8717 tablet_replica.cc:330] T b4318238b0e94b718881e5c6d0afd0e5 P 8292ec3a6791482b847b5728af5c3012: stopping tablet replica
I1207 02:40:47.922904  8717 raft_consensus.cc:2229] T b4318238b0e94b718881e5c6d0afd0e5 P 8292ec3a6791482b847b5728af5c3012 [term 1 LEADER]: Raft consensus shutting down.
I1207 02:40:47.922933  8717 raft_consensus.cc:2258] T b4318238b0e94b718881e5c6d0afd0e5 P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus is shut down!
I1207 02:40:47.923015  8717 ts_tablet_manager.cc:1824] T b4318238b0e94b718881e5c6d0afd0e5 P 8292ec3a6791482b847b5728af5c3012: Deleting tablet data with delete state TABLET_DATA_DELETED
I1207 02:40:47.923732  8717 ts_tablet_manager.cc:1837] T b4318238b0e94b718881e5c6d0afd0e5 P 8292ec3a6791482b847b5728af5c3012: tablet deleted with delete type TABLET_DATA_DELETED: last-logged OpId 1.2
I1207 02:40:47.923758  8717 log.cc:1192] T b4318238b0e94b718881e5c6d0afd0e5 P 8292ec3a6791482b847b5728af5c3012: Deleting WAL directory at /home/ubuntu/Impala/testdata/cluster/cdh7/node-3/var/lib/kudu/ts/wal/wals/b4318238b0e94b718881e5c6d0afd0e5
I1207 02:40:47.923826  8717 ts_tablet_manager.cc:1858] T b4318238b0e94b718881e5c6d0afd0e5 P 8292ec3a6791482b847b5728af5c3012: Deleting consensus metadata
I1207 02:40:47.924152  8717 tablet_replica.cc:330] T daf8246510f14af1bab937551059d615 P 8292ec3a6791482b847b5728af5c3012: stopping tablet replica
I1207 02:40:47.924182  8717 raft_consensus.cc:2229] T daf8246510f14af1bab937551059d615 P 8292ec3a6791482b847b5728af5c3012 [term 1 LEADER]: Raft consensus shutting down.
I1207 02:40:47.924201  8717 raft_consensus.cc:2258] T daf8246510f14af1bab937551059d615 P 8292ec3a6791482b847b5728af5c3012 [term 1 FOLLOWER]: Raft consensus is shut down!
I1207 02:40:47.924249  8717 ts_tablet_manager.cc:1824] T daf8246510f14af1bab937551059d615 P 8292ec3a6791482b847b5728af5c3012: Deleting tablet data with delete state TABLET_DATA_DELETED
I1207 02:40:47.924993  8717 ts_tablet_manager.cc:1837] T daf8246510f14af1bab937551059d615 P 8292ec3a6791482b847b5728af5c3012: tablet deleted with delete type TABLET_DATA_DELETED: last-logged OpId 1.2
I1207 02:40:47.925007  8717 log.cc:1192] T daf8246510f14af1bab937551059d615 P 8292ec3a6791482b847b5728af5c3012: Deleting WAL directory at /home/ubuntu/Impala/testdata/cluster/cdh7/node-3/var/lib/kudu/ts/wal/wals/daf8246510f14af1bab937551059d615
I1207 02:40:47.925066  8717 ts_tablet_manager.cc:1858] T daf8246510f14af1bab937551059d615 P 8292ec3a6791482b847b5728af5c3012: Deleting consensus metadata
{code}

Timing-wise, that looks like it's after all query attempts have completed and the database is being cleaned up with {{DROP DATABASE  `test_read_modes_53f93f33` CASCADE}}.

The test does an insert before that at 02:40:44.548391. A little bit before that, there does seem to have been a leader change:
{code}
I1207 02:40:40.212661  7090 raft_consensus.cc:2781] T fa987644f7554383a1a9b6984e54cfa2 P 8e14bd2c381248c8a5a89a7d17069d0e [term 1 FOLLOWER]: Leader election won for term 1
I1207 02:40:40.212677  7090 raft_consensus.cc:686] T fa987644f7554383a1a9b6984e54cfa2 P 8e14bd2c381248c8a5a89a7d17069d0e [term 1 LEADER]: Becoming Leader. State: Replica: 8e14bd2c381248c8a5a89a7d17069d0e, State: Running, Role: LEADER
I1207 02:40:40.212697  7090 consensus_queue.cc:235] T fa987644f7554383a1a9b6984e54cfa2 P 8e14bd2c381248c8a5a89a7d17069d0e [LEADER]: Queue going to LEADER mode. State: All replicated index: 0, Majority replicated index: 0, Committed index: 0, Last appended: 0.0, Last appended by leader: 0, Current term: 1, Majority size: 2, State: 0, Mode: LEADER, active raft config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: "b1100c8a2e57495f936302b0a5eeb707" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31202 } } peers { permanent_uuid: "8e14bd2c381248c8a5a89a7d17069d0e" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31201 } } peers { permanent_uuid: "8292ec3a6791482b847b5728af5c3012" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31200 } }
I1207 02:40:43.224936  7526 consensus_queue.cc:1033] T fa987644f7554383a1a9b6984e54cfa2 P 8e14bd2c381248c8a5a89a7d17069d0e [LEADER]: Connected to new peer: Peer: permanent_uuid: "b1100c8a2e57495f936302b0a5eeb707" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31202 }, Status: LMP_MISMATCH, Last received: 0.0, Next index: 1, Last known committed idx: 0, Time since last communication: 0.000s
I1207 02:40:43.225529  7714 consensus_queue.cc:1033] T fa987644f7554383a1a9b6984e54cfa2 P 8e14bd2c381248c8a5a89a7d17069d0e [LEADER]: Connected to new peer: Peer: permanent_uuid: "8292ec3a6791482b847b5728af5c3012" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31200 }, Status: LMP_MISMATCH, Last received: 0.0, Next index: 1, Last known committed idx: 0, Time since last communication: 0.000s
{code}

The Kudu master shows
{code}
W1207 02:40:38.368923 17161 catalog_manager.cc:6233] The number of live tablet servers is not enough to re-replicate a tablet replica of the newly created table impala::test_read_modes_53f93f33.test_read_latest in case of a server failure: 4 tablet servers would be needed, 3 are available. Consider bringing up more tablet servers.
I1207 02:40:38.571925 17159 catalog_manager.cc:4964] T 3dac2fd02f6045b3aa01d06e00d7fdf2 P 8292ec3a6791482b847b5728af5c3012 reported cstate change: term changed from 0 to 1, leader changed from <none> to 8292ec3a6791482b847b5728af5c3012 (127.0.0.1). New cstate: current_term: 1 leader_uuid: "8292ec3a6791482b847b5728af5c3012" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: "8e14bd2c381248c8a5a89a7d17069d0e" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31201 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "b1100c8a2e57495f936302b0a5eeb707" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31202 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "8292ec3a6791482b847b5728af5c3012" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31200 } health_report { overall_health: HEALTHY } } }
I1207 02:40:38.899076 17159 catalog_manager.cc:4964] T a010773aad414657975ec8fcd20b84fc P b1100c8a2e57495f936302b0a5eeb707 reported cstate change: term changed from 0 to 1, leader changed from <none> to b1100c8a2e57495f936302b0a5eeb707 (127.0.0.1). New cstate: current_term: 1 leader_uuid: "b1100c8a2e57495f936302b0a5eeb707" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: "8e14bd2c381248c8a5a89a7d17069d0e" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31201 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "b1100c8a2e57495f936302b0a5eeb707" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31202 } health_report { overall_health: HEALTHY } } peers { permanent_uuid: "8292ec3a6791482b847b5728af5c3012" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31200 } health_report { overall_health: UNKNOWN } } }
I1207 02:40:38.941711 17159 catalog_manager.cc:4964] T 387108e0cf094873b8d61319178fcfe1 P b1100c8a2e57495f936302b0a5eeb707 reported cstate change: term changed from 0 to 1, leader changed from <none> to b1100c8a2e57495f936302b0a5eeb707 (127.0.0.1). New cstate: current_term: 1 leader_uuid: "b1100c8a2e57495f936302b0a5eeb707" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: "b1100c8a2e57495f936302b0a5eeb707" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31202 } health_report { overall_health: HEALTHY } } peers { permanent_uuid: "8e14bd2c381248c8a5a89a7d17069d0e" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31201 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "8292ec3a6791482b847b5728af5c3012" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31200 } health_report { overall_health: UNKNOWN } } }
I1207 02:40:39.073591 17159 catalog_manager.cc:4964] T daf8246510f14af1bab937551059d615 P 8292ec3a6791482b847b5728af5c3012 reported cstate change: term changed from 0 to 1, leader changed from <none> to 8292ec3a6791482b847b5728af5c3012 (127.0.0.1). New cstate: current_term: 1 leader_uuid: "8292ec3a6791482b847b5728af5c3012" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: "8e14bd2c381248c8a5a89a7d17069d0e" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31201 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "b1100c8a2e57495f936302b0a5eeb707" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31202 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "8292ec3a6791482b847b5728af5c3012" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31200 } health_report { overall_health: HEALTHY } } }
I1207 02:40:39.184866 17159 catalog_manager.cc:4964] T b4318238b0e94b718881e5c6d0afd0e5 P 8292ec3a6791482b847b5728af5c3012 reported cstate change: term changed from 0 to 1, leader changed from <none> to 8292ec3a6791482b847b5728af5c3012 (127.0.0.1). New cstate: current_term: 1 leader_uuid: "8292ec3a6791482b847b5728af5c3012" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: "8e14bd2c381248c8a5a89a7d17069d0e" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31201 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "b1100c8a2e57495f936302b0a5eeb707" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31202 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "8292ec3a6791482b847b5728af5c3012" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31200 } health_report { overall_health: HEALTHY } } }
I1207 02:40:39.251101 17159 catalog_manager.cc:4964] T 63d011a0c5a340f2909efd885c06b0ed P 8292ec3a6791482b847b5728af5c3012 reported cstate change: term changed from 0 to 1, leader changed from <none> to 8292ec3a6791482b847b5728af5c3012 (127.0.0.1). New cstate: current_term: 1 leader_uuid: "8292ec3a6791482b847b5728af5c3012" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: "b1100c8a2e57495f936302b0a5eeb707" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31202 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "8e14bd2c381248c8a5a89a7d17069d0e" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31201 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "8292ec3a6791482b847b5728af5c3012" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31200 } health_report { overall_health: HEALTHY } } }
I1207 02:40:40.075673 17159 catalog_manager.cc:4964] T 377b32f8e9c542338249be052aae9d20 P b1100c8a2e57495f936302b0a5eeb707 reported cstate change: term changed from 0 to 1, leader changed from <none> to b1100c8a2e57495f936302b0a5eeb707 (127.0.0.1). New cstate: current_term: 1 leader_uuid: "b1100c8a2e57495f936302b0a5eeb707" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: "8e14bd2c381248c8a5a89a7d17069d0e" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31201 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "b1100c8a2e57495f936302b0a5eeb707" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31202 } health_report { overall_health: HEALTHY } } peers { permanent_uuid: "8292ec3a6791482b847b5728af5c3012" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31200 } health_report { overall_health: UNKNOWN } } }
I1207 02:40:40.212988 17159 catalog_manager.cc:4964] T fa987644f7554383a1a9b6984e54cfa2 P 8e14bd2c381248c8a5a89a7d17069d0e reported cstate change: term changed from 0 to 1, leader changed from <none> to 8e14bd2c381248c8a5a89a7d17069d0e (127.0.0.1). New cstate: current_term: 1 leader_uuid: "8e14bd2c381248c8a5a89a7d17069d0e" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: "b1100c8a2e57495f936302b0a5eeb707" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31202 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: "8e14bd2c381248c8a5a89a7d17069d0e" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31201 } health_report { overall_health: HEALTHY } } peers { permanent_uuid: "8292ec3a6791482b847b5728af5c3012" member_type: VOTER last_known_addr { host: "127.0.0.1" port: 31200 } health_report { overall_health: UNKNOWN } } }
{code}
around that time. The warning and UNKNOWN health notices are interesting.

