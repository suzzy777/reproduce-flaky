[https://github.com/apache/hadoop/pull/2241]

Don't know why there is no automatic association

Hi [~fanrui] The performance improvement seems awesome. Thanks for sharing.

One fundamental question I have is, will this work for cases where the DN host changes its IP? This happens a lot in containerized environment where the hostname is fixed (keeping either externalDNS or StatefulSet Pod identity) but the IP could change for each restart.

Hi [~liuml07], thanks for your comment. 

The case you provided may happen. My idea is to only cache URI objects, and the URL does not contain ip. From FlameGraph, we can see that create URI is the most time-consuming operation, so caching URI can provide performance.
I have updated the code. Could you please review it, thank you.

[https://github.com/apache/hadoop/pull/2241]

Hi [~liuml07], [~hexiaoqiao] , thank you very much for your suggestions and review.

I have added the unit test related to change ip. If Cache InetSocketAddress, the unit test will fail. Now modify it to Cache URI and the unit test can succeed.

Yes, I totally see the value of caching. That is amazing results.

OK, caching the URI instead of {{InetSocketAddress}} makes more sense. I think this will be safe and more generic. Still, providing a tube for enabling this feature and disabling it by default will be suggested. Or at least provide two APIs and use the new one by "opt-in" and existing use cases are by default "opt-out".

Thanks!

[~fanrui] I have added you to Hadoop Contributor list in JIRA, and also assigned this JIRA to you. Thanks,

[~liuml07], thank you very much. I will do my best to do it.

I prefer `providing a tube for enabling this feature and disabling it by default will be suggested.`
But I have some doubts now. If enabling this feature, it will affect all callers of `NetUtils.createSocketAddr` and not just the hdfs client, right?

{quote}
it will affect all callers of `NetUtils.createSocketAddr` and not just the hdfs client, right?
{quote}

Yes that is true...

Is it overkill if we:
# in hadoop-common/NetUtils we have two API of {{createSocketAddr(...) \{ return createSocketAddr(..., false);\}}} and {{createSocketAddr(..., useCacheIfPresent)}}
# in HDFS we have a new config which will decide the {{useCacheIfPresent}} being false or true

If we make DFS code path use this only by default, perhaps the 2 point is not necessary. I'm also fine with that.

Hi [~liuml07], thank you very much for your suggestions.

add method: createSocketAddr(..., useCacheIfPresent), just called by DFSInputStream.getBestNodeDNAddrPair().In other cases, useCacheIfPresent is always false.

Added the configuration `dfs.client.read.uri.cache.enable` to determine whether the hdfs client uses the current Cache.

{quote}1. in hadoop-common/NetUtils we have two API of createSocketAddr(...) { return createSocketAddr(..., false);} and createSocketAddr(..., useCacheIfPresent)
2. in HDFS we have a new config which will decide the useCacheIfPresent being false or true{quote}
+1, Another side, I still suggest to add evict policy for the cache, I am concerned if it will involve any overhead for long time application when one datanode is decommissioned or offline but still cached at Client. Please correct me if something I am missed. Thanks.

[~hexiaoqiao], thanks for your suggestions.

I agree with you, if the evict policy is added, the code will be more robust. For the default cache size and expire time, can you provide some suggestions? Thanks.

Yes an evict policy for the cache is good for long-running clients and large or dynamic HDFS clusters. I don't have any good estimation, but the expiration time can be up to 12 or even 24 hours.

{quote}the expiration time can be up to 12 or even 24 hours.{quote}
+1, it makes sense to me. or we can expose one parameter to config for end users. 

ok.I will expose the `cache size` and `expire time` parameters to the user. The default value of `expire time` is set to 12 hours.
URI objects do not occupy much memory. Can the default value of cache size be set to 5000 or even 10000? Can you provide some suggestions for the default cache size? Thanks.

I’m thinking two new configs seem a bit overkill - unrelated to this but Hadoop has too many configs while we seldom clean them up...

Cache max size being 1000 is very enough for me.

thanks for the reporting the issue [~fanrui]
{quote}The CPU usage of DFSInputStream.getBestNodeDNAddrPair has been optimized from 4.86% to 0.54%.
{quote}
can you report the CPU usage for same number of samples for DFSInputStream.getBestNodeDNAddrPair 
{quote}I’m thinking two new configs seem a bit overkill - unrelated to this but Hadoop has too many configs while we seldom clean them up
{quote}
agree with this , there has been increasing number of configurations ,  we can avoid configs here 

 

Hi [~hemanthboyina], thanks for your comment.

My test case IO is the bottleneck, and the CPU has not reached the bottleneck.
Before Optimization: CPU usage is about 40.2%
After Optimization: CPU usage is about 38.9%
The CPU usage rate is reduced by about 3.2%, which is similar to the theoretical analysis of the flame graph.

Hi, [~liuml07] [~hexiaoqiao] [~hemanthboyina], thanks everyone for guidance and suggestions.

I have updated the code. add parameter of `uri cache expire time`, default is 12 hours. The cache size is 1000 and cannot be modified.
Could you have time to help review it? Thanks.

[~fanrui] I have merged this to {{trunk}} branch. Also I added the Release Note for this JIRA. Please review and update the "Release Note" field if it applies. Thanks for your contribution!

Hi [~liuml07], thanks for your work. I think "Release Note" is good.

This seems like a great improvement. I have a couple of questions:

1. The cache size is hard coded to 1000. For a long running client like HBase, if the cluster is > 1000 nodes, then the cache may become less effective. I see the earlier comments about trying to avoid more config flags, but I think there are many clusters out there with > 1000 nodes. Maybe we should hardcode this to something a bit larger?

2. Are there any objections to backporting this to at least branch 3.3? Seems a shame for it to be only on trunk when 3.4 is likely a long time away? There is a minor import conflict for branch-3.3 which will be easily resolved.

Thanks [~sodonnell]. 

I guess I do not play with clusters with >1000 nodes nowdays (we have more and small clusters) and do not have preference about >1000 cache size. But totally understand your use case. Since this is just a max size (or capacity) instead of the typical real size, I think it makes sense if you want to increase the hard limit from 1000 to something like 3000. The number is more an art so anything above 2000 makes sense I assume.

Yes, I think this can be backported in old branches like 3.3. I recall vaguely that there is not 3.4 specific logic or assumption. Will defer to [~fanrui] and [~hexiaoqiao] to confirm. I can help backport. So, is 3.2/3.1 also doable? 

 

Reopening to backport to branch-3.3

[~liuml07] Thanks for the reply. I created a PR for branch-3.3:

https://github.com/apache/hadoop/pull/2817

The only conflicts were on the imports on NetUtils.java, but as this change was committed some time ago I think it makes sense to do a fill pre-commit run.

Can you please review the PR? 

Yes thank you!

I merged this onto branch-3.3 from the PR. Hence closing this Jira again.

[~sodonnell] I was oncall recently so did not have time to review that in time. I just checked and the backport looks great. Thanks!

