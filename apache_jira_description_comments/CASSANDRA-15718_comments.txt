[~dcapwell] I've started making adjustments to use quick theories and interestingly the assertion of the histogram fails as you point out that the "current test is wrong". I started thinking about those assertions:

{code}
assertTrue(assertionMessage,partitionsPerLoggedBatchCountPre <= metrics.partitionsPerLoggedBatch.getSnapshot().getMax());
        assertTrue(assertionMessage,partitionsPerUnloggedBatchCountPre <= metrics.partitionsPerUnloggedBatch.getSnapshot().getMax());
        assertTrue(assertionMessage,partitionsPerCounterBatchCountPre <= metrics.partitionsPerCounterBatch.getSnapshot().getMax());
{code} 

and started to wonder if the histograms need to be asserted at all. I mostly just copied the semantics of the old tests, but didn't really stop to consider them. Is the assertion of the histogram fulfilling any purpose in your view compared to what is already being asserted in terms of the direct expectations of the metrics:

{code}
        assertEquals(assertionMessage,partitionsPerUnloggedBatchCountPost, metrics.partitionsPerUnloggedBatch.getCount());
        assertEquals(assertionMessage, partitionsPerLoggedBatchCountPost, metrics.partitionsPerLoggedBatch.getCount());
        assertEquals(assertionMessage, partitionsPerCounterBatchCountPost, metrics.partitionsPerCounterBatch.getCount());
{code}

Count shows that the histograms are updated at the same frequency as expected, but doesn't show that the actual metrics are working.  Testing the actual histogram's data would show if the updates are correct or not; so I do feel that we want histogram data checks as well.

Now, do I think we need to test if histograms from the metrics library are correct?  No, only our usage.  To be clear, since the test really provides fixed values, then min/max are well known so should be enough to test.

I've pushed some changes to my branch (link in description) addressing the points that were made in the initial review. I converted to quick theories rather than my custom use of {{Random}}. I wasn't quite sure how to tune the parameters to the test exactly as running these tests over larger parameter spaces adds to the total run time of the test. I ended up settling on parameters that routinely ran in less than one to two seconds total for the entire test class. If someone thinks there is value in tuning quick theories to run more examples with wider parameters it's easy enough to change. For my own purposes/testing, I've run these tests with significantly wider testing parameters and they pass without issues. 

As for this point:

> the batch has distinctPartitions mutations, so shouldn't max reflect that?

I spent a fair amount of time trying to understand exactly what was happening there and I could still have it wrong but I think that the semantics of {{<=}} for the assertion might have been correct, though the numbers I asserted in my test were originally wrong. Given the underlying use of {{DecayingEstimatedHistogramReservoir}} for the batch metrics it seems that it's possible for the actual value recorded in the reservoir to be different from the results returned from {{getMin()}} and {[getMax()}} so even though we know the {{distinctPartitions}} I'm not sure that we know exactly what the returned value might be. Of course, it's entirely possible that I've completely misunderstood something here.

Looking forward to the next round of review on this and if some aspect of this can be further improved.

[~dcapwell] (or others who might be interested) I was wonder if you'd had a moment to consider my comments above as well as the changes in the branch?

sorry I have not looked sooner, checking out now.

bq. even though we know the distinctPartitions I'm not sure that we know exactly what the returned value might be

{code}
public void update(long value)
    {
        long now = clock.getTime();
        rescaleIfNeeded(now);

        int index = findIndex(bucketOffsets, value);

        updateBucket(decayingBuckets, index, Math.round(forwardDecayWeight(now)));
        updateBucket(buckets, index, 1);
    }

public long getMax()
        {
            final int lastBucket = decayingBuckets.length - 1;

            if (decayingBuckets[lastBucket] > 0)
                return Long.MAX_VALUE;

            for (int i = lastBucket - 1; i >= 0; i--)
            {
                if (decayingBuckets[i] > 0)
                    return bucketOffsets[i];
            }
            return 0;
        }
{code}

Yep, we loose the original value and min/max actually reflect the bucket, so we would need to know the bucketing to actually be able to correctly assert the value.  The only clean way I can see is if the snapshot had a method to convert a value to bucket, that would at least let us make sure we saw data for the correct bucket.

* https://github.com/apache/cassandra/compare/trunk...spmallette:CASSANDRA-15582-trunk-batchmetrics#diff-8948cec1f9d33f10b15c38de80141548R170. Should call snapshot once, else you do more memory copies than needed.
* https://github.com/apache/cassandra/compare/trunk...spmallette:CASSANDRA-15582-trunk-batchmetrics#diff-8948cec1f9d33f10b15c38de80141548R133 Can we remove ".withExamples(10)", should rely on the default.  We currently disable shrinking so that won't case the GC to freak out.  If you also fix the above statement you drop the amount of memory considerably (clone 9 times per iteration, could be 1 time).
* https://github.com/apache/cassandra/compare/trunk...spmallette:CASSANDRA-15582-trunk-batchmetrics#diff-8948cec1f9d33f10b15c38de80141548R178 this doesn't look stable it assumes the very first test case has 1 partition, if the first test case has 2 or more partitions then it should fail (since the value is now 2 [1]).
* Would be good to call "org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoir#clear" so each test doesn't see the results of the previous tests.

For min/max the only thing I can think of is to expose a test method to get the bucket for a specific value, that would let us refine min/max.  Something like the below would work

{code}
class EstimatedHistogramReservoirSnapshot
...
public long getBucketValue(long value)
        {
            int index = findIndex(bucketOffsets, value);
            return bucketOffsets[index];
        }
{code}

[~jrwest] since you have worked with this recently; thoughts?  Should we leave the max as is?

[1] - here are the first 30 buckets: 

{code}
0 = 1
1 = 2
2 = 3
3 = 4
4 = 5
5 = 6
6 = 7
7 = 8
8 = 10
9 = 12
10 = 14
11 = 17
12 = 20
13 = 24
14 = 29
15 = 35
16 = 42
17 = 50
18 = 60
19 = 72
20 = 86
21 = 103
22 = 124
23 = 149
24 = 179
25 = 215
26 = 258
27 = 310
28 = 372
29 = 446
{code}

[~spmallette] sorry for the late replay; my comments are mostly small so overall LGTM.  The main open one is if we could do better with min/max assert but if thats too much trouble we can leave it as just max.

I wouldn't change the behavior of min/max just for the tests (if I understand right). That has been the historical behavior even before my recent changes. If you want to add a method like `getBucketValue` I would recommend a different name since `bucketValue` is already used to mean the count for a given bucket summed across stripes.

sorry [~jrwest], only meant modify the test so it can assert the min/max range based off the input.  Right now a regression in the value of the metric (not the histogram, but the input to it) wouldn't be caught.

> Should call snapshot once, else you do more memory copies than needed.

fixed

> Can we remove ".withExamples(10)", should rely on the default.

I can remove it, but it will drastically increase the test time from 3 seconds to almost 3 minutes on my system. Is this test validating behavior and ranges worth 3 minutes? I wasn't sure, which is why i figured hitting a subset of examples would be a sufficient improvement over the more static approach that existed before I began my changes. Perhaps I could change other parameters that would produce less tests instead (e.g. reduce partition counts tested instead)? I've left that as is for now, but happy to change it to whatever makes sense. Just let me know. 

> this doesn't look stable it assumes the very first test case has 1 partition, if the first test case has 2 or more partitions then it should fail (since the value is now 2 [1]).

min/max has been tricky but I ran with your approach to include that test method you suggested - if you have a better name for that method I'm fine to take suggestions. it deals with the max pretty nicely. min seemed to require a bit more thought but basically it appeared to require that i retrieve the bucketing prior to the max and then assert a {{<=}} sort of range as existed before. I'm not quite sure if I'm missing something obvious in how to assert the exact min expectation, but my debug sessions never quite got to the bottom of that. :(

> Would be good to call "org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoir#clear" so each test doesn't see the results of the previous tests.

I added that call to {{clear()}} and I think that makes for a better test. I'd thought about that but didn't realize the approach to do so was so straightforward. Thanks for pointing me at that. 

Looking forward to hearing what folks think of this latest revision.

> Can we remove ".withExamples(10)", should rely on the default.

you bumped to 25, so feel thats a fair compromise; thanks =)

Overall the patch LGTM, there is a very small change to the test to simplify the min/max logic; I attached a patch to your branch to do that (also, the tests are flaky caused by timeouts, so bump the timeout value).  The basic idea is that getBucketingForValue doesn't return the bucket value, but the range of values.  Since we always apply a fixed partition size to the test min is always max_bucket-1 (exception when bucket == 0), so by making the function return the range, we can do a simple equals check on the pair to assert min/max at the same time.

[~spmallette]

[~djoshi] can you also review?  Think the patch is basically ready, so would be good to get your eyes as well.

I've applied that patch so that it is viewable in the branch diff for review. Thanks. 

I can squash all those commits to tidy up, when folks are happy with what's there. 

Thanks for all the hard work! 

I am +1.

Hi [~spmallette], thanks for the contribution. I have kicked off a CI build [here|https://circleci.com/workflow-run/cc16d125-eaff-4395-a8b5-79f7c38994d9].

[~spmallette] I have a little feedback which I have mocked up in this [branch|https://github.com/apache/cassandra/compare/trunk...dineshjoshi:CASSANDRA-15582-trunk-batchmetrics?expand=1]. If it looks good, I can squash and commit it.

[~djoshi] I'm fine with the introduction of {{Range}} there rather than using {{Pair}}  +1 

Committed. Thanks for your contribution [~spmallette]! Thanks for the review [~dcapwell]!

FYI: CI was green [here|https://circleci.com/workflow-run/10c17f89-a4f3-4b37-9bf4-7abd6834579f].

