they used to use their own space. But I think Sebastian did a fix to use some methods from abstract mahout test class an
d now they land somewhere else. I can take a look, provided I know what the best practice really is for using temp directories for tests using local Mr jobs.

oh I think this particular thing is different. There's semiofficial rule in map red how to access task temporary space.  Maoreduce boxes each task in a way that its temporary space can be accessed thru a system variable.

Apparently, local Mr spends much less effort to box a task properly. We can probably figure a workaround for the local mode task boxing.


Yeah, it is different.  It's not actually in the test, it just manifested itself for me via the test.

it's actually will only manifest in test with Jenkins for some reason. There's actually nothing to be fixed (except for maybe the local mode of the mapreduce itself), and it will work exactly as it is supposed in distributed mode. 

but we can probably squeeze in a workaround into mahout abstract test class. Just another piece of black magic to plug a hole in local mapreduce mode.

I can take a look at it next week, but I am not sure how to verify it with Jenkins other than committing it first.

Ok, I actually don't know what the best way to fix this. 

Hadoop boxes tasks so that java.io.tmpdir points to the task's temporary directory in the mapred folder (in distributed mode). That's basically their contract for task temporary space. 

My idea was to hack that in the case of local mapred to point something more suitable with mahout. 

But that more suitable turns out to be... /tmp+something as in the following code: 
{code:title=MahoutTestCase.java}

  protected final File getTestTempDir() throws IOException {
    if (testTempDir == null) {
      String systemTmpDir = System.getProperty("java.io.tmpdir");
      long simpleRandomLong = (long) (Long.MAX_VALUE * Math.random());
      testTempDir = new File(systemTmpDir, "mahout-" + getClass().getSimpleName() + '-' + simpleRandomLong);
      if (!testTempDir.mkdir()) {
        throw new IOException("Could not create " + testTempDir);
      }
      testTempDir.deleteOnExit();
    }
    return testTempDir;
  }

{code}

So... it looks like Mahout's test framework is already hooked on that (which in its turn is deregulated by Mahout, so it points to /tmp perhaps in test mode already). 

So it looks like i cannot override java.io.tmpdir Mahout-wide because Mahout already attributes some meaning to that variable.

I don't immediately see the best solution here. 

i can probably change the solvers so that they don't necessarily write to the task's root folder but create another folder there, but that still doesn't guarantee absence of clashes during tests (because only getTestTempDir() would guarantee that).

So i would want to solicit some discussion here.


Also -- I am not sure if I am construing the exception correctly in this sense -- the original stack trace doesn't look like there's a clash, but rather that in that Jenkins' environment, java.io.tmp is not writable by the process. which would be a problem regardless of what we do.

One hack that can probably overcome this. i am still not convinced that's the best way to do it, but at least the task wouldn't be writing directly into java.io.tmpdir when specifically requested not to.

Github snapshot is here: https://github.com/dlyubimov/mahout-commits/tree/MAHOUT-814


another workaround, per tutorial, seems to be to construct this task per the rule below but does it have to be that complicated just to access task scratchpad path? besides, i looked at the history of this and it really subject to change. So using java.io.tmpdir really seems like the safest bet. 

{quote}

$${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid/work/tmp : The temporary directory for the task. (User can specify the property mapred.child.tmp to set the value of temporary directory for map and reduce tasks. This defaults to ./tmp. If the value is not an absolute path, it is prepended with task's working directory. Otherwise, it is directly assigned. The directory will be created if it doesn't exist. Then, the child java tasks are executed with option -Djava.io.tmpdir='the absolute path of the tmp dir'. Anp pipes and streaming are set with environment variable, TMPDIR='the absolute path of the tmp dir'). This directory is created, if mapred.child.tmp has the value ./tmp


{quote}

Your analysis is completely correct. java.io.tmpdir is more than that -- it's the definition of "writable temp space" for Java itself. There should be nothing wrong with using this; in fact, it's the only reliable temp space available.

1. If it's not writable, that's a problem with the host machine. As you might have noticed, Jenkins seems to be quite flaky; a few times a week it will fail for some internal machine-specific reason.

2. However I too am not sure that's the issue; it could still be a clash somehow. Can you change the job that uses q-temp.seq to stash it in a file that maybe includes the timestamp? like q-temp-125095095090.seq. In general I think this is a good strategy, and why the test framework does exactly this.

It's either nothing for us to fix (1), or, I think, a case of making temp files more unique (2). You shouldn't have to do anything more complex.

+1 on Sean's option #2.  The only question then is do we want to make it a little more forceful in the cleanup of temp files?  Perhaps a deleteOnExit?  Not sure what that would mean in distributed mode (I would assume it's safe, but what happens if you are reusing the JVM?)

Actually, from what i am reading, even java.io.tmpdir may be shared between different tasks even in distributed mode in case jvm sharing is enabled (forget the property name, but i use it also at value of ~5-10 to speed up small tasks setup). 

In which case using java.io.tmp/q-temp may potentially be a problem as well. 

+What i propose is it probably would be safer to use a directory *$java.io.tmp/$taskAttemptID* which would guarantee task uniqueness (but perhaps not with local tests, if it is not unique with local tests, i will add some random numbers).+

{panel}
A little background: having to write a temporary file in this task is a corner case only arising when Q block height set is smaller than the number of input rows of A coming in in one split, which should never be the case with normal block sizes but may be a case with minSplitSize splits set at 1G or something, or if A input is extremely sparse (such as one non-zero element per row on average, then yeah, Q blocks, which are k+p wide (the number of eigenvalues requested), could be rather memory intensive, which is not a very good use case for this method, i'd rather try to transpose first to see if it helps row-wise sparsity). 

Only with those corner cases out-of-core version of task's local computation of what i call Q-hat blocks is used. But that pass is always sequential so it should capitalize on sequential read speeds and potentially OS I/O cache if there's enough memory installed.

The test however is set up intentionally the way that Q block height is set extremely small to test both blocking within a split and among the splits.
{panel}

or timestamp approach may work too, of course.

Not sure it this is related, but sounds similar.  I can't run more than one power iteration, ie q=2 produces

11/09/21 11:25:46 INFO mapred.LocalJobRunner: reduce > reduce
11/09/21 11:25:46 INFO mapred.Task: Task 'attempt_local_0004_r_000000_0' done.
11/09/21 11:25:50 INFO mapred.JobClient: Cleaning up the staging area file:/tmp/hadoop-nathanhalko/mapred/staging/nathanhalko-200181280/.staging/job_local_0005
Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory temp/ABt-job-1 already exists
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:134)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:830)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:791)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:791)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:465)
	at org.apache.mahout.math.hadoop.stochasticsvd.ABtJob.run(ABtJob.java:454)
	at org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver.run(SSVDSolver.java:312)
	at org.apache.mahout.math.hadoop.stochasticsvd.SSVDCli.run(SSVDCli.java:118)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.mahout.math.hadoop.stochasticsvd.SSVDCli.main(SSVDCli.java:163)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
	at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

for q=0,1 everything works fine.  I am running with --overwrite and I rm -rf the temp dir before running.

no it isn't related. Good catch. q=1 has put such a good showing i never tried q>1, not even in local tests. That should be a typo-kind fix.  

Nathan, your issue is now resolved in MAHOUT-816 (pending commit)

Implementation per Grant's suggestion. 

BTW, part of the suggestion (cleaning out task temp files) has already been implemented before. And that actually happens on cleanup() of the mapper.

I am still kind of dubious of the reason of the exception. looks like permission issues on java.io.tmpdir to me.



Ok, i take it there's no objections, then i will commit it. it is a simple patch, and that's what Grant wanted...

Integrated in Mahout-Quality #1057 (See [https://builds.apache.org/job/Mahout-Quality/1057/])
    MAHOUT-814 (patch rev.1)

dlyubimov : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1174452
Files : 
* /mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/QRFirstStep.java
* /mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverDenseTest.java


