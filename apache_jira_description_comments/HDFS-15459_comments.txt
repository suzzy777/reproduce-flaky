The failure is related to {{StrippedBlockUtil.java}}.

I noticed the following issue for failing executions:

1- While Creating a file, an error happens in with the replicas

{code:bash}
2020-10-13 14:38:26,437 [IPC Server handler 2 on default port 18020] INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(891)) - Not enough replicas was chosen. Reason: {NODE_TOO_BUSY=4}
2020-10-13 14:38:26,438 [IPC Server handler 2 on default port 18020] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(471)) - Failed to place enough replicas, still in need of 2 to reach 9 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-10-13 14:38:26,438 [IPC Server handler 2 on default port 18020] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=9, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2020-10-13 14:38:26,438 [IPC Server handler 2 on default port 18020] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(471)) - Failed to place enough replicas, still in need of 2 to reach 9 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2020-10-13 14:38:26,446 [Listener at localhost/65076] WARN  hdfs.DFSOutputStream (DFSStripedOutputStream.java:allocateNewBlock(531)) - Cannot allocate parity block(index=7, policy=RS-6-3-1024k). Exclude nodes=[]. There may not be enough datanodes or racks. You can check if the cluster topology supports the enabled erasure coding policies by running the command 'hdfs ec -verifyClusterSetup'.
2020-10-13 14:38:26,447 [Listener at localhost/65076] WARN  hdfs.DFSOutputStream (DFSStripedOutputStream.java:allocateNewBlock(531)) - Cannot allocate parity block(index=8, policy=RS-6-3-1024k). Exclude nodes=[]. There may not be enough datanodes or racks. You can check if the cluster topology supports the enabled erasure coding policies by running the command 'hdfs ec -verifyClusterSetup'.
{code}

2- some RBWs to be done. 7 replicas out of 9 will be created for each block.

3- Every-time this error happens, later in the JUnit, an input stream on the same file will have a blockLocation that is"null".
The NPE comes because of the internal blocks parsed inside {{StripedBlock.java}}.
An array is initialized with size {{dataBlkNum + parityBlkNum}} (which is 9), while {{LocatedStripedBlock}} length is 7. Therefore two entries are left uninitialized which causes the NPE.

4- I checked other places in the code where {{parseStripedBlockGroup()}} is called and it seems that it is taken for granted that entries are not-null. 

[~elgoiri], I am not very familiar with the DFStriped logic. Do you know if this is a major bug? Or fixing the {{parseStripedBlockGroup}} loop is a simple fix for that scenario?

| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime ||  Logfile || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 32m 30s{color} |  | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} || ||
| {color:green}+1{color} | {color:green} dupname {color} | {color:green}  0m  0s{color} |  | {color:green} No case conflicting files found. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} |  | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} |  | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} || ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 21m 14s{color} |  | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 17s{color} |  | {color:green} trunk passed with JDK Ubuntu-11.0.8+10-post-Ubuntu-0ubuntu118.04.1 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  9s{color} |  | {color:green} trunk passed with JDK Private Build-1.8.0_265-8u265-b01-0ubuntu2~18.04-b01 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 45s{color} |  | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 15s{color} |  | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 17m 25s{color} |  | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} |  | {color:green} trunk passed with JDK Ubuntu-11.0.8+10-post-Ubuntu-0ubuntu118.04.1 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 21s{color} |  | {color:green} trunk passed with JDK Private Build-1.8.0_265-8u265-b01-0ubuntu2~18.04-b01 {color} |
| {color:blue}0{color} | {color:blue} spotbugs {color} | {color:blue}  3m  7s{color} |  | {color:blue} Used deprecated FindBugs config; considering switching to SpotBugs. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m  5s{color} |  | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} || ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  9s{color} |  | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 10s{color} |  | {color:green} the patch passed with JDK Ubuntu-11.0.8+10-post-Ubuntu-0ubuntu118.04.1 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 10s{color} |  | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  2s{color} |  | {color:green} the patch passed with JDK Private Build-1.8.0_265-8u265-b01-0ubuntu2~18.04-b01 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  2s{color} |  | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} blanks {color} | {color:green}  0m  0s{color} |  | {color:green} The patch has no blanks issues. {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 40s{color} |  | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  8s{color} |  | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 17m 56s{color} |  | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 46s{color} |  | {color:green} the patch passed with JDK Ubuntu-11.0.8+10-post-Ubuntu-0ubuntu118.04.1 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 15s{color} |  | {color:green} the patch passed with JDK Private Build-1.8.0_265-8u265-b01-0ubuntu2~18.04-b01 {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m  8s{color} |  | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} || ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}109m 27s{color} | [/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt|https://ci-hadoop.apache.org/job/PreCommit-HDFS-Build/231/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt] | {color:red} hadoop-hdfs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 36s{color} |  | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}217m 11s{color} |  | {color:black}{color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.TestFileChecksum |
|   | hadoop.hdfs.TestDFSInotifyEventInputStreamKerberized |
|   | hadoop.hdfs.TestFileChecksumCompositeCrc |
|   | hadoop.hdfs.TestRollingUpgrade |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | ClientAPI=1.40 ServerAPI=1.40 base: https://ci-hadoop.apache.org/job/PreCommit-HDFS-Build/231/artifact/out/Dockerfile |
| JIRA Issue | HDFS-15459 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/13013535/HDFS-15459.001.patch |
| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient findbugs checkstyle |
| uname | Linux 9fd633b04547 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | personality/hadoop.sh |
| git revision | trunk / a308a1ec22befa919ad8b6509f1b8c888c80f350 |
| Default Java | Private Build-1.8.0_265-8u265-b01-0ubuntu2~18.04-b01 |
| Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.8+10-post-Ubuntu-0ubuntu118.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_265-8u265-b01-0ubuntu2~18.04-b01 |
|  Test Results | https://ci-hadoop.apache.org/job/PreCommit-HDFS-Build/231/testReport/ |
| Max. process+thread count | 2919 (vs. ulimit of 5500) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://ci-hadoop.apache.org/job/PreCommit-HDFS-Build/231/console |
| versions | git=2.17.1 maven=3.6.0 findbugs=4.1.3 |
| Powered by | Apache Yetus 0.13.0-SNAPSHOT https://yetus.apache.org |


This message was automatically generated.



[~weichiu], [~inigoiri] Can you please take a look that small patch to fix the broken test?
The Failed Units are not related to the patch.
If there is something wrong in the way SDFSStriped are parsed, then we should file a different Jira. 

I'm just slightly familiar with EC so I cannot tell if this is critical or not.
For the fix itself, the other option would be for isBlockTokenExpired() to handle the null properly too.

{quote}  For the fix itself, the other option would be for isBlockTokenExpired() to handle the null properly too.
{quote}
 I prefer the solution in the patch because I think that the semantic of {{isBlockTokenExpired}} should not be handling null. Also, handling the null in {{isBlockTokenExpired}} will affect other JUnits hiding other NPEs that can shed some lights on a potential bug.

[~kihwal], [~weichiu] can you guys take a look at that patch? this JUnit keeps failing on Yetus for sometime now and it will be good to get rid of those flaky tests.

[~ahussein], OK, let's go with this fix; I like being able to catch NPEs in other places.
+1 on [^HDFS-15459.001.patch].

Thanks [~inigoiri] for the review! Can we please go ahead and merge it to [3.3.1|https://issues.apache.org/jira/issues/?jql=project+%3D+HDFS+AND+fixVersion+%3D+3.3.1], [3.4.0|https://issues.apache.org/jira/issues/?jql=project+%3D+HDFS+AND+fixVersion+%3D+3.4.0], [3.1.5|https://issues.apache.org/jira/issues/?jql=project+%3D+HDFS+AND+fixVersion+%3D+3.1.5], [3.2.3|https://issues.apache.org/jira/issues/?jql=project+%3D+HDFS+AND+fixVersion+%3D+3.2.3]

Thanks [~ahussein] for the fix, committed and pushed to trunk, branch-3.3, branch-3.2, and branch-3.1.

cherry-pick to branch-3.2.2 and verify at local, Thanks [~ahussein].

[~ahussein]/[~elgoiri] 

This same tests fails now also, though there is no NPE(since you have added a check)

[https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2424/3/testReport/org.apache.hadoop.hdfs.server.blockmanagement/TestBlockTokenWithDFSStriped/testRead/]

Give a check once

