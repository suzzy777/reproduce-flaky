-1 to going back to this. This threaded mechanism caused a stream of escalations in practice and does not ensure a bound to the number of seed messages.

I suggest to investigate the purpose of the seeding thread. Why do we need at least one seed ? Would it change if we could figure out the end-offset on a topic ? Couldn't we just avoid the seed by assuming the cache ready without consuming an actual message ?

The current code also uses a seeder thread. So on first start it acts exactly the same. The current code though involves a lot of additional complexity like talking to the sling repo. It has a lot of different state and with this a lot of possible errors.Â 

So I do not see a valid reason why my change is worse than the current code in master.

Â 

I am on PTO until July 21st, sorry. The -1 still holds, the current code does ensure a bound (1 seed max), the old code did not (unbounded number of seeds). We could also remove the 1 seed with the current code if we considered cache seeded without consuming an actual message.

I do not consider this a valid -1 as you were not able to explain how my change is worse than the current code. Especially to explain why it would be so bad that it needs a blocking -1 that holds my progress for 11 days.
The worst case behaviour of both new and old code is the same (unbounded seeding thread). I will thoroughly test this and if I see no issues I intend to merge.

I have read the apache rule about vetoes. Your -1 in fact is a veto until you redraw it. That sucks. Many thanks.

Explaining is only possible with people who listen. The reason in in the thread. Also, I added a way forward which is to avoid the seed on the first place. This is a refactoring effort, you could also refactor that part in 11 days.

The threaded approach has caused us many CSOs even though it was thoroughly tested ;-) 

My further changes to extract common code depend on this change. If I continue those without this change I will have to do this change completely from scratch.Â 

I added an upper limit of seeding messages.Â 
[https://github.com/apache/sling-org-apache-sling-distribution-journal/pull/52/commits/74bbf2c7cedd993859dd4863e464b7d476634c57]

This will avoid that we try forever.

You also added a bug ðŸ˜¥ it can be that none of the the bounded set of messages is consumed. I really think we should dedicate our time to look forward: make sure no seed is required by changing the way we consider the cache to be seeded. 

When it comes to the Apache rules, by this message I allow you to override my veto if you consider the consensus is reached and its really impossible to extract common code without changing the code semantic.

I found that in case of an outage of the messaging system the seeder thread will not succeed. (This happens if the messaging allows sending but refuses consume with 503 error). For this case I wanted to make sure we do not overload the system.Â 

The same situation could happen with the current code when the single message sent out does not arrive for some reason.

{quote}The same situation could happen with the current code when the single message sent out does not arrive for some reason.
{quote}
>No because it's an assign request and we assign to an offset that we know correspond to a >message created before the new seed message.

Yes .. but in case of an overload the consume might not work while the send works. So our queues are still stuck as they are not considered being seeded.

Btw. I think my observation might be another possible reason why we saw the extreme number of seeding messages before. If the messaging system blocks consume requests (in case of too high load) but allows send requests then the seeding thread continued until the messaging system is allowing consume requests again.

Â 

I tested the current code in the cloud and it behaves correctly. I was not able to reproduce the journal unavailable case again though.Â 

I would not like to have an unbounded seeder thread as there is the very real risk that it never stops. Do you think the limited number of seeding tries is acceptable? An alternative would be to use exponential backoff (simply increasing delays) to make sure we never flood the messaging system with seed messages.

What I really like in the new code is to print the retry numbers. This way we can very easily see in the logging when we produce higher number of seeding messages than expected.

An unbounded + exponential backoff would be correct and better than previous threaded mechanism.

I won t be able to reply further. To not block you, i remove my veto and will review before the Sling release is cut (ideally in 11+ days).

Sounds good. I will do exponential backoff with a large maximum delay. That should make sure that we never overload the messaging system while also making sure we get seeded eventually. The new seeding code will then be only in the QueueCacheSeeder. So we can easily remove it once we have a solution that does not require seeding.

Â 

The latest state looks good. I tested in in the cloud. We currently have a very flaky messaging backend. So I got a lot of cases where seeding did not work immediately. The system always worked eventually and only produced a low number of seeding messages. I plan to merge and deploy this next week. In the mean time I will continue with the extraction of the queuing code in a new issue / branch based on this branch.

