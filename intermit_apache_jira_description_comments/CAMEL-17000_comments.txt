Also 5.7.2 causes problems, so we can stay on 5.7.1 for 3.12.0 release.

Ok. Let's revert

I updated to 5.8.1 yesterday and i was waiting for ci

Yeah it was a surprise to see core failures, but we didn't have a lot of other changes so could point to junit quicker. 

So even 5.7.1 to 5.7.2 causes a problem

I've had a look at this. The errors  and also increased number of "flakes" are due to the parallel execution. It's not clear exactly what changed, but I've found a way to set a custom parallelization strategy on Junit5 which at least gets rid of the errors. I've also analyzed a lot of the cases causing flakiness in the tests and found a root causes for some of them, for example, setting the  System property CamelSedaPollTimeout to 10 for tests using seda consumer isn't always working since a test running in parallel unsets it when it finishes, so that some tests end up running with the default 1s timeout which can cause failures, especially related to suspending & restarting.

Ah thanks for find out about this. Yeah it smells like a problem with that seda timeout. I guess we can should look at an alternative way to set that poll timeout to make seda exit quicker

I removed the JVM system CamelSedaPollTimeout and replaced it with a component level configuration, so we can configure this in a better way.
Then we should no longer have troubles with those.

However there may be other places where the JUnit upgrade is causing trouble. Karen maybe if you have time to test again and see which tests that is causing trouble.

I created a PR https://github.com/apache/camel/pull/6350 to upgrade to Jupiter 5.8.1 and set a custom strategy for parallel execution which is used in camel-core tests only for now. Using this on my environment (Ubuntu 20.04.3, 16Gb RAM, 8 CPU, openjdk 11.0.11) I typically get around 30 flakes but no errors.
Most of them appear to be related to heavy use of asynchronous parallel processing in Camel itself. For example in MulticastParallelAllTimeoutAwareTest it appears that the exchanges are not even sent to the multicast targets before the timeout on the processing expires.
Typically the failing tests succeed on the second try because surefire reruns them after the launching all tests the first time, and at this point, there is much less contention for the processor.
I created CAMEL-17064 to further investigate flake issues. 

You can maybe group some of these high contention concurrent tests to run later. Or we can add flakly test count from 3 to 5 to let it try a few times more

As we cut 3.13 next week and all works is done except for flakly core tests which has another JIRA then lets resolve this

